[{"client_msg_id":"5f2dd458-edd5-4815-bfb2-73f7e686d6d0","type":"message","text":"I'm seeing a rather fun behaviour with tasks.  in a system with AMD EPYC 7502 32-Core Processor, I launch the script in the:thread:below, with different number of threads and get the following timings:\n```% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  285.718 ms (7020 allocations: 543.05 KiB)\n  143.495 ms (8020 allocations: 574.30 KiB)\n  71.975 ms (8020 allocations: 574.30 KiB)\n  36.460 ms (8020 allocations: 574.30 KiB)\n  18.987 ms (8020 allocations: 574.30 KiB)\n  24.721 ms (8021 allocations: 574.33 KiB)\n  80.087 ms (8024 allocations: 574.42 KiB)\n  217.695 ms (8022 allocations: 574.36 KiB)```\nit scales perfectly up to 16 threads, then it starts getting worse and worse, up to 128 threads (the number of threads available in the system), where it's close to the performance of a single thread.\n\nthe equivalent function which uses `Threads.@threads` instead of `Thread.@spawn` has a much more straightforward behaviour (but scaling is worse, because of load imbalance):\n```% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  286.673 ms (2020 allocations: 113.64 KiB)\n  222.603 ms (2026 allocations: 114.11 KiB)\n  132.780 ms (2038 allocations: 115.06 KiB)\n  72.050 ms (2060 allocations: 116.91 KiB)\n  37.192 ms (2101 allocations: 120.50 KiB)\n  20.731 ms (2193 allocations: 128.00 KiB)\n  11.464 ms (2348 allocations: 142.12 KiB)\n  9.128 ms (2659 allocations: 170.38 KiB)```\ndo you have an idea why tasks are behaving this way?","user":"UDB26738Q","ts":"1608134434.180500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"k7Gj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm seeing a rather fun behaviour with tasks.  in a system with AMD EPYC 7502 32-Core Processor, I launch the script in the"},{"type":"emoji","name":"thread"},{"type":"text","text":"below, with different number of threads and get the following timings:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  285.718 ms (7020 allocations: 543.05 KiB)\n  143.495 ms (8020 allocations: 574.30 KiB)\n  71.975 ms (8020 allocations: 574.30 KiB)\n  36.460 ms (8020 allocations: 574.30 KiB)\n  18.987 ms (8020 allocations: 574.30 KiB)\n  24.721 ms (8021 allocations: 574.33 KiB)\n  80.087 ms (8024 allocations: 574.42 KiB)\n  217.695 ms (8022 allocations: 574.36 KiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"it scales perfectly up to 16 threads, then it starts getting worse and worse, up to 128 threads (the number of threads available in the system), where it's close to the performance of a single thread.\n\nthe equivalent function which uses "},{"type":"text","text":"Threads.@threads","style":{"code":true}},{"type":"text","text":" instead of "},{"type":"text","text":"Thread.@spawn","style":{"code":true}},{"type":"text","text":" has a much more straightforward behaviour (but scaling is worse, because of load imbalance):\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  286.673 ms (2020 allocations: 113.64 KiB)\n  222.603 ms (2026 allocations: 114.11 KiB)\n  132.780 ms (2038 allocations: 115.06 KiB)\n  72.050 ms (2060 allocations: 116.91 KiB)\n  37.192 ms (2101 allocations: 120.50 KiB)\n  20.731 ms (2193 allocations: 128.00 KiB)\n  11.464 ms (2348 allocations: 142.12 KiB)\n  9.128 ms (2659 allocations: 170.38 KiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\ndo you have an idea why tasks are behaving this way?"}]}]}],"thread_ts":"1608134434.180500","reply_count":39,"reply_users_count":4,"latest_reply":"1608148411.189600","reply_users":["UDB26738Q","U67BJLYCS","U01GRS159T8","UC7AF7NSU"],"subscribed":false,"reactions":[{"name":"eyes","users":["UEP056STX"],"count":1}]},{"client_msg_id":"fa577030-9eed-4af0-ab37-0db033861a75","type":"message","text":"```using BenchmarkTools, Base.Threads\n\n# Return the fractional part of x, modulo 1, always positive\nfpart(x) = mod(x, one(x))\n\nfunction Σ(n, j)\n    # Compute the finite sum\n    s = 0.0\n    denom = j\n    for k in 0:n\n        s = fpart(s + powermod(16, n - k, denom) / denom)\n        denom += 8\n    end\n    # Compute the infinite sum\n    num = 1 / 16\n\twhile (frac = num / denom) &gt; eps(s)\n        s     += frac\n        num   /= 16\n        denom += 8\n    end\n    return fpart(s)\nend\n\npi_digit(n) =\n    floor(Int, 16 * fpart(4Σ(n-1, 1) - 2Σ(n-1, 4) - Σ(n-1, 5) - Σ(n-1, 6)))\n\npi_string(n) = \"0x3.\" * join(string.(pi_digit.(1:n), base = 16)) * \"p0\"\n\nfunction pi_string_tasks(N)\n    tasks = [Threads.@spawn pi_digit(n) for n in 1:N]\n    digits = [fetch(t) for t in tasks]\n    return \"0x3.\" * join(string.(digits, base = 16)) * \"p0\"\nend\n\nfunction pi_string_threads(N)\n    digits = Vector{Int}(undef, N)\n    Threads.@threads for n in eachindex(digits)\n        digits[n] = pi_digit(n)\n    end\n    return \"0x3.\" * join(string.(digits, base = 16)) * \"p0\"\nend\n\n@btime pi_string_tasks(1_000)\n# @btime pi_string_threads(1_000)```","user":"UDB26738Q","ts":"1608134449.180600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wBww","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using BenchmarkTools, Base.Threads\n\n# Return the fractional part of x, modulo 1, always positive\nfpart(x) = mod(x, one(x))\n\nfunction Σ(n, j)\n    # Compute the finite sum\n    s = 0.0\n    denom = j\n    for k in 0:n\n        s = fpart(s + powermod(16, n - k, denom) / denom)\n        denom += 8\n    end\n    # Compute the infinite sum\n    num = 1 / 16\n\twhile (frac = num / denom) > eps(s)\n        s     += frac\n        num   /= 16\n        denom += 8\n    end\n    return fpart(s)\nend\n\npi_digit(n) =\n    floor(Int, 16 * fpart(4Σ(n-1, 1) - 2Σ(n-1, 4) - Σ(n-1, 5) - Σ(n-1, 6)))\n\npi_string(n) = \"0x3.\" * join(string.(pi_digit.(1:n), base = 16)) * \"p0\"\n\nfunction pi_string_tasks(N)\n    tasks = [Threads.@spawn pi_digit(n) for n in 1:N]\n    digits = [fetch(t) for t in tasks]\n    return \"0x3.\" * join(string.(digits, base = 16)) * \"p0\"\nend\n\nfunction pi_string_threads(N)\n    digits = Vector{Int}(undef, N)\n    Threads.@threads for n in eachindex(digits)\n        digits[n] = pi_digit(n)\n    end\n    return \"0x3.\" * join(string.(digits, base = 16)) * \"p0\"\nend\n\n@btime pi_string_tasks(1_000)\n# @btime pi_string_threads(1_000)"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"344e81d3-cc5d-4972-aa12-a07d7ff75b61","type":"message","text":"I would look at the scheduling. Maybe collect the time it takes to run each task + the thread id it run on","user":"U67BJLYCS","ts":"1608134737.181000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FVzZh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I would look at the scheduling. Maybe collect the time it takes to run each task + the thread id it run on"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"8bd7d1e2-2b3f-4e06-af0d-cd7c94951c73","type":"message","text":"I'm now running this script (`pi_digits` is defined above):\n```function test(N)\n    tasks = [Threads.@spawn (threadid(), @elapsed pi_digit(n)) for n in 1:N]\n    ids_times = [fetch(t) for t in tasks]\n    d = Dict(id =&gt; 0.0 for id in 1:nthreads())\n    for (id, time) in ids_times\n        d[id] += time\n    end\n    println()\n    @show nthreads()\n    @show sort([d...]; by=last)\nend```\nthis is the output for different numbers of threads:\n","user":"UDB26738Q","ts":"1608137342.181400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"anpnj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm now running this script ("},{"type":"text","text":"pi_digits","style":{"code":true}},{"type":"text","text":" is defined above):\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function test(N)\n    tasks = [Threads.@spawn (threadid(), @elapsed pi_digit(n)) for n in 1:N]\n    ids_times = [fetch(t) for t in tasks]\n    d = Dict(id => 0.0 for id in 1:nthreads())\n    for (id, time) in ids_times\n        d[id] += time\n    end\n    println()\n    @show nthreads()\n    @show sort([d...]; by=last)\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"this is the output for different numbers of threads:\n"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"8bd7d1e2-2b3f-4e06-af0d-cd7c94951c73","type":"message","text":"```% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n\nnthreads() = 1\nsort([d...]; by = last) = [1 =&gt; 0.2865000009999997]\n\nnthreads() = 2\nsort([d...]; by = last) = [1 =&gt; 0.12960901099999997, 2 =&gt; 0.156104224]\n\nnthreads() = 4\nsort([d...]; by = last) = [1 =&gt; 0.05195538599999998, 4 =&gt; 0.07460718500000002, 3 =&gt; 0.07856373500000005, 2 =&gt; 0.08093613]\n\nnthreads() = 8\nsort([d...]; by = last) = [1 =&gt; 0.013739732000000001, 7 =&gt; 0.03946424299999999, 8 =&gt; 0.042188045, 5 =&gt; 0.04337162700000001, 6 =&gt; 0.044675275999999986, 4 =&gt; 0.045851848999999986, 3 =&gt; 0.04686953400000002, 2 =&gt; 0.04811377600000001]\n\nnthreads() = 16\nsort([d...]; by = last) = [1 =&gt; 2.79e-6, 12 =&gt; 0.010826886, 11 =&gt; 0.011894768999999998, 6 =&gt; 0.016018859, 15 =&gt; 0.017071656, 13 =&gt; 0.018111586999999995, 9 =&gt; 0.0190903, 5 =&gt; 0.021575394000000005, 7 =&gt; 0.022863109, 3 =&gt; 0.02408427, 10 =&gt; 0.025287639000000004, 8 =&gt; 0.026511469000000003, 4 =&gt; 0.027777469999999995, 16 =&gt; 0.029137271000000006, 14 =&gt; 0.030289042, 2 =&gt; 0.031713884]\n\nnthreads() = 32\nsort([d...]; by = last) = [22 =&gt; 2.87e-6, 25 =&gt; 4.82e-6, 24 =&gt; 9.21e-6, 18 =&gt; 1.604e-5, 19 =&gt; 1.6819e-5, 15 =&gt; 1.994e-5, 28 =&gt; 2.204e-5, 14 =&gt; 2.574e-5, 23 =&gt; 2.738e-5, 1 =&gt; 0.000269686, 30 =&gt; 0.002488692, 12 =&gt; 0.003507347, 9 =&gt; 0.005645804, 13 =&gt; 0.0067288569999999995, 31 =&gt; 0.007839411999999999, 32 =&gt; 0.009144662, 6 =&gt; 0.010429423, 27 =&gt; 0.011287489000000001, 10 =&gt; 0.012320203999999998, 7 =&gt; 0.013576885000000002, 29 =&gt; 0.01458821, 16 =&gt; 0.015805203, 17 =&gt; 0.016860724, 8 =&gt; 0.0179101, 5 =&gt; 0.019193019, 26 =&gt; 0.020444430999999996, 21 =&gt; 0.021835921999999997, 3 =&gt; 0.022924733999999995, 11 =&gt; 0.024104807000000002, 20 =&gt; 0.025241179, 4 =&gt; 0.026562499000000003, 2 =&gt; 0.027821791000000002]\n\nnthreads() = 64\nsort([d...]; by = last) = [60 =&gt; 2.3e-6, 59 =&gt; 2.52e-6, 45 =&gt; 3.27e-6, 33 =&gt; 3.42e-6, 58 =&gt; 4.3e-6, 31 =&gt; 4.37e-6, 47 =&gt; 4.47e-6, 55 =&gt; 4.71e-6, 40 =&gt; 5.119e-6, 54 =&gt; 5.33e-6, 38 =&gt; 6.31e-6, 52 =&gt; 7.07e-6, 57 =&gt; 7.079e-6, 51 =&gt; 7.779e-6, 26 =&gt; 7.99e-6, 24 =&gt; 8.7e-6, 37 =&gt; 8.88e-6, 12 =&gt; 9.47e-6, 27 =&gt; 1.007e-5, 25 =&gt; 1.0279e-5, 2 =&gt; 1.161e-5, 18 =&gt; 1.306e-5, 13 =&gt; 1.351e-5, 15 =&gt; 1.3739e-5, 6 =&gt; 1.404e-5, 9 =&gt; 1.549e-5, 8 =&gt; 1.7749e-5, 44 =&gt; 1.816e-5, 19 =&gt; 1.876e-5, 17 =&gt; 1.9769e-5, 10 =&gt; 2.03e-5, 36 =&gt; 2.1909e-5, 7 =&gt; 2.2229e-5, 30 =&gt; 2.244e-5, 49 =&gt; 2.341e-5, 22 =&gt; 2.394e-5, 23 =&gt; 2.6499e-5, 1 =&gt; 4.482e-5, 39 =&gt; 4.7349e-5, 32 =&gt; 6.4799e-5, 41 =&gt; 0.000462813, 34 =&gt; 0.0015850859999999999, 28 =&gt; 0.002755808, 53 =&gt; 0.003726433, 16 =&gt; 0.004724889, 5 =&gt; 0.006039851, 56 =&gt; 0.007188380999999999, 21 =&gt; 0.008231126, 11 =&gt; 0.009587935, 46 =&gt; 0.010963883999999998, 35 =&gt; 0.01187375, 50 =&gt; 0.013074562999999999, 64 =&gt; 0.014300674999999999, 4 =&gt; 0.015357207000000001, 48 =&gt; 0.016683039, 14 =&gt; 0.017996006000000002, 61 =&gt; 0.019183738999999998, 42 =&gt; 0.020310674000000004, 62 =&gt; 0.021559233, 29 =&gt; 0.022816473999999993, 20 =&gt; 0.024007465999999998, 63 =&gt; 0.026350746000000005, 43 =&gt; 0.027420987000000008, 3 =&gt; 0.028706525]\n\nnthreads() = 128\nsort([d...]; by = last) = [90 =&gt; 1.1e-6, 94 =&gt; 1.38e-6, 109 =&gt; 1.49e-6, 111 =&gt; 1.9e-6, 102 =&gt; 2.12e-6, 124 =&gt; 2.28e-6, 122 =&gt; 2.95e-6, 117 =&gt; 3.0e-6, 113 =&gt; 3.27e-6, 114 =&gt; 3.73e-6, 100 =&gt; 4.28e-6, 106 =&gt; 4.33e-6, 96 =&gt; 4.33e-6, 105 =&gt; 4.53e-6, 60 =&gt; 5.23e-6, 89 =&gt; 5.35e-6, 101 =&gt; 5.43e-6, 61 =&gt; 6.76e-6, 95 =&gt; 6.9e-6, 71 =&gt; 7.65e-6, 66 =&gt; 8.619e-6, 75 =&gt; 8.65e-6, 86 =&gt; 9.21e-6, 58 =&gt; 9.6e-6, 27 =&gt; 9.6e-6, 110 =&gt; 9.76e-6, 87 =&gt; 1.168e-5, 88 =&gt; 1.182e-5, 67 =&gt; 1.207e-5, 36 =&gt; 1.2459e-5, 72 =&gt; 1.292e-5, 84 =&gt; 1.323e-5, 57 =&gt; 1.3389e-5, 68 =&gt; 1.359e-5, 78 =&gt; 1.371e-5, 104 =&gt; 1.422e-5, 53 =&gt; 1.4299e-5, 65 =&gt; 1.552e-5, 92 =&gt; 1.565e-5, 31 =&gt; 1.631e-5, 77 =&gt; 1.664e-5, 47 =&gt; 1.693e-5, 24 =&gt; 1.775e-5, 33 =&gt; 1.8829e-5, 91 =&gt; 1.906e-5, 19 =&gt; 1.92e-5, 126 =&gt; 1.943e-5, 69 =&gt; 1.965e-5, 7 =&gt; 2.015e-5, 35 =&gt; 2.036e-5, 41 =&gt; 2.084e-5, 81 =&gt; 2.132e-5, 42 =&gt; 2.212e-5, 18 =&gt; 2.337e-5, 59 =&gt; 2.363e-5, 44 =&gt; 2.369e-5, 98 =&gt; 2.3699e-5, 76 =&gt; 2.402e-5, 21 =&gt; 2.468e-5, 119 =&gt; 2.574e-5, 48 =&gt; 2.648e-5, 49 =&gt; 2.6629e-5, 82 =&gt; 2.691e-5, 127 =&gt; 2.8909e-5, 115 =&gt; 2.9369e-5, 70 =&gt; 2.9829e-5, 39 =&gt; 2.9969e-5, 97 =&gt; 3.0009e-5, 54 =&gt; 3.1859e-5, 38 =&gt; 3.2289e-5, 121 =&gt; 3.233e-5, 13 =&gt; 3.2439e-5, 52 =&gt; 3.2919e-5, 32 =&gt; 3.3659e-5, 63 =&gt; 3.473e-5, 5 =&gt; 3.518e-5, 10 =&gt; 3.605e-5, 123 =&gt; 3.826e-5, 25 =&gt; 3.8289e-5, 80 =&gt; 3.8749e-5, 64 =&gt; 3.8919e-5, 8 =&gt; 3.91e-5, 4 =&gt; 4.04e-5, 6 =&gt; 4.0659e-5, 56 =&gt; 4.0849e-5, 74 =&gt; 4.1959e-5, 128 =&gt; 4.22e-5, 28 =&gt; 4.348e-5, 26 =&gt; 4.391e-5, 15 =&gt; 4.4179e-5, 45 =&gt; 4.4509e-5, 30 =&gt; 4.5009e-5, 112 =&gt; 4.5309e-5, 43 =&gt; 4.6559e-5, 11 =&gt; 4.7499e-5, 37 =&gt; 4.792e-5, 40 =&gt; 4.8049e-5, 79 =&gt; 4.874e-5, 55 =&gt; 4.9549e-5, 16 =&gt; 5.012e-5, 73 =&gt; 5.0249e-5, 118 =&gt; 5.0699e-5, 3 =&gt; 5.135e-5, 50 =&gt; 5.4409e-5, 23 =&gt; 5.6679e-5, 120 =&gt; 5.7529e-5, 9 =&gt; 6.3809e-5, 22 =&gt; 7.0009e-5, 125 =&gt; 8.0659e-5, 62 =&gt; 8.1988e-5, 116 =&gt; 8.4859e-5, 12 =&gt; 9.6789e-5, 17 =&gt; 9.7339e-5, 85 =&gt; 0.000106758, 1 =&gt; 0.000114469, 34 =&gt; 0.000384214, 20 =&gt; 0.004097987, 46 =&gt; 0.004801278, 107 =&gt; 0.005457957, 29 =&gt; 0.012370773000000002, 14 =&gt; 0.031825250000000006, 99 =&gt; 0.040151674, 103 =&gt; 0.04455443799999998, 51 =&gt; 0.04884664399999999, 108 =&gt; 0.058635777000000014, 83 =&gt; 0.062696137, 2 =&gt; 0.06695400100000001, 93 =&gt; 0.06932269099999999]```","user":"UDB26738Q","ts":"1608137342.181600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"anpnj-+8R","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n\nnthreads() = 1\nsort([d...]; by = last) = [1 => 0.2865000009999997]\n\nnthreads() = 2\nsort([d...]; by = last) = [1 => 0.12960901099999997, 2 => 0.156104224]\n\nnthreads() = 4\nsort([d...]; by = last) = [1 => 0.05195538599999998, 4 => 0.07460718500000002, 3 => 0.07856373500000005, 2 => 0.08093613]\n\nnthreads() = 8\nsort([d...]; by = last) = [1 => 0.013739732000000001, 7 => 0.03946424299999999, 8 => 0.042188045, 5 => 0.04337162700000001, 6 => 0.044675275999999986, 4 => 0.045851848999999986, 3 => 0.04686953400000002, 2 => 0.04811377600000001]\n\nnthreads() = 16\nsort([d...]; by = last) = [1 => 2.79e-6, 12 => 0.010826886, 11 => 0.011894768999999998, 6 => 0.016018859, 15 => 0.017071656, 13 => 0.018111586999999995, 9 => 0.0190903, 5 => 0.021575394000000005, 7 => 0.022863109, 3 => 0.02408427, 10 => 0.025287639000000004, 8 => 0.026511469000000003, 4 => 0.027777469999999995, 16 => 0.029137271000000006, 14 => 0.030289042, 2 => 0.031713884]\n\nnthreads() = 32\nsort([d...]; by = last) = [22 => 2.87e-6, 25 => 4.82e-6, 24 => 9.21e-6, 18 => 1.604e-5, 19 => 1.6819e-5, 15 => 1.994e-5, 28 => 2.204e-5, 14 => 2.574e-5, 23 => 2.738e-5, 1 => 0.000269686, 30 => 0.002488692, 12 => 0.003507347, 9 => 0.005645804, 13 => 0.0067288569999999995, 31 => 0.007839411999999999, 32 => 0.009144662, 6 => 0.010429423, 27 => 0.011287489000000001, 10 => 0.012320203999999998, 7 => 0.013576885000000002, 29 => 0.01458821, 16 => 0.015805203, 17 => 0.016860724, 8 => 0.0179101, 5 => 0.019193019, 26 => 0.020444430999999996, 21 => 0.021835921999999997, 3 => 0.022924733999999995, 11 => 0.024104807000000002, 20 => 0.025241179, 4 => 0.026562499000000003, 2 => 0.027821791000000002]\n\nnthreads() = 64\nsort([d...]; by = last) = [60 => 2.3e-6, 59 => 2.52e-6, 45 => 3.27e-6, 33 => 3.42e-6, 58 => 4.3e-6, 31 => 4.37e-6, 47 => 4.47e-6, 55 => 4.71e-6, 40 => 5.119e-6, 54 => 5.33e-6, 38 => 6.31e-6, 52 => 7.07e-6, 57 => 7.079e-6, 51 => 7.779e-6, 26 => 7.99e-6, 24 => 8.7e-6, 37 => 8.88e-6, 12 => 9.47e-6, 27 => 1.007e-5, 25 => 1.0279e-5, 2 => 1.161e-5, 18 => 1.306e-5, 13 => 1.351e-5, 15 => 1.3739e-5, 6 => 1.404e-5, 9 => 1.549e-5, 8 => 1.7749e-5, 44 => 1.816e-5, 19 => 1.876e-5, 17 => 1.9769e-5, 10 => 2.03e-5, 36 => 2.1909e-5, 7 => 2.2229e-5, 30 => 2.244e-5, 49 => 2.341e-5, 22 => 2.394e-5, 23 => 2.6499e-5, 1 => 4.482e-5, 39 => 4.7349e-5, 32 => 6.4799e-5, 41 => 0.000462813, 34 => 0.0015850859999999999, 28 => 0.002755808, 53 => 0.003726433, 16 => 0.004724889, 5 => 0.006039851, 56 => 0.007188380999999999, 21 => 0.008231126, 11 => 0.009587935, 46 => 0.010963883999999998, 35 => 0.01187375, 50 => 0.013074562999999999, 64 => 0.014300674999999999, 4 => 0.015357207000000001, 48 => 0.016683039, 14 => 0.017996006000000002, 61 => 0.019183738999999998, 42 => 0.020310674000000004, 62 => 0.021559233, 29 => 0.022816473999999993, 20 => 0.024007465999999998, 63 => 0.026350746000000005, 43 => 0.027420987000000008, 3 => 0.028706525]\n\nnthreads() = 128\nsort([d...]; by = last) = [90 => 1.1e-6, 94 => 1.38e-6, 109 => 1.49e-6, 111 => 1.9e-6, 102 => 2.12e-6, 124 => 2.28e-6, 122 => 2.95e-6, 117 => 3.0e-6, 113 => 3.27e-6, 114 => 3.73e-6, 100 => 4.28e-6, 106 => 4.33e-6, 96 => 4.33e-6, 105 => 4.53e-6, 60 => 5.23e-6, 89 => 5.35e-6, 101 => 5.43e-6, 61 => 6.76e-6, 95 => 6.9e-6, 71 => 7.65e-6, 66 => 8.619e-6, 75 => 8.65e-6, 86 => 9.21e-6, 58 => 9.6e-6, 27 => 9.6e-6, 110 => 9.76e-6, 87 => 1.168e-5, 88 => 1.182e-5, 67 => 1.207e-5, 36 => 1.2459e-5, 72 => 1.292e-5, 84 => 1.323e-5, 57 => 1.3389e-5, 68 => 1.359e-5, 78 => 1.371e-5, 104 => 1.422e-5, 53 => 1.4299e-5, 65 => 1.552e-5, 92 => 1.565e-5, 31 => 1.631e-5, 77 => 1.664e-5, 47 => 1.693e-5, 24 => 1.775e-5, 33 => 1.8829e-5, 91 => 1.906e-5, 19 => 1.92e-5, 126 => 1.943e-5, 69 => 1.965e-5, 7 => 2.015e-5, 35 => 2.036e-5, 41 => 2.084e-5, 81 => 2.132e-5, 42 => 2.212e-5, 18 => 2.337e-5, 59 => 2.363e-5, 44 => 2.369e-5, 98 => 2.3699e-5, 76 => 2.402e-5, 21 => 2.468e-5, 119 => 2.574e-5, 48 => 2.648e-5, 49 => 2.6629e-5, 82 => 2.691e-5, 127 => 2.8909e-5, 115 => 2.9369e-5, 70 => 2.9829e-5, 39 => 2.9969e-5, 97 => 3.0009e-5, 54 => 3.1859e-5, 38 => 3.2289e-5, 121 => 3.233e-5, 13 => 3.2439e-5, 52 => 3.2919e-5, 32 => 3.3659e-5, 63 => 3.473e-5, 5 => 3.518e-5, 10 => 3.605e-5, 123 => 3.826e-5, 25 => 3.8289e-5, 80 => 3.8749e-5, 64 => 3.8919e-5, 8 => 3.91e-5, 4 => 4.04e-5, 6 => 4.0659e-5, 56 => 4.0849e-5, 74 => 4.1959e-5, 128 => 4.22e-5, 28 => 4.348e-5, 26 => 4.391e-5, 15 => 4.4179e-5, 45 => 4.4509e-5, 30 => 4.5009e-5, 112 => 4.5309e-5, 43 => 4.6559e-5, 11 => 4.7499e-5, 37 => 4.792e-5, 40 => 4.8049e-5, 79 => 4.874e-5, 55 => 4.9549e-5, 16 => 5.012e-5, 73 => 5.0249e-5, 118 => 5.0699e-5, 3 => 5.135e-5, 50 => 5.4409e-5, 23 => 5.6679e-5, 120 => 5.7529e-5, 9 => 6.3809e-5, 22 => 7.0009e-5, 125 => 8.0659e-5, 62 => 8.1988e-5, 116 => 8.4859e-5, 12 => 9.6789e-5, 17 => 9.7339e-5, 85 => 0.000106758, 1 => 0.000114469, 34 => 0.000384214, 20 => 0.004097987, 46 => 0.004801278, 107 => 0.005457957, 29 => 0.012370773000000002, 14 => 0.031825250000000006, 99 => 0.040151674, 103 => 0.04455443799999998, 51 => 0.04884664399999999, 108 => 0.058635777000000014, 83 => 0.062696137, 2 => 0.06695400100000001, 93 => 0.06932269099999999]"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"8bd7d1e2-2b3f-4e06-af0d-cd7c94951c73","type":"message","text":"is this what you were suggesting?","user":"UDB26738Q","ts":"1608137342.181800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"anpnj-+8R-1a9w","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is this what you were suggesting?"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"type":"message","subtype":"thread_broadcast","text":"it looks like that with more threads available the scheduler is overloading few threads when using the tasks?","user":"UDB26738Q","ts":"1608137594.182000","thread_ts":"1608134434.180500","root":{"client_msg_id":"5f2dd458-edd5-4815-bfb2-73f7e686d6d0","type":"message","text":"I'm seeing a rather fun behaviour with tasks.  in a system with AMD EPYC 7502 32-Core Processor, I launch the script in the:thread:below, with different number of threads and get the following timings:\n```% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  285.718 ms (7020 allocations: 543.05 KiB)\n  143.495 ms (8020 allocations: 574.30 KiB)\n  71.975 ms (8020 allocations: 574.30 KiB)\n  36.460 ms (8020 allocations: 574.30 KiB)\n  18.987 ms (8020 allocations: 574.30 KiB)\n  24.721 ms (8021 allocations: 574.33 KiB)\n  80.087 ms (8024 allocations: 574.42 KiB)\n  217.695 ms (8022 allocations: 574.36 KiB)```\nit scales perfectly up to 16 threads, then it starts getting worse and worse, up to 128 threads (the number of threads available in the system), where it's close to the performance of a single thread.\n\nthe equivalent function which uses `Threads.@threads` instead of `Thread.@spawn` has a much more straightforward behaviour (but scaling is worse, because of load imbalance):\n```% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  286.673 ms (2020 allocations: 113.64 KiB)\n  222.603 ms (2026 allocations: 114.11 KiB)\n  132.780 ms (2038 allocations: 115.06 KiB)\n  72.050 ms (2060 allocations: 116.91 KiB)\n  37.192 ms (2101 allocations: 120.50 KiB)\n  20.731 ms (2193 allocations: 128.00 KiB)\n  11.464 ms (2348 allocations: 142.12 KiB)\n  9.128 ms (2659 allocations: 170.38 KiB)```\ndo you have an idea why tasks are behaving this way?","user":"UDB26738Q","ts":"1608134434.180500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"k7Gj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm seeing a rather fun behaviour with tasks.  in a system with AMD EPYC 7502 32-Core Processor, I launch the script in the"},{"type":"emoji","name":"thread"},{"type":"text","text":"below, with different number of threads and get the following timings:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  285.718 ms (7020 allocations: 543.05 KiB)\n  143.495 ms (8020 allocations: 574.30 KiB)\n  71.975 ms (8020 allocations: 574.30 KiB)\n  36.460 ms (8020 allocations: 574.30 KiB)\n  18.987 ms (8020 allocations: 574.30 KiB)\n  24.721 ms (8021 allocations: 574.33 KiB)\n  80.087 ms (8024 allocations: 574.42 KiB)\n  217.695 ms (8022 allocations: 574.36 KiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"it scales perfectly up to 16 threads, then it starts getting worse and worse, up to 128 threads (the number of threads available in the system), where it's close to the performance of a single thread.\n\nthe equivalent function which uses "},{"type":"text","text":"Threads.@threads","style":{"code":true}},{"type":"text","text":" instead of "},{"type":"text","text":"Thread.@spawn","style":{"code":true}},{"type":"text","text":" has a much more straightforward behaviour (but scaling is worse, because of load imbalance):\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  286.673 ms (2020 allocations: 113.64 KiB)\n  222.603 ms (2026 allocations: 114.11 KiB)\n  132.780 ms (2038 allocations: 115.06 KiB)\n  72.050 ms (2060 allocations: 116.91 KiB)\n  37.192 ms (2101 allocations: 120.50 KiB)\n  20.731 ms (2193 allocations: 128.00 KiB)\n  11.464 ms (2348 allocations: 142.12 KiB)\n  9.128 ms (2659 allocations: 170.38 KiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\ndo you have an idea why tasks are behaving this way?"}]}]}],"thread_ts":"1608134434.180500","reply_count":39,"reply_users_count":4,"latest_reply":"1608148411.189600","reply_users":["UDB26738Q","U67BJLYCS","U01GRS159T8","UC7AF7NSU"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"giJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it looks like that with more threads available the scheduler is overloading few threads when using the tasks?"}]}]}],"client_msg_id":"bea24a8a-f47f-4a0b-b9d7-1a32b26d6178"},{"client_msg_id":"cd7067a5-939f-42cd-9a7f-5b3fab92c97e","type":"message","text":"I would try the task thing but using ThreadPools.@tspawnat","user":"U01GRS159T8","ts":"1608138762.182300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sHHOG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I would try the task thing but using ThreadPools.@tspawnat"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"21ed42cd-6c1b-4cf4-b9b5-a2f6a8b5431d","type":"message","text":"not sure what's the advantage","user":"UDB26738Q","ts":"1608139060.182500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KIV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"not sure what's the advantage"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"575bc683-5bb4-42f8-aefb-fd4ebcc935b3","type":"message","text":"you can make sure the tasks are put on different threads","user":"U01GRS159T8","ts":"1608139096.182700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OHXyj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you can make sure the tasks are put on different threads"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"53d70f4a-b33f-4a11-950d-e31fed01b4c4","type":"message","text":"I thought thats what you thought the problem was?","user":"U01GRS159T8","ts":"1608139112.182900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"95U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I thought thats what you thought the problem was?"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"d5d43660-6cc4-4acd-be5b-c03692d3ef86","type":"message","text":"can you do a histogram of the thread ids","user":"U67BJLYCS","ts":"1608139133.183100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QnvRV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"can you do a histogram of the thread ids"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"3d9878ff-90de-4688-9be6-2f1dfe34dcab","type":"message","text":"and a culmulative time spend on each thread?","user":"U67BJLYCS","ts":"1608139141.183300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OmxiE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"and a culmulative time spend on each thread?"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"9f9280cd-e37b-4ab8-ac78-3e261b760c03","type":"message","text":"the cumulative time is shown above","user":"UDB26738Q","ts":"1608139154.183500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uL2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"the cumulative time is shown above"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"3c881803-f77e-490c-9e7d-fcf676dba6b9","type":"message","text":"not as a histogram, but a dictionary :sweat_smile:","user":"UDB26738Q","ts":"1608139170.183700","team":"T68168MUP","edited":{"user":"UDB26738Q","ts":"1608139178.000000"},"blocks":[{"type":"rich_text","block_id":"KLUsc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"not as a histogram, but a dictionary "},{"type":"emoji","name":"sweat_smile"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"27d65cb4-0897-45df-8ffc-2deba8565fb3","type":"message","text":"one fun idea would be to multi-thread the spawning","user":"U67BJLYCS","ts":"1608139206.184000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Bku+C","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"one fun idea would be to multi-thread the spawning"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"d8cbf23a-ff5b-4cf7-a8bb-f93f35c093ff","type":"message","text":"one question from me was whether we are spawning fast enough","user":"U67BJLYCS","ts":"1608139355.184200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qGH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"one question from me was whether we are spawning fast enough"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"3af709fd-3184-4d68-8d19-7746ad9397f1","type":"message","text":"how do I measure that?","user":"UDB26738Q","ts":"1608139399.184400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+5pT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"how do I measure that?"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"b2299b84-8f3b-442e-9ab1-c1e46862e38c","type":"message","text":"time `tasks = [Threads.@spawn (threadid(), @elapsed pi_digit(n)) for n in 1:N]`","user":"U67BJLYCS","ts":"1608139426.184600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ZjgxV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"time "},{"type":"text","text":"tasks = [Threads.@spawn (threadid(), @elapsed pi_digit(n)) for n in 1:N]","style":{"code":true}}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"4225439c-1264-4470-bf82-b9577de63283","type":"message","text":"also the serial wait is a bit meh","user":"U67BJLYCS","ts":"1608139453.184800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PNW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"also the serial wait is a bit meh"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"bb24bac7-1a69-44ff-b17f-be2033ccbfb1","type":"message","text":"<@UC7AF7NSU> is Floops using binary trees for splitting and waiting?","user":"U67BJLYCS","ts":"1608139488.185000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iM3","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UC7AF7NSU"},{"type":"text","text":" is Floops using binary trees for splitting and waiting?"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"fa9a9ab1-cd96-4898-be78-c3abfb54f535","type":"message","text":"oh and also <@UDB26738Q> are you seeing the same with `floops`?","user":"U67BJLYCS","ts":"1608139514.185200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"AVo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"oh and also "},{"type":"user","user_id":"UDB26738Q"},{"type":"text","text":" are you seeing the same with "},{"type":"text","text":"floops","style":{"code":true}},{"type":"text","text":"?"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"1294cac7-f6a5-42c2-86bd-86282a318476","type":"message","text":"didn't try that one yet, but I was curious to play with it as well.  plain `Threads.@threads` works well, the problem is that tasks aren't uniform, so simple threading gets a lot of load imbalance","user":"UDB26738Q","ts":"1608139616.185400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UutKc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"didn't try that one yet, but I was curious to play with it as well.  plain "},{"type":"text","text":"Threads.@threads","style":{"code":true}},{"type":"text","text":" works well, the problem is that tasks aren't uniform, so simple threading gets a lot of load imbalance"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"0388a596-7753-4ad8-8c2d-43d24257f6d5","type":"message","text":"ThreadPools.@qbthreads","user":"U01GRS159T8","ts":"1608139633.185600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DsuLY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ThreadPools.@qbthreads"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"ae1cace3-d3a1-412b-9cc8-7340602366aa","type":"message","text":"gives oyu a proper thread pool that handles heterogenous jobs well","user":"U01GRS159T8","ts":"1608139658.185800","team":"T68168MUP","edited":{"user":"U01GRS159T8","ts":"1608139675.000000"},"blocks":[{"type":"rich_text","block_id":"bJK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"gives oyu a proper thread pool that handles heterogenous jobs well"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"3434381e-8bf2-46e7-bb76-c03a3c66e785","type":"message","text":"there's a couple of variants, I always use qbthreads because it leaves thread 1 free so the repl remains responsive","user":"U01GRS159T8","ts":"1608139731.186100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Fl6F","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"there's a couple of variants, I always use qbthreads because it leaves thread 1 free so the repl remains responsive"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"3a726ba3-206c-4129-883d-9e038e201e92","type":"message","text":"`ThreadPools.@qthreads` has exactly the same behaviour as `Threads.@spawn`","user":"UDB26738Q","ts":"1608140195.186300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rse","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ThreadPools.@qthreads","style":{"code":true}},{"type":"text","text":" has exactly the same behaviour as "},{"type":"text","text":"Threads.@spawn","style":{"code":true}}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"type":"message","text":"this is how long spawning takes:\n```% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  0.000145 seconds (5.00 k allocations: 429.812 KiB)\n  0.000201 seconds (5.01 k allocations: 430.953 KiB)\n  0.000427 seconds (5.21 k allocations: 443.594 KiB)\n  0.000743 seconds (5.19 k allocations: 442.078 KiB)\n  0.001024 seconds (5.38 k allocations: 454.391 KiB)\n  0.000565 seconds (5.02 k allocations: 431.719 KiB)\n  0.008058 seconds (9.59 k allocations: 741.875 KiB)\n  0.012611 seconds (27.52 k allocations: 1.895 MiB)```\nthe plot shows the cumulative runtime and the number of tasks run by each thread","files":[{"id":"F01H8LGNY4C","created":1608143268,"timestamp":1608143268,"name":"distributions.png","title":"distributions.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"UDB26738Q","editable":false,"size":118948,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01H8LGNY4C/distributions.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01H8LGNY4C/download/distributions.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01H8LGNY4C-df6b550f07/distributions_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01H8LGNY4C-df6b550f07/distributions_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01H8LGNY4C-df6b550f07/distributions_360.png","thumb_360_w":360,"thumb_360_h":240,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01H8LGNY4C-df6b550f07/distributions_480.png","thumb_480_w":480,"thumb_480_h":320,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01H8LGNY4C-df6b550f07/distributions_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01H8LGNY4C-df6b550f07/distributions_720.png","thumb_720_w":720,"thumb_720_h":480,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01H8LGNY4C-df6b550f07/distributions_800.png","thumb_800_w":800,"thumb_800_h":533,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01H8LGNY4C-df6b550f07/distributions_960.png","thumb_960_w":960,"thumb_960_h":640,"thumb_1024":"https://files.slack.com/files-tmb/T68168MUP-F01H8LGNY4C-df6b550f07/distributions_1024.png","thumb_1024_w":1024,"thumb_1024_h":683,"original_w":1800,"original_h":1200,"thumb_tiny":"AwAgADCzc+bvGzdjHaosT/8ATT9atPI6uqhCQep9KhZpmVsgggjGBitYvQyklfqJF53mru34zznNTSySIQETcMVGvm+amd+MDNSyO6kBUzx1pPca0RGzyPE2UKsMYx1pkXneau7fjPOc0ZmOM76kieQbVZDj1NPZC3fUdOXG3Zuz3xUOZx/e/Kp5GYMgAOM84FIrnd8wbHP8NSnoU1d7kaNMZF3bsZ54qSUyB125x3wKcrknBB68cUpbaec/gM0m9QS03ImeTd8oOOf4aFaXzFBDY4ycfnUnmD0b/vk04OCcc/kaL+Q7eZ//2Q==","permalink":"https://julialang.slack.com/files/UDB26738Q/F01H8LGNY4C/distributions.png","permalink_public":"https://slack-files.com/T68168MUP-F01H8LGNY4C-49cff28872","is_starred":false,"has_rich_preview":false},{"id":"F01HSB1K6KA","created":1608143270,"timestamp":1608143270,"name":"runtimes.png","title":"runtimes.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"UDB26738Q","editable":false,"size":136782,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01HSB1K6KA/runtimes.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01HSB1K6KA/download/runtimes.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01HSB1K6KA-ac270c5486/runtimes_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01HSB1K6KA-ac270c5486/runtimes_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01HSB1K6KA-ac270c5486/runtimes_360.png","thumb_360_w":360,"thumb_360_h":240,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01HSB1K6KA-ac270c5486/runtimes_480.png","thumb_480_w":480,"thumb_480_h":320,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01HSB1K6KA-ac270c5486/runtimes_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01HSB1K6KA-ac270c5486/runtimes_720.png","thumb_720_w":720,"thumb_720_h":480,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01HSB1K6KA-ac270c5486/runtimes_800.png","thumb_800_w":800,"thumb_800_h":533,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01HSB1K6KA-ac270c5486/runtimes_960.png","thumb_960_w":960,"thumb_960_h":640,"thumb_1024":"https://files.slack.com/files-tmb/T68168MUP-F01HSB1K6KA-ac270c5486/runtimes_1024.png","thumb_1024_w":1024,"thumb_1024_h":683,"original_w":1800,"original_h":1200,"thumb_tiny":"AwAgADC1ceZuXZu6c4qHE/8A00/WrUjyKyhU3A9T6VB+++f7+c8fnWsXoYyWvUSLzvNXdvxnnOamneVSPLBIxzxmmL5vmR534wM1JMzbdqhs9cik9xrZke6Zon3A7uMcYNNi87zV3b8Z5zmjM3H3/wDJp0Zm3ru34yM5p99hdVuST79q7N2e+KgzP/t1PMzgoFDHnJxTVkbdko+Oe1SnoU1djUMxkXdvxnmnzF9ybd3vilRyWwQ/J7ilkkZCBsduP4RQ3qNLQjZpNw2hsd+KFaXzEBDbeM5FL55PWKT/AL5qQS5YDY4z3K0rhbzP/9k=","permalink":"https://julialang.slack.com/files/UDB26738Q/F01HSB1K6KA/runtimes.png","permalink_public":"https://slack-files.com/T68168MUP-F01HSB1K6KA-54bb7276ee","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"JtLv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"this is how long spawning takes:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  0.000145 seconds (5.00 k allocations: 429.812 KiB)\n  0.000201 seconds (5.01 k allocations: 430.953 KiB)\n  0.000427 seconds (5.21 k allocations: 443.594 KiB)\n  0.000743 seconds (5.19 k allocations: 442.078 KiB)\n  0.001024 seconds (5.38 k allocations: 454.391 KiB)\n  0.000565 seconds (5.02 k allocations: 431.719 KiB)\n  0.008058 seconds (9.59 k allocations: 741.875 KiB)\n  0.012611 seconds (27.52 k allocations: 1.895 MiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"the plot shows the cumulative runtime and the number of tasks run by each thread"}]}]}],"user":"UDB26738Q","ts":"1608143291.186500","thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"6bce5f8e-d779-474c-bbcc-34ab3f31d777","type":"message","text":"you're right that spawning the tasks takes quite a bit.  is there a better approach?","user":"UDB26738Q","ts":"1608143377.187100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"B/MV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you're right that spawning the tasks takes quite a bit.  is there a better approach?"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"2950e5a5-2fcc-4e2c-8739-d2d7e691ad36","type":"message","text":"start a task per thread","user":"U67BJLYCS","ts":"1608143511.187300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ig0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"start a task per thread"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"9c597a54-3ed2-4165-b717-584841df1c9a","type":"message","text":"and use channels","user":"U67BJLYCS","ts":"1608143517.187500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+oEZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"and use channels"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"8ae3479d-09dd-473c-8bf6-b4797d33ded3","type":"message","text":"with FLoops\n```function pi_string_floops(N)\n    @floop for n in 1:N\n        @reduce(digits = append!!(EmptyVector(), SingletonVector(pi_digit(n))))\n    end\n    return \"0x3.\" * join(string.(digits, base = 16)) * \"p0\"\nend```\nI get exactly the same behaviour as with `Threads.@threads` (but it's nicer to write):\n```% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  285.331 ms (2040 allocations: 122.36 KiB)\n  225.346 ms (2112 allocations: 133.34 KiB)\n  132.689 ms (2254 allocations: 147.19 KiB)\n  71.927 ms (2533 allocations: 166.62 KiB)\n  34.477 ms (3614 allocations: 222.97 KiB)\n  19.649 ms (4683 allocations: 273.19 KiB)\n  11.304 ms (8887 allocations: 460.50 KiB)\n  7.376 ms (17144 allocations: 821.47 KiB)```","user":"UDB26738Q","ts":"1608144590.187700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ygrx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"with FLoops\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function pi_string_floops(N)\n    @floop for n in 1:N\n        @reduce(digits = append!!(EmptyVector(), SingletonVector(pi_digit(n))))\n    end\n    return \"0x3.\" * join(string.(digits, base = 16)) * \"p0\"\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I get exactly the same behaviour as with "},{"type":"text","text":"Threads.@threads","style":{"code":true}},{"type":"text","text":" (but it's nicer to write):\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  285.331 ms (2040 allocations: 122.36 KiB)\n  225.346 ms (2112 allocations: 133.34 KiB)\n  132.689 ms (2254 allocations: 147.19 KiB)\n  71.927 ms (2533 allocations: 166.62 KiB)\n  34.477 ms (3614 allocations: 222.97 KiB)\n  19.649 ms (4683 allocations: 273.19 KiB)\n  11.304 ms (8887 allocations: 460.50 KiB)\n  7.376 ms (17144 allocations: 821.47 KiB)"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"16a32e2a-ba81-449d-86a2-4828630e089f","type":"message","text":"I think I'll delve into channels another time, but at least I have a better understanding of the problem (not ideal spawning of the tasks)","user":"UDB26738Q","ts":"1608144776.187900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=iGjb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think I'll delve into channels another time, but at least I have a better understanding of the problem (not ideal spawning of the tasks)"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"a955318c-a54f-4c1c-9390-0d09cd630a47","type":"message","text":"<@U67BJLYCS> `@floop` with `ThreadedEx` (default for parallel) is binary tree (with the \"leaf\" being arbitrary sized block). It's not coupled to particular scheduling, though; e.g., `DistributedEx` does not use tree.","user":"UC7AF7NSU","ts":"1608147433.188200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pZz","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":" "},{"type":"text","text":"@floop","style":{"code":true}},{"type":"text","text":" with "},{"type":"text","text":"ThreadedEx","style":{"code":true}},{"type":"text","text":" (default for parallel) is binary tree (with the \"leaf\" being arbitrary sized block). It's not coupled to particular scheduling, though; e.g., "},{"type":"text","text":"DistributedEx","style":{"code":true}},{"type":"text","text":" does not use tree."}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"c2979702-f04a-44f8-8abd-2e57f8242e79","type":"message","text":"<@UDB26738Q> If you write `@floop ThreadedEx(basesize=1) for n in 1:N ...` I think it'd be closer to manual spawn (though a bit more inefficient because it'd spawn more tasks).  By default it's something like `basesize = N ÷ nthreads()`. Maybe it's possible to improve the performance a little bit by tweaking `basesize` to use something in the middle.","user":"UC7AF7NSU","ts":"1608147688.188400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qbav","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UDB26738Q"},{"type":"text","text":" If you write "},{"type":"text","text":"@floop ThreadedEx(basesize=1) for n in 1:N ...","style":{"code":true}},{"type":"text","text":" I think it'd be closer to manual spawn (though a bit more inefficient because it'd spawn more tasks).  By default it's something like "},{"type":"text","text":"basesize = N ÷ nthreads()","style":{"code":true}},{"type":"text","text":". Maybe it's possible to improve the performance a little bit by tweaking "},{"type":"text","text":"basesize","style":{"code":true}},{"type":"text","text":" to use something in the middle."}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"44de78e5-e9b5-4750-ab05-3e46ac464174","type":"message","text":"with `ThreadedEx(basesize=1)`:\n```% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  295.095 ms (64736 allocations: 2.85 MiB)\n  148.564 ms (65735 allocations: 2.88 MiB)\n  74.747 ms (65738 allocations: 2.88 MiB)\n  38.245 ms (65748 allocations: 2.89 MiB)\n  19.820 ms (65762 allocations: 2.89 MiB)\n  10.695 ms (65774 allocations: 2.89 MiB)\n  6.781 ms (65806 allocations: 2.89 MiB)\n  4.547 ms (65847 allocations: 2.89 MiB)```\nfastest solution so far :gotta-go-fast:","user":"UDB26738Q","ts":"1608147911.188600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8FIk0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"with "},{"type":"text","text":"ThreadedEx(basesize=1)","style":{"code":true}},{"type":"text","text":":\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  295.095 ms (64736 allocations: 2.85 MiB)\n  148.564 ms (65735 allocations: 2.88 MiB)\n  74.747 ms (65738 allocations: 2.88 MiB)\n  38.245 ms (65748 allocations: 2.89 MiB)\n  19.820 ms (65762 allocations: 2.89 MiB)\n  10.695 ms (65774 allocations: 2.89 MiB)\n  6.781 ms (65806 allocations: 2.89 MiB)\n  4.547 ms (65847 allocations: 2.89 MiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"fastest solution so far "},{"type":"emoji","name":"gotta-go-fast"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q","reactions":[{"name":"raised_hands","users":["U01H9PZ7ZN0"],"count":1}]},{"client_msg_id":"2414ec06-7e3b-43e3-90d2-8e478902aebb","type":"message","text":"For the channel-based approach, take a look at `Threads.foreach` if you have in Julia 1.6. IIRC, I never had a MWE where `push!(::Channel, _)` performs better than `@spawn` when `@spawn` is the bottleneck, though. It's useful for managing resources (e.g. RAM) though.","user":"UC7AF7NSU","ts":"1608147940.188800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TlFe","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"For the channel-based approach, take a look at "},{"type":"text","text":"Threads.foreach","style":{"code":true}},{"type":"text","text":" if you have in Julia 1.6. IIRC, I never had a MWE where "},{"type":"text","text":"push!(::Channel, _)","style":{"code":true}},{"type":"text","text":" performs better than "},{"type":"text","text":"@spawn","style":{"code":true}},{"type":"text","text":" when "},{"type":"text","text":"@spawn","style":{"code":true}},{"type":"text","text":" is the bottleneck, though. It's useful for managing resources (e.g. RAM) though."}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q","reactions":[{"name":"+1","users":["UDB26738Q"],"count":1}]},{"client_msg_id":"1b4cf4ba-fa76-4e4a-be4f-e867367b6d75","type":"message","text":"Oh, interesting! I thought `basesize=1` would be a bit worse than sequential `@spawn`. Though I've heard tree-based solution was better in Cilk so maybe it's conceivable.","user":"UC7AF7NSU","ts":"1608148045.189100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3AaW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oh, interesting! I thought "},{"type":"text","text":"basesize=1","style":{"code":true}},{"type":"text","text":" would be a bit worse than sequential "},{"type":"text","text":"@spawn","style":{"code":true}},{"type":"text","text":". Though I've heard tree-based solution was better in Cilk so maybe it's conceivable."}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"c01fcf42-4a6d-43a1-bbe6-ae5358f18a31","type":"message","text":"well, it is slightly worse up to 16 threads, after that `@spawn` behaves quite badly, but `@floops` is still pretty good","user":"UDB26738Q","ts":"1608148178.189300","team":"T68168MUP","edited":{"user":"UDB26738Q","ts":"1608148185.000000"},"blocks":[{"type":"rich_text","block_id":"A5N","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"well, it is slightly worse up to 16 threads, after that "},{"type":"text","text":"@spawn","style":{"code":true}},{"type":"text","text":" behaves quite badly, but "},{"type":"text","text":"@floops","style":{"code":true}},{"type":"text","text":" is still pretty good"}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"},{"client_msg_id":"c81e0b67-f111-4fc9-a6ce-2dd9ce9bf065","type":"message","text":"Ah, I see! I guess `@spawn`ing from multiple threads independently is beneficial when you have many workers.","user":"UC7AF7NSU","ts":"1608148411.189600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pjFe","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah, I see! I guess "},{"type":"text","text":"@spawn","style":{"code":true}},{"type":"text","text":"ing from multiple threads independently is beneficial when you have many workers."}]}]}],"thread_ts":"1608134434.180500","parent_user_id":"UDB26738Q"}]