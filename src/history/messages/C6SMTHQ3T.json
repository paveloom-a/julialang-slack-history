[{"type":"message","subtype":"channel_join","ts":"1608550686.229900","user":"U01GX4L17PW","text":"<@U01GX4L17PW> has joined the channel","inviter":"USU9FRPEU"},{"client_msg_id":"ec8c656a-3bd2-4c64-aa47-d147f633b1ec","type":"message","text":"Fantastic, thanks <@UBVE598BC>. I'll try the dict ordering  solution, and I like the termination message idea.","user":"UCNPT22MQ","ts":"1608403418.228100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"S2m06","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Fantastic, thanks "},{"type":"user","user_id":"UBVE598BC"},{"type":"text","text":". I'll try the dict ordering  solution, and I like the termination message idea."}]}]}]},{"client_msg_id":"24869ca3-ca49-4920-88f2-dabe7577473c","type":"message","text":"I've found this to be a useful abstraction.\n```struct TerminationMessage end\n\nstruct WorkerPool\n    channel::Channel{Any}\n    workers::Vector{Task}\nend\n\nfunction workerpool(f::Function, n::Integer)\n    channel = Channel(n)\n    workers = Task[Base.Threads.@spawn _worker(f, channel) for _ = 1:n]\n    return WorkerPool(channel, workers)\nend\n\nfunction _worker(f::Function, channel::Channel)\n    while true\n        message = take!(channel)\n        message == TerminationMessage &amp;&amp; return\n        f(message)\n    end\nend\n\nfunction finish(pool::WorkerPool)\n    for worker in pool.workers\n        put!(pool.channel, TerminationMessage)\n    end\n    for worker in pool.workers\n        wait(worker)\n    end\nend```","user":"UBVE598BC","ts":"1608399047.226500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oQ59R","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I've found this to be a useful abstraction.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"struct TerminationMessage end\n\nstruct WorkerPool\n    channel::Channel{Any}\n    workers::Vector{Task}\nend\n\nfunction workerpool(f::Function, n::Integer)\n    channel = Channel(n)\n    workers = Task[Base.Threads.@spawn _worker(f, channel) for _ = 1:n]\n    return WorkerPool(channel, workers)\nend\n\nfunction _worker(f::Function, channel::Channel)\n    while true\n        message = take!(channel)\n        message == TerminationMessage && return\n        f(message)\n    end\nend\n\nfunction finish(pool::WorkerPool)\n    for worker in pool.workers\n        put!(pool.channel, TerminationMessage)\n    end\n    for worker in pool.workers\n        wait(worker)\n    end\nend"}]}]}]},{"client_msg_id":"60e3e13c-1404-4f88-970e-afbf32a0c6fe","type":"message","text":"Yes, channels and workers are a good design. I've done exactly that for writing compressed multi-level tiff files. The output ordering I handled by having a dedicated worker receiving the output from the compressor workers. It stored the compressed tiles in an internal dict, keyed by their sequence number. Whenever it had tiles available in order it forwarded those to the next step, out of order tiles had to wait in the dict until everything before them was ready.","user":"UBVE598BC","ts":"1608398714.225900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mCm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes, channels and workers are a good design. I've done exactly that for writing compressed multi-level tiff files. The output ordering I handled by having a dedicated worker receiving the output from the compressor workers. It stored the compressed tiles in an internal dict, keyed by their sequence number. Whenever it had tiles available in order it forwarded those to the next step, out of order tiles had to wait in the dict until everything before them was ready."}]}]}]},{"client_msg_id":"3f0507e6-d204-4624-8e73-1fa5955a3c24","type":"message","text":"What's a nice design for a multithreaded iterator over a dataset (i.e. sampling rows, fetching them, doing some unpacking / processing)? I feel like channels could work? A channel for indices to fetch, then an output channel, and some number of workers in the middle that run in a loop? One thing I need is to able to guarantee ordering is preserved between the input and output channels - any easy way to do that?","user":"UCNPT22MQ","ts":"1608397061.221000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"olVbu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What's a nice design for a multithreaded iterator over a dataset (i.e. sampling rows, fetching them, doing some unpacking / processing)? I feel like channels could work? A channel for indices to fetch, then an output channel, and some number of workers in the middle that run in a loop? One thing I need is to able to guarantee ordering is preserved between the input and output channels - any easy way to do that?"}]}]}]},{"client_msg_id":"ea211e5a-9332-4881-8c8c-671641960ef1","type":"message","text":"Crossposting my announcement of <https://github.com/JuliaActors/Actors.jl|Actors 0.2> on Discourse:\n\n<https://discourse.julialang.org/t/ann-actors-0-2-genservers-and-guards/52056>","user":"UP9P4JFNJ","ts":"1608315969.204800","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"[ANN] Actors 0.2, GenServers and Guards","title_link":"https://discourse.julialang.org/t/ann-actors-0-2-genservers-and-guards/52056","text":"I’m happy to announce the release of Actors 0.2 together with two actor infrastructure libraries GenServers and Guards. Actors: Concurrency based on the Actor Model Actors implements the classical Actor model in Julia. It builds on Julia’s concurrency primitives, provides a message based programming model for making concurrency easy to understand and reason about and integrates well with Julia’s features for multi-threading and distributed computing. Actors expresses actor behavior as a fun...","fallback":"JuliaLang: [ANN] Actors 0.2, GenServers and Guards","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","fields":[{"title":"Reading time","value":"1 mins :clock2:","short":true},{"title":"Likes","value":"3 :heart:","short":true}],"ts":1608308480,"from_url":"https://discourse.julialang.org/t/ann-actors-0-2-genservers-and-guards/52056","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/ann-actors-0-2-genservers-and-guards/52056"}],"blocks":[{"type":"rich_text","block_id":"QPA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Crossposting my announcement of "},{"type":"link","url":"https://github.com/JuliaActors/Actors.jl","text":"Actors 0.2"},{"type":"text","text":" on Discourse:\n\n"},{"type":"link","url":"https://discourse.julialang.org/t/ann-actors-0-2-genservers-and-guards/52056"}]}]}],"thread_ts":"1608315969.204800","reply_count":15,"reply_users_count":3,"latest_reply":"1608576509.230800","reply_users":["U01CQTKB86N","UP9P4JFNJ","U6QGE7S86"],"subscribed":false,"reactions":[{"name":"wow","users":["U680THK2S","U01CQTKB86N","UDGT4PM41","U6QGE7S86"],"count":4}]},{"client_msg_id":"dc41eb5c-93fe-46ba-90f7-3589b4146b9e","type":"message","text":"I’ve looked into the parallel computing sections of the manual. Then I see that there is JuliaFolds that has a different view on how to do this, and there might be others. How should one go about thinking this, what’s the actual difference, and which is better for what?","user":"U01CQTKB86N","ts":"1608312913.201700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DOOxu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’ve looked into the parallel computing sections of the manual. Then I see that there is JuliaFolds that has a different view on how to do this, and there might be others. How should one go about thinking this, what’s the actual difference, and which is better for what?"}]}]}],"thread_ts":"1608312913.201700","reply_count":10,"reply_users_count":2,"latest_reply":"1608494907.228200","reply_users":["U8D9768Q6","U01CQTKB86N"],"subscribed":false},{"client_msg_id":"73f835fe-f00d-40fb-a87c-224fb32471c3","type":"message","text":"thanks, that makes sense.","user":"UH2HTNM50","ts":"1608227552.198800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"A0lv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"thanks, that makes sense."}]}]}]},{"client_msg_id":"fb90468a-18bc-48b6-bef7-f4735b0caed9","type":"message","text":"oh, indeed, it does appear to be thread-local. that wasn't obvious from the julia side of the code, i think (none of the comments around those callsites indicate it):\n<https://github.com/JuliaLang/julia/blob/c9923cda97064072ecf9eaf3e243bf58c140a89b/src/gc.c#L2807>","user":"U89GY9W1J","ts":"1608227472.198400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2Th85","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"oh, indeed, it does appear to be thread-local. that wasn't obvious from the julia side of the code, i think (none of the comments around those callsites indicate it):\n"},{"type":"link","url":"https://github.com/JuliaLang/julia/blob/c9923cda97064072ecf9eaf3e243bf58c140a89b/src/gc.c#L2807"}]}]}]},{"client_msg_id":"daa171b3-7ad2-4773-ad20-e922cea3fe01","type":"message","text":"I thought it was thread-local","user":"U67BXBF99","ts":"1608222567.196900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"BKwDF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I thought it was thread-local"}]}]}],"reactions":[{"name":"man-facepalming","users":["UH2HTNM50"],"count":1}]},{"client_msg_id":"4866efa3-4d0b-4e96-8eef-ad4481e3e529","type":"message","text":"I was looking at `jl_gc_enable()` this morning and it doesn't seem threadsafe- is this a known issue?","user":"UH2HTNM50","ts":"1608220740.196600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"XWl5Q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I was looking at "},{"type":"text","text":"jl_gc_enable()","style":{"code":true}},{"type":"text","text":" this morning and it doesn't seem threadsafe- is this a known issue?"}]}]}]},{"type":"message","subtype":"thread_broadcast","text":"it looks like that with more threads available the scheduler is overloading few threads when using the tasks?","user":"UDB26738Q","ts":"1608137594.182000","thread_ts":"1608134434.180500","root":{"client_msg_id":"5f2dd458-edd5-4815-bfb2-73f7e686d6d0","type":"message","text":"I'm seeing a rather fun behaviour with tasks.  in a system with AMD EPYC 7502 32-Core Processor, I launch the script in the:thread:below, with different number of threads and get the following timings:\n```% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  285.718 ms (7020 allocations: 543.05 KiB)\n  143.495 ms (8020 allocations: 574.30 KiB)\n  71.975 ms (8020 allocations: 574.30 KiB)\n  36.460 ms (8020 allocations: 574.30 KiB)\n  18.987 ms (8020 allocations: 574.30 KiB)\n  24.721 ms (8021 allocations: 574.33 KiB)\n  80.087 ms (8024 allocations: 574.42 KiB)\n  217.695 ms (8022 allocations: 574.36 KiB)```\nit scales perfectly up to 16 threads, then it starts getting worse and worse, up to 128 threads (the number of threads available in the system), where it's close to the performance of a single thread.\n\nthe equivalent function which uses `Threads.@threads` instead of `Thread.@spawn` has a much more straightforward behaviour (but scaling is worse, because of load imbalance):\n```% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  286.673 ms (2020 allocations: 113.64 KiB)\n  222.603 ms (2026 allocations: 114.11 KiB)\n  132.780 ms (2038 allocations: 115.06 KiB)\n  72.050 ms (2060 allocations: 116.91 KiB)\n  37.192 ms (2101 allocations: 120.50 KiB)\n  20.731 ms (2193 allocations: 128.00 KiB)\n  11.464 ms (2348 allocations: 142.12 KiB)\n  9.128 ms (2659 allocations: 170.38 KiB)```\ndo you have an idea why tasks are behaving this way?","user":"UDB26738Q","ts":"1608134434.180500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"k7Gj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm seeing a rather fun behaviour with tasks.  in a system with AMD EPYC 7502 32-Core Processor, I launch the script in the"},{"type":"emoji","name":"thread"},{"type":"text","text":"below, with different number of threads and get the following timings:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  285.718 ms (7020 allocations: 543.05 KiB)\n  143.495 ms (8020 allocations: 574.30 KiB)\n  71.975 ms (8020 allocations: 574.30 KiB)\n  36.460 ms (8020 allocations: 574.30 KiB)\n  18.987 ms (8020 allocations: 574.30 KiB)\n  24.721 ms (8021 allocations: 574.33 KiB)\n  80.087 ms (8024 allocations: 574.42 KiB)\n  217.695 ms (8022 allocations: 574.36 KiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"it scales perfectly up to 16 threads, then it starts getting worse and worse, up to 128 threads (the number of threads available in the system), where it's close to the performance of a single thread.\n\nthe equivalent function which uses "},{"type":"text","text":"Threads.@threads","style":{"code":true}},{"type":"text","text":" instead of "},{"type":"text","text":"Thread.@spawn","style":{"code":true}},{"type":"text","text":" has a much more straightforward behaviour (but scaling is worse, because of load imbalance):\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  286.673 ms (2020 allocations: 113.64 KiB)\n  222.603 ms (2026 allocations: 114.11 KiB)\n  132.780 ms (2038 allocations: 115.06 KiB)\n  72.050 ms (2060 allocations: 116.91 KiB)\n  37.192 ms (2101 allocations: 120.50 KiB)\n  20.731 ms (2193 allocations: 128.00 KiB)\n  11.464 ms (2348 allocations: 142.12 KiB)\n  9.128 ms (2659 allocations: 170.38 KiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\ndo you have an idea why tasks are behaving this way?"}]}]}],"thread_ts":"1608134434.180500","reply_count":39,"reply_users_count":4,"latest_reply":"1608148411.189600","reply_users":["UDB26738Q","U67BJLYCS","U01GRS159T8","UC7AF7NSU"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"giJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it looks like that with more threads available the scheduler is overloading few threads when using the tasks?"}]}]}],"client_msg_id":"bea24a8a-f47f-4a0b-b9d7-1a32b26d6178"},{"client_msg_id":"5f2dd458-edd5-4815-bfb2-73f7e686d6d0","type":"message","text":"I'm seeing a rather fun behaviour with tasks.  in a system with AMD EPYC 7502 32-Core Processor, I launch the script in the:thread:below, with different number of threads and get the following timings:\n```% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  285.718 ms (7020 allocations: 543.05 KiB)\n  143.495 ms (8020 allocations: 574.30 KiB)\n  71.975 ms (8020 allocations: 574.30 KiB)\n  36.460 ms (8020 allocations: 574.30 KiB)\n  18.987 ms (8020 allocations: 574.30 KiB)\n  24.721 ms (8021 allocations: 574.33 KiB)\n  80.087 ms (8024 allocations: 574.42 KiB)\n  217.695 ms (8022 allocations: 574.36 KiB)```\nit scales perfectly up to 16 threads, then it starts getting worse and worse, up to 128 threads (the number of threads available in the system), where it's close to the performance of a single thread.\n\nthe equivalent function which uses `Threads.@threads` instead of `Thread.@spawn` has a much more straightforward behaviour (but scaling is worse, because of load imbalance):\n```% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  286.673 ms (2020 allocations: 113.64 KiB)\n  222.603 ms (2026 allocations: 114.11 KiB)\n  132.780 ms (2038 allocations: 115.06 KiB)\n  72.050 ms (2060 allocations: 116.91 KiB)\n  37.192 ms (2101 allocations: 120.50 KiB)\n  20.731 ms (2193 allocations: 128.00 KiB)\n  11.464 ms (2348 allocations: 142.12 KiB)\n  9.128 ms (2659 allocations: 170.38 KiB)```\ndo you have an idea why tasks are behaving this way?","user":"UDB26738Q","ts":"1608134434.180500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"k7Gj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm seeing a rather fun behaviour with tasks.  in a system with AMD EPYC 7502 32-Core Processor, I launch the script in the"},{"type":"emoji","name":"thread"},{"type":"text","text":"below, with different number of threads and get the following timings:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  285.718 ms (7020 allocations: 543.05 KiB)\n  143.495 ms (8020 allocations: 574.30 KiB)\n  71.975 ms (8020 allocations: 574.30 KiB)\n  36.460 ms (8020 allocations: 574.30 KiB)\n  18.987 ms (8020 allocations: 574.30 KiB)\n  24.721 ms (8021 allocations: 574.33 KiB)\n  80.087 ms (8024 allocations: 574.42 KiB)\n  217.695 ms (8022 allocations: 574.36 KiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"it scales perfectly up to 16 threads, then it starts getting worse and worse, up to 128 threads (the number of threads available in the system), where it's close to the performance of a single thread.\n\nthe equivalent function which uses "},{"type":"text","text":"Threads.@threads","style":{"code":true}},{"type":"text","text":" instead of "},{"type":"text","text":"Thread.@spawn","style":{"code":true}},{"type":"text","text":" has a much more straightforward behaviour (but scaling is worse, because of load imbalance):\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  286.673 ms (2020 allocations: 113.64 KiB)\n  222.603 ms (2026 allocations: 114.11 KiB)\n  132.780 ms (2038 allocations: 115.06 KiB)\n  72.050 ms (2060 allocations: 116.91 KiB)\n  37.192 ms (2101 allocations: 120.50 KiB)\n  20.731 ms (2193 allocations: 128.00 KiB)\n  11.464 ms (2348 allocations: 142.12 KiB)\n  9.128 ms (2659 allocations: 170.38 KiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\ndo you have an idea why tasks are behaving this way?"}]}]}],"thread_ts":"1608134434.180500","reply_count":39,"reply_users_count":4,"latest_reply":"1608148411.189600","reply_users":["UDB26738Q","U67BJLYCS","U01GRS159T8","UC7AF7NSU"],"subscribed":false,"reactions":[{"name":"eyes","users":["UEP056STX"],"count":1}]},{"type":"message","subtype":"thread_broadcast","text":"In 40 minutes","user":"U67BXBF99","ts":"1608133869.175300","thread_ts":"1607974366.170100","root":{"client_msg_id":"81604d55-3ef2-4654-af0e-635e5776ded0","type":"message","text":"Hey, all! The <#C6SMTHQ3T|multithreading> users’ BoF for December is planned for Wednesday, at 4:30 PM UTC (11:30a EST+5). Link at <https://meet.google.com/ugr-sbmu-wts> and I’ll update minutes at <https://docs.google.com/document/d/1rDgdFH94QL4wlEAdzBbdxJJ0Wk2CkBhWuptzPETSRc0/edit?usp=sharing>","user":"U67BXBF99","ts":"1607974366.170100","team":"T68168MUP","edited":{"user":"U67BXBF99","ts":"1607974384.000000"},"blocks":[{"type":"rich_text","block_id":"LQfL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey, all! The "},{"type":"channel","channel_id":"C6SMTHQ3T"},{"type":"text","text":" users’ BoF for December is planned for Wednesday, at 4:30 PM UTC (11:30a EST+5). Link at "},{"type":"link","url":"https://meet.google.com/ugr-sbmu-wts"},{"type":"text","text":" and I’ll update minutes at "},{"type":"link","url":"https://docs.google.com/document/d/1rDgdFH94QL4wlEAdzBbdxJJ0Wk2CkBhWuptzPETSRc0/edit?usp=sharing"}]}]}],"thread_ts":"1607974366.170100","reply_count":1,"reply_users_count":1,"latest_reply":"1608133869.175300","reply_users":["U67BXBF99"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"/M1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In 40 minutes"}]}]}],"client_msg_id":"8df728fd-c24e-47e3-9453-6bc0a413d6a8"},{"client_msg_id":"8cffd73b-0a4f-4b3a-a938-788be37cde45","type":"message","text":"Does julia natively use pthreads? I'm getting some profiling done of my code, and the guy helping me wants to know what threading model we use...","user":"U7C8KRZKL","ts":"1608132266.174100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FBa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does julia natively use pthreads? I'm getting some profiling done of my code, and the guy helping me wants to know what threading model we use..."}]}]}],"thread_ts":"1608132266.174100","reply_count":7,"reply_users_count":3,"latest_reply":"1608136093.181200","reply_users":["U6795JH6H","U67BXBF99","U7C8KRZKL"],"subscribed":false},{"client_msg_id":"a9ec60bf-4bdf-48f0-b6e6-abce09c11da7","type":"message","text":"Let me know, preferably in advance, if you have any topics you’d like to have discussed!","user":"U67BXBF99","ts":"1607974394.170800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4POs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Let me know, preferably in advance, if you have any topics you’d like to have discussed!"}]}]}]},{"client_msg_id":"81604d55-3ef2-4654-af0e-635e5776ded0","type":"message","text":"Hey, all! The <#C6SMTHQ3T|multithreading> users’ BoF for December is planned for Wednesday, at 4:30 PM UTC (11:30a EST+5). Link at <https://meet.google.com/ugr-sbmu-wts> and I’ll update minutes at <https://docs.google.com/document/d/1rDgdFH94QL4wlEAdzBbdxJJ0Wk2CkBhWuptzPETSRc0/edit?usp=sharing>","user":"U67BXBF99","ts":"1607974366.170100","team":"T68168MUP","edited":{"user":"U67BXBF99","ts":"1607974384.000000"},"blocks":[{"type":"rich_text","block_id":"LQfL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey, all! The "},{"type":"channel","channel_id":"C6SMTHQ3T"},{"type":"text","text":" users’ BoF for December is planned for Wednesday, at 4:30 PM UTC (11:30a EST+5). Link at "},{"type":"link","url":"https://meet.google.com/ugr-sbmu-wts"},{"type":"text","text":" and I’ll update minutes at "},{"type":"link","url":"https://docs.google.com/document/d/1rDgdFH94QL4wlEAdzBbdxJJ0Wk2CkBhWuptzPETSRc0/edit?usp=sharing"}]}]}],"thread_ts":"1607974366.170100","reply_count":1,"reply_users_count":1,"latest_reply":"1608133869.175300","reply_users":["U67BXBF99"],"subscribed":false},{"client_msg_id":"33872a18-2f12-4c0b-becc-1e5a2d8b39af","type":"message","text":"I tested 1.5 and 1.6. But my problem isn't the exception, which is correct: it's how I could rethrow it *outside of the task* without losing the original backtrace.","user":"U67431ELR","ts":"1607972444.168600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Joo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I tested 1.5 and 1.6. But my problem isn't the exception, which is correct: it's how I could rethrow it *outside of the task* without losing the original backtrace."}]}]}],"thread_ts":"1607972444.168600","reply_count":8,"reply_users_count":2,"latest_reply":"1607980415.172400","reply_users":["UP9P4JFNJ","U67431ELR"],"subscribed":false},{"client_msg_id":"f897bf92-61d3-4747-b238-5aac3b224413","type":"message","text":"I remember that being the case in former versions of Julia. Now with 1.5.3 I’m getting:\n```julia &gt; fail(x) = nonexistent(x)\n\njulia&gt; t = Threads.@spawn fail(1)\nTask (runnable) @0x0000000120b19690\n\njulia&gt; t\nTask (failed) @0x0000000120b19690\nUndefVarError: nonexistent not defined\nfail(::Int64) at /Users/paul/.julia/dev/tmp/exception.jl:1\n(::var\"#45#46\")() at ./threadingconstructs.jl:169\n\njulia&gt; t.exception\nUndefVarError(:nonexistent)\n\njulia&gt; stacktrace(t.backtrace)\n2-element Array{Base.StackTraces.StackFrame,1}:\n fail(::Int64) at exception.jl:1\n (::var\"#45#46\")() at threadingconstructs.jl:169```\nThe same now with `rethrow`:\n```function fail1(x)\n    try\n        nonexistent(x)\n    catch exc\n        rethrow(exc)\n    end\nend\n\njulia&gt; t1 = Threads.@spawn fail1(1)\nTask (runnable) @0x0000000120b19210\n\njulia&gt; t1\nTask (failed) @0x0000000120b19210\nUndefVarError: nonexistent not defined\nfail1(::Int64) at /Users/paul/.julia/dev/tmp/exception.jl:5\n(::var\"#47#48\")() at ./threadingconstructs.jl:169\n\njulia&gt; t1.exception\nUndefVarError(:nonexistent)\n\njulia&gt; stacktrace(t1.backtrace)\n2-element Array{Base.StackTraces.StackFrame,1}:\n fail1(::Int64) at exception.jl:5\n (::var\"#47#48\")() at threadingconstructs.jl:169```\nWhich version are you using?","user":"UP9P4JFNJ","ts":"1607930935.168100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Q/0T/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I remember that being the case in former versions of Julia. Now with 1.5.3 I’m getting:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia > fail(x) = nonexistent(x)\n\njulia> t = Threads.@spawn fail(1)\nTask (runnable) @0x0000000120b19690\n\njulia> t\nTask (failed) @0x0000000120b19690\nUndefVarError: nonexistent not defined\nfail(::Int64) at /Users/paul/.julia/dev/tmp/exception.jl:1\n(::var\"#45#46\")() at ./threadingconstructs.jl:169\n\njulia> t.exception\nUndefVarError(:nonexistent)\n\njulia> stacktrace(t.backtrace)\n2-element Array{Base.StackTraces.StackFrame,1}:\n fail(::Int64) at exception.jl:1\n (::var\"#45#46\")() at threadingconstructs.jl:169"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"The same now with "},{"type":"text","text":"rethrow","style":{"code":true}},{"type":"text","text":":\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function fail1(x)\n    try\n        nonexistent(x)\n    catch exc\n        rethrow(exc)\n    end\nend\n\njulia> t1 = Threads.@spawn fail1(1)\nTask (runnable) @0x0000000120b19210\n\njulia> t1\nTask (failed) @0x0000000120b19210\nUndefVarError: nonexistent not defined\nfail1(::Int64) at /Users/paul/.julia/dev/tmp/exception.jl:5\n(::var\"#47#48\")() at ./threadingconstructs.jl:169\n\njulia> t1.exception\nUndefVarError(:nonexistent)\n\njulia> stacktrace(t1.backtrace)\n2-element Array{Base.StackTraces.StackFrame,1}:\n fail1(::Int64) at exception.jl:5\n (::var\"#47#48\")() at threadingconstructs.jl:169"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Which version are you using?"}]}]}]}]