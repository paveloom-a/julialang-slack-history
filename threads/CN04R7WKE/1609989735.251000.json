[{"client_msg_id":"4212879b-39a2-43b1-848e-de02a0f83860","type":"message","text":"you'll want to find out what broke the link in the derivatives.","user":"U69BL50BF","ts":"1609989735.251000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1QI3x","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you'll want to find out what broke the link in the derivatives."}]}]}],"thread_ts":"1609989735.251000","reply_count":1,"reply_users_count":1,"latest_reply":"1610524310.286000","reply_users":["UG194JC5S"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"As the loss function for the training of neural DEs on snap shot data, I implemented this counter function in a very naive manner. Something like:\n`lims = Array(range(1, step = 1, stop = 4)) # the \"buckets\"`\n`counter_of_prediction = [count(x -&gt; lims[ind_lim] &lt;= x &lt; lims[ind_lim + 1], prediction) for ind_lim in 1:length(lims) - 1] # count elements in each bucket for current prediction`\n`loss = sum(abs2, counter_of_prediction .- counter_of_observed_data) # compare number of elements in each bucket for observation and prediction`\nIn practice I would then define “lims” more dynamically in function of the observed data and adjust the bucket size, too. However, if I’m not mistaken, the result of this idea would always include the fact that the loss is defined as a step function, even if the steps get very small. Does the gradient function of Zygote only handle differentiable, continuous functions and therefore every loss function for DiffEqFlux has to be like that? Are there any ticks I should be aware of that could help me here?","user":"UG194JC5S","ts":"1610524310.286000","thread_ts":"1609989735.251000","root":{"client_msg_id":"4212879b-39a2-43b1-848e-de02a0f83860","type":"message","text":"you'll want to find out what broke the link in the derivatives.","user":"U69BL50BF","ts":"1609989735.251000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1QI3x","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you'll want to find out what broke the link in the derivatives."}]}]}],"thread_ts":"1609989735.251000","reply_count":1,"reply_users_count":1,"latest_reply":"1610524310.286000","reply_users":["UG194JC5S"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"HPs54","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"As the loss function for the training of neural DEs on snap shot data, I implemented this counter function in a very naive manner. Something like:\n"},{"type":"text","text":"lims = Array(range(1, step = 1, stop = 4)) # the \"buckets\"","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"counter_of_prediction = [count(x -> lims[ind_lim] <= x < lims[ind_lim + 1], prediction) for ind_lim in 1:length(lims) - 1] # count elements in each bucket for current prediction","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"loss = sum(abs2, counter_of_prediction .- counter_of_observed_data) # compare number of elements in each bucket for observation and prediction","style":{"code":true}},{"type":"text","text":"\nIn practice I would then define “lims” more dynamically in function of the observed data and adjust the bucket size, too. However, if I’m not mistaken, the result of this idea would always include the fact that the loss is defined as a step function, even if the steps get very small. Does the gradient function of Zygote only handle differentiable, continuous functions and therefore every loss function for DiffEqFlux has to be like that? Are there any ticks I should be aware of that could help me here?"}]}]}],"client_msg_id":"0c89e310-6adc-4173-83e3-d5e40b303430"}]