[{"client_msg_id":"bc2c696d-27ba-417b-a658-a1f8c58ff91c","type":"message","text":"What's the difference between training in these 2 ways?\n```iter = 0\nresult_neuralode = DiffEqFlux.sciml_train(loss_neuralode, prob_neuralode.p,\n                                          ADAM(0.05), cb = callback,\n                                          maxiters = 300)\n\nresult_neuralode2 = DiffEqFlux.sciml_train(loss_neuralode, result_neuralode.minimizer,\n                                          ADAM(0.05), cb = callback,\n                                          maxiters = 200)```\n```iter = 0\nresult_neuralode = DiffEqFlux.sciml_train(loss_neuralode, prob_neuralode.p,\n                                          ADAM(0.05), cb = callback,\n                                          maxiters = 500)```\nBoth of them have 500 iterations and same loss functions but the first approach is giving me a much better prediction and converges relatively faster. Does this happen in general or in some special loss functions?","user":"U01KF8TJEN4","ts":"1613418261.187300","team":"T68168MUP","edited":{"user":"U01KF8TJEN4","ts":"1613418331.000000"},"blocks":[{"type":"rich_text","block_id":"Tx4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What's the difference between training in these 2 ways?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"iter = 0\nresult_neuralode = DiffEqFlux.sciml_train(loss_neuralode, prob_neuralode.p,\n                                          ADAM(0.05), cb = callback,\n                                          maxiters = 300)\n\nresult_neuralode2 = DiffEqFlux.sciml_train(loss_neuralode, result_neuralode.minimizer,\n                                          ADAM(0.05), cb = callback,\n                                          maxiters = 200)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"iter = 0\nresult_neuralode = DiffEqFlux.sciml_train(loss_neuralode, prob_neuralode.p,\n                                          ADAM(0.05), cb = callback,\n                                          maxiters = 500)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Both of them have 500 iterations and same loss functions but the first approach is giving me a much better prediction and converges relatively faster. Does this happen in general or in some special loss functions?"}]}]}],"thread_ts":"1613418261.187300","reply_count":3,"reply_users_count":2,"latest_reply":"1613427184.188100","reply_users":["UNG2XJJP3","U01KF8TJEN4"],"subscribed":false},{"client_msg_id":"ecd1633a-11a7-42d3-b648-b67fe1bd902c","type":"message","text":"in the first way after 300 iters adam has adapted itself and the step is no longer 0.05","user":"UNG2XJJP3","ts":"1613422899.187700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"J92WY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"in the first way after 300 iters adam has adapted itself and the step is no longer 0.05"}]}]}],"thread_ts":"1613418261.187300","parent_user_id":"U01KF8TJEN4"},{"client_msg_id":"681b0365-47ee-48ce-8710-e2f40e508668","type":"message","text":"So in general wouldn't it be better to train models in smaller iterations for example- 500 iters or 1000 iters, again and again like in a loop instead of training the model with a single big iteration value?","user":"U01KF8TJEN4","ts":"1613424881.187900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hjnzp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So in general wouldn't it be better to train models in smaller iterations for example- 500 iters or 1000 iters, again and again like in a loop instead of training the model with a single big iteration value?"}]}]}],"thread_ts":"1613418261.187300","parent_user_id":"U01KF8TJEN4"},{"client_msg_id":"b9d30f41-6147-4844-8ccf-bd37df84f803","type":"message","text":"As far as I know, it makes sense if you modify data to avoid local minima or optimizer (adam to bfgs) during training. If everything keeps same, no reason to be better.","user":"UNG2XJJP3","ts":"1613427184.188100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"S6ZI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"As far as I know, it makes sense if you modify data to avoid local minima or optimizer (adam to bfgs) during training. If everything keeps same, no reason to be better."}]}]}],"thread_ts":"1613418261.187300","parent_user_id":"U01KF8TJEN4"}]