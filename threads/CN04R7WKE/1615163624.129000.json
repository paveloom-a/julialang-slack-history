[{"client_msg_id":"e37783e1-5c5b-4d25-8b28-4f3da49d0904","type":"message","text":"This is pretty cool (I mean not the RL part - it's pretty basic, but the application to control theory specifically). Is anyone here doing something in this direction?\n<https://youtu.be/NZNl-M040NY>","user":"U01BG0NN34J","ts":"1615163624.129000","team":"T68168MUP","attachments":[{"service_name":"YouTube","service_url":"https://www.youtube.com/","title":"Deep Reinforcement Learning for Fluid Dynamics and Control","title_link":"https://youtu.be/NZNl-M040NY","author_name":"Steve Brunton","author_link":"https://www.youtube.com/channel/UCm5mt-A4w61lknZ9lCsZtBw","thumb_url":"https://i.ytimg.com/vi/NZNl-M040NY/hqdefault.jpg","thumb_width":480,"thumb_height":360,"fallback":"YouTube Video: Deep Reinforcement Learning for Fluid Dynamics and Control","video_html":"<iframe width=\"400\" height=\"225\" src=\"https://www.youtube.com/embed/NZNl-M040NY?feature=oembed&autoplay=1&iv_load_policy=3\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","video_html_width":400,"video_html_height":225,"from_url":"https://youtu.be/NZNl-M040NY","service_icon":"https://a.slack-edge.com/80588/img/unfurl_icons/youtube.png","id":1,"original_url":"https://youtu.be/NZNl-M040NY"}],"blocks":[{"type":"rich_text","block_id":"hnZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This is pretty cool (I mean not the RL part - it's pretty basic, but the application to control theory specifically). Is anyone here doing something in this direction?\n"},{"type":"link","url":"https://youtu.be/NZNl-M040NY"}]}]}],"thread_ts":"1615163624.129000","reply_count":6,"reply_users_count":3,"latest_reply":"1615388482.198700","reply_users":["UDDSTBX19","U01A38DLY8N","U01BG0NN34J"],"subscribed":false,"reactions":[{"name":"+1","users":["U01724Q3PGW","US4A6G6B0"],"count":2}]},{"client_msg_id":"d223cfb1-4af7-444d-b0b1-fff5c54e0a78","type":"message","text":"<@U01BG0NN34J> RL for control would likely involve state space models, so those would be markov decision processes or some model free thing like Q-learning. There is the QuickPOMDPs package in Julia for things like that. I did not watch the video yet, but it came up on my feed too. After your recommendation I will take a look.","user":"UDDSTBX19","ts":"1615219322.129800","team":"T68168MUP","edited":{"user":"UDDSTBX19","ts":"1615219688.000000"},"blocks":[{"type":"rich_text","block_id":"XmtlQ","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U01BG0NN34J"},{"type":"text","text":" RL for control would likely involve state space models, so those would be markov decision processes or some model free thing like Q-learning. There is the QuickPOMDPs package in Julia for things like that. I did not watch the video yet, but it came up on my feed too. After your recommendation I will take a look."}]}]}],"thread_ts":"1615163624.129000","parent_user_id":"U01BG0NN34J"},{"client_msg_id":"5A37B26D-D00B-4131-AEC6-0D443EEA6545","type":"message","text":"Yea Ive just started my PhD now and it’s in an adjacent field - or at least I hope to get to these topics soon! ","user":"U01A38DLY8N","ts":"1615234986.139400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QtD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yea Ive just started my PhD now and it’s in an adjacent field - or at least I hope to get to these topics soon! "}]}]}],"thread_ts":"1615163624.129000","parent_user_id":"U01BG0NN34J","reactions":[{"name":"+1","users":["UDDSTBX19"],"count":1}]},{"client_msg_id":"64029010-7850-4113-A900-B8181C7A667E","type":"message","text":"Well I'm mostly interested in the connections between control theory and RL. I'm familiar with RL but not control theory (just a bit). In particular I want to know how concepts like \"controllability\" and stability translate to RL.","user":"U01BG0NN34J","ts":"1615316901.158500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"n0u","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Well I'm mostly interested in the connections between control theory and RL. I'm familiar with RL but not control theory (just a bit). In particular I want to know how concepts like \"controllability\" and stability translate to RL."}]}]}],"thread_ts":"1615163624.129000","parent_user_id":"U01BG0NN34J"},{"client_msg_id":"bb0bf6a3-a104-4c9c-948c-bce030a4d4ed","type":"message","text":"I am by no means an expert <@U01BG0NN34J>, but a lot of modern control theory focuses on the state space. And so you are trying to find a sequence of actions to drive the system to the desired state. In different interpretations, RL tries to be model free, because sometimes trying to estimate the transition matrix for a state space model can be time consuming or inaccurate, or there could just be a huge number of states. Solving model free RL problems can still be hard, since you have a lot of different policies to try out--hence the policy evaluation and value iteration methods. I think that one approach to finding faster solutions to these problems is using neural nets. But I do think it is a challenge to try and solve these problems in a model-free way. At least with a model you can figure out the eigenstructure and get guarantees on the controllability. without that, you might have these really long delays between taking actions and getting the rewards, which is the common challenge or issue with RL methods.","user":"UDDSTBX19","ts":"1615320524.162800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VSvgN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am by no means an expert "},{"type":"user","user_id":"U01BG0NN34J"},{"type":"text","text":", but a lot of modern control theory focuses on the state space. And so you are trying to find a sequence of actions to drive the system to the desired state. In different interpretations, RL tries to be model free, because sometimes trying to estimate the transition matrix for a state space model can be time consuming or inaccurate, or there could just be a huge number of states. Solving model free RL problems can still be hard, since you have a lot of different policies to try out--hence the policy evaluation and value iteration methods. I think that one approach to finding faster solutions to these problems is using neural nets. But I do think it is a challenge to try and solve these problems in a model-free way. At least with a model you can figure out the eigenstructure and get guarantees on the controllability. without that, you might have these really long delays between taking actions and getting the rewards, which is the common challenge or issue with RL methods."}]}]}],"thread_ts":"1615163624.129000","parent_user_id":"U01BG0NN34J"},{"client_msg_id":"CA396172-45E1-42AF-84E5-8AFFC8185547","type":"message","text":"Well from RL point of view whether the experience comes from the real world or the model is not relevant. \nWhen the reward comes has to do with whether the mdp is a sparse reward problem or not. If it is sparse a model of the reward function will not be able to give you a less sparse reward. \nOne way in which a model may help with rewards is by augmenting them, via curiosity for example, or minimization of surprise - where the agent visits those states which a model is not able to reliably predict. \n","user":"U01BG0NN34J","ts":"1615322947.178500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qAF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Well from RL point of view whether the experience comes from the real world or the model is not relevant. \n"},{"type":"text","text":"When the reward comes has to do with whether the mdp is a sparse reward problem or not. If it is sparse a model of the reward function will not be able to give you a less sparse reward. \nOne way in which a model may help with rewards is by augmenting them, via curiosity for example, or minimization of surprise - where the agent visits those states which a model is not able to reliably predict. \n"}]}]}],"thread_ts":"1615163624.129000","parent_user_id":"U01BG0NN34J"},{"client_msg_id":"80EA04D0-F600-41DE-AC47-43FD2382065D","type":"message","text":"Mm there’s a bunch of hot research happening in labs to try to merge rl and optimal/classical control in a common framework. ","user":"U01A38DLY8N","ts":"1615388482.198700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QY62","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Mm there’s a bunch of hot research happening in labs to try to merge rl and optimal/classical control in a common framework. "}]}]}],"thread_ts":"1615163624.129000","parent_user_id":"U01BG0NN34J"}]