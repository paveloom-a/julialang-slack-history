[{"client_msg_id":"fdf4913c-8c28-47a2-80a7-0213662897ee","type":"message","text":"Hi, everyone!\nI have a question about the word embedding layer.\nWhat is the standard way to deal with the words out of vocabulary when using the embedding layer? Are all words out of vocabulary mapping all-zero vector? Or there is a special key in the vocabulary to record these unknown words?","user":"USAH2P9E1","ts":"1615732900.014200","team":"T68168MUP","edited":{"user":"USAH2P9E1","ts":"1615733112.000000"},"blocks":[{"type":"rich_text","block_id":"gyB3A","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, everyone!\nI have a question about the word embedding layer.\nWhat is the standard way to deal with the words out of vocabulary when using the embedding layer? Are all words out of vocabulary mapping all-zero vector? Or there is a special key in the vocabulary to record these unknown words?"}]}]}],"thread_ts":"1615732900.014200","reply_count":6,"reply_users_count":4,"latest_reply":"1615988158.006500","reply_users":["U01FAHWCMFF","USAH2P9E1","URXNULJKS","U69F2VCFJ"],"subscribed":false},{"client_msg_id":"1e1e58db-c9c1-435e-8b3f-a2490eebb2d4","type":"message","text":"The pytorch implementation of embeddings is as follows.\nSpecify the padding idx during init.\nThat idx within the embedding layer is all zeros.\n\nKnowing this, youd have to set all unknown words to have the idx of the padding idx, prior to model input/fitting\n\nHope this helps.","user":"U01FAHWCMFF","ts":"1615742813.014600","team":"T68168MUP","edited":{"user":"U01FAHWCMFF","ts":"1615742855.000000"},"blocks":[{"type":"rich_text","block_id":"1fiGG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The pytorch implementation of embeddings is as follows.\nSpecify the padding idx during init.\nThat idx within the embedding layer is all zeros.\n\nKnowing this, youd have to set all unknown words to have the idx of the padding idx, prior to model input/fitting\n\nHope this helps."}]}]}],"thread_ts":"1615732900.014200","parent_user_id":"USAH2P9E1"},{"client_msg_id":"4d1ca5dc-6bcf-43da-a1ad-8f49ddd91edd","type":"message","text":"Thanks! It helps me!\nI'm learning NLP and I find that there is no embedding layer in Flux. So I'm trying to implement one.\nWill this vector be updated during training?","user":"USAH2P9E1","ts":"1615770909.014900","team":"T68168MUP","edited":{"user":"USAH2P9E1","ts":"1615770992.000000"},"blocks":[{"type":"rich_text","block_id":"yqM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks! It helps me!\nI'm learning NLP and I find that there is no embedding layer in Flux. So I'm trying to implement one.\nWill this vector be updated during training?"}]}]}],"thread_ts":"1615732900.014200","parent_user_id":"USAH2P9E1"},{"client_msg_id":"74893683-3140-4195-ba7e-3c27650600d7","type":"message","text":"Lucky for us I've been working on an NLP problem this week! Here is my implementation of the embedding layer (note: there are other implementations out there).\n\n```struct Embedding\n    W::Array{Float32, 2}\n    Embedding(embedding_size::Integer, vocab_size::Integer) = new(randn(Float32, embedding_size, vocab_size))\nend\n\nFlux.@functor Embedding\n\n(m::Embedding)(x::Vector{&lt;:Integer}) = m.W[:, x]```\nThere are three parts to embedding.\n1. Defining the embedding \n2. Putting the functor wrapper around embedding. This allows flux to treat the layer as an actual flux layer. Now these will be updated via backpropogation as well as have other flux magic enabled such as allowing for them to be stored in GPU.\n3. Allowing embedding to be 'called' on (__ call __ () in python) to do its lookup.\nHope this helps :slightly_smiling_face:","user":"U01FAHWCMFF","ts":"1615813545.015400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gQ8Hc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Lucky for us I've been working on an NLP problem this week! Here is my implementation of the embedding layer (note: there are other implementations out there).\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"struct Embedding\n    W::Array{Float32, 2}\n    Embedding(embedding_size::Integer, vocab_size::Integer) = new(randn(Float32, embedding_size, vocab_size))\nend\n\nFlux.@functor Embedding\n\n(m::Embedding)(x::Vector{<:Integer}) = m.W[:, x]"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"There are three parts to embedding.\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Defining the embedding "}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Putting the functor wrapper around embedding. This allows flux to treat the layer as an actual flux layer. Now these will be updated via backpropogation as well as have other flux magic enabled such as allowing for them to be stored in GPU."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Allowing embedding to be 'called' on (__ call __ () in python) to do its lookup."}]}],"style":"ordered","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"\nHope this helps "},{"type":"emoji","name":"slightly_smiling_face"}]}]}],"thread_ts":"1615732900.014200","parent_user_id":"USAH2P9E1"},{"client_msg_id":"29ce9403-71ac-4ea5-8e18-2576b0825942","type":"message","text":"check out : (Transformers.Basic.Embed)\n<https://github.com/chengchingwen/Transformers.jl/tree/master/src/basic/embeds>","user":"URXNULJKS","ts":"1615837543.015600","team":"T68168MUP","edited":{"user":"URXNULJKS","ts":"1615837636.000000"},"blocks":[{"type":"rich_text","block_id":"YuQBp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"check out : (Transformers.Basic.Embed)\n"},{"type":"link","url":"https://github.com/chengchingwen/Transformers.jl/tree/master/src/basic/embeds"}]}]}],"thread_ts":"1615732900.014200","parent_user_id":"USAH2P9E1"},{"client_msg_id":"e090e00f-5777-4650-b486-d990082969ac","type":"message","text":"<@U01FAHWCMFF> <@URXNULJKS> Thanks! I'm going to try these.","user":"USAH2P9E1","ts":"1615877238.000100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"k5Q","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U01FAHWCMFF"},{"type":"text","text":" "},{"type":"user","user_id":"URXNULJKS"},{"type":"text","text":" Thanks! I'm going to try these."}]}]}],"thread_ts":"1615732900.014200","parent_user_id":"USAH2P9E1"},{"client_msg_id":"884BB947-CF77-4E60-86D1-A5105712CE65","type":"message","text":"It depends on the model and tokenizer you used. For example those transformer-based pretrain model usually use some kinds of sub-word tokenization method (like BPE or WordPieces) to break unknown word into known sub-words. ","user":"U69F2VCFJ","ts":"1615988158.006500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ooDy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It depends on the model and tokenizer you used. For example those transformer-based pretrain model usually use some kinds of sub-word tokenization method (like BPE or WordPieces) to break unknown word into known sub-words. "}]}]}],"thread_ts":"1615732900.014200","parent_user_id":"USAH2P9E1","reactions":[{"name":"+1","users":["URXNULJKS","USAH2P9E1"],"count":2}]}]