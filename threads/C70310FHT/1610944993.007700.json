[{"client_msg_id":"e97557b6-2fc2-4abe-ba0c-a311f4890519","type":"message","text":"Hey <@U679VPJ8L> - out of curiosity, what would be needed to bolster the Julia NLP ecosystem? I instead had to go over to Spacy in python for some work that I had to do with tokenization but am curious what it would take to continue building the Julia NLP ecosystem. Thoughts?","user":"US64J0NPQ","ts":"1610944993.007700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"brt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey "},{"type":"user","user_id":"U679VPJ8L"},{"type":"text","text":" - out of curiosity, what would be needed to bolster the Julia NLP ecosystem? I instead had to go over to Spacy in python for some work that I had to do with tokenization but am curious what it would take to continue building the Julia NLP ecosystem. Thoughts?"}]}]}],"thread_ts":"1610944993.007700","reply_count":10,"reply_users_count":3,"latest_reply":"1610991877.009700","reply_users":["U6A936746","US64J0NPQ","U679VPJ8L"],"subscribed":false},{"client_msg_id":"af1089e3-5c7d-4f9b-bc1d-a3fc7118130a","type":"message","text":"What tokenization need was unmet by WordTokenizers.jl?","user":"U6A936746","ts":"1610963897.007800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Vcf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What tokenization need was unmet by WordTokenizers.jl?"}]}]}],"thread_ts":"1610944993.007700","parent_user_id":"US64J0NPQ"},{"client_msg_id":"dab0b9bf-85af-491d-9d62-1619c4d2434c","type":"message","text":"Hey <@U6A936746> - for me, I was trying to take a PDF, use Taro.jl or PDFIO.jl to extract text from it, and then use WordTokenizers.jl to split sentences. I found that the rule-based approach of splitting sentences wasn't able to handle the raw extracted text terribly well which prompted me to fall back to using Spacy. Spacy gave me better results for sentence tokenization \"out of the box\". Comparing WordTokenizers.jl to Spacy is certainly not a fair comparison so that was why I was wondering what could be done to make the Julia NLP ecosystem a bit stronger.","user":"US64J0NPQ","ts":"1610986812.008000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Um3E","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey "},{"type":"user","user_id":"U6A936746"},{"type":"text","text":" - for me, I was trying to take a PDF, use Taro.jl or PDFIO.jl to extract text from it, and then use WordTokenizers.jl to split sentences. I found that the rule-based approach of splitting sentences wasn't able to handle the raw extracted text terribly well which prompted me to fall back to using Spacy. Spacy gave me better results for sentence tokenization \"out of the box\". Comparing WordTokenizers.jl to Spacy is certainly not a fair comparison so that was why I was wondering what could be done to make the Julia NLP ecosystem a bit stronger."}]}]}],"thread_ts":"1610944993.007700","parent_user_id":"US64J0NPQ"},{"client_msg_id":"99401484-9f84-4949-a51a-9d9ff2a507ab","type":"message","text":"It would be nice to have more sentence splitters.\nSentence splitting is ridiculously under-researched (at least it was when I was working on it)  particularly given that most word tokenizers expected things to be pre-segemented into sentences.\nThe current one one is based on something <@U677R5Q5A> gave me, which IIRC  was supposed to be a correcting pass for some ML based splitter, but actually was better than that ML part.\n\nIn general applying ML to tokenization is something people didn’t really do til Spacy.\n(and then after that a bunch of research work came out)","user":"U6A936746","ts":"1610987293.008200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YYD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It would be nice to have more sentence splitters.\nSentence splitting is ridiculously under-researched (at least it was when I was working on it)  particularly given that most word tokenizers expected things to be pre-segemented into sentences.\nThe current one one is based on something "},{"type":"user","user_id":"U677R5Q5A"},{"type":"text","text":" gave me, which IIRC  was supposed to be a correcting pass for some ML based splitter, but actually was better than that ML part.\n\nIn general applying ML to tokenization is something people didn’t really do til Spacy.\n(and then after that a bunch of research work came out)"}]}]}],"thread_ts":"1610944993.007700","parent_user_id":"US64J0NPQ"},{"client_msg_id":"7b933f32-b694-4013-8120-9e012c921a20","type":"message","text":"Yea - <@U6A936746> I would be willing to look into this some more to see if improvements could be made for WordTokenizers's sentence tokenization methods. Would that be something of interest to JuliaText?","user":"US64J0NPQ","ts":"1610988651.008400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Cq6n","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yea - "},{"type":"user","user_id":"U6A936746"},{"type":"text","text":" I would be willing to look into this some more to see if improvements could be made for WordTokenizers's sentence tokenization methods. Would that be something of interest to JuliaText?"}]}]}],"thread_ts":"1610944993.007700","parent_user_id":"US64J0NPQ"},{"client_msg_id":"9b8b1bdb-bba6-4be7-9a18-502398f17471","type":"message","text":"I don’t do NLP any more, I have too many other projects.\nBut probably","user":"U6A936746","ts":"1610988684.008600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rpMOg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I don’t do NLP any more, I have too many other projects.\nBut probably"}]}]}],"thread_ts":"1610944993.007700","parent_user_id":"US64J0NPQ"},{"client_msg_id":"1a578c1b-0f09-4889-beb9-80880c8a7af5","type":"message","text":"Ok, great! This is just a hobby interest of mine so it would just be contributions, if any, little by little. Do you know who I should speak with about WordTokenizers.jl <@U6A936746>?","user":"US64J0NPQ","ts":"1610988867.008800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"O3B9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ok, great! This is just a hobby interest of mine so it would just be contributions, if any, little by little. Do you know who I should speak with about WordTokenizers.jl "},{"type":"user","user_id":"U6A936746"},{"type":"text","text":"?"}]}]}],"thread_ts":"1610944993.007700","parent_user_id":"US64J0NPQ"},{"client_msg_id":"6ccb814a-13eb-4bb6-b7f2-84cd6434d5cf","type":"message","text":"To answer your original question, Jacob, what is needed to bolster the ecosystem is more users and more contributors. I can't say 'we lack this feature, this is priority' ... We'll know that only when we have a larger mass of users using this.","user":"U679VPJ8L","ts":"1610988880.009000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Z417","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"To answer your original question, Jacob, what is needed to bolster the ecosystem is more users and more contributors. I can't say 'we lack this feature, this is priority' ... We'll know that only when we have a larger mass of users using this."}]}]}],"thread_ts":"1610944993.007700","parent_user_id":"US64J0NPQ"},{"client_msg_id":"229e406d-7b8c-458f-b19a-7ad8b8f3d541","type":"message","text":"For wordtokenizers, just do a PR if you're interested","user":"U679VPJ8L","ts":"1610988919.009200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8DxI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"For wordtokenizers, just do a PR if you're interested"}]}]}],"thread_ts":"1610944993.007700","parent_user_id":"US64J0NPQ"},{"client_msg_id":"881c864f-3085-485f-8658-944897c2ad65","type":"message","text":"<@UEMBFC48P> has been managing that project for some time now.","user":"U679VPJ8L","ts":"1610988998.009400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DVGaO","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UEMBFC48P"},{"type":"text","text":" has been managing that project for some time now."}]}]}],"thread_ts":"1610944993.007700","parent_user_id":"US64J0NPQ","reactions":[{"name":"point_up","users":["U6A936746"],"count":1}]},{"client_msg_id":"7ce3f716-3b4f-4f17-9a31-99bbb05ef47b","type":"message","text":"Thanks <@U679VPJ8L> - well, I shall try to be a user and contributor then! I agree with your sentiment too <@U6A936746> - I will try investigating new sentence tokenizers to see if I can make something useful. Do you have any suggestions about papers, books, or other resources I could consult to assist me in this? Thanks!","user":"US64J0NPQ","ts":"1610991877.009700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Nwxo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks "},{"type":"user","user_id":"U679VPJ8L"},{"type":"text","text":" - well, I shall try to be a user and contributor then! I agree with your sentiment too "},{"type":"user","user_id":"U6A936746"},{"type":"text","text":" - I will try investigating new sentence tokenizers to see if I can make something useful. Do you have any suggestions about papers, books, or other resources I could consult to assist me in this? Thanks!"}]}]}],"thread_ts":"1610944993.007700","parent_user_id":"US64J0NPQ"}]