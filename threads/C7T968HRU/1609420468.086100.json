[{"client_msg_id":"37b80e27-ba6b-44c2-a253-fda30e6f7e25","type":"message","text":"you can `remake` to have a shorter `tspan`","user":"U69BL50BF","ts":"1609420468.086100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Nb4y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you can "},{"type":"text","text":"remake","style":{"code":true}},{"type":"text","text":" to have a shorter "},{"type":"text","text":"tspan","style":{"code":true}}]}]}],"thread_ts":"1609420468.086100","reply_count":6,"reply_users_count":3,"latest_reply":"1609522339.088700","reply_users":["U01HTAK9DNG","U9MD78Z9N","U69BL50BF"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"But this means that I will also optimise on the shorter time span. Isn't it possible to do something like intelligent search pruning a-la <https://ml.informatik.uni-freiburg.de/papers/15-IJCAI-Extrapolation_of_Learning_Curves.pdf>?","user":"U01HTAK9DNG","ts":"1609441976.086800","thread_ts":"1609420468.086100","root":{"client_msg_id":"37b80e27-ba6b-44c2-a253-fda30e6f7e25","type":"message","text":"you can `remake` to have a shorter `tspan`","user":"U69BL50BF","ts":"1609420468.086100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Nb4y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you can "},{"type":"text","text":"remake","style":{"code":true}},{"type":"text","text":" to have a shorter "},{"type":"text","text":"tspan","style":{"code":true}}]}]}],"thread_ts":"1609420468.086100","reply_count":6,"reply_users_count":3,"latest_reply":"1609522339.088700","reply_users":["U01HTAK9DNG","U9MD78Z9N","U69BL50BF"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"wkk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But this means that I will also optimise on the shorter time span. Isn't it possible to do something like intelligent search pruning a-la "},{"type":"link","url":"https://ml.informatik.uni-freiburg.de/papers/15-IJCAI-Extrapolation_of_Learning_Curves.pdf"},{"type":"text","text":"?"}]}]}],"client_msg_id":"ddcee4ed-2bcf-4e56-b7ec-67e81c45ecf1"},{"client_msg_id":"2e410e32-7956-452c-b2d3-4cabd3ff7bdd","type":"message","text":"Yes it is possible. However from how i am understanding the algorithm proposed in the paper stopping happens inside the \"apply gradient update in back prop\" loop, so you would still run the 300k long simulation multiple times per parameter exploration and just stop early sometimes if the convergence curve looks shitty to an automated algorithm.","user":"U9MD78Z9N","ts":"1609499370.087700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nExvN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes it is possible. However from how i am understanding the algorithm proposed in the paper stopping happens inside the \"apply gradient update in back prop\" loop, so you would still run the 300k long simulation multiple times per parameter exploration and just stop early sometimes if the convergence curve looks shitty to an automated algorithm."}]}]}],"thread_ts":"1609420468.086100","parent_user_id":"U69BL50BF"},{"client_msg_id":"1122ef88-3774-4d2d-b491-41682cf56078","type":"message","text":"This could be implemented as a callback to SciML train but your code would have to remember and the training curves by itself and return true to stop","user":"U9MD78Z9N","ts":"1609499607.087900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KedT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This could be implemented as a callback to SciML train but your code would have to remember and the training curves by itself and return true to stop"}]}]}],"thread_ts":"1609420468.086100","parent_user_id":"U69BL50BF"},{"client_msg_id":"d1dbef72-0a58-4e69-86fe-635b3c5a5cdc","type":"message","text":"I referred to that paper as to the general idea of making shortcuts in the parameter search. My problem doesn't involve any neural networks, I just optimise a few problem domain parameters. So I'm not sure what \"convergence curve\" are you talking about in this case.","user":"U01HTAK9DNG","ts":"1609521522.088100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JXsl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I referred to that paper as to the general idea of making shortcuts in the parameter search. My problem doesn't involve any neural networks, I just optimise a few problem domain parameters. So I'm not sure what \"convergence curve\" are you talking about in this case."}]}]}],"thread_ts":"1609420468.086100","parent_user_id":"U69BL50BF"},{"type":"message","text":"The only shortcuts i can imagine for an ODE problem is:\n - Solve over shorter time frames as Chris proposed\n - Solve a simplified system\n - Solve to lower precision requirements\n - stop early if you think you detect pathologically wrong solutions (callback in the ODE solver)\n - Reduce average solve time but solving with higher throughput by solving an essamble of solutions (on GPU/SIMD)\n - relax the parameter search space by adding more dimensions to make it easier to escpae local minima (f.e. multiple shooting)","user":"U9MD78Z9N","ts":"1609522232.088500","team":"T68168MUP","thread_ts":"1609420468.086100","parent_user_id":"U69BL50BF"},{"client_msg_id":"adadf5f4-9710-4b95-9fd4-a873d4850ce4","type":"message","text":"yeah that's a good list","user":"U69BL50BF","ts":"1609522339.088700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ElluN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yeah that's a good list"}]}]}],"thread_ts":"1609420468.086100","parent_user_id":"U69BL50BF"}]