[{"client_msg_id":"37b80e27-ba6b-44c2-a253-fda30e6f7e25","type":"message","text":"you can `remake` to have a shorter `tspan`","user":"U69BL50BF","ts":"1609420468.086100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Nb4y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you can "},{"type":"text","text":"remake","style":{"code":true}},{"type":"text","text":" to have a shorter "},{"type":"text","text":"tspan","style":{"code":true}}]}]}],"thread_ts":"1609420468.086100","reply_count":3,"reply_users_count":2,"latest_reply":"1609499607.087900","reply_users":["U01HTAK9DNG","U9MD78Z9N"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"But this means that I will also optimise on the shorter time span. Isn't it possible to do something like intelligent search pruning a-la <https://ml.informatik.uni-freiburg.de/papers/15-IJCAI-Extrapolation_of_Learning_Curves.pdf>?","user":"U01HTAK9DNG","ts":"1609441976.086800","thread_ts":"1609420468.086100","root":{"client_msg_id":"37b80e27-ba6b-44c2-a253-fda30e6f7e25","type":"message","text":"you can `remake` to have a shorter `tspan`","user":"U69BL50BF","ts":"1609420468.086100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Nb4y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you can "},{"type":"text","text":"remake","style":{"code":true}},{"type":"text","text":" to have a shorter "},{"type":"text","text":"tspan","style":{"code":true}}]}]}],"thread_ts":"1609420468.086100","reply_count":3,"reply_users_count":2,"latest_reply":"1609499607.087900","reply_users":["U01HTAK9DNG","U9MD78Z9N"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"wkk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But this means that I will also optimise on the shorter time span. Isn't it possible to do something like intelligent search pruning a-la "},{"type":"link","url":"https://ml.informatik.uni-freiburg.de/papers/15-IJCAI-Extrapolation_of_Learning_Curves.pdf"},{"type":"text","text":"?"}]}]}],"client_msg_id":"ddcee4ed-2bcf-4e56-b7ec-67e81c45ecf1"},{"client_msg_id":"2e410e32-7956-452c-b2d3-4cabd3ff7bdd","type":"message","text":"Yes it is possible. However from how i am understanding the algorithm proposed in the paper stopping happens inside the \"apply gradient update in back prop\" loop, so you would still run the 300k long simulation multiple times per parameter exploration and just stop early sometimes if the convergence curve looks shitty to an automated algorithm.","user":"U9MD78Z9N","ts":"1609499370.087700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nExvN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes it is possible. However from how i am understanding the algorithm proposed in the paper stopping happens inside the \"apply gradient update in back prop\" loop, so you would still run the 300k long simulation multiple times per parameter exploration and just stop early sometimes if the convergence curve looks shitty to an automated algorithm."}]}]}],"thread_ts":"1609420468.086100","parent_user_id":"U69BL50BF"},{"client_msg_id":"1122ef88-3774-4d2d-b491-41682cf56078","type":"message","text":"This could be implemented as a callback to SciML train but your code would have to remember and the training curves by itself and return true to stop","user":"U9MD78Z9N","ts":"1609499607.087900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KedT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This could be implemented as a callback to SciML train but your code would have to remember and the training curves by itself and return true to stop"}]}]}],"thread_ts":"1609420468.086100","parent_user_id":"U69BL50BF"}]