[{"client_msg_id":"84de9945-58a8-4479-a59e-35a8337b1bf9","type":"message","text":"I've been trying to get one-GPU-per-thread parallelization working (mainly to avoid paying the N-process Julia startup cost of the standard one-GPU-per-process way which is otherwise very robust). Between unified memory, <https://github.com/JuliaGPU/CUDA.jl/issues/731|#731> getting fixed, and recent stuff on master, quite a lot is working well! I'm stuck on the following thing though, wondering if I might get some hints of what might be going wrong that could help get a MWE. The general outline of what I'm running is\n```x = move_to_unified_memory(CuArray(...))\n\nThreads.@sync begin\n    Threads.@spawn begin\n        device!(0)\n        some_gpu_code(x)\n    end\n    Threads.@spawn begin\n        device!(1)\n        some_gpu_code(x)\n    end\nend```\nFor `some_gpu_code` being simple FFTs or array broadcasts, it works. For my more complex actual calculations (which in theory should just be a whole bunch of these chained together in some complex way), I get the segfault I attach in the thread. Any ideas? This is CUDA#master.","user":"UUMJUCYRK","ts":"1614162324.041500","team":"T68168MUP","edited":{"user":"UUMJUCYRK","ts":"1614162388.000000"},"blocks":[{"type":"rich_text","block_id":"/hNZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I've been trying to get one-GPU-per-thread parallelization working (mainly to avoid paying the N-process Julia startup cost of the standard one-GPU-per-process way which is otherwise very robust). Between unified memory, "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/issues/731","text":"#731"},{"type":"text","text":" getting fixed, and recent stuff on master, quite a lot is working well! I'm stuck on the following thing though, wondering if I might get some hints of what might be going wrong that could help get a MWE. The general outline of what I'm running is\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"x = move_to_unified_memory(CuArray(...))\n\nThreads.@sync begin\n    Threads.@spawn begin\n        device!(0)\n        some_gpu_code(x)\n    end\n    Threads.@spawn begin\n        device!(1)\n        some_gpu_code(x)\n    end\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"For "},{"type":"text","text":"some_gpu_code","style":{"code":true}},{"type":"text","text":" being simple FFTs or array broadcasts, it works. For my more complex actual calculations (which in theory should just be a whole bunch of these chained together in some complex way), I get the segfault I attach in the thread. Any ideas? This is CUDA#master."}]}]}],"thread_ts":"1614162324.041500","reply_count":19,"reply_users_count":2,"latest_reply":"1614165497.046700","reply_users":["UUMJUCYRK","U68A3ASP9"],"subscribed":false},{"client_msg_id":"01cbcc97-0b5e-4fad-8faf-fee7a3b2116c","type":"message","text":"```signal (11): Segmentation fault\nin expression starting at In[112]:1\nraise at /lib64/libpthread.so.0 (unknown line)\n_IO_funlockfile at /lib64/libpthread.so.0 (unknown line)\nmaybe_collect at /buildworker/worker/package_linux64/build/src/julia_threads.h:313 [inlined]\njl_gc_pool_alloc at /buildworker/worker/package_linux64/build/src/gc.c:1176\nBlock#266 at /global/u1/m/marius/work/clem/dev/CUDA/src/pool.jl:101 [inlined]\nType##kw at /global/u1/m/marius/work/clem/dev/CUDA/src/pool.jl:101 [inlined]\nactual_alloc at /global/u1/m/marius/work/clem/dev/CUDA/src/pool.jl:219\nactual_alloc at /global/u1/m/marius/work/clem/dev/CUDA/src/pool.jl:193 [inlined]\nmacro expansion at /global/u1/m/marius/work/clem/dev/CUDA/src/pool/binned.jl:143 [inlined]\nmacro expansion at /global/homes/m/marius/.julia/packages/TimerOutputs/ZmKD7/src/TimerOutput.jl:206 [inlined]\npool_alloc at /global/u1/m/marius/work/clem/dev/CUDA/src/pool/binned.jl:142\nmacro expansion at /global/homes/m/marius/.julia/packages/TimerOutputs/ZmKD7/src/TimerOutput.jl:206 [inlined]\nmacro expansion at /global/u1/m/marius/work/clem/dev/CUDA/src/pool.jl:318 [inlined]\nmacro expansion at ./timing.jl:279 [inlined]\nalloc at /global/u1/m/marius/work/clem/dev/CUDA/src/pool.jl:317 [inlined]\nCuArray at /global/u1/m/marius/work/clem/dev/CUDA/src/array.jl:20\nCuArray at /global/u1/m/marius/work/clem/dev/CUDA/src/array.jl:78 [inlined]\nsimilar at ./abstractarray.jl:779 [inlined]\nsimilar at ./abstractarray.jl:778 [inlined]\nsimilar at /global/u1/m/marius/work/clem/dev/CUDA/src/broadcast.jl:11 [inlined]\ncopy at ./broadcast.jl:908 [inlined]\nmaterialize at ./broadcast.jl:883 [inlined]```","user":"UUMJUCYRK","ts":"1614162360.041600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lVGm+","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"signal (11): Segmentation fault\nin expression starting at In[112]:1\nraise at /lib64/libpthread.so.0 (unknown line)\n_IO_funlockfile at /lib64/libpthread.so.0 (unknown line)\nmaybe_collect at /buildworker/worker/package_linux64/build/src/julia_threads.h:313 [inlined]\njl_gc_pool_alloc at /buildworker/worker/package_linux64/build/src/gc.c:1176\nBlock#266 at /global/u1/m/marius/work/clem/dev/CUDA/src/pool.jl:101 [inlined]\nType##kw at /global/u1/m/marius/work/clem/dev/CUDA/src/pool.jl:101 [inlined]\nactual_alloc at /global/u1/m/marius/work/clem/dev/CUDA/src/pool.jl:219\nactual_alloc at /global/u1/m/marius/work/clem/dev/CUDA/src/pool.jl:193 [inlined]\nmacro expansion at /global/u1/m/marius/work/clem/dev/CUDA/src/pool/binned.jl:143 [inlined]\nmacro expansion at /global/homes/m/marius/.julia/packages/TimerOutputs/ZmKD7/src/TimerOutput.jl:206 [inlined]\npool_alloc at /global/u1/m/marius/work/clem/dev/CUDA/src/pool/binned.jl:142\nmacro expansion at /global/homes/m/marius/.julia/packages/TimerOutputs/ZmKD7/src/TimerOutput.jl:206 [inlined]\nmacro expansion at /global/u1/m/marius/work/clem/dev/CUDA/src/pool.jl:318 [inlined]\nmacro expansion at ./timing.jl:279 [inlined]\nalloc at /global/u1/m/marius/work/clem/dev/CUDA/src/pool.jl:317 [inlined]\nCuArray at /global/u1/m/marius/work/clem/dev/CUDA/src/array.jl:20\nCuArray at /global/u1/m/marius/work/clem/dev/CUDA/src/array.jl:78 [inlined]\nsimilar at ./abstractarray.jl:779 [inlined]\nsimilar at ./abstractarray.jl:778 [inlined]\nsimilar at /global/u1/m/marius/work/clem/dev/CUDA/src/broadcast.jl:11 [inlined]\ncopy at ./broadcast.jl:908 [inlined]\nmaterialize at ./broadcast.jl:883 [inlined]"}]}]}],"thread_ts":"1614162324.041500","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"13873be2-1709-4b41-bc5f-4e59d1c4ae98","type":"message","text":"I've encountered spurious segfaults in the GC too, but haven't been able to pinpoint them yet","user":"U68A3ASP9","ts":"1614163245.041900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7ewY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I've encountered spurious segfaults in the GC too, but haven't been able to pinpoint them yet"}]}]}],"thread_ts":"1614162324.041500","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"027fa9d8-7b8a-4456-9d53-a480fab5b411","type":"message","text":"curious how it's in `raise` here, maybe try a Julia build with assertions enabled?","user":"U68A3ASP9","ts":"1614163257.042100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"u3U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"curious how it's in "},{"type":"text","text":"raise","style":{"code":true}},{"type":"text","text":" here, maybe try a Julia build with assertions enabled?"}]}]}],"thread_ts":"1614162324.041500","parent_user_id":"UUMJUCYRK","reactions":[{"name":"+1","users":["UUMJUCYRK"],"count":1}]},{"client_msg_id":"a38d2916-7409-4376-af6b-d6ce6bf0d55e","type":"message","text":"also, which version of Julia is this?","user":"U68A3ASP9","ts":"1614163264.042300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uCy1=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"also, which version of Julia is this?"}]}]}],"thread_ts":"1614162324.041500","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"ef4cf809-382d-41c0-984e-cda9b3c385fe","type":"message","text":"1.6.0-beta1","user":"UUMJUCYRK","ts":"1614163302.042600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5UaX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"1.6.0-beta1"}]}]}],"thread_ts":"1614162324.041500","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"e9450cd0-61ab-436d-baff-d61b9ba8f416","type":"message","text":"you basically need the backports-for-rc2 branch, because in older versions there's a bug with `@cfunction`","user":"U68A3ASP9","ts":"1614163325.042800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TUgN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you basically need the backports-for-rc2 branch, because in older versions there's a bug with "},{"type":"text","text":"@cfunction","style":{"code":true}}]}]}],"thread_ts":"1614162324.041500","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"74ec61f7-44fe-487b-9f69-9ff0e4091220","type":"message","text":"also, <https://github.com/JuliaGPU/CUDA.jl/pull/734> has some additional fixes I discovered yesterday :-)","user":"U68A3ASP9","ts":"1614163345.043000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wn0L","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"also, "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/pull/734"},{"type":"text","text":" has some additional fixes I discovered yesterday :-)"}]}]}],"thread_ts":"1614162324.041500","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"f6dea7cf-3580-48fb-8f28-62643e3f09f9","type":"message","text":"the likely source for the segfault here is that we're touching some global array from different threads. those array intrinsics are not thread safe, and can lead to these segfaults","user":"U68A3ASP9","ts":"1614163428.043200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jkA2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"the likely source for the segfault here is that we're touching some global array from different threads. those array intrinsics are not thread safe, and can lead to these segfaults"}]}]}],"thread_ts":"1614162324.041500","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"1706a560-1eeb-46c7-9a38-5e96b36cad8c","type":"message","text":"and on another note, you may not need threads; task-based concurrency is now possible too. there might be some cost due to having to switch CUDA contexts whenever a Julia task switch occurs, but the multithreading code paths likely have some inefficiencies too (basically all this stuff is fairly new)","user":"U68A3ASP9","ts":"1614163493.043400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bUb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"and on another note, you may not need threads; task-based concurrency is now possible too. there might be some cost due to having to switch CUDA contexts whenever a Julia task switch occurs, but the multithreading code paths likely have some inefficiencies too (basically all this stuff is fairly new)"}]}]}],"thread_ts":"1614162324.041500","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"88df6f74-18f0-448b-b7ed-a074c6b04eac","type":"message","text":"Thanks, will give that PR/branch/assertions a go.","user":"UUMJUCYRK","ts":"1614163541.043600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jLsr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks, will give that PR/branch/assertions a go."}]}]}],"thread_ts":"1614162324.041500","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"9f88fe2c-82cd-4c19-9a20-dc5049263ca7","type":"message","text":"I did try task-based first, but I think I did see alot of overhead, or even weirder, sequential stuff like where one GPU would clearly do the whole calculation and only then would the other GPU start. Tbh I didn't really understand what was going on so maybe worth another try. Just to check, in theory while one task is waiting on a synchronize or something, it'll switch to the other task which can be launching kernels on another (or even same) GPU, right?","user":"UUMJUCYRK","ts":"1614163797.043800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eRgw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I did try task-based first, but I think I did see alot of overhead, or even weirder, sequential stuff like where one GPU would clearly do the whole calculation and only then would the other GPU start. Tbh I didn't really understand what was going on so maybe worth another try. Just to check, in theory while one task is waiting on a synchronize or something, it'll switch to the other task which can be launching kernels on another (or even same) GPU, right?"}]}]}],"thread_ts":"1614162324.041500","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"86c28289-4f2e-495f-b937-870220412180","type":"message","text":"correct","user":"U68A3ASP9","ts":"1614163924.044000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oCdi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"correct"}]}]}],"thread_ts":"1614162324.041500","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"d5fc602d-5288-4ec5-8569-119f64334330","type":"message","text":"not all operations have been made fully asynchronous though, that's a WIP","user":"U68A3ASP9","ts":"1614163935.044200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+zlU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"not all operations have been made fully asynchronous though, that's a WIP"}]}]}],"thread_ts":"1614162324.041500","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"9250d12a-efc1-446d-9d90-74521b4d8af1","type":"message","text":"crucially, without <https://github.com/JuliaGPU/CUDA.jl/issues/735> memory copies are still synchronous","user":"U68A3ASP9","ts":"1614163958.044400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ZxR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"crucially, without "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/issues/735"},{"type":"text","text":" memory copies are still synchronous"}]}]}],"thread_ts":"1614162324.041500","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"6e6ccc0c-5d62-4997-aabd-9e7de8942eac","type":"message","text":"so that might have been what prevented switching to another task in your case","user":"U68A3ASP9","ts":"1614163980.044600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"XMP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"so that might have been what prevented switching to another task in your case"}]}]}],"thread_ts":"1614162324.041500","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"e3d4e5b1-8897-448e-830c-a26f276db952","type":"message","text":"I see, yea that sounds plausible. Seems like I should just wait a few weeks since so much of this looks to be really bleeding edge but man I just cant take the Distributed startup costs anymore ha. Anyway, all this is super helpful.","user":"UUMJUCYRK","ts":"1614164149.045900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vSR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I see, yea that sounds plausible. Seems like I should just wait a few weeks since so much of this looks to be really bleeding edge but man I just cant take the Distributed startup costs anymore ha. Anyway, all this is super helpful."}]}]}],"thread_ts":"1614162324.041500","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"5821ce91-65f8-48cc-84fc-e84d28ce0b13","type":"message","text":"I'll probably release CUDA.jl 3.0 without the required memory pinning feature though, as the release might be slightly breaking enough already","user":"U68A3ASP9","ts":"1614164725.046300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iOm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'll probably release CUDA.jl 3.0 without the required memory pinning feature though, as the release might be slightly breaking enough already"}]}]}],"thread_ts":"1614162324.041500","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"4a6209a5-085c-4f3d-99e4-ad51f19d4d4f","type":"message","text":"for some more details, here's the initial version of a release blog post: <https://juliagpu.org/2021-03-01-cuda_3.0/>","user":"U68A3ASP9","ts":"1614164847.046500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8Ph","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"for some more details, here's the initial version of a release blog post: "},{"type":"link","url":"https://juliagpu.org/2021-03-01-cuda_3.0/"}]}]}],"thread_ts":"1614162324.041500","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"16ee35ad-035e-4745-b7fd-f6ec09f85f78","type":"message","text":"Ahh I think that segfault is related to something else weird on this system where I think some different versions of libraries get loaded (maybe pthread?) depending on if I run in REPL or Jupyterlab. Eg the following is a 100% reproducer of the segfault on Jupyterlab, but is fine on the REPL:\n```Threads.@sync begin\n    Threads.@spawn begin\n        device!(0)\n        CUDA.rand(10,10)\n        GC.gc(true)\n    end\n    Threads.@spawn begin\n        device!(1)\n        CUDA.rand(10,10)\n        GC.gc(true)\n    end\nend```\nAnyway, almost definitely not a CUDA.jl problem.","user":"UUMJUCYRK","ts":"1614165497.046700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eabw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ahh I think that segfault is related to something else weird on this system where I think some different versions of libraries get loaded (maybe pthread?) depending on if I run in REPL or Jupyterlab. Eg the following is a 100% reproducer of the segfault on Jupyterlab, but is fine on the REPL:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Threads.@sync begin\n    Threads.@spawn begin\n        device!(0)\n        CUDA.rand(10,10)\n        GC.gc(true)\n    end\n    Threads.@spawn begin\n        device!(1)\n        CUDA.rand(10,10)\n        GC.gc(true)\n    end\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Anyway, almost definitely not a CUDA.jl problem."}]}]}],"thread_ts":"1614162324.041500","parent_user_id":"UUMJUCYRK"}]