[{"client_msg_id":"ad51cebb-c618-43e4-99d1-646708fd719a","type":"message","text":"ah, I see, so this (<https://juliagpu.github.io/CUDA.jl/dev/api/kernel/#Dynamic-parallelism>) is the low-level building block one would use to achieve that? I think it'd be useful to have a more high-level DAG-based formalism implemented on top of it, but I have no idea how easy / hard that is","user":"U6BJ9E351","ts":"1612128322.100200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rMI8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ah, I see, so this ("},{"type":"link","url":"https://juliagpu.github.io/CUDA.jl/dev/api/kernel/#Dynamic-parallelism"},{"type":"text","text":") is the low-level building block one would use to achieve that? I think it'd be useful to have a more high-level DAG-based formalism implemented on top of it, but I have no idea how easy / hard that is"}]}]}],"thread_ts":"1612128322.100200","reply_count":19,"reply_users_count":3,"latest_reply":"1612187366.105900","reply_users":["U6A0PD8CR","UC7AF7NSU","U6BJ9E351"],"subscribed":false},{"client_msg_id":"de206a46-e56f-467f-9e4a-c7099c30f2df","type":"message","text":"My hope is that <@UC7AF7NSU>’s work on Tapir will eventually let us do something fancy like execute Dagger graphs on the GPU via dynamic execution","user":"U6A0PD8CR","ts":"1612142589.101000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"CJ8HV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"My hope is that "},{"type":"user","user_id":"UC7AF7NSU"},{"type":"text","text":"’s work on Tapir will eventually let us do something fancy like execute Dagger graphs on the GPU via dynamic execution"}]}]}],"thread_ts":"1612128322.100200","parent_user_id":"U6BJ9E351"},{"client_msg_id":"5393d342-2547-48fb-b6cc-b6c2e4225d08","type":"message","text":"Until then, you could probably implement a basic on-device task scheduler with CUDA.jl","user":"U6A0PD8CR","ts":"1612142647.101200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Pbyb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Until then, you could probably implement a basic on-device task scheduler with CUDA.jl"}]}]}],"thread_ts":"1612128322.100200","parent_user_id":"U6BJ9E351"},{"client_msg_id":"e2488c93-fdb2-4944-ad02-8220ca1056f2","type":"message","text":"Yeah, I imagine that we can lower `@sync`/`@spawn` syntax to outline the functions and then compile it via `dynamic_cufunction`. Tapir would be handy for that. Though Tapir is probably not strictly required and maybe CUDA.jl can already implement `@spawn` on its own (just guessing).","user":"UC7AF7NSU","ts":"1612144958.101400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mqZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah, I imagine that we can lower "},{"type":"text","text":"@sync","style":{"code":true}},{"type":"text","text":"/"},{"type":"text","text":"@spawn","style":{"code":true}},{"type":"text","text":" syntax to outline the functions and then compile it via "},{"type":"text","text":"dynamic_cufunction","style":{"code":true}},{"type":"text","text":". Tapir would be handy for that. Though Tapir is probably not strictly required and maybe CUDA.jl can already implement "},{"type":"text","text":"@spawn","style":{"code":true}},{"type":"text","text":" on its own (just guessing)."}]}]}],"thread_ts":"1612128322.100200","parent_user_id":"U6BJ9E351"},{"client_msg_id":"f226dc63-f4b4-4627-96f8-0995216e4c93","type":"message","text":"That was another thing I was curious about, whether the multithreading approach with `@spawn` can be used on the GPU to do some work stealing. Do you know if there are any examples / PR I could look at, or is it all experimental at the moment?","user":"U6BJ9E351","ts":"1612165656.101900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"a5ql0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That was another thing I was curious about, whether the multithreading approach with "},{"type":"text","text":"@spawn","style":{"code":true}},{"type":"text","text":" can be used on the GPU to do some work stealing. Do you know if there are any examples / PR I could look at, or is it all experimental at the moment?"}]}]}],"thread_ts":"1612128322.100200","parent_user_id":"U6BJ9E351"},{"client_msg_id":"9b175f80-970c-4964-9ab7-b54b13203bda","type":"message","text":"(this looks interesting <https://github.com/JuliaGPU/CUDA.jl/pull/662|https://github.com/JuliaGPU/CUDA.jl/pull/662>)","user":"U6BJ9E351","ts":"1612167401.102300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SeOkV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(this looks interesting "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/pull/662","text":"https://github.com/JuliaGPU/CUDA.jl/pull/662"},{"type":"text","text":")"}]}]}],"thread_ts":"1612128322.100200","parent_user_id":"U6BJ9E351"},{"client_msg_id":"4da7e321-e158-4130-9973-d602caef0359","type":"message","text":"isn't the PR about the host API?","user":"UC7AF7NSU","ts":"1612167683.102700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"42e","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"isn't the PR about the host API?"}]}]}],"thread_ts":"1612128322.100200","parent_user_id":"U6BJ9E351"},{"client_msg_id":"819f4efe-d36b-4a36-b52d-743d40513443","type":"message","text":"I think it's possible (\"theoretically straightforward\") to create a divide-and-conquer algorithmic skeleton on GPU using work stealing. Work stealing for generic arbitrary and heterogeneous tasks sound hard as it looks like you'd need to specify `threads`/`blocks`/`shmem` for launching a device kernel. But I'm no GPU expert.","user":"UC7AF7NSU","ts":"1612169519.102900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WVs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think it's possible (\"theoretically straightforward\") to create a divide-and-conquer algorithmic skeleton on GPU using work stealing. Work stealing for generic arbitrary and heterogeneous tasks sound hard as it looks like you'd need to specify "},{"type":"text","text":"threads","style":{"code":true}},{"type":"text","text":"/"},{"type":"text","text":"blocks","style":{"code":true}},{"type":"text","text":"/"},{"type":"text","text":"shmem","style":{"code":true}},{"type":"text","text":" for launching a device kernel. But I'm no GPU expert."}]}]}],"thread_ts":"1612128322.100200","parent_user_id":"U6BJ9E351"},{"client_msg_id":"021d3924-b74f-4de0-bb87-558ff43ab09a","type":"message","text":"&gt; isn't the PR about the host API?\nIdeally I'd everything on the GPU in one go, but doing the scheduling on the host is probably a decent compromise (but I also need to learn more about this, will play a bit with multithreading plus gpu and see how it goes)","user":"U6BJ9E351","ts":"1612171572.103300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"F48ER","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"isn't the PR about the host API?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Ideally I'd everything on the GPU in one go, but doing the scheduling on the host is probably a decent compromise (but I also need to learn more about this, will play a bit with multithreading plus gpu and see how it goes)"}]}]}],"thread_ts":"1612128322.100200","parent_user_id":"U6BJ9E351"},{"client_msg_id":"8dad26c8-6174-486a-b1ef-73b1610b9aa3","type":"message","text":"ah, gotcha. yeah, controlling the DAG on host using Julia tasks sound like a nice approach","user":"UC7AF7NSU","ts":"1612172987.103500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sjdmx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ah, gotcha. yeah, controlling the DAG on host using Julia tasks sound like a nice approach"}]}]}],"thread_ts":"1612128322.100200","parent_user_id":"U6BJ9E351"},{"client_msg_id":"14b7332c-71c1-46cb-9894-22156e6ecfc3","type":"message","text":"If you're not trying to reinvent the wheel <@U6BJ9E351>, definitely give Dagger (and DaggerGPU) a try","user":"U6A0PD8CR","ts":"1612186419.103900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tV/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If you're not trying to reinvent the wheel "},{"type":"user","user_id":"U6BJ9E351"},{"type":"text","text":", definitely give Dagger (and DaggerGPU) a try"}]}]}],"thread_ts":"1612128322.100200","parent_user_id":"U6BJ9E351"},{"client_msg_id":"2547079a-fa39-4250-8942-31cc8e6cc19c","type":"message","text":"I'm rebasing some work right now which will make Dagger much better at allocating GPU resources fairly and effectively","user":"U6A0PD8CR","ts":"1612186455.104100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hbv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm rebasing some work right now which will make Dagger much better at allocating GPU resources fairly and effectively"}]}]}],"thread_ts":"1612128322.100200","parent_user_id":"U6BJ9E351"},{"client_msg_id":"25776ca4-9c28-492c-9063-06c81c3460a1","type":"message","text":"lol, no, I'm definitely very eager to not reinvent the wheel if the machinery exists already. I haven't used Dagger in a while (at the time, lack of docs was a big issue, it looks much more mature now), didn't know it supported GPU.","user":"U6BJ9E351","ts":"1612186934.104300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9HM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"lol, no, I'm definitely very eager to not reinvent the wheel if the machinery exists already. I haven't used Dagger in a while (at the time, lack of docs was a big issue, it looks much more mature now), didn't know it supported GPU."}]}]}],"thread_ts":"1612128322.100200","parent_user_id":"U6BJ9E351"},{"client_msg_id":"4351f0cb-7d99-407f-a803-bf457e47376c","type":"message","text":"so it is no longer just about many processes and distributed computing, but can also support multithreading and GPU?","user":"U6BJ9E351","ts":"1612187105.104500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TAm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"so it is no longer just about many processes and distributed computing, but can also support multithreading and GPU?"}]}]}],"thread_ts":"1612128322.100200","parent_user_id":"U6BJ9E351"},{"client_msg_id":"14b2d762-c1f4-4900-ac20-c88e0a7ae475","type":"message","text":"Yeah, I've been working on making Dagger the best possible API for heterogeneous computing (backed by JuliaLab). We've improved scheduling efficiency and overhead significantly so far, and I've got a long list of changes on my todo list still.","user":"U6A0PD8CR","ts":"1612187123.104700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lTFn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah, I've been working on making Dagger the best possible API for heterogeneous computing (backed by JuliaLab). We've improved scheduling efficiency and overhead significantly so far, and I've got a long list of changes on my todo list still."}]}]}],"thread_ts":"1612128322.100200","parent_user_id":"U6BJ9E351"},{"client_msg_id":"e5f2465e-43ea-4787-8a8d-b1323bcd4d76","type":"message","text":"Yep, multithreading is automatic (just set `JULIA_NUM_THREADS` or `-t` as usual), and GPU support comes via DaggerGPU","user":"U6A0PD8CR","ts":"1612187160.104900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mgkt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yep, multithreading is automatic (just set "},{"type":"text","text":"JULIA_NUM_THREADS","style":{"code":true}},{"type":"text","text":" or "},{"type":"text","text":"-t","style":{"code":true}},{"type":"text","text":" as usual), and GPU support comes via DaggerGPU"}]}]}],"thread_ts":"1612128322.100200","parent_user_id":"U6BJ9E351"},{"client_msg_id":"cc3f4609-c42d-4e96-b569-70da87c2302d","type":"message","text":"The GPU support is still experimental and Dagger/DaggerGPU doesn't actually do much for you. The GPU integration is mostly about tracking GPU utilization right now.","user":"U6A0PD8CR","ts":"1612187232.105100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4gj9i","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The GPU support is still experimental and Dagger/DaggerGPU doesn't actually do much for you. The GPU integration is mostly about tracking GPU utilization right now."}]}]}],"thread_ts":"1612128322.100200","parent_user_id":"U6BJ9E351"},{"client_msg_id":"2fb09b70-62cf-4ba9-9cb0-9c8c23c03816","type":"message","text":"But in the future I'll be integrating better with KernelAbstractions so its event mechanism works seamlessly across a cluster","user":"U6A0PD8CR","ts":"1612187268.105300","team":"T68168MUP","edited":{"user":"U6A0PD8CR","ts":"1612187307.000000"},"blocks":[{"type":"rich_text","block_id":"GPy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But in the future I'll be integrating better with KernelAbstractions so its event mechanism works seamlessly across a cluster"}]}]}],"thread_ts":"1612128322.100200","parent_user_id":"U6BJ9E351"},{"client_msg_id":"9625ed69-b1e5-4122-aa42-1d911d5b4a33","type":"message","text":"glad to hear that, it's really cool progress! I'll probably just use Dagger for the scheduling part (esp. since I'd like to have multithreading on CPU or GPU with a unique implementation)","user":"U6BJ9E351","ts":"1612187326.105600","team":"T68168MUP","edited":{"user":"U6BJ9E351","ts":"1612187338.000000"},"blocks":[{"type":"rich_text","block_id":"LYxbF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"glad to hear that, it's really cool progress! I'll probably just use Dagger for the scheduling part (esp. since I'd like to have multithreading on CPU or GPU with a unique implementation)"}]}]}],"thread_ts":"1612128322.100200","parent_user_id":"U6BJ9E351"},{"client_msg_id":"e0f7acb6-1061-4fbe-8d99-5805d0ab33c9","type":"message","text":"Sounds good! Let me know if you need any help with Dagger","user":"U6A0PD8CR","ts":"1612187366.105900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Hwass","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Sounds good! Let me know if you need any help with Dagger"}]}]}],"thread_ts":"1612128322.100200","parent_user_id":"U6BJ9E351","reactions":[{"name":"thankyou","users":["U6BJ9E351"],"count":1}]}]