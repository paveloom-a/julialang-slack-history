[{"client_msg_id":"a7e1e627-b461-41a0-af1e-9dadfec34f3a","type":"message","text":"Currently the only really robust way to do multi-GPU is with separate processes controlling each GPU. One downside is that transferring memory between GPUs has to go through the CPU as its serialized/deserialized by Julia (unless I'm missing something). Is there any other way to do transfers between GPUs? Ideally what I'm looking for is something like the pseudo-code:\n```device!(0)\nx = unified_memory(CuArray(...))\nx_handle = some_sort_of_handle(x)\n\npmap(workers()) do i\n    device!(i)\n    local_x = CuArray(...)\n    copy_to!(local_x, x_handle)\n    ...\nend```\nI started reading about CUDA MPS but I don't know enough if its the right thing to devote more time to understanding, or if it can be used from CUDA.jl at all. I'm also aware of CUDA-aware MPI, but since in my case most of my work is from Jupyter notebooks, I don't think it can really work. I've also been playing alot lately with threads instead of processes, but that doesn't seem stable enough to rely on quite yet. Curious if there's some other (easy?) option with processes I'm missing?","user":"UUMJUCYRK","ts":"1614672449.074900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gLvc3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Currently the only really robust way to do multi-GPU is with separate processes controlling each GPU. One downside is that transferring memory between GPUs has to go through the CPU as its serialized/deserialized by Julia (unless I'm missing something). Is there any other way to do transfers between GPUs? Ideally what I'm looking for is something like the pseudo-code:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"device!(0)\nx = unified_memory(CuArray(...))\nx_handle = some_sort_of_handle(x)\n\npmap(workers()) do i\n    device!(i)\n    local_x = CuArray(...)\n    copy_to!(local_x, x_handle)\n    ...\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I started reading about CUDA MPS but I don't know enough if its the right thing to devote more time to understanding, or if it can be used from CUDA.jl at all. I'm also aware of CUDA-aware MPI, but since in my case most of my work is from Jupyter notebooks, I don't think it can really work. I've also been playing alot lately with threads instead of processes, but that doesn't seem stable enough to rely on quite yet. Curious if there's some other (easy?) option with processes I'm missing?"}]}]}],"thread_ts":"1614672449.074900","reply_count":3,"reply_users_count":3,"latest_reply":"1614676441.075600","reply_users":["UMY1LV01G","U68A3ASP9","UUMJUCYRK"],"subscribed":false},{"client_msg_id":"39e11b8e-1ba8-4651-b8b8-0af2918bf62d","type":"message","text":"I'm excited for <https://github.com/JuliaParallel/UCX.jl/pull/35> to land so we can do device-device transfers for ML training","user":"UMY1LV01G","ts":"1614672901.075000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rmnnA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm excited for "},{"type":"link","url":"https://github.com/JuliaParallel/UCX.jl/pull/35"},{"type":"text","text":" to land so we can do device-device transfers for ML training"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK","reactions":[{"name":"+1","users":["UUMJUCYRK"],"count":1}]},{"client_msg_id":"a4f44240-f750-4f38-b068-8e20812b8919","type":"message","text":"there's an IPC API, e.g. <https://github.com/ndd314/cuda_examples/blob/master/0_Simple/simpleIPC/simpleIPC.cu>, but I haven't tried it","user":"U68A3ASP9","ts":"1614673800.075200","team":"T68168MUP","edited":{"user":"U68A3ASP9","ts":"1614673809.000000"},"blocks":[{"type":"rich_text","block_id":"GdzPF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"there's an IPC API, e.g. "},{"type":"link","url":"https://github.com/ndd314/cuda_examples/blob/master/0_Simple/simpleIPC/simpleIPC.cu"},{"type":"text","text":", but I haven't tried it"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"ebae5633-e856-4342-82b4-b2096f8d4745","type":"message","text":"Thanks, will check that out.","user":"UUMJUCYRK","ts":"1614676441.075600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"V8ZPV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks, will check that out."}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK"}]