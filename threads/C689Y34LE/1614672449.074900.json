[{"client_msg_id":"a7e1e627-b461-41a0-af1e-9dadfec34f3a","type":"message","text":"Currently the only really robust way to do multi-GPU is with separate processes controlling each GPU. One downside is that transferring memory between GPUs has to go through the CPU as its serialized/deserialized by Julia (unless I'm missing something). Is there any other way to do transfers between GPUs? Ideally what I'm looking for is something like the pseudo-code:\n```device!(0)\nx = unified_memory(CuArray(...))\nx_handle = some_sort_of_handle(x)\n\npmap(workers()) do i\n    device!(i)\n    local_x = CuArray(...)\n    copy_to!(local_x, x_handle)\n    ...\nend```\nI started reading about CUDA MPS but I don't know enough if its the right thing to devote more time to understanding, or if it can be used from CUDA.jl at all. I'm also aware of CUDA-aware MPI, but since in my case most of my work is from Jupyter notebooks, I don't think it can really work. I've also been playing alot lately with threads instead of processes, but that doesn't seem stable enough to rely on quite yet. Curious if there's some other (easy?) option with processes I'm missing?","user":"UUMJUCYRK","ts":"1614672449.074900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gLvc3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Currently the only really robust way to do multi-GPU is with separate processes controlling each GPU. One downside is that transferring memory between GPUs has to go through the CPU as its serialized/deserialized by Julia (unless I'm missing something). Is there any other way to do transfers between GPUs? Ideally what I'm looking for is something like the pseudo-code:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"device!(0)\nx = unified_memory(CuArray(...))\nx_handle = some_sort_of_handle(x)\n\npmap(workers()) do i\n    device!(i)\n    local_x = CuArray(...)\n    copy_to!(local_x, x_handle)\n    ...\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I started reading about CUDA MPS but I don't know enough if its the right thing to devote more time to understanding, or if it can be used from CUDA.jl at all. I'm also aware of CUDA-aware MPI, but since in my case most of my work is from Jupyter notebooks, I don't think it can really work. I've also been playing alot lately with threads instead of processes, but that doesn't seem stable enough to rely on quite yet. Curious if there's some other (easy?) option with processes I'm missing?"}]}]}],"thread_ts":"1614672449.074900","reply_count":22,"reply_users_count":4,"latest_reply":"1614729415.080400","reply_users":["UMY1LV01G","U68A3ASP9","UUMJUCYRK","U67BJLYCS"],"subscribed":false},{"client_msg_id":"39e11b8e-1ba8-4651-b8b8-0af2918bf62d","type":"message","text":"I'm excited for <https://github.com/JuliaParallel/UCX.jl/pull/35> to land so we can do device-device transfers for ML training","user":"UMY1LV01G","ts":"1614672901.075000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rmnnA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm excited for "},{"type":"link","url":"https://github.com/JuliaParallel/UCX.jl/pull/35"},{"type":"text","text":" to land so we can do device-device transfers for ML training"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK","reactions":[{"name":"+1","users":["UUMJUCYRK"],"count":1}]},{"client_msg_id":"a4f44240-f750-4f38-b068-8e20812b8919","type":"message","text":"there's an IPC API, e.g. <https://github.com/ndd314/cuda_examples/blob/master/0_Simple/simpleIPC/simpleIPC.cu>, but I haven't tried it","user":"U68A3ASP9","ts":"1614673800.075200","team":"T68168MUP","edited":{"user":"U68A3ASP9","ts":"1614673809.000000"},"blocks":[{"type":"rich_text","block_id":"GdzPF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"there's an IPC API, e.g. "},{"type":"link","url":"https://github.com/ndd314/cuda_examples/blob/master/0_Simple/simpleIPC/simpleIPC.cu"},{"type":"text","text":", but I haven't tried it"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"ebae5633-e856-4342-82b4-b2096f8d4745","type":"message","text":"Thanks, will check that out.","user":"UUMJUCYRK","ts":"1614676441.075600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"V8ZPV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks, will check that out."}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"26292770-c80e-41f0-b548-36f51444ba78","type":"message","text":"Re <https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#interprocess-communication|IPC>,\n&gt; Note that the IPC API is not supported for cudaMallocManaged allocations.\na bit of a bummer, since this was part of the idea, but not a total showstopper since you can still do the D2D copy manually","user":"UUMJUCYRK","ts":"1614677672.075800","team":"T68168MUP","edited":{"user":"UUMJUCYRK","ts":"1614677771.000000"},"blocks":[{"type":"rich_text","block_id":"zPQ+k","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Re "},{"type":"link","url":"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#interprocess-communication","text":"IPC"},{"type":"text","text":",\n"}]},{"type":"rich_text_quote","elements":[{"type":"text","text":"Note that the IPC API is not supported for cudaMallocManaged allocations."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"a bit of a bummer, since this was part of the idea, but not a total showstopper since you can still do the D2D copy manually"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"11d7adcf-858e-4116-afd8-e5490f7afa0e","type":"message","text":"Ok, nice, that was remarkably easy:\n```using Distributed\naddprocs(1, exeflags=\"--project=@.\")\n@everywhere using CUDA\n\nhandle = @fetchfrom 2 begin\n    x = cu([1,2,3])\n    handle = Ref{CUDA.CUipcMemHandle}()\n    CUDA.cuIpcGetMemHandle(handle, pointer(x))\n    handle\nend\n\nbuf = Ref{CUDA.CUdeviceptr}()\nCUDA.cuIpcOpenMemHandle(buf, handle[], CUDA.CU_IPC_MEM_LAZY_ENABLE_PEER_ACCESS)\nremote_x = unsafe_wrap(CuArray{Int,1}, convert(CuPtr{Int}, buf[]), (3,); own=false)\nprintln(remote_x)```","user":"UUMJUCYRK","ts":"1614687523.076300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HvN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ok, nice, that was remarkably easy:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Distributed\naddprocs(1, exeflags=\"--project=@.\")\n@everywhere using CUDA\n\nhandle = @fetchfrom 2 begin\n    x = cu([1,2,3])\n    handle = Ref{CUDA.CUipcMemHandle}()\n    CUDA.cuIpcGetMemHandle(handle, pointer(x))\n    handle\nend\n\nbuf = Ref{CUDA.CUdeviceptr}()\nCUDA.cuIpcOpenMemHandle(buf, handle[], CUDA.CU_IPC_MEM_LAZY_ENABLE_PEER_ACCESS)\nremote_x = unsafe_wrap(CuArray{Int,1}, convert(CuPtr{Int}, buf[]), (3,); own=false)\nprintln(remote_x)"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"3c555db9-95d8-431e-8c61-c8c73cabab3f","type":"message","text":"nice! what kind of communication mechanism does that use?","user":"U68A3ASP9","ts":"1614687654.076500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DkBsE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"nice! what kind of communication mechanism does that use?"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"cedea775-435d-48bb-9b89-b88da992a014","type":"message","text":"also, since you've switched to multiprocessing again I take it multithreading had issues still?","user":"U68A3ASP9","ts":"1614687674.076700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MvSZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"also, since you've switched to multiprocessing again I take it multithreading had issues still?"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"f7e36d8b-ac65-4847-990b-78601f8e2793","type":"message","text":"communication mechanism?","user":"UUMJUCYRK","ts":"1614687685.076900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"M2g4w","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"communication mechanism?"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"0c6d36c3-832c-447a-be95-fbe8bf21a825","type":"message","text":"yea, quite alot of non-trivial stuff worked with threading, but not quite the full machinery of what Im doing","user":"UUMJUCYRK","ts":"1614687739.077100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ipA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yea, quite alot of non-trivial stuff worked with threading, but not quite the full machinery of what Im doing"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"e20f2458-57e1-4e5f-a747-5dfe23171bbf","type":"message","text":"will no doubt continue to poke at it","user":"UUMJUCYRK","ts":"1614687795.077300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YmD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"will no doubt continue to poke at it"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"3e839f27-537e-43e6-acfd-a9840027ee04","type":"message","text":"ok great, it's good to have users pushing on this, as its hard to flush out issues with just CI","user":"U68A3ASP9","ts":"1614687827.077500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GuOY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ok great, it's good to have users pushing on this, as its hard to flush out issues with just CI"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"026e092f-8394-4edb-b84b-bdbdb35e5449","type":"message","text":"&gt; communication mechanism?\ndo you just get a raw device pointer, or does it do a device copy between contexts, etc","user":"U68A3ASP9","ts":"1614687857.077700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kyP3","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"communication mechanism?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"do you just get a raw device pointer, or does it do a device copy between contexts, etc"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"61a22660-1dc2-404c-8106-f2620b3e712b","type":"message","text":"ah, that I havent checked, also this was just 1 GPU","user":"UUMJUCYRK","ts":"1614687901.077900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IrM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ah, that I havent checked, also this was just 1 GPU"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"63d19a9d-6867-4184-8155-df9cbe6ce034","type":"message","text":"any reason not to make serialization always use IPC? you could make a copy after deserialization then close the memHandle right away so GC is ok and its fine if the original process free's the memory, but you still win over the current method since its all on GPU?","user":"UUMJUCYRK","ts":"1614688007.078100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"aXO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"any reason not to make serialization always use IPC? you could make a copy after deserialization then close the memHandle right away so GC is ok and its fine if the original process free's the memory, but you still win over the current method since its all on GPU?"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"803ba9fb-f0ec-419e-b5c0-845d84c03c83","type":"message","text":"IPC only works in the same host and you just get a raw pointer","user":"U67BJLYCS","ts":"1614688348.078300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kqOnM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"IPC only works in the same host and you just get a raw pointer"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"2f0ca89e-fcd7-4ec8-a180-4b59f86ac2d1","type":"message","text":"So GC is an issue ;)","user":"U67BJLYCS","ts":"1614688375.078500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Z8R","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So GC is an issue ;)"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"0ed25e94-4baa-47af-8b88-c07651ce4fe4","type":"message","text":"But with Distributed.jl we don't know that you are on the same host and so we can't use IPC as a default","user":"U67BJLYCS","ts":"1614688408.078700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LhF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But with Distributed.jl we don't know that you are on the same host and so we can't use IPC as a default"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"c0f6e696-8b6c-463d-ad6a-5d86e3947844","type":"message","text":"UCX is abstracting that away from us (eventually)","user":"U67BJLYCS","ts":"1614688474.078900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"inn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"UCX is abstracting that away from us (eventually)"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"24e9f10e-4549-4052-a4c1-55182245dcda","type":"message","text":"&gt; we don't know that you are on the same host\nAh right, duh. Also, I guess the issue with my idea of just immediately copying the memory on the deserialized end is that technically the memory could have been freed at the origin in the time it takes to copy? Even still, might be nice to have an option to switch serialization to IPC (meaning the user guarantees its the same host)? Or maybe a custom serializer?","user":"UUMJUCYRK","ts":"1614688812.079100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0W+bL","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"we don't know that you are on the same host"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Ah right, duh. Also, I guess the issue with my idea of just immediately copying the memory on the deserialized end is that technically the memory could have been freed at the origin in the time it takes to copy? Even still, might be nice to have an option to switch serialization to IPC (meaning the user guarantees its the same host)? Or maybe a custom serializer?"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"9d813ca6-ea79-4454-90ae-05dc770966eb","type":"message","text":"I think documenting what you have above in an issue would be good (before the slack hole eats it), but I am hesitant to add anything official that might break and confuse people, I think IPC has a couple of other requirements (like all the CUDA devices being visible, which some HPC schedulers make really annoying)","user":"U67BJLYCS","ts":"1614690608.079300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DYDb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think documenting what you have above in an issue would be good (before the slack hole eats it), but I am hesitant to add anything official that might break and confuse people, I think IPC has a couple of other requirements (like all the CUDA devices being visible, which some HPC schedulers make really annoying)"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK"},{"client_msg_id":"55937ed3-4d62-4493-b5e8-3929dfbcaf57","type":"message","text":"Soon enough UCX.jl will likely be the official solution","user":"U67BJLYCS","ts":"1614690629.079500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hgdTc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Soon enough UCX.jl will likely be the official solution"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK","reactions":[{"name":"+1","users":["UUMJUCYRK"],"count":1}]},{"client_msg_id":"f1eb517f-b3f4-4de5-9176-fb2dd5d27958","type":"message","text":"&gt; I think documenting what you have above in an issue would be good\n<https://github.com/JuliaGPU/CUDA.jl/issues/747>","user":"UUMJUCYRK","ts":"1614729415.080400","team":"T68168MUP","edited":{"user":"UUMJUCYRK","ts":"1614729422.000000"},"blocks":[{"type":"rich_text","block_id":"aI5","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"I think documenting what you have above in an issue would be good"}]},{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/issues/747"}]}]}],"thread_ts":"1614672449.074900","parent_user_id":"UUMJUCYRK"}]