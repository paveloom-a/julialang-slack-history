[{"client_msg_id":"2d40c8a7-c2d0-410b-b86a-7cc0a67d26af","type":"message","text":"I just cloned CUDA.jl and I can't get it to build successfully. Normally I'd do\n```] activate Project.toml\ninclude(\"test/setup.jl\")```\nand it everything would work as though I had just done `using CUDA`.\n\nNow, however, I get:\n```julia&gt; include(\"test/setup.jl\")\n[ Info: Precompiling CUDA [052768ef-5323-5732-b1bb-66c8b64840ba]\nERROR: LoadError: LoadError: UndefVarError: ci_cache not defined\nStacktrace:\n  [1] getproperty(x::Module, f::Symbol)\n    @ Base ./Base.jl:26\n  [2] top-level scope\n    @ ~/CUDA.jl/src/compiler/gpucompiler.jl:43\n  [3] include(mod::Module, _path::String)\n    @ Base ./Base.jl:386\n  [4] include(x::String)\n    @ CUDA ~/CUDA.jl/src/CUDA.jl:1\n  [5] top-level scope\n    @ ~/CUDA.jl/src/CUDA.jl:69\n  [6] include\n    @ ./Base.jl:386 [inlined]\n  [7] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String)\n    @ Base ./loading.jl:1209\n  [8] top-level scope\n    @ none:1\n  [9] eval\n    @ ./boot.jl:360 [inlined]\n [10] eval(x::Expr)\n    @ Base.MainInclude ./client.jl:446\n [11] top-level scope\n    @ none:1\nin expression starting at /home/ec2-user/CUDA.jl/src/compiler/gpucompiler.jl:43\nin expression starting at /home/ec2-user/CUDA.jl/src/CUDA.jl:1\nERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to /home/ec2-user/.julia/compiled/v1.6/CUDA/jl_LSCwP5.\nStacktrace:\n [1] error(s::String)\n   @ Base ./error.jl:33\n [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::Base.TTY, internal_stdout::Base.TTY)\n   @ Base ./loading.jl:1356\n [3] compilecache(pkg::Base.PkgId, path::String)\n   @ Base ./loading.jl:1302\n [4] _require(pkg::Base.PkgId)\n   @ Base ./loading.jl:1017\n [5] require(uuidkey::Base.PkgId)\n   @ Base ./loading.jl:910\n [6] require(into::Module, mod::Symbol)\n   @ Base ./loading.jl:897\n [7] include(fname::String)\n   @ Base.MainInclude ./client.jl:444\n [8] top-level scope\n   @ REPL[4]:1\nin expression starting at /home/ec2-user/CUDA.jl/test/setup.jl:1```\nAlternatively, if I try `] build`:\n```(CUDA) pkg&gt; build\nPrecompiling project...\n  Progress [========================================&gt;]  1/1\n  ✗ CUDA\n0 dependencies successfully precompiled in 17 seconds (26 already precompiled)\n1 dependency errored```\nWhat's the recommended way to clone and build the repo now?","user":"U01G39CC63F","ts":"1616173015.025900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"t8fG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I just cloned CUDA.jl and I can't get it to build successfully. Normally I'd do\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"] activate Project.toml\ninclude(\"test/setup.jl\")"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"and it everything would work as though I had just done "},{"type":"text","text":"using CUDA","style":{"code":true}},{"type":"text","text":".\n\nNow, however, I get:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> include(\"test/setup.jl\")\n[ Info: Precompiling CUDA [052768ef-5323-5732-b1bb-66c8b64840ba]\nERROR: LoadError: LoadError: UndefVarError: ci_cache not defined\nStacktrace:\n  [1] getproperty(x::Module, f::Symbol)\n    @ Base ./Base.jl:26\n  [2] top-level scope\n    @ ~/CUDA.jl/src/compiler/gpucompiler.jl:43\n  [3] include(mod::Module, _path::String)\n    @ Base ./Base.jl:386\n  [4] include(x::String)\n    @ CUDA ~/CUDA.jl/src/CUDA.jl:1\n  [5] top-level scope\n    @ ~/CUDA.jl/src/CUDA.jl:69\n  [6] include\n    @ ./Base.jl:386 [inlined]\n  [7] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String)\n    @ Base ./loading.jl:1209\n  [8] top-level scope\n    @ none:1\n  [9] eval\n    @ ./boot.jl:360 [inlined]\n [10] eval(x::Expr)\n    @ Base.MainInclude ./client.jl:446\n [11] top-level scope\n    @ none:1\nin expression starting at /home/ec2-user/CUDA.jl/src/compiler/gpucompiler.jl:43\nin expression starting at /home/ec2-user/CUDA.jl/src/CUDA.jl:1\nERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to /home/ec2-user/.julia/compiled/v1.6/CUDA/jl_LSCwP5.\nStacktrace:\n [1] error(s::String)\n   @ Base ./error.jl:33\n [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::Base.TTY, internal_stdout::Base.TTY)\n   @ Base ./loading.jl:1356\n [3] compilecache(pkg::Base.PkgId, path::String)\n   @ Base ./loading.jl:1302\n [4] _require(pkg::Base.PkgId)\n   @ Base ./loading.jl:1017\n [5] require(uuidkey::Base.PkgId)\n   @ Base ./loading.jl:910\n [6] require(into::Module, mod::Symbol)\n   @ Base ./loading.jl:897\n [7] include(fname::String)\n   @ Base.MainInclude ./client.jl:444\n [8] top-level scope\n   @ REPL[4]:1\nin expression starting at /home/ec2-user/CUDA.jl/test/setup.jl:1"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nAlternatively, if I try "},{"type":"text","text":"] build","style":{"code":true}},{"type":"text","text":":\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"(CUDA) pkg> build\nPrecompiling project...\n  Progress [========================================>]  1/1\n  ✗ CUDA\n0 dependencies successfully precompiled in 17 seconds (26 already precompiled)\n1 dependency errored"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nWhat's the recommended way to clone and build the repo now?"}]}]}],"thread_ts":"1616173015.025900","reply_count":9,"reply_users_count":3,"latest_reply":"1616182102.028000","reply_users":["U6A0PD8CR","U01G39CC63F","U68A3ASP9"],"is_locked":false,"subscribed":false},{"client_msg_id":"e8f17a95-4c4c-4a8c-812a-8a81bf108b35","type":"message","text":"I believe you may need a newer version of Julia or GPUCompiler. What version of Julia are you on? And what version is GPUCompiler in your manifest?","user":"U6A0PD8CR","ts":"1616174031.026200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LlJGo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I believe you may need a newer version of Julia or GPUCompiler. What version of Julia are you on? And what version is GPUCompiler in your manifest?"}]}]}],"thread_ts":"1616173015.025900","parent_user_id":"U01G39CC63F"},{"client_msg_id":"21ea5794-708b-46dc-8d77-1675a69ff1ce","type":"message","text":"```julia&gt; versioninfo()\nJulia Version 1.6.0-beta1.85\nCommit 5062627e5a* (2021-02-02 07:22 UTC)```\nand\n```  [61eb1bfa] GPUCompiler v0.10.0```","user":"U01G39CC63F","ts":"1616174458.026400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Vm6G+","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> versioninfo()\nJulia Version 1.6.0-beta1.85\nCommit 5062627e5a* (2021-02-02 07:22 UTC)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"and\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"  [61eb1bfa] GPUCompiler v0.10.0"}]}]}],"thread_ts":"1616173015.025900","parent_user_id":"U01G39CC63F"},{"client_msg_id":"6c87279f-c28d-4561-90fb-6402eb7c63d6","type":"message","text":"And what version of CUDA is this?","user":"U6A0PD8CR","ts":"1616175049.026600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1x86","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And what version of CUDA is this?"}]}]}],"thread_ts":"1616173015.025900","parent_user_id":"U01G39CC63F"},{"client_msg_id":"c6e0a811-de9a-4b37-813a-6a548f5ef282","type":"message","text":"Oh you might actually need GPUCompiler#master if CUDA is on master","user":"U6A0PD8CR","ts":"1616175107.026800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"P7t3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oh you might actually need GPUCompiler#master if CUDA is on master"}]}]}],"thread_ts":"1616173015.025900","parent_user_id":"U01G39CC63F"},{"client_msg_id":"93f2fd36-9f76-4fc9-bc8c-3362039530d7","type":"message","text":"Try `] instantiate` first","user":"U6A0PD8CR","ts":"1616175128.027000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QuG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Try "},{"type":"text","text":"] instantiate","style":{"code":true}},{"type":"text","text":" first"}]}]}],"thread_ts":"1616173015.025900","parent_user_id":"U01G39CC63F"},{"client_msg_id":"8ca07355-7230-455f-9470-31384c51fa6a","type":"message","text":"With CUDA activated","user":"U6A0PD8CR","ts":"1616175137.027200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ANO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"With CUDA activated"}]}]}],"thread_ts":"1616173015.025900","parent_user_id":"U01G39CC63F"},{"client_msg_id":"6482a9e5-1f3a-4192-9dca-eca82d883b11","type":"message","text":"Yeah GPUCompiler master branch is needed.","user":"U68A3ASP9","ts":"1616178877.027600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HThJk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah GPUCompiler master branch is needed."}]}]}],"thread_ts":"1616173015.025900","parent_user_id":"U01G39CC63F"},{"client_msg_id":"d3f15c4e-d4d7-48b9-82a6-485737268a04","type":"message","text":"awesome thank you both","user":"U01G39CC63F","ts":"1616182091.027800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ZCva3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"awesome thank you both"}]}]}],"thread_ts":"1616173015.025900","parent_user_id":"U01G39CC63F"},{"client_msg_id":"56a29e4c-3ffb-47b3-9b8a-8b21b5f463bb","type":"message","text":"adding #master did it","user":"U01G39CC63F","ts":"1616182102.028000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"e7b+A","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"adding #master did it"}]}]}],"thread_ts":"1616173015.025900","parent_user_id":"U01G39CC63F"}]