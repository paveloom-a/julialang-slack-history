[{"client_msg_id":"0d640423-7ab9-4f43-8f8d-25e188cf0f79","type":"message","text":"Do any of you know what/where I need to start to make a kernel implementation using Julia as fast as nvcc implementation? I have two different kernel implementations, one is written in C using nvcc and the other in Julia. They are almost the same, except inevitable changes such as a pointer to array in C being implemented using CuDeviceArray in Julia. They were launched using the same kernel configuration. Currently, nvcc implementation is 6 times faster than Julia, and I would like to reduce this performance gap.\n\nIt seems that generated code is very different. In the case of nvcc, the maximum number of registers was 190 but it was 250 for Julia. When I used nvcc, I set `maxrregcount`  to 96 for better occupancy and saw about 30% improvement. I tried similar things for Julia implementation by trying different values of `maxregs` but with no luck. Any help will be greatly appreciated.","user":"U01FXSDEXN3","ts":"1615569301.043900","team":"T68168MUP","edited":{"user":"U01FXSDEXN3","ts":"1615569513.000000"},"blocks":[{"type":"rich_text","block_id":"vZb51","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do any of you know what/where I need to start to make a kernel implementation using Julia as fast as nvcc implementation? I have two different kernel implementations, one is written in C using nvcc and the other in Julia. They are almost the same, except inevitable changes such as a pointer to array in C being implemented using CuDeviceArray in Julia. They were launched using the same kernel configuration. Currently, nvcc implementation is 6 times faster than Julia, and I would like to reduce this performance gap.\n\nIt seems that generated code is very different. In the case of nvcc, the maximum number of registers was 190 but it was 250 for Julia. When I used nvcc, I set "},{"type":"text","text":"maxrregcount","style":{"code":true}},{"type":"text","text":"  to 96 for better occupancy and saw about 30% improvement. I tried similar things for Julia implementation by trying different values of "},{"type":"text","text":"maxregs","style":{"code":true}},{"type":"text","text":" but with no luck. Any help will be greatly appreciated."}]}]}],"thread_ts":"1615569301.043900","reply_count":9,"reply_users_count":2,"latest_reply":"1615570867.046700","reply_users":["U68A3ASP9","U01FXSDEXN3"],"subscribed":false,"reactions":[{"name":"+1","users":["UN2U72Q3F"],"count":1}]},{"client_msg_id":"4fd90423-6283-4f2b-80cd-8e7e94a4993a","type":"message","text":"are you using `@inbounds` etc? also try to avoid unnecessary type conversions","user":"U68A3ASP9","ts":"1615569954.044400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1EJw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"are you using "},{"type":"text","text":"@inbounds","style":{"code":true}},{"type":"text","text":" etc? also try to avoid unnecessary type conversions"}]}]}],"thread_ts":"1615569301.043900","parent_user_id":"U01FXSDEXN3"},{"client_msg_id":"1dc4635f-cdf3-44d8-bb61-bbb7bf3abe17","type":"message","text":"generally Julia GPU code can be as fast or faster than C code compiler with nvcc","user":"U68A3ASP9","ts":"1615569967.044600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nluQC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"generally Julia GPU code can be as fast or faster than C code compiler with nvcc"}]}]}],"thread_ts":"1615569301.043900","parent_user_id":"U01FXSDEXN3"},{"client_msg_id":"d8fb4056-1ec0-46e9-8f5d-1c55a3122db0","type":"message","text":"Yes, I'm using `@inbounds` for every for loop. I think I do one type conversion for shared memory at the beginning of my kernel; a chunk of dynamic shared memory (amount to 2KB) is allocated and is assigned to two different data types, Int and Float64. Except that there's no type conversion I think...","user":"U01FXSDEXN3","ts":"1615570183.044800","team":"T68168MUP","edited":{"user":"U01FXSDEXN3","ts":"1615570386.000000"},"blocks":[{"type":"rich_text","block_id":"0Gj7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes, I'm using "},{"type":"text","text":"@inbounds","style":{"code":true}},{"type":"text","text":" for every for loop. I think I do one type conversion for shared memory at the beginning of my kernel; a chunk of dynamic shared memory (amount to 2KB) is allocated and is assigned to two different data types, Int and Float64. Except that there's no type conversion I think..."}]}]}],"thread_ts":"1615569301.043900","parent_user_id":"U01FXSDEXN3"},{"client_msg_id":"b3fa70f0-1163-4bf6-811a-793db508cb55","type":"message","text":"there's a couple of area's where we emit worse code than nvcc, but those should not lead to a 6x regression","user":"U68A3ASP9","ts":"1615570656.045600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xqis","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"there's a couple of area's where we emit worse code than nvcc, but those should not lead to a 6x regression"}]}]}],"thread_ts":"1615569301.043900","parent_user_id":"U01FXSDEXN3"},{"client_msg_id":"593059ea-9e8f-4328-868a-b0924ffb4f4c","type":"message","text":"for example, our indices being Int64 results in some additional register pressure, and makes it impossible to overlap them with other ops","user":"U68A3ASP9","ts":"1615570684.045800","team":"T68168MUP","edited":{"user":"U68A3ASP9","ts":"1615570696.000000"},"blocks":[{"type":"rich_text","block_id":"tCKC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"for example, our indices being Int64 results in some additional register pressure, and makes it impossible to overlap them with other ops"}]}]}],"thread_ts":"1615569301.043900","parent_user_id":"U01FXSDEXN3"},{"client_msg_id":"6857026f-df6a-4806-9371-f62b399f833e","type":"message","text":"maybe you're really unlucky and you just run into the case where it starts spilling, wrecking performance?","user":"U68A3ASP9","ts":"1615570720.046100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oGfN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"maybe you're really unlucky and you just run into the case where it starts spilling, wrecking performance?"}]}]}],"thread_ts":"1615569301.043900","parent_user_id":"U01FXSDEXN3"},{"client_msg_id":"44bae84f-384c-473b-8762-7633e547d8c8","type":"message","text":"anyway, if you can reduce it to some Julia vs C code I'm happy to take a look","user":"U68A3ASP9","ts":"1615570740.046300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Z8Lo8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"anyway, if you can reduce it to some Julia vs C code I'm happy to take a look"}]}]}],"thread_ts":"1615569301.043900","parent_user_id":"U01FXSDEXN3"},{"client_msg_id":"60b19d96-10f5-4cbe-8de7-d82a0fad4122","type":"message","text":"there was this though, <https://github.com/JuliaGPU/GPUCompiler.jl/issues/92>, so make sure you're using GPUCompiler.jl v0.9.2+","user":"U68A3ASP9","ts":"1615570819.046500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fLpOW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"there was this though, "},{"type":"link","url":"https://github.com/JuliaGPU/GPUCompiler.jl/issues/92"},{"type":"text","text":", so make sure you're using GPUCompiler.jl v0.9.2+"}]}]}],"thread_ts":"1615569301.043900","parent_user_id":"U01FXSDEXN3"},{"client_msg_id":"1cc097fa-3350-4d14-aec8-03efa79f6e78","type":"message","text":"It's somewhat complicated, because it calls several different device functions inside a kernel (but I do not call other libraries such as cublas). I'll see what I could do to give you a simple example. Thanks!","user":"U01FXSDEXN3","ts":"1615570867.046700","team":"T68168MUP","edited":{"user":"U01FXSDEXN3","ts":"1615570921.000000"},"blocks":[{"type":"rich_text","block_id":"IvE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It's somewhat complicated, because it calls several different device functions inside a kernel (but I do not call other libraries such as cublas). I'll see what I could do to give you a simple example. Thanks!"}]}]}],"thread_ts":"1615569301.043900","parent_user_id":"U01FXSDEXN3"}]