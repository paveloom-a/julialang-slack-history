[{"client_msg_id":"e2c171fd-cb4c-4908-b42a-5a30d3ab803f","type":"message","text":"As FYI, <https://github.com/omlins/ParallelStencil.jl|ParallelStencil.jl> is now registered (Threads and CUDA backends). Enthusiastic to see if in the near future one could add the AMDGPU backend as well. Would mainly require to have functional AMD equivalent of `CuArrays` , and equivalent of thread/block/grid (index) as well as kernel launch params. Next steps may include “AMD-aware”  MPI (GPU pointer exchange) and streams priority handling for asynchronous execution if this will be available at some point to interact with <https://github.com/eth-cscs/ImplicitGlobalGrid.jl|ImplicitGlobalGrid.jl>.","user":"U019A495ZM3","ts":"1617219372.088900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"f8xn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"As FYI, "},{"type":"link","url":"https://github.com/omlins/ParallelStencil.jl","text":"ParallelStencil.jl"},{"type":"text","text":" is now registered (Threads and CUDA backends). Enthusiastic to see if in the near future one could add the AMDGPU backend as well. Would mainly require to have functional AMD equivalent of "},{"type":"text","text":"CuArrays","style":{"code":true}},{"type":"text","text":" , and equivalent of thread/block/grid (index) as well as kernel launch params. Next steps may include “AMD-aware”  MPI (GPU pointer exchange) and streams priority handling for asynchronous execution if this will be available at some point to interact with "},{"type":"link","url":"https://github.com/eth-cscs/ImplicitGlobalGrid.jl","text":"ImplicitGlobalGrid.jl"},{"type":"text","text":"."}]}]}],"thread_ts":"1617219372.088900","reply_count":13,"reply_users_count":3,"latest_reply":"1617309334.112700","reply_users":["U67BJLYCS","U019A495ZM3","U6A0PD8CR"],"is_locked":false,"subscribed":false,"reactions":[{"name":"+1","users":["UM5RTMK6V"],"count":1}]},{"client_msg_id":"5542e990-a11a-45bf-bce9-454f544a8776","type":"message","text":"We recently added a ROC backend to KernelAbstractions.jl","user":"U67BJLYCS","ts":"1617221518.089000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Hyny","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We recently added a ROC backend to KernelAbstractions.jl"}]}]}],"thread_ts":"1617219372.088900","parent_user_id":"U019A495ZM3"},{"client_msg_id":"6ab6be4a-2832-4118-afd8-755b90df269c","type":"message","text":"Cool - just read about it :slightly_smiling_face:\nHope it performs similarly to CUDA. Would be nice to see if one could add AMDGPU backend to ParallelStencil.jl and later do the AMDGPU MPI with ImplicitGlobalGrid.jl. We use fairly “basic” features in there ()with excpetion of the async streams for communication / computation overlap.","user":"U019A495ZM3","ts":"1617223932.089300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"D0fae","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Cool - just read about it "},{"type":"emoji","name":"slightly_smiling_face"},{"type":"text","text":"\nHope it performs similarly to CUDA. Would be nice to see if one could add AMDGPU backend to ParallelStencil.jl and later do the AMDGPU MPI with ImplicitGlobalGrid.jl. We use fairly “basic” features in there ()with excpetion of the async streams for communication / computation overlap."}]}]}],"thread_ts":"1617219372.088900","parent_user_id":"U019A495ZM3"},{"client_msg_id":"b206a656-3d0d-4826-9f23-cf1c3d71d9f6","type":"message","text":"Yeah KA helps with that since the dependency management is explicit","user":"U67BJLYCS","ts":"1617223980.089500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"o9c","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah KA helps with that since the dependency management is explicit"}]}]}],"thread_ts":"1617219372.088900","parent_user_id":"U019A495ZM3"},{"client_msg_id":"c17a0f66-8e37-44e7-b851-37df10a7687f","type":"message","text":"so you get async streams \"for free\"","user":"U67BJLYCS","ts":"1617224013.089700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"W7K","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"so you get async streams \"for free\""}]}]}],"thread_ts":"1617219372.088900","parent_user_id":"U019A495ZM3"},{"client_msg_id":"b38613fd-41b4-4489-a8f1-d0c8a2a26f74","type":"message","text":"Cool :slightly_smiling_face:","user":"U019A495ZM3","ts":"1617224186.089900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"A6hX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Cool "},{"type":"emoji","name":"slightly_smiling_face"}]}]}],"thread_ts":"1617219372.088900","parent_user_id":"U019A495ZM3"},{"client_msg_id":"74a557e6-1726-43dd-8ac6-eaa7212cc8e5","type":"message","text":"Where free is explicitly reasoning about dependencies everywhere :wink:","user":"U67BJLYCS","ts":"1617224318.090100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tACzw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Where free is explicitly reasoning about dependencies everywhere "},{"type":"emoji","name":"wink"}]}]}],"thread_ts":"1617219372.088900","parent_user_id":"U019A495ZM3"},{"client_msg_id":"2af17994-adfb-4abb-9bdf-c60025983ace","type":"message","text":"Nice. I was wondering about the mapping of functions like `Base.sin` (“are mapped to `CUDA.sin`”); is there an “automated” way to get those or do you implement it via a macro (or function) ?","user":"U019A495ZM3","ts":"1617224648.090400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"I=nES","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Nice. I was wondering about the mapping of functions like "},{"type":"text","text":"Base.sin","style":{"code":true}},{"type":"text","text":" (“are mapped to "},{"type":"text","text":"CUDA.sin","style":{"code":true}},{"type":"text","text":"”); is there an “automated” way to get those or do you implement it via a macro (or function) ?"}]}]}],"thread_ts":"1617219372.088900","parent_user_id":"U019A495ZM3"},{"client_msg_id":"ccbb3044-c92e-42ff-8d85-4e5b11aefc7f","type":"message","text":"CUDA 3.0 implements a solution for that","user":"U67BJLYCS","ts":"1617224910.090600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"woklf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CUDA 3.0 implements a solution for that"}]}]}],"thread_ts":"1617219372.088900","parent_user_id":"U019A495ZM3","reactions":[{"name":"raised_hands","users":["U019A495ZM3"],"count":1}]},{"client_msg_id":"683ac262-fbd8-444a-a575-edeaec9c9342","type":"message","text":"and KA does it using Cassette","user":"U67BJLYCS","ts":"1617224915.090800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yii1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"and KA does it using Cassette"}]}]}],"thread_ts":"1617219372.088900","parent_user_id":"U019A495ZM3","reactions":[{"name":"+1","users":["U019A495ZM3"],"count":1}]},{"client_msg_id":"734c461f-866f-4a28-bd90-c6272f9b9741","type":"message","text":"AMDGPU won't be implementing the same overrides as CUDA until we lower-bound Julia to 1.7, because the implementation with 1.6 breaks when both CUDA and AMDGPU are loaded.","user":"U6A0PD8CR","ts":"1617285106.092800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VimF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"AMDGPU won't be implementing the same overrides as CUDA until we lower-bound Julia to 1.7, because the implementation with 1.6 breaks when both CUDA and AMDGPU are loaded."}]}]}],"thread_ts":"1617219372.088900","parent_user_id":"U019A495ZM3"},{"client_msg_id":"d4b4d1d1-fb82-46ce-8ce9-d4fa67f39f0b","type":"message","text":"But KA has a working 1.6 implementation, so I'd recommend using that","user":"U6A0PD8CR","ts":"1617285125.093000","team":"T68168MUP","edited":{"user":"U6A0PD8CR","ts":"1617285138.000000"},"blocks":[{"type":"rich_text","block_id":"bRhm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But KA has a working 1.6 implementation, so I'd recommend using that"}]}]}],"thread_ts":"1617219372.088900","parent_user_id":"U019A495ZM3"},{"client_msg_id":"949d9fc0-e6bb-4c84-a125-96cc50a841c2","type":"message","text":"Thanks <@U6A0PD8CR> for the advise and looking fwd to further AMD support (a Julia GPU “HPC” battle horse) in these times where cheaper AMD-based solutions are economically attractive but running codes are missing…","user":"U019A495ZM3","ts":"1617309197.112300","team":"T68168MUP","edited":{"user":"U019A495ZM3","ts":"1617309210.000000"},"blocks":[{"type":"rich_text","block_id":"PrMRE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks "},{"type":"user","user_id":"U6A0PD8CR"},{"type":"text","text":" for the advise and looking fwd to further AMD support (a Julia GPU “HPC” battle horse) in these times where cheaper AMD-based solutions are economically attractive but running codes are missing…"}]}]}],"thread_ts":"1617219372.088900","parent_user_id":"U019A495ZM3","reactions":[{"name":"+1","users":["U6A0PD8CR"],"count":1}]},{"client_msg_id":"be94f690-efbc-4c65-a551-93c4183dcb37","type":"message","text":"Please do try out ROCKernels and file lots of issues about bugs and missing features! I too wish most Julia code worked on my favorite kind of GPUs","user":"U6A0PD8CR","ts":"1617309334.112700","team":"T68168MUP","edited":{"user":"U6A0PD8CR","ts":"1617309362.000000"},"blocks":[{"type":"rich_text","block_id":"TNA+J","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Please do try out ROCKernels and file lots of issues about bugs and missing features! I too wish most Julia code worked on my favorite kind of GPUs"}]}]}],"thread_ts":"1617219372.088900","parent_user_id":"U019A495ZM3","reactions":[{"name":"+1","users":["U019A495ZM3"],"count":1}]}]