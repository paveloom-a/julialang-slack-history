[{"client_msg_id":"bf588df5-e1bd-438b-b246-fdb230fed0a2","type":"message","text":"is fp16 working for codegen or just precompiled cuda kernels?","user":"UDGT4PM41","ts":"1618171453.184000","team":"T68168MUP","edited":{"user":"UDGT4PM41","ts":"1618172098.000000"},"blocks":[{"type":"rich_text","block_id":"GXQg/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is fp16 working for codegen or just precompiled cuda kernels?"}]}]}],"thread_ts":"1618171453.184000","reply_count":7,"reply_users_count":3,"latest_reply":"1618234822.187500","reply_users":["UDGT4PM41","U67BJLYCS","U68A3ASP9"],"is_locked":false,"subscribed":false},{"client_msg_id":"5356abd5-b025-4390-a503-2366acf0b51a","type":"message","text":"<https://github.com/JuliaGPU/CUDA.jl/issues/391> This is the latest I assume?","user":"UDGT4PM41","ts":"1618172187.184200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QfNC","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/issues/391"},{"type":"text","text":" This is the latest I assume?"}]}]}],"thread_ts":"1618171453.184000","parent_user_id":"UDGT4PM41"},{"client_msg_id":"f85ae8c4-5d67-495c-960e-7b28e8c471bd","type":"message","text":"fp16 is \"working\"","user":"U67BJLYCS","ts":"1618174557.184500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vlA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"fp16 is \"working\""}]}]}],"thread_ts":"1618171453.184000","parent_user_id":"UDGT4PM41"},{"client_msg_id":"a684680c-8741-4322-a326-1e0c8a9a2ea7","type":"message","text":"but it is legalized by promoting to FP32 and then truncating","user":"U67BJLYCS","ts":"1618174597.184700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Bmc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"but it is legalized by promoting to FP32 and then truncating"}]}]}],"thread_ts":"1618171453.184000","parent_user_id":"UDGT4PM41"},{"client_msg_id":"85ddcfa7-cc82-443d-90e7-abad68ba1f7b","type":"message","text":"ah, I see","user":"UDGT4PM41","ts":"1618175397.184900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FD9PX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ah, I see"}]}]}],"thread_ts":"1618171453.184000","parent_user_id":"UDGT4PM41"},{"client_msg_id":"9f12a8ca-c3ee-4642-bc34-f4972e66d74a","type":"message","text":"no, we don't run the demote pass for PTX","user":"U68A3ASP9","ts":"1618206477.185200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8Shj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"no, we don't run the demote pass for PTX"}]}]}],"thread_ts":"1618171453.184000","parent_user_id":"UDGT4PM41"},{"client_msg_id":"4667a04b-4d79-40fa-b4d8-c503fa5b1d86","type":"message","text":"so you really do get Float16:\n```julia&gt; function kernel(a)\n       @inbounds a[] += one(eltype(a))\n       return\n       end\nkernel (generic function with 1 method)\n\njulia&gt; @device_code_llvm debuginfo=:none @cuda kernel(CuArray{Float16}([1]))\n; PTX CompilerJob of kernel kernel(CuDeviceVector{Float16, 1}) for sm_75\ndefine ptx_kernel void @_Z17julia_kernel_355313CuDeviceArrayI7Float16Li1ELi1EE({ [1 x i64], i8 addrspace(1)* }* nocapture nonnull readonly byval align 8 dereferenceable(16) %0) local_unnamed_addr {\ntop:\n  %1 = getelementptr inbounds { [1 x i64], i8 addrspace(1)* }, { [1 x i64], i8 addrspace(1)* }* %0, i64 0, i32 1\n  %2 = bitcast i8 addrspace(1)** %1 to half addrspace(1)**\n  %3 = load half addrspace(1)*, half addrspace(1)** %2, align 8\n  %4 = load half, half addrspace(1)* %3, align 2\n  %5 = fadd half %4, 0xH3C00\n  store half %5, half addrspace(1)* %3, align 2\n  ret void\n}```","user":"U68A3ASP9","ts":"1618206488.185400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gAvtd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"so you really do get Float16:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> function kernel(a)\n       @inbounds a[] += one(eltype(a))\n       return\n       end\nkernel (generic function with 1 method)\n\njulia> @device_code_llvm debuginfo=:none @cuda kernel(CuArray{Float16}([1]))\n; PTX CompilerJob of kernel kernel(CuDeviceVector{Float16, 1}) for sm_75\ndefine ptx_kernel void @_Z17julia_kernel_355313CuDeviceArrayI7Float16Li1ELi1EE({ [1 x i64], i8 addrspace(1)* }* nocapture nonnull readonly byval align 8 dereferenceable(16) %0) local_unnamed_addr {\ntop:\n  %1 = getelementptr inbounds { [1 x i64], i8 addrspace(1)* }, { [1 x i64], i8 addrspace(1)* }* %0, i64 0, i32 1\n  %2 = bitcast i8 addrspace(1)** %1 to half addrspace(1)**\n  %3 = load half addrspace(1)*, half addrspace(1)** %2, align 8\n  %4 = load half, half addrspace(1)* %3, align 2\n  %5 = fadd half %4, 0xH3C00\n  store half %5, half addrspace(1)* %3, align 2\n  ret void\n}"}]}]}],"thread_ts":"1618171453.184000","parent_user_id":"UDGT4PM41"},{"client_msg_id":"390f0e38-59e2-4be8-ba4c-a919cd5a224f","type":"message","text":"Oh that's why we split it it o two pipeline steps. Forgotten about that","user":"U67BJLYCS","ts":"1618234822.187500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WwL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oh that's why we split it it o two pipeline steps. Forgotten about that"}]}]}],"thread_ts":"1618171453.184000","parent_user_id":"UDGT4PM41"}]