[{"client_msg_id":"1497e32b-9874-4695-a33f-1900f439e0b4","type":"message","text":"Some Googling suggests that (at least as of May 2018) people were having trouble getting CUBLAS to use Tensor Cores for Int8 GEMM: <https://forums.developer.nvidia.com/t/cublas-int8-tensor-core-mode-vs-fp16-mode/70510>","user":"U7THT3TM3","ts":"1612823218.140900","team":"T68168MUP","attachments":[{"service_name":"NVIDIA Developer Forums","title":"cuBLAS INT8 tensor core mode vs. FP16 mode","title_link":"https://forums.developer.nvidia.com/t/cublas-int8-tensor-core-mode-vs-fp16-mode/70510","text":"Hi all, I recently acquired an RTX card and was testing the new INT8 tensor core mode supported by Turing. I put together a simple test program (based on the “Programming Tensor Cores” devblogs article) to compare the execution times of INT8 mode vs. FP16 mode using the tensor cores. Strangely the execution times of tensor-FP16 mode and tensor-INT8 mode are practically the same. I was expecting much better execution times for tensor-INT8 mode since it’s supposed to have nearly twice the through...","fallback":"NVIDIA Developer Forums: cuBLAS INT8 tensor core mode vs. FP16 mode","thumb_url":"https://aws1.discourse-cdn.com/nvidia/original/2X/8/8f17cc8f1a724d6ecea8a197a267ec8a05ef1490.png","ts":1550251569,"from_url":"https://forums.developer.nvidia.com/t/cublas-int8-tensor-core-mode-vs-fp16-mode/70510","thumb_width":237,"thumb_height":42,"service_icon":"https://aws1.discourse-cdn.com/nvidia/optimized/2X/8/819b2855e1f1f3249e77dc713405cf77d1eda57c_2_180x180.png","id":1,"original_url":"https://forums.developer.nvidia.com/t/cublas-int8-tensor-core-mode-vs-fp16-mode/70510"}],"blocks":[{"type":"rich_text","block_id":"Qnl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Some Googling suggests that (at least as of May 2018) people were having trouble getting CUBLAS to use Tensor Cores for Int8 GEMM: "},{"type":"link","url":"https://forums.developer.nvidia.com/t/cublas-int8-tensor-core-mode-vs-fp16-mode/70510"}]}]}],"thread_ts":"1612823218.140900","reply_count":1,"reply_users_count":1,"latest_reply":"1612823809.141800","reply_users":["U01C67AS6F7"],"subscribed":false,"reactions":[{"name":"thumbsup_all","users":["U01C67AS6F7"],"count":1}]},{"client_msg_id":"f1760bd7-151b-4d81-80a4-ce0fecce501f","type":"message","text":"I would think something would have been added since then. I'd be somewhat surprised if that's not the case.","user":"U01C67AS6F7","ts":"1612823809.141800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yKyrt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I would think something would have been added since then. I'd be somewhat surprised if that's not the case."}]}]}],"thread_ts":"1612823218.140900","parent_user_id":"U7THT3TM3"}]