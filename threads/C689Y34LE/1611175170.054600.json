[{"client_msg_id":"cca8e87b-2874-48d1-a97c-e6898eb25a81","type":"message","text":"I'm wanting to run a RL algorithm (PPO), where the data gathering step is run on multiple processes to speed it up. However, if I try to run the inference step for a batch of states (ie `actions = model[states]` ) on the gpu, I get this error:\n```CUBLASError: the GPU program failed to execute (code 13, CUBLAS_STATUS_EXECUTION_FAILED)```\nDoes anyone know the reason for this error, and how I could get around it? I have tried making a copy of the model for each process, but this doesn't seem to make a difference.","user":"UQR5DTD99","ts":"1611175170.054600","team":"T68168MUP","edited":{"user":"UQR5DTD99","ts":"1611175246.000000"},"blocks":[{"type":"rich_text","block_id":"EbW9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm wanting to run a RL algorithm (PPO), where the data gathering step is run on multiple processes to speed it up. However, if I try to run the inference step for a batch of states (ie "},{"type":"text","text":"actions = model[states]","style":{"code":true}},{"type":"text","text":" ) on the gpu, I get this error:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"CUBLASError: the GPU program failed to execute (code 13, CUBLAS_STATUS_EXECUTION_FAILED)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Does anyone know the reason for this error, and how I could get around it? I have tried making a copy of the model for each process, but this doesn't seem to make a difference."}]}]}],"thread_ts":"1611175170.054600","reply_count":8,"reply_users_count":3,"latest_reply":"1611252041.001900","reply_users":["U68A3ASP9","UQR5DTD99","UC4QQPG4A"],"subscribed":false},{"client_msg_id":"eee9649a-9273-47fe-b6ab-7921442483f6","type":"message","text":"that's probably a bug in CUDA.jl, it shouldn't be possible to invoke CUBLAS incorrectly. I guess it'd be hard to reduce to something minimal?","user":"U68A3ASP9","ts":"1611175790.054900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"A8eS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"that's probably a bug in CUDA.jl, it shouldn't be possible to invoke CUBLAS incorrectly. I guess it'd be hard to reduce to something minimal?"}]}]}],"thread_ts":"1611175170.054600","parent_user_id":"UQR5DTD99"},{"client_msg_id":"c6c25cf6-f4e8-4561-88d4-4f62e08d573c","type":"message","text":"you can also try running with `JULIA_DEBUG=CUBLAS` to see which of the API calls is invoked incorrectly, and hopefully be able to see which argument that is","user":"U68A3ASP9","ts":"1611175816.055100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iWLx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you can also try running with "},{"type":"text","text":"JULIA_DEBUG=CUBLAS","style":{"code":true}},{"type":"text","text":" to see which of the API calls is invoked incorrectly, and hopefully be able to see which argument that is"}]}]}],"thread_ts":"1611175170.054600","parent_user_id":"UQR5DTD99"},{"client_msg_id":"064a4059-087b-434b-86c4-de8ab4030a42","type":"message","text":"do file an issue if you find more info about the issue","user":"U68A3ASP9","ts":"1611175829.055300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IsDp7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"do file an issue if you find more info about the issue"}]}]}],"thread_ts":"1611175170.054600","parent_user_id":"UQR5DTD99"},{"client_msg_id":"bb1f3a3b-bf3c-4cbd-baa2-9443cadce763","type":"message","text":"```ERROR: LoadError: TaskFailedException:\nOn worker 3:\nCUBLASError: the GPU program failed to execute (code 13, CUBLAS_STATUS_EXECUTION_FAILED)\nthrow_api_error at C:\\Users\\benmc\\.julia\\packages\\CUDA\\dZvbp\\lib\\cublas\\error.jl:47\nmacro expansion at C:\\Users\\benmc\\.julia\\packages\\CUDA\\dZvbp\\lib\\cublas\\error.jl:58 [inlined]\ncublasSgemm_v2 at C:\\Users\\benmc\\.julia\\packages\\CUDA\\dZvbp\\lib\\utils\\call.jl:93\ngemm! at C:\\Users\\benmc\\.julia\\packages\\CUDA\\dZvbp\\lib\\cublas\\wrappers.jl:721\ngemm_wrapper! at C:\\Users\\benmc\\.julia\\packages\\CUDA\\dZvbp\\lib\\cublas\\linalg.jl:188\nmul! at C:\\Users\\benmc\\.julia\\packages\\CUDA\\dZvbp\\lib\\cublas\\linalg.jl:222 [inlined]\nmul! at D:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\LinearAlgebra\\src\\matmul.jl:208 [inlined]\n* at D:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\LinearAlgebra\\src\\matmul.jl:160 [inlined]\nDense at C:\\Users\\benmc\\.julia\\packages\\Flux\\05b38\\src\\layers\\basic.jl:123 [inlined]\nDense at C:\\Users\\benmc\\.julia\\packages\\Flux\\05b38\\src\\layers\\basic.jl:134 [inlined]\napplychain at C:\\Users\\benmc\\.julia\\packages\\Flux\\05b38\\src\\layers\\basic.jl:36\nChain at C:\\Users\\benmc\\.julia\\packages\\Flux\\05b38\\src\\layers\\basic.jl:38\nget_parallel_batch at C:\\prestart-agent\\training\\multiproc-train.jl:331\nmacro expansion at C:\\prestart-agent\\training\\multiproc-train.jl:437 [inlined]\n#3 at D:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\Distributed\\src\\macros.jl:301\n#160 at D:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\Distributed\\src\\macros.jl:87\n#103 at D:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\Distributed\\src\\process_messages.jl:290\nrun_work_thunk at D:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\Distributed\\src\\process_messages.jl:79\nrun_work_thunk at D:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\Distributed\\src\\process_messages.jl:88\n#96 at .\\task.jl:356\nStacktrace:\n [1] sync_end(::Channel{Any}) at .\\task.jl:314\n [2] (::Distributed.var\"#159#161\"{Main.TrainAgent.var\"#3#4\"{String,Chain{Tuple{Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(identity),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}}}},Array{Float32,1},Array{Float32,1},Array{Float32,2},Int64,SharedArrays.SharedArray{Float32,2},SharedArrays.SharedArray{Float32,2},SharedArrays.SharedArray{Float32,1},SharedArrays.SharedArray{Float32,1},SharedArrays.SharedArray{Float32,1},SharedArrays.SharedArray{Float32,2},SharedArrays.SharedArray{Int64,1}},UnitRange{Int64}})() at .\\task.jl:333\n [1] sync_end(::Channel{Any}) at .\\task.jl:314\n [2] macro expansion at .\\task.jl:333 [inlined]\n [3] get_batch(::String, ::Chain{Tuple{Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(identity),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}}}}, ::Array{Float32,1}, ::Array{Float32,1}, ::Array{Float32,1}, ::Array{Float32,2}, ::Array{Float32,2}) at C:\\prestart-agent\\training\\multiproc-train.jl:435\n [4] train(::String, ::String) at C:\\prestart-agent\\training\\multiproc-train.jl:536\n [5] top-level scope at C:\\prestart-agent\\training\\multiproc-main.jl:18\n [6] include(::Function, ::Module, ::String) at .\\Base.jl:380\n [7] include(::Module, ::String) at .\\Base.jl:368\n [8] exec_options(::Base.JLOptions) at .\\client.jl:296\n [9] _start() at .\\client.jl:506\nin expression starting at C:\\prestart-agent\\training\\multiproc-main.jl:18```","user":"UQR5DTD99","ts":"1611179555.055500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uCRjF","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: LoadError: TaskFailedException:\nOn worker 3:\nCUBLASError: the GPU program failed to execute (code 13, CUBLAS_STATUS_EXECUTION_FAILED)\nthrow_api_error at C:\\Users\\benmc\\.julia\\packages\\CUDA\\dZvbp\\lib\\cublas\\error.jl:47\nmacro expansion at C:\\Users\\benmc\\.julia\\packages\\CUDA\\dZvbp\\lib\\cublas\\error.jl:58 [inlined]\ncublasSgemm_v2 at C:\\Users\\benmc\\.julia\\packages\\CUDA\\dZvbp\\lib\\utils\\call.jl:93\ngemm! at C:\\Users\\benmc\\.julia\\packages\\CUDA\\dZvbp\\lib\\cublas\\wrappers.jl:721\ngemm_wrapper! at C:\\Users\\benmc\\.julia\\packages\\CUDA\\dZvbp\\lib\\cublas\\linalg.jl:188\nmul! at C:\\Users\\benmc\\.julia\\packages\\CUDA\\dZvbp\\lib\\cublas\\linalg.jl:222 [inlined]\nmul! at D:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\LinearAlgebra\\src\\matmul.jl:208 [inlined]\n* at D:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\LinearAlgebra\\src\\matmul.jl:160 [inlined]\nDense at C:\\Users\\benmc\\.julia\\packages\\Flux\\05b38\\src\\layers\\basic.jl:123 [inlined]\nDense at C:\\Users\\benmc\\.julia\\packages\\Flux\\05b38\\src\\layers\\basic.jl:134 [inlined]\napplychain at C:\\Users\\benmc\\.julia\\packages\\Flux\\05b38\\src\\layers\\basic.jl:36\nChain at C:\\Users\\benmc\\.julia\\packages\\Flux\\05b38\\src\\layers\\basic.jl:38\nget_parallel_batch at C:\\prestart-agent\\training\\multiproc-train.jl:331\nmacro expansion at C:\\prestart-agent\\training\\multiproc-train.jl:437 [inlined]\n#3 at D:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\Distributed\\src\\macros.jl:301\n#160 at D:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\Distributed\\src\\macros.jl:87\n#103 at D:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\Distributed\\src\\process_messages.jl:290\nrun_work_thunk at D:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\Distributed\\src\\process_messages.jl:79\nrun_work_thunk at D:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\Distributed\\src\\process_messages.jl:88\n#96 at .\\task.jl:356\nStacktrace:\n [1] sync_end(::Channel{Any}) at .\\task.jl:314\n [2] (::Distributed.var\"#159#161\"{Main.TrainAgent.var\"#3#4\"{String,Chain{Tuple{Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(identity),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}}}},Array{Float32,1},Array{Float32,1},Array{Float32,2},Int64,SharedArrays.SharedArray{Float32,2},SharedArrays.SharedArray{Float32,2},SharedArrays.SharedArray{Float32,1},SharedArrays.SharedArray{Float32,1},SharedArrays.SharedArray{Float32,1},SharedArrays.SharedArray{Float32,2},SharedArrays.SharedArray{Int64,1}},UnitRange{Int64}})() at .\\task.jl:333\n [1] sync_end(::Channel{Any}) at .\\task.jl:314\n [2] macro expansion at .\\task.jl:333 [inlined]\n [3] get_batch(::String, ::Chain{Tuple{Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(tanh),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}},Dense{typeof(identity),CUDA.CuArray{Float32,2},CUDA.CuArray{Float32,1}}}}, ::Array{Float32,1}, ::Array{Float32,1}, ::Array{Float32,1}, ::Array{Float32,2}, ::Array{Float32,2}) at C:\\prestart-agent\\training\\multiproc-train.jl:435\n [4] train(::String, ::String) at C:\\prestart-agent\\training\\multiproc-train.jl:536\n [5] top-level scope at C:\\prestart-agent\\training\\multiproc-main.jl:18\n [6] include(::Function, ::Module, ::String) at .\\Base.jl:380\n [7] include(::Module, ::String) at .\\Base.jl:368\n [8] exec_options(::Base.JLOptions) at .\\client.jl:296\n [9] _start() at .\\client.jl:506\nin expression starting at C:\\prestart-agent\\training\\multiproc-main.jl:18"}]}]}],"thread_ts":"1611175170.054600","parent_user_id":"UQR5DTD99"},{"client_msg_id":"bb1f3a3b-bf3c-4cbd-baa2-9443cadce763","type":"message","text":"","user":"UQR5DTD99","ts":"1611179555.055700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uCRjF-W2L","elements":[{"type":"rich_text_section","elements":[]}]}],"thread_ts":"1611175170.054600","parent_user_id":"UQR5DTD99"},{"client_msg_id":"82009bb2-942f-46ad-b89d-8f2256fb855c","type":"message","text":"^ this is the error I get. It doesn't occur immediately, but only after the GPU memory fills up","user":"UQR5DTD99","ts":"1611179576.055900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Lix5s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"^ this is the error I get. It doesn't occur immediately, but only after the GPU memory fills up"}]}]}],"thread_ts":"1611175170.054600","parent_user_id":"UQR5DTD99"},{"client_msg_id":"4293482a-b85e-4e88-a47f-20921853e97d","type":"message","text":"that's not a debug log","user":"U68A3ASP9","ts":"1611214111.000900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fSw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"that's not a debug log"}]}]}],"thread_ts":"1611175170.054600","parent_user_id":"UQR5DTD99"},{"client_msg_id":"7f3255e9-7728-4f42-a7e3-347cc32415b3","type":"message","text":"Could you point at the model implementation you are running?","user":"UC4QQPG4A","ts":"1611252041.001900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=pW2J","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Could you point at the model implementation you are running?"}]}]}],"thread_ts":"1611175170.054600","parent_user_id":"UQR5DTD99"}]