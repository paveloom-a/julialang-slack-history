[{"client_msg_id":"309b053d-e9e0-4189-b492-60c813d92ece","type":"message","text":"Does anyone know how `CUDA.jl` handles shared memory between GPU and CPU? I have the suspicion that it ignores that it is shared, thereby allocating more memory than actually is free and crashing. Unfortunately, I’m not expert enough to be able to prove that somehow…","user":"UNZKG0909","ts":"1611688634.053900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Et0=F","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does anyone know how "},{"type":"text","text":"CUDA.jl","style":{"code":true}},{"type":"text","text":" handles shared memory between GPU and CPU? I have the suspicion that it ignores that it is shared, thereby allocating more memory than actually is free and crashing. Unfortunately, I’m not expert enough to be able to prove that somehow…"}]}]}],"thread_ts":"1611688634.053900","reply_count":9,"reply_users_count":3,"latest_reply":"1611733958.055900","reply_users":["U6A0PD8CR","UNZKG0909","U68A3ASP9"],"subscribed":false},{"client_msg_id":"9861851e-6b41-42ec-9b77-8f8c589caf1a","type":"message","text":"Do you mean shared memory or unified memory?","user":"U6A0PD8CR","ts":"1611688788.054000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=s5W","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do you mean shared memory or unified memory?"}]}]}],"thread_ts":"1611688634.053900","parent_user_id":"UNZKG0909"},{"client_msg_id":"0d103ebf-7074-47a6-9c44-5238eef5982f","type":"message","text":"They're two very different things","user":"U6A0PD8CR","ts":"1611688795.054200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ZVvnN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"They're two very different things"}]}]}],"thread_ts":"1611688634.053900","parent_user_id":"UNZKG0909"},{"client_msg_id":"ad25eb07-6c84-455b-bfc7-7ed4a3ba4232","type":"message","text":"Good question - I'm actually not sure. I am talking about a Jetson Nano development board that is having 4GB of memory. Both the CPU and GPU use that memory, so one of these units can only use what the other left of it.","user":"UNZKG0909","ts":"1611691098.054400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"T3Oe7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Good question - I'm actually not sure. I am talking about a Jetson Nano development board that is having 4GB of memory. Both the CPU and GPU use that memory, so one of these units can only use what the other left of it."}]}]}],"thread_ts":"1611688634.053900","parent_user_id":"UNZKG0909"},{"client_msg_id":"3ac229ac-5a70-45f0-ab07-1b279eb69ce3","type":"message","text":"CUDA.jl just allocates using CUDA's default allocator. we currently don't attempt to create managed/unified allocations that are shared between CPU and GPU, even on systems where the hardware is actually shared","user":"U68A3ASP9","ts":"1611694863.054800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2gE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CUDA.jl just allocates using CUDA's default allocator. we currently don't attempt to create managed/unified allocations that are shared between CPU and GPU, even on systems where the hardware is actually shared"}]}]}],"thread_ts":"1611688634.053900","parent_user_id":"UNZKG0909"},{"client_msg_id":"d842311d-9fd2-44e9-a9be-5fb43a21d583","type":"message","text":"that's somewhat wasteful in your case because you need to allocate a CPU array to transfer data back from the GPU whereas that's a \"free\" operation on that specific hardware.","user":"U68A3ASP9","ts":"1611694940.055000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uoTr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"that's somewhat wasteful in your case because you need to allocate a CPU array to transfer data back from the GPU whereas that's a \"free\" operation on that specific hardware."}]}]}],"thread_ts":"1611688634.053900","parent_user_id":"UNZKG0909"},{"client_msg_id":"ddc6e551-46af-491b-b98c-d6303be1b404","type":"message","text":"it would be good to support unified memory. in the mean time, you can create unified buffers yourself and \"wrap\" a CuArray or Array around it. it's a bit cumbersome though. e.g. <https://juliagpu.github.io/CUDA.jl/stable/usage/multigpu/#Scenario-2:-Multiple-GPUs-per-process>, for Array you'd `convert` to a `Ptr` and use `unsafe_wrap(Array, ..)`","user":"U68A3ASP9","ts":"1611695032.055200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"XiFL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it would be good to support unified memory. in the mean time, you can create unified buffers yourself and \"wrap\" a CuArray or Array around it. it's a bit cumbersome though. e.g. "},{"type":"link","url":"https://juliagpu.github.io/CUDA.jl/stable/usage/multigpu/#Scenario-2:-Multiple-GPUs-per-process"},{"type":"text","text":", for Array you'd "},{"type":"text","text":"convert","style":{"code":true}},{"type":"text","text":" to a "},{"type":"text","text":"Ptr","style":{"code":true}},{"type":"text","text":" and use "},{"type":"text","text":"unsafe_wrap(Array, ..)","style":{"code":true}}]}]}],"thread_ts":"1611688634.053900","parent_user_id":"UNZKG0909"},{"client_msg_id":"1410f947-bb23-41e7-ba7e-8011111db2ea","type":"message","text":"What about garbage collection, is that also coming from CUDA itself? What I believe I see is that on a GPU with separate memory of 4GB, my program allocates 300MB per iteration. At about 3.7GB, GC kicks in and frees memory again. On the Nano, only about 2GB of memory are actually free. The program still allocates 300MB per iteration, but near the limit no memory is freed, and at one point it tries to allocate memory that is not there and crashes.","user":"UNZKG0909","ts":"1611727374.055400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"p=48I","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What about garbage collection, is that also coming from CUDA itself? What I believe I see is that on a GPU with separate memory of 4GB, my program allocates 300MB per iteration. At about 3.7GB, GC kicks in and frees memory again. On the Nano, only about 2GB of memory are actually free. The program still allocates 300MB per iteration, but near the limit no memory is freed, and at one point it tries to allocate memory that is not there and crashes."}]}]}],"thread_ts":"1611688634.053900","parent_user_id":"UNZKG0909"},{"client_msg_id":"a8781f09-429c-49e3-9f32-8e8da6929cac","type":"message","text":"We use Julia GC. so it's likely some of your data is still reachable, and as such can't be freed.","user":"U68A3ASP9","ts":"1611733942.055700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8kuT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We use Julia GC. so it's likely some of your data is still reachable, and as such can't be freed."}]}]}],"thread_ts":"1611688634.053900","parent_user_id":"UNZKG0909"},{"client_msg_id":"b63e86e9-14b8-4380-a452-fc3a2babd3fc","type":"message","text":"when you allocate unified memory as in the docs I listed, you are responsible for freeing it yourself.","user":"U68A3ASP9","ts":"1611733958.055900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ket","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"when you allocate unified memory as in the docs I listed, you are responsible for freeing it yourself."}]}]}],"thread_ts":"1611688634.053900","parent_user_id":"UNZKG0909"}]