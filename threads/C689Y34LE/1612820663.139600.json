[{"client_msg_id":"e1b199d1-9696-45da-91ba-26d5b299ddcc","type":"message","text":"I'm working to benchmark basic MMA calculations for varying input types and whether or not tensor cores are available. However, I'm struggling to determine when a CUBLAS gemmEx function is utilizing tensor cores for integer computations. When I run Float32 matrix calculations, there is an expected ~3.6x increase when allowing for tensor core usage vs when the pedantic math flag is used. For Int8 calculations though, the runtime is nearly identical. I thought CUBLAS supported dispatching appropriate Int8 matrix multiplications to tensor cores. Is this not the case? Can anyone point out some error I'm making?\n\nI've also somewhat looked into what's required to add Integer support into GemmKernels by wrapping the PTX instructions for each possible type and matrix size combos. However, if I were to continue the way all the PTX instructions are wrapped with intrinsics, the Turing/Ampere tensor core matrix size and type combos would make the work difficult. Generating the possible combos while not trying to generate invalid combos seems like it would lead to a large amount of if statements. Could the system be revamped to be a list of all possibilities rather than nested iterations through type and sizes? This also seems somewhat redundant. Perhaps when I get these benchmarks working, I'll start honest work on that addition, but per this thread (<https://github.com/JuliaGPU/GemmKernels.jl/issues/64>), maybe that's not worth the work if some other method is preferred. <@U68A3ASP9> <@UPMLA9F9S> Any thoughts?\n\nThe end goal for my work is to create a real-time correlator for a radio astronomy array. This involves large amounts of correlation on Complex Int8 data. We're looking for any way to accelerate these computations and see tensor cores as a promising route. I'm trying to come up with initial benchmarks to assess feasibility.\n\nCUDA versioninfo() output:\n```julia&gt; CUDA.versioninfo()\nCUDA toolkit 11.1.1, artifact installation\nCUDA driver 11.1.0\nNVIDIA driver 455.23.4\n\nLibraries: \n- CUBLAS: 11.3.0\n- CURAND: 10.2.2\n- CUFFT: 10.3.0\n- CUSOLVER: 11.0.1\n- CUSPARSE: 11.3.0\n- CUPTI: 14.0.0\n- NVML: 11.0.0+455.23.4\n- CUDNN: 8.0.4 (for CUDA 11.1.0)\n- CUTENSOR: 1.2.1 (for CUDA 11.1.0)\n\nToolchain:\n- Julia: 1.5.3\n- LLVM: 9.0.1\n- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4\n- Device support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75\n\n2 devices:\n  0: GeForce RTX 3090 (sm_86, 22.463 GiB / 23.700 GiB available)\n  1: GeForce GTX 1080 Ti (sm_61, 10.914 GiB / 10.917 GiB available)```\n\nMost relevant code:\n```# CUBLAS math flags:\n# CUBLAS.cublasSetMathMode(CUBLAS.handle(), CUBLAS.CUBLAS_PEDANTIC_MATH)\n# CUBLAS.cublasSetMathMode(CUBLAS.handle(), CUBLAS.CUBLAS_TENSOR_OP_MATH)\n\n# Test integer tensor core speedup\nfunction test_mma_cublas_i8(_size)\n    a     = rand(Int8, _size)\n    b     = rand(Int8, _size)\n    c     = rand(Int32, _size)\n\n    a_dev = CuArray(a)\n    b_dev = CuArray(b)\n    c_dev = CuArray(c)\n    d_dev = similar(c_dev)\n\n    CUDA.CUBLAS.gemmEx!('N', 'N', 1, a_dev, b_dev, 1, c_dev)\n\n    bench = @benchmark CUDA.@sync blocking=false CUDA.CUBLAS.gemmEx!($'N', $'N', $1, $a_dev, $b_dev, $1, $c_dev)\nend```\n\nFull benchmarking code:\n","user":"U01C67AS6F7","ts":"1612820663.139600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DI7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm working to benchmark basic MMA calculations for varying input types and whether or not tensor cores are available. However, I'm struggling to determine when a CUBLAS gemmEx function is utilizing tensor cores for integer computations. When I run Float32 matrix calculations, there is an expected ~3.6x increase when allowing for tensor core usage vs when the pedantic math flag is used. For Int8 calculations though, the runtime is nearly identical. I thought CUBLAS supported dispatching appropriate Int8 matrix multiplications to tensor cores. Is this not the case? Can anyone point out some error I'm making?\n\nI've also somewhat looked into what's required to add Integer support into GemmKernels by wrapping the PTX instructions for each possible type and matrix size combos. However, if I were to continue the way all the PTX instructions are wrapped with intrinsics, the Turing/Ampere tensor core matrix size and type combos would make the work difficult. Generating the possible combos while not trying to generate invalid combos seems like it would lead to a large amount of if statements. Could the system be revamped to be a list of all possibilities rather than nested iterations through type and sizes? This also seems somewhat redundant. Perhaps when I get these benchmarks working, I'll start honest work on that addition, but per this thread ("},{"type":"link","url":"https://github.com/JuliaGPU/GemmKernels.jl/issues/64"},{"type":"text","text":"), maybe that's not worth the work if some other method is preferred. "},{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":" "},{"type":"user","user_id":"UPMLA9F9S"},{"type":"text","text":" Any thoughts?\n\nThe end goal for my work is to create a real-time correlator for a radio astronomy array. This involves large amounts of correlation on Complex Int8 data. We're looking for any way to accelerate these computations and see tensor cores as a promising route. I'm trying to come up with initial benchmarks to assess feasibility.\n\nCUDA versioninfo() output:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> CUDA.versioninfo()\nCUDA toolkit 11.1.1, artifact installation\nCUDA driver 11.1.0\nNVIDIA driver 455.23.4\n\nLibraries: \n- CUBLAS: 11.3.0\n- CURAND: 10.2.2\n- CUFFT: 10.3.0\n- CUSOLVER: 11.0.1\n- CUSPARSE: 11.3.0\n- CUPTI: 14.0.0\n- NVML: 11.0.0+455.23.4\n- CUDNN: 8.0.4 (for CUDA 11.1.0)\n- CUTENSOR: 1.2.1 (for CUDA 11.1.0)\n\nToolchain:\n- Julia: 1.5.3\n- LLVM: 9.0.1\n- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4\n- Device support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75\n\n2 devices:\n  0: GeForce RTX 3090 (sm_86, 22.463 GiB / 23.700 GiB available)\n  1: GeForce GTX 1080 Ti (sm_61, 10.914 GiB / 10.917 GiB available)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\n\nMost relevant code:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"# CUBLAS math flags:\n# CUBLAS.cublasSetMathMode(CUBLAS.handle(), CUBLAS.CUBLAS_PEDANTIC_MATH)\n# CUBLAS.cublasSetMathMode(CUBLAS.handle(), CUBLAS.CUBLAS_TENSOR_OP_MATH)\n\n# Test integer tensor core speedup\nfunction test_mma_cublas_i8(_size)\n    a     = rand(Int8, _size)\n    b     = rand(Int8, _size)\n    c     = rand(Int32, _size)\n\n    a_dev = CuArray(a)\n    b_dev = CuArray(b)\n    c_dev = CuArray(c)\n    d_dev = similar(c_dev)\n\n    CUDA.CUBLAS.gemmEx!('N', 'N', 1, a_dev, b_dev, 1, c_dev)\n\n    bench = @benchmark CUDA.@sync blocking=false CUDA.CUBLAS.gemmEx!($'N', $'N', $1, $a_dev, $b_dev, $1, $c_dev)\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\n\nFull benchmarking code:\n"}]}]}],"thread_ts":"1612820663.139600","reply_count":2,"reply_users_count":2,"latest_reply":"1612890657.145400","reply_users":["UPMLA9F9S","U01C67AS6F7"],"subscribed":false,"reactions":[{"name":"+1","users":["UPMLA9F9S","U7THT3TM3"],"count":2}]},{"client_msg_id":"860c276b-bb0f-480c-9c6d-7f1914e0e8d2","type":"message","text":"&gt; However, I'm struggling to determine when a CUBLAS gemmEx function is utilizing tensor cores for integer computations.\nOne way to do this is to run Julia under NVIDIA Nsight Compute, and checking whether the executed kernel executes `HMMA.*` SASS instructions. There's also a category for resource usage, which lists Tensor Core usage, I believe.\n\n&gt; However, if I were to continue the way all the PTX instructions are wrapped with intrinsics, the Turing/Ampere tensor core matrix size and type combos would make the work difficult. Generating the possible combos while not trying to generate invalid combos seems like it would lead to a large amount of if statements. Could the system be revamped to be a list of all possibilities rather than nested iterations through type and sizes?\nI would say that depends on how \"sparse\" the matrix of possibilities for type/size combinations is. If there are only a few combinations allowed, it would definitely be better to just use a single list of allowed type-size combinations, rather than a separate type and size list + a large number of if statements.","user":"UPMLA9F9S","ts":"1612859869.143600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OrJx","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"However, I'm struggling to determine when a CUBLAS gemmEx function is utilizing tensor cores for integer computations."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"One way to do this is to run Julia under NVIDIA Nsight Compute, and checking whether the executed kernel executes "},{"type":"text","text":"HMMA.*","style":{"code":true}},{"type":"text","text":" SASS instructions. There's also a category for resource usage, which lists Tensor Core usage, I believe.\n\n"}]},{"type":"rich_text_quote","elements":[{"type":"text","text":"However, if I were to continue the way all the PTX instructions are wrapped with intrinsics, the Turing/Ampere tensor core matrix size and type combos would make the work difficult. Generating the possible combos while not trying to generate invalid combos seems like it would lead to a large amount of if statements. Could the system be revamped to be a list of all possibilities rather than nested iterations through type and sizes?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I would say that depends on how \"sparse\" the matrix of possibilities for type/size combinations is. If there are only a few combinations allowed, it would definitely be better to just use a single list of allowed type-size combinations, rather than a separate type and size list + a large number of if statements."}]}]}],"thread_ts":"1612820663.139600","parent_user_id":"U01C67AS6F7"},{"client_msg_id":"1705825E-5FD5-4118-9D3D-D754A5B26E7B","type":"message","text":"Thanks for the nsight compute suggestion. I’ll look into both suggestions this week.","user":"U01C67AS6F7","ts":"1612890657.145400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"itB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for the nsight compute suggestion. I’ll look into both suggestions this week."}]}]}],"thread_ts":"1612820663.139600","parent_user_id":"U01C67AS6F7"}]