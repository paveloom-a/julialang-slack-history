[{"client_msg_id":"ad6c9da5-65ad-458a-b9f9-6ea933eb15bd","type":"message","text":"A while ago I remember running across a paper where the goal was to learn an optimization algorithm, e.g. . a neural network that can be iteratively queried with f(x) and gradf(x) values to produce a series of solution estimates towards the optimum of f(x). It was then favorably compared to human-produced algorithms such as Adam or Adagrad. I cannot find it now, does anyone know which one I mean? And has anyone seen the same idea but for constrained optimization, where a constraint c(x) and its gradient is also supplied to an iterative algorithm we want to learn?","user":"UFCNUVC67","ts":"1608821900.159700","team":"T68168MUP","edited":{"user":"UFCNUVC67","ts":"1608821948.000000"},"blocks":[{"type":"rich_text","block_id":"/Ij","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"A while ago I remember running across a paper where the goal was to learn an optimization algorithm, e.g. . a neural network that can be iteratively queried with f(x) and gradf(x) values to produce a series of solution estimates towards the optimum of f(x). It was then favorably compared to human-produced algorithms such as Adam or Adagrad. I cannot find it now, does anyone know which one I mean? And has anyone seen the same idea but for constrained optimization, where a constraint c(x) and its gradient is also supplied to an iterative algorithm we want to learn?"}]}]}],"thread_ts":"1608821900.159700","reply_count":3,"reply_users_count":2,"latest_reply":"1608823996.160500","reply_users":["U6A936746","UFCNUVC67"],"subscribed":false},{"client_msg_id":"273387c0-f264-4dd1-b0f3-f80a6b289d34","type":"message","text":"That sounds like something from  meta-learning.\nI thought the paper was called \"learning to learn\" maybe","user":"U6A936746","ts":"1608822318.160000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MQ8w","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That sounds like something from  meta-learning.\nI thought the paper was called \"learning to learn\" maybe"}]}]}],"thread_ts":"1608821900.159700","parent_user_id":"UFCNUVC67","reactions":[{"name":"white_check_mark","users":["UFCNUVC67"],"count":1}]},{"client_msg_id":"7fb14442-d835-4249-b5db-ca0c247f3640","type":"message","text":"Ah yes, it was probably that paper \"Learning to learn by gradient descent by gradient descent\" to be exact. I will try to see if a constrained optimization version has been examined. Do you know of any papers in that direction?","user":"UFCNUVC67","ts":"1608822964.160300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PPJnD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah yes, it was probably that paper \"Learning to learn by gradient descent by gradient descent\" to be exact. I will try to see if a constrained optimization version has been examined. Do you know of any papers in that direction?"}]}]}],"thread_ts":"1608821900.159700","parent_user_id":"UFCNUVC67"},{"client_msg_id":"d72c6faf-83ed-4f86-8dae-cd777148f49b","type":"message","text":"nope, this is the full extent of my knowledge’","user":"U6A936746","ts":"1608823996.160500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yeVD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"nope, this is the full extent of my knowledge’"}]}]}],"thread_ts":"1608821900.159700","parent_user_id":"UFCNUVC67"}]