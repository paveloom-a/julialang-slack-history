[{"client_msg_id":"0d806891-8e69-4eb8-a418-8231b2ec106b","type":"message","text":"hello! (new to flux). for flux when trying to pass different parts of an input to different cells, does it need to be wrapped as a single chained model? (as opposed to being encoded during loss calculation) i currently get a dimension mismatch error (`ERROR: LoadError: DimensionMismatch(\"arrays could not be broadcast to a common size; got a dimension with lengths 12 and 16\")`) when trying to combine gru with dense layers. i have 2 grus (processing part of the input) + an unprocessed input being fed into a dense layer. loss calculation works fine, so i'm a little confused as to where this mismatch is coming from. i've tried to put something close to a minimum working example in the reply thread below.","user":"UPKKW4H7B","ts":"1615929368.006800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"A45s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hello! (new to flux). for flux when trying to pass different parts of an input to different cells, does it need to be wrapped as a single chained model? (as opposed to being encoded during loss calculation) i currently get a dimension mismatch error ("},{"type":"text","text":"ERROR: LoadError: DimensionMismatch(\"arrays could not be broadcast to a common size; got a dimension with lengths 12 and 16\")","style":{"code":true}},{"type":"text","text":") when trying to combine gru with dense layers. i have 2 grus (processing part of the input) + an unprocessed input being fed into a dense layer. loss calculation works fine, so i'm a little confused as to where this mismatch is coming from. i've tried to put something close to a minimum working example in the reply thread below."}]}]}],"thread_ts":"1615929368.006800","reply_count":1,"reply_users_count":1,"latest_reply":"1615929381.006900","reply_users":["UPKKW4H7B"],"subscribed":false},{"client_msg_id":"52c8fbf3-d16c-432d-b1ab-e5444086a687","type":"message","text":"```\ninput_size::Int64 = size(train_data_x, 1)\nbatch_size::Int64 = 16\nnumber_of_epochs::Int64 = 1000\nfeature_size::Int64 = 20\nrecurrent_output_size::Int64 = 4\ngru_1 = Flux.GRU(feature_size, recurrent_output_size)\ngru_2 = Flux.GRU(feature_size, recurrent_output_size)\nmlp_input_size::Int64 = recurrent_output_size*2 + 1\nmlp_neural_network_model = Flux.Chain(Flux.Dense(mlp_input_size, mlp_input_size, Flux.tanh),\n                                      Flux.AlphaDropout(0.5),\n                                      Flux.Dense(mlp_input_size, 1, Flux.sigmoid))\n\nfunction loss(x, y)\n    x_1_data = x[1:feature_size,:]\n    x_2_data = x[feature_size+1:feature_size*2,:]\n    dense_input = [gru_1(x_1_data); gru_2(x_2_data); permutedims(x[end,:])]\n    loss_value = Flux.Losses.binarycrossentropy(mlp_neural_network_model(dense_input)[1,:], y)\n\n    return loss_value\nend\nfunction callback()\n    @debug \"Loss callback\" loss(train_data_x, train_data_y)\nend\n\noptimizer = Flux.ADAGrad()\n_train_data = Flux.Data.DataLoader((train_data_x, train_data_y), batchsize=batch_size)\nFlux.@epochs number_of_epochs Flux.train!(loss,\n                                          Flux.params(gru_1, gru_2, mlp_neural_network_model),\n                                          _train_data,\n                                          optimizer,\n                                          cb=Flux.throttle(callback, 5))```","user":"UPKKW4H7B","ts":"1615929381.006900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"BfxuJ","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"\ninput_size::Int64 = size(train_data_x, 1)\nbatch_size::Int64 = 16\nnumber_of_epochs::Int64 = 1000\nfeature_size::Int64 = 20\nrecurrent_output_size::Int64 = 4\ngru_1 = Flux.GRU(feature_size, recurrent_output_size)\ngru_2 = Flux.GRU(feature_size, recurrent_output_size)\nmlp_input_size::Int64 = recurrent_output_size*2 + 1\nmlp_neural_network_model = Flux.Chain(Flux.Dense(mlp_input_size, mlp_input_size, Flux.tanh),\n                                      Flux.AlphaDropout(0.5),\n                                      Flux.Dense(mlp_input_size, 1, Flux.sigmoid))\n\nfunction loss(x, y)\n    x_1_data = x[1:feature_size,:]\n    x_2_data = x[feature_size+1:feature_size*2,:]\n    dense_input = [gru_1(x_1_data); gru_2(x_2_data); permutedims(x[end,:])]\n    loss_value = Flux.Losses.binarycrossentropy(mlp_neural_network_model(dense_input)[1,:], y)\n\n    return loss_value\nend\nfunction callback()\n    @debug \"Loss callback\" loss(train_data_x, train_data_y)\nend\n\noptimizer = Flux.ADAGrad()\n_train_data = Flux.Data.DataLoader((train_data_x, train_data_y), batchsize=batch_size)\nFlux.@epochs number_of_epochs Flux.train!(loss,\n                                          Flux.params(gru_1, gru_2, mlp_neural_network_model),\n                                          _train_data,\n                                          optimizer,\n                                          cb=Flux.throttle(callback, 5))"}]}]}],"thread_ts":"1615929368.006800","parent_user_id":"UPKKW4H7B"}]