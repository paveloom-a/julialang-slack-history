[{"client_msg_id":"dd1f97e2-1f26-4987-834d-69c6f75b6424","type":"message","text":"So, I'm trying to implement a GAN using Flux. I got the code from the model_zoo, and was adapting to make it similar to what I already had programmed in Python. But it seems that my Generator is not being trained properly. The generated images are just noise, while in Python, things seems to start \"moving\" after the first epoch. Here is the code for the generator network and training:\n```function Generator(latent_dim::Int = 100) # latent_dim is the size of the vector noise. The default is 100\n    return Chain(\n            Dense(latent_dim, 256,x-&gt;leakyrelu.(x, 0.2f0)),\n            Dense(256, 512,x-&gt;leakyrelu.(x, 0.2f0)),\n            Dense(512, 1024,x-&gt;leakyrelu.(x, 0.2f0)),\n            Dense(1024,784,x-&gt;σ.(x))\n            )\nend\ngenerator_loss(fake_output) = logitbinarycrossentropy(fake_output, 1)\nfunction train_generator!(gen, dscr, x, opt_gen, hparams)\n    noise = randn!(similar(x, (hparams.latent_dim, hparams.batch_size)))\n    ps = Flux.params(gen)    \n    # pullback(ps,value) returns ps and the gradient of ps, where loss = ps, and back(value) = ∇ps(value)\n    # Taking gradient\n    loss, back = Flux.pullback(ps) do \n        generator_loss(dscr(gen(noise))) # Thithe generator_loss\n    end\n    grad = back(1f0)\n    update!(opt_gen, ps, grad)\n    return loss\nend```\nAny ideas on why this might be happening? I'm training on MNIST using the 60.000 images.","user":"U01CMBH4MQE","ts":"1611838797.057400","team":"T68168MUP","edited":{"user":"U01CMBH4MQE","ts":"1611838828.000000"},"blocks":[{"type":"rich_text","block_id":"5ZKE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So, I'm trying to implement a GAN using Flux. I got the code from the model_zoo, and was adapting to make it similar to what I already had programmed in Python. But it seems that my Generator is not being trained properly. The generated images are just noise, while in Python, things seems to start \"moving\" after the first epoch. Here is the code for the generator network and training:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function Generator(latent_dim::Int = 100) # latent_dim is the size of the vector noise. The default is 100\n    return Chain(\n            Dense(latent_dim, 256,x->leakyrelu.(x, 0.2f0)),\n            Dense(256, 512,x->leakyrelu.(x, 0.2f0)),\n            Dense(512, 1024,x->leakyrelu.(x, 0.2f0)),\n            Dense(1024,784,x->σ.(x))\n            )\nend\ngenerator_loss(fake_output) = logitbinarycrossentropy(fake_output, 1)\nfunction train_generator!(gen, dscr, x, opt_gen, hparams)\n    noise = randn!(similar(x, (hparams.latent_dim, hparams.batch_size)))\n    ps = Flux.params(gen)    \n    # pullback(ps,value) returns ps and the gradient of ps, where loss = ps, and back(value) = ∇ps(value)\n    # Taking gradient\n    loss, back = Flux.pullback(ps) do \n        generator_loss(dscr(gen(noise))) # Thithe generator_loss\n    end\n    grad = back(1f0)\n    update!(opt_gen, ps, grad)\n    return loss\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Any ideas on why this might be happening? I'm training on MNIST using the 60.000 images."}]}]}],"thread_ts":"1611838797.057400","reply_count":1,"reply_users_count":1,"latest_reply":"1611861567.061000","reply_users":["UMY1LV01G"],"subscribed":false},{"client_msg_id":"44ace138-f541-4345-9f77-dea0898b32bf","type":"message","text":"How do the gradients look? I would compare magnitudes (e.g. norms) between Keras and Flux to make sure everything isn't just 0","user":"UMY1LV01G","ts":"1611861567.061000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ZOv9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How do the gradients look? I would compare magnitudes (e.g. norms) between Keras and Flux to make sure everything isn't just 0"}]}]}],"thread_ts":"1611838797.057400","parent_user_id":"U01CMBH4MQE"}]