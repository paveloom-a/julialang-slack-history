[{"client_msg_id":"794d6404-70c5-4009-8c6b-27cf235adb8c","type":"message","text":"<https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron>","user":"UH24GRBLL","ts":"1615373326.171300","team":"T68168MUP","attachments":[{"service_name":"The Verge","title":"OpenAI’s state-of-the-art machine vision AI is fooled by handwritten notes","title_link":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron","text":"Reading is believing.","fallback":"The Verge: OpenAI’s state-of-the-art machine vision AI is fooled by handwritten notes","image_url":"https://cdn.vox-cdn.com/thumbor/SArfDFeVMgOfre78OYFqZX3MjmI=/0x146:1196x772/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/22353458/apple_ipod_test.jpg","image_width":478,"image_height":250,"ts":1615211398,"from_url":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron","image_bytes":65169,"service_icon":"https://cdn.vox-cdn.com/uploads/chorus_asset/file/7395359/ios-icon.0.png","id":1,"original_url":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron"}],"blocks":[{"type":"rich_text","block_id":"n6Ti","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron"}]}]}],"thread_ts":"1615373326.171300","reply_count":17,"reply_users_count":7,"latest_reply":"1615391265.219000","reply_users":["UH24GRBLL","U7JQGPGCQ","U7HAYKY9X","UPUBAM63X","UC4QQPG4A","U017LQ3A59U","U017D621ELC"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"&gt; “We refer to these attacks as typographic attacks,” write OpenAI’s researchers in a blog post. “By exploiting the model’s ability to read text robustly, we find that even photographs of hand-written text can often fool the model.”\n","user":"UH24GRBLL","ts":"1615373372.171500","thread_ts":"1615373326.171300","root":{"client_msg_id":"794d6404-70c5-4009-8c6b-27cf235adb8c","type":"message","text":"<https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron>","user":"UH24GRBLL","ts":"1615373326.171300","team":"T68168MUP","attachments":[{"service_name":"The Verge","title":"OpenAI’s state-of-the-art machine vision AI is fooled by handwritten notes","title_link":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron","text":"Reading is believing.","fallback":"The Verge: OpenAI’s state-of-the-art machine vision AI is fooled by handwritten notes","image_url":"https://cdn.vox-cdn.com/thumbor/SArfDFeVMgOfre78OYFqZX3MjmI=/0x146:1196x772/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/22353458/apple_ipod_test.jpg","image_width":478,"image_height":250,"ts":1615211398,"from_url":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron","image_bytes":65169,"service_icon":"https://cdn.vox-cdn.com/uploads/chorus_asset/file/7395359/ios-icon.0.png","id":1,"original_url":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron"}],"blocks":[{"type":"rich_text","block_id":"n6Ti","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron"}]}]}],"thread_ts":"1615373326.171300","reply_count":17,"reply_users_count":7,"latest_reply":"1615391265.219000","reply_users":["UH24GRBLL","U7JQGPGCQ","U7HAYKY9X","UPUBAM63X","UC4QQPG4A","U017LQ3A59U","U017D621ELC"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"/hwU","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"“We refer to these attacks as typographic attacks,” write OpenAI’s researchers in a blog post. “By exploiting the model’s ability to read text robustly, we find that even photographs of hand-written text can often fool the model.”"}]},{"type":"rich_text_section","elements":[]}]}],"client_msg_id":"623ad1b9-479f-4213-be4f-aaf442899ec4"},{"type":"message","subtype":"thread_broadcast","text":"and the original source <https://openai.com/blog/multimodal-neurons/>","user":"UH24GRBLL","ts":"1615373481.171800","thread_ts":"1615373326.171300","root":{"client_msg_id":"794d6404-70c5-4009-8c6b-27cf235adb8c","type":"message","text":"<https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron>","user":"UH24GRBLL","ts":"1615373326.171300","team":"T68168MUP","attachments":[{"service_name":"The Verge","title":"OpenAI’s state-of-the-art machine vision AI is fooled by handwritten notes","title_link":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron","text":"Reading is believing.","fallback":"The Verge: OpenAI’s state-of-the-art machine vision AI is fooled by handwritten notes","image_url":"https://cdn.vox-cdn.com/thumbor/SArfDFeVMgOfre78OYFqZX3MjmI=/0x146:1196x772/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/22353458/apple_ipod_test.jpg","image_width":478,"image_height":250,"ts":1615211398,"from_url":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron","image_bytes":65169,"service_icon":"https://cdn.vox-cdn.com/uploads/chorus_asset/file/7395359/ios-icon.0.png","id":1,"original_url":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron"}],"blocks":[{"type":"rich_text","block_id":"n6Ti","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron"}]}]}],"thread_ts":"1615373326.171300","reply_count":17,"reply_users_count":7,"latest_reply":"1615391265.219000","reply_users":["UH24GRBLL","U7JQGPGCQ","U7HAYKY9X","UPUBAM63X","UC4QQPG4A","U017LQ3A59U","U017D621ELC"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"r1s4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"and the original source "},{"type":"link","url":"https://openai.com/blog/multimodal-neurons/"}]}]}],"client_msg_id":"e8261ed5-8875-4971-8413-e0a110a284dd","edited":{"user":"UH24GRBLL","ts":"1615373523.000000"}},{"client_msg_id":"8072a106-0262-44e9-aba5-6da835920778","type":"message","text":"Tbh I find this hugely impressive rather than a reason to ridicule the model - I always thought image recognitions was based on just comparing some picture with other labeled pictures, so to have an algorithm that goes beyond that to say \"hold on there's some text here, let's read that\" seems quite a feat to me","user":"U7JQGPGCQ","ts":"1615373499.172100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yIP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Tbh I find this hugely impressive rather than a reason to ridicule the model - I always thought image recognitions was based on just comparing some picture with other labeled pictures, so to have an algorithm that goes beyond that to say \"hold on there's some text here, let's read that\" seems quite a feat to me"}]}]}],"thread_ts":"1615373326.171300","parent_user_id":"UH24GRBLL"},{"client_msg_id":"87b3a185-be25-415a-aa1f-a89b4f449771","type":"message","text":"that it is","user":"UH24GRBLL","ts":"1615373514.172300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"n4C=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"that it is"}]}]}],"thread_ts":"1615373326.171300","parent_user_id":"UH24GRBLL"},{"client_msg_id":"356384dc-9527-47d6-ae0b-9e8b05464ad8","type":"message","text":"on the flipside, it really does mean that it has no concept of what an iPod is or looks like memorized","user":"UH24GRBLL","ts":"1615373542.172700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7phF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"on the flipside, it really does mean that it has no concept of what an iPod is or looks like memorized"}]}]}],"thread_ts":"1615373326.171300","parent_user_id":"UH24GRBLL"},{"client_msg_id":"f0ea46a3-f266-4144-b93d-5e52c41d71aa","type":"message","text":"there's no crosschecking with other, known knowledge","user":"UH24GRBLL","ts":"1615373565.172900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ib+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"there's no crosschecking with other, known knowledge"}]}]}],"thread_ts":"1615373326.171300","parent_user_id":"UH24GRBLL"},{"client_msg_id":"1ed00b35-3ab9-4843-a985-0bbf44c0a2a4","type":"message","text":"That's amazing. Have you heard about the \"bird or bicycle\" challenge? Or single-pixel attacks? Apparently, it's fairly easy to fool image captioning NNs by changing one single pixel. Conversely, it's basically impossible to make a NN that can distinguish between a bird and a bicycle with 100% accuracy","user":"U7HAYKY9X","ts":"1615375011.176000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"u6V","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That's amazing. Have you heard about the \"bird or bicycle\" challenge? Or single-pixel attacks? Apparently, it's fairly easy to fool image captioning NNs by changing one single pixel. Conversely, it's basically impossible to make a NN that can distinguish between a bird and a bicycle with 100% accuracy"}]}]}],"thread_ts":"1615373326.171300","parent_user_id":"UH24GRBLL"},{"client_msg_id":"35784a6b-0b4e-4000-a486-fc1f9dcbf354","type":"message","text":"yeah, I know about those","user":"UH24GRBLL","ts":"1615375435.176600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Bfpx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yeah, I know about those"}]}]}],"thread_ts":"1615373326.171300","parent_user_id":"UH24GRBLL"},{"client_msg_id":"923db631-30a6-4eaa-9f83-2dcea54c7907","type":"message","text":"the catch is that you have to basically train a NN to fool the NN","user":"UH24GRBLL","ts":"1615375449.176800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"t3=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"the catch is that you have to basically train a NN to fool the NN"}]}]}],"thread_ts":"1615373326.171300","parent_user_id":"UH24GRBLL"},{"client_msg_id":"e46b15f6-0a4e-43c9-9080-62aedb0ccdc9","type":"message","text":"this is much simpler","user":"UH24GRBLL","ts":"1615375453.177000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UnP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"this is much simpler"}]}]}],"thread_ts":"1615373326.171300","parent_user_id":"UH24GRBLL"},{"client_msg_id":"eebc7950-2af9-4454-ae4e-eba806fa993a","type":"message","text":"There's a paper where people printed out some neural slop patterns taped them to their hoodies and were defeating CV models very easily","user":"UPUBAM63X","ts":"1615375531.178200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"deqt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"There's a paper where people printed out some neural slop patterns taped them to their hoodies and were defeating CV models very easily"}]}]}],"thread_ts":"1615373326.171300","parent_user_id":"UH24GRBLL"},{"client_msg_id":"d199acd9-e209-454a-9183-24d05a40cf72","type":"message","text":"Those have a name, although I forget that atm, is it just adversarial examples? I don't think so, but yeah those are pretty common.","user":"UC4QQPG4A","ts":"1615377530.180000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8Hyz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Those have a name, although I forget that atm, is it just adversarial examples? I don't think so, but yeah those are pretty common."}]}]}],"thread_ts":"1615373326.171300","parent_user_id":"UH24GRBLL","reactions":[{"name":"+1","users":["U017D621ELC"],"count":1}]},{"client_msg_id":"88a3c047-568a-4da8-af80-28ee187dd33a","type":"message","text":"Couldn't this just indicate that many of the image in the training set had labelled inside them?","user":"U017LQ3A59U","ts":"1615381612.180700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1Ox","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Couldn't this just indicate that many of the image in the training set had labelled inside them?"}]}]}],"thread_ts":"1615373326.171300","parent_user_id":"UH24GRBLL"},{"client_msg_id":"a01445dc-bf77-49b7-91c6-6d54c8083ee2","type":"message","text":"<@UPUBAM63X> <@UC4QQPG4A> These are <https://arxiv.org/pdf/1712.09665.pdf|Adversarial Patches>..","user":"U017D621ELC","ts":"1615390892.216500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"V0Nd","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UPUBAM63X"},{"type":"text","text":" "},{"type":"user","user_id":"UC4QQPG4A"},{"type":"text","text":" These are "},{"type":"link","url":"https://arxiv.org/pdf/1712.09665.pdf","text":"Adversarial Patches"},{"type":"text","text":".."}]}]}],"thread_ts":"1615373326.171300","parent_user_id":"UH24GRBLL"},{"client_msg_id":"4306b939-c981-4fa0-a409-9085c3aa2258","type":"message","text":"There was a story on YT by yannic kilcher, where he gave an extra option of \"apple with ipod pasted\" while choosing, and the network chose that as its main prediction lol","user":"U017D621ELC","ts":"1615391045.216900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PQj+c","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"There was a story on YT by yannic kilcher, where he gave an extra option of \"apple with ipod pasted\" while choosing, and the network chose that as its main prediction lol"}]}]}],"thread_ts":"1615373326.171300","parent_user_id":"UH24GRBLL"},{"client_msg_id":"cd5f6c7f-d5dd-4dff-b61d-150b55b53a44","type":"message","text":"the catch is that the network has to be aware of that option","user":"UH24GRBLL","ts":"1615391141.217100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Px1A","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"the catch is that the network has to be aware of that option"}]}]}],"thread_ts":"1615373326.171300","parent_user_id":"UH24GRBLL","reactions":[{"name":"+1","users":["U017D621ELC"],"count":1}]},{"client_msg_id":"26bb0d1b-877b-4b81-9b43-1d1ee84eb304","type":"message","text":"I personally feel that its hard to define what an adversarial example is in the case of CLIP, when this model tries to blur the line between Text and Vision. As <@U7JQGPGCQ> said, it might as well be that the model decided to read out text from the image instead of recognizing it","user":"U017D621ELC","ts":"1615391265.219000","team":"T68168MUP","edited":{"user":"U017D621ELC","ts":"1615391280.000000"},"blocks":[{"type":"rich_text","block_id":"KV0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I personally feel that its hard to define what an adversarial example is in the case of CLIP, when this model tries to blur the line between Text and Vision. As "},{"type":"user","user_id":"U7JQGPGCQ"},{"type":"text","text":" said, it might as well be that the model decided to read out text from the image instead of recognizing it"}]}]}],"thread_ts":"1615373326.171300","parent_user_id":"UH24GRBLL"}]