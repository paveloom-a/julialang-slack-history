[{"client_msg_id":"794d6404-70c5-4009-8c6b-27cf235adb8c","type":"message","text":"<https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron>","user":"UH24GRBLL","ts":"1615373326.171300","team":"T68168MUP","attachments":[{"service_name":"The Verge","title":"OpenAI’s state-of-the-art machine vision AI is fooled by handwritten notes","title_link":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron","text":"Reading is believing.","fallback":"The Verge: OpenAI’s state-of-the-art machine vision AI is fooled by handwritten notes","image_url":"https://cdn.vox-cdn.com/thumbor/SArfDFeVMgOfre78OYFqZX3MjmI=/0x146:1196x772/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/22353458/apple_ipod_test.jpg","image_width":478,"image_height":250,"ts":1615211398,"from_url":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron","image_bytes":65169,"service_icon":"https://cdn.vox-cdn.com/uploads/chorus_asset/file/7395359/ios-icon.0.png","id":1,"original_url":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron"}],"blocks":[{"type":"rich_text","block_id":"n6Ti","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron"}]}]}],"thread_ts":"1615373326.171300","reply_count":7,"reply_users_count":3,"latest_reply":"1615375011.176000","reply_users":["UH24GRBLL","U7JQGPGCQ","U7HAYKY9X"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"&gt; “We refer to these attacks as typographic attacks,” write OpenAI’s researchers in a blog post. “By exploiting the model’s ability to read text robustly, we find that even photographs of hand-written text can often fool the model.”\n","user":"UH24GRBLL","ts":"1615373372.171500","thread_ts":"1615373326.171300","root":{"client_msg_id":"794d6404-70c5-4009-8c6b-27cf235adb8c","type":"message","text":"<https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron>","user":"UH24GRBLL","ts":"1615373326.171300","team":"T68168MUP","attachments":[{"service_name":"The Verge","title":"OpenAI’s state-of-the-art machine vision AI is fooled by handwritten notes","title_link":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron","text":"Reading is believing.","fallback":"The Verge: OpenAI’s state-of-the-art machine vision AI is fooled by handwritten notes","image_url":"https://cdn.vox-cdn.com/thumbor/SArfDFeVMgOfre78OYFqZX3MjmI=/0x146:1196x772/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/22353458/apple_ipod_test.jpg","image_width":478,"image_height":250,"ts":1615211398,"from_url":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron","image_bytes":65169,"service_icon":"https://cdn.vox-cdn.com/uploads/chorus_asset/file/7395359/ios-icon.0.png","id":1,"original_url":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron"}],"blocks":[{"type":"rich_text","block_id":"n6Ti","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron"}]}]}],"thread_ts":"1615373326.171300","reply_count":7,"reply_users_count":3,"latest_reply":"1615375011.176000","reply_users":["UH24GRBLL","U7JQGPGCQ","U7HAYKY9X"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"/hwU","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"“We refer to these attacks as typographic attacks,” write OpenAI’s researchers in a blog post. “By exploiting the model’s ability to read text robustly, we find that even photographs of hand-written text can often fool the model.”"}]},{"type":"rich_text_section","elements":[]}]}],"client_msg_id":"623ad1b9-479f-4213-be4f-aaf442899ec4"},{"type":"message","subtype":"thread_broadcast","text":"and the original source <https://openai.com/blog/multimodal-neurons/>","user":"UH24GRBLL","ts":"1615373481.171800","thread_ts":"1615373326.171300","root":{"client_msg_id":"794d6404-70c5-4009-8c6b-27cf235adb8c","type":"message","text":"<https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron>","user":"UH24GRBLL","ts":"1615373326.171300","team":"T68168MUP","attachments":[{"service_name":"The Verge","title":"OpenAI’s state-of-the-art machine vision AI is fooled by handwritten notes","title_link":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron","text":"Reading is believing.","fallback":"The Verge: OpenAI’s state-of-the-art machine vision AI is fooled by handwritten notes","image_url":"https://cdn.vox-cdn.com/thumbor/SArfDFeVMgOfre78OYFqZX3MjmI=/0x146:1196x772/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/22353458/apple_ipod_test.jpg","image_width":478,"image_height":250,"ts":1615211398,"from_url":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron","image_bytes":65169,"service_icon":"https://cdn.vox-cdn.com/uploads/chorus_asset/file/7395359/ios-icon.0.png","id":1,"original_url":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron"}],"blocks":[{"type":"rich_text","block_id":"n6Ti","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://www.theverge.com/2021/3/8/22319173/openai-machine-vision-adversarial-typographic-attacka-clip-multimodal-neuron"}]}]}],"thread_ts":"1615373326.171300","reply_count":7,"reply_users_count":3,"latest_reply":"1615375011.176000","reply_users":["UH24GRBLL","U7JQGPGCQ","U7HAYKY9X"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"r1s4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"and the original source "},{"type":"link","url":"https://openai.com/blog/multimodal-neurons/"}]}]}],"client_msg_id":"e8261ed5-8875-4971-8413-e0a110a284dd","edited":{"user":"UH24GRBLL","ts":"1615373523.000000"}},{"client_msg_id":"8072a106-0262-44e9-aba5-6da835920778","type":"message","text":"Tbh I find this hugely impressive rather than a reason to ridicule the model - I always thought image recognitions was based on just comparing some picture with other labeled pictures, so to have an algorithm that goes beyond that to say \"hold on there's some text here, let's read that\" seems quite a feat to me","user":"U7JQGPGCQ","ts":"1615373499.172100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yIP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Tbh I find this hugely impressive rather than a reason to ridicule the model - I always thought image recognitions was based on just comparing some picture with other labeled pictures, so to have an algorithm that goes beyond that to say \"hold on there's some text here, let's read that\" seems quite a feat to me"}]}]}],"thread_ts":"1615373326.171300","parent_user_id":"UH24GRBLL"},{"client_msg_id":"87b3a185-be25-415a-aa1f-a89b4f449771","type":"message","text":"that it is","user":"UH24GRBLL","ts":"1615373514.172300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"n4C=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"that it is"}]}]}],"thread_ts":"1615373326.171300","parent_user_id":"UH24GRBLL"},{"client_msg_id":"356384dc-9527-47d6-ae0b-9e8b05464ad8","type":"message","text":"on the flipside, it really does mean that it has no concept of what an iPod is or looks like memorized","user":"UH24GRBLL","ts":"1615373542.172700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7phF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"on the flipside, it really does mean that it has no concept of what an iPod is or looks like memorized"}]}]}],"thread_ts":"1615373326.171300","parent_user_id":"UH24GRBLL"},{"client_msg_id":"f0ea46a3-f266-4144-b93d-5e52c41d71aa","type":"message","text":"there's no crosschecking with other, known knowledge","user":"UH24GRBLL","ts":"1615373565.172900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ib+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"there's no crosschecking with other, known knowledge"}]}]}],"thread_ts":"1615373326.171300","parent_user_id":"UH24GRBLL"},{"client_msg_id":"1ed00b35-3ab9-4843-a985-0bbf44c0a2a4","type":"message","text":"That's amazing. Have you heard about the \"bird or bicycle\" challenge? Or single-pixel attacks? Apparently, it's fairly easy to fool image captioning NNs by changing one single pixel. Conversely, it's basically impossible to make a NN that can distinguish between a bird and a bicycle with 100% accuracy","user":"U7HAYKY9X","ts":"1615375011.176000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"u6V","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That's amazing. Have you heard about the \"bird or bicycle\" challenge? Or single-pixel attacks? Apparently, it's fairly easy to fool image captioning NNs by changing one single pixel. Conversely, it's basically impossible to make a NN that can distinguish between a bird and a bicycle with 100% accuracy"}]}]}],"thread_ts":"1615373326.171300","parent_user_id":"UH24GRBLL"}]