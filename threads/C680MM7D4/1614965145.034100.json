[{"client_msg_id":"7565e6ef-56ec-426b-9019-2a26b8ceef55","type":"message","text":"PyTorch is getting JAX like code transforms: <https://twitter.com/cHHillee/status/1367621541693857794>","user":"UDGT4PM41","ts":"1614965145.034100","team":"T68168MUP","attachments":[{"fallback":"<https://twitter.com/cHHillee|@cHHillee>: Since your transformation takes in Python code and outputs Python source code, they're\n1. Automatically composable with each other\n2. Automatically composable with anything else (like Torchscript).\n3. Able to be dumped out as Python code. \n\nHere's an example (2/4) <https://pbs.twimg.com/media/EvrDp5TVkAAJt7z.png>","ts":1614901359,"author_name":"Horace He","author_link":"https://twitter.com/cHHillee/status/1367621541693857794","author_icon":"https://pbs.twimg.com/profile_images/918522465478987776/4rqdxVqQ_normal.jpg","author_subname":"@cHHillee","text":"Since your transformation takes in Python code and outputs Python source code, they're\n1. Automatically composable with each other\n2. Automatically composable with anything else (like Torchscript).\n3. Able to be dumped out as Python code. \n\nHere's an example (2/4) <https://pbs.twimg.com/media/EvrDp5TVkAAJt7z.png>","service_name":"twitter","service_url":"https://twitter.com/","from_url":"https://twitter.com/cHHillee/status/1367621541693857794","image_url":"https://pbs.twimg.com/media/EvrDp5TVkAAJt7z.png","image_width":811,"image_height":433,"image_bytes":49251,"id":1,"original_url":"https://twitter.com/cHHillee/status/1367621541693857794","footer":"Twitter","footer_icon":"https://a.slack-edge.com/80588/img/services/twitter_pixel_snapped_32.png"}],"blocks":[{"type":"rich_text","block_id":"=cO9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"PyTorch is getting JAX like code transforms: "},{"type":"link","url":"https://twitter.com/cHHillee/status/1367621541693857794"}]}]}],"thread_ts":"1614965145.034100","reply_count":70,"reply_users_count":6,"latest_reply":"1615730370.451600","reply_users":["UDGT4PM41","U017D621ELC","UMY1LV01G","U011V2YN59N","UN3R6SDDW","U69BL50BF"],"subscribed":false},{"client_msg_id":"1a0f1b94-a685-48da-b2c2-9f7f92740e40","type":"message","text":"<@UMY1LV01G>","user":"UDGT4PM41","ts":"1614965399.034900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"irr","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UMY1LV01G"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"4ca47b9f-0a78-4c99-985b-65af067fe8dd","type":"message","text":"TF includes dynamic graphs from Pytorch\nPytorch includes functional transforms from JAX\nWhat next?","user":"U017D621ELC","ts":"1614966282.035800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1ilM2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"TF includes dynamic graphs from Pytorch\nPytorch includes functional transforms from JAX\nWhat next?"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"7c3cb641-767f-4c30-84f6-d08d915f4002","type":"message","text":"The convergence between both frameworks was inevitable once PyTorch started eating TF's lunch for research and looking into edge/mobile deployment","user":"UMY1LV01G","ts":"1614966383.036000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MCRdn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The convergence between both frameworks was inevitable once PyTorch started eating TF's lunch for research and looking into edge/mobile deployment"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"8453bd4f-519a-483f-bb2e-9f55a080ff8a","type":"message","text":"I'm not sure how I feel about the new graph framework. It looks nice, but does feel a bit \"bolted on\" and adds to the kitchen sink","user":"UMY1LV01G","ts":"1614966494.036200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hV8I3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm not sure how I feel about the new graph framework. It looks nice, but does feel a bit \"bolted on\" and adds to the kitchen sink"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41","reactions":[{"name":"confused","users":["U017D621ELC","UKG4WF8PJ"],"count":2}]},{"client_msg_id":"ef8091a7-1f2a-4dc1-a7ad-a155c0a3bb47","type":"message","text":"JAX definitely has the cleanest conceptual model and least amount of API bloat. I guess we'll see if their strategy of outsourcing higher level libraries to the community pays off, or if the core library accumulates more and more orthogonal features.","user":"UMY1LV01G","ts":"1614966678.036700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"w5IL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"JAX definitely has the cleanest conceptual model and least amount of API bloat. I guess we'll see if their strategy of outsourcing higher level libraries to the community pays off, or if the core library accumulates more and more orthogonal features."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41","reactions":[{"name":"heart","users":["U017D621ELC","UKG4WF8PJ"],"count":2}]},{"client_msg_id":"b0217498-953e-4907-b55c-ae66a596db7b","type":"message","text":"PyTorch started off small, after all. Though in fairness they never claimed to be just a library like JAX does","user":"UMY1LV01G","ts":"1614966722.036900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"b+A","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"PyTorch started off small, after all. Though in fairness they never claimed to be just a library like JAX does"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"d373c6a9-1830-4e64-be56-339345b913a4","type":"message","text":"I hurts how much money and effort are being put into making python fast.","user":"U011V2YN59N","ts":"1614973220.041300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zcc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I hurts how much money and effort are being put into making python fast."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41","reactions":[{"name":"100","users":["UDGT4PM41","U93BUBZP0"],"count":2},{"name":"disappointed_relieved","users":["U017D621ELC"],"count":1}]},{"client_msg_id":"85d12e6a-17c4-4b1f-9f26-6b0e9a73b5e8","type":"message","text":"when like python is very unpleasant to write","user":"U011V2YN59N","ts":"1614973389.042100","team":"T68168MUP","edited":{"user":"U011V2YN59N","ts":"1614973396.000000"},"blocks":[{"type":"rich_text","block_id":"6mXQt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"when like python is very unpleasant to write"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41","reactions":[{"name":"+1","users":["U93BUBZP0"],"count":1}]},{"client_msg_id":"c4c4f3b0-005d-43ec-bc2c-8e4f764b10a8","type":"message","text":"<@UMY1LV01G> Definitely agree that Jax the least amount of API float - not sure if I agree that it has the cleanest conceptual model :stuck_out_tongue: Or well, I'd argue that perhaps it's the cleanest, but that doesn't make it the easiest to reason about.\n\nI do sympathize with your concerns about FX haha - in some sense it's 100% bolted on. PyTorch was never meant to have a dynamic graph representation.\n\nEssentially, the hope is that there are a subset of programs that users want function transformations for, where they're willing to deal with the limitations that come from trying to acquire a graph in Python.","user":"UN3R6SDDW","ts":"1615438766.268100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7Fg","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UMY1LV01G"},{"type":"text","text":" Definitely agree that Jax the least amount of API float - not sure if I agree that it has the cleanest conceptual model "},{"type":"emoji","name":"stuck_out_tongue"},{"type":"text","text":" Or well, I'd argue that perhaps it's the cleanest, but that doesn't make it the easiest to reason about.\n\nI do sympathize with your concerns about FX haha - in some sense it's 100% bolted on. PyTorch was never meant to have a dynamic graph representation.\n\nEssentially, the hope is that there are a subset of programs that users want function transformations for, where they're willing to deal with the limitations that come from trying to acquire a graph in Python."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"0f0c80b4-6670-4a1d-95a3-eddc47fd182a","type":"message","text":"Hi Horace :)\nFWIW I do think fx is a worthwhile addition and will eliminate whole class of nasty hacks people have been doing in the name of getting good performance out of network primitives. I think my point about JAX would've been better phrased as \"fewer moving parts\" since Torch's autograd is pretty dang simple conceptually (while running circles around e.g. TF when it comes to resource usage in practice). What I hope is that we're seeing a breakdown of the old \"private island\" model of incompatible library ecosystems with these newer libraries. Julia obviously has the benefit of hindsight and not being the first mover in that respect.","user":"UMY1LV01G","ts":"1615441620.269100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RPbj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi Horace :)\nFWIW I do think fx is a worthwhile addition and will eliminate whole class of nasty hacks people have been doing in the name of getting good performance out of network primitives. I think my point about JAX would've been better phrased as \"fewer moving parts\" since Torch's autograd is pretty dang simple conceptually (while running circles around e.g. TF when it comes to resource usage in practice). What I hope is that we're seeing a breakdown of the old \"private island\" model of incompatible library ecosystems with these newer libraries. Julia obviously has the benefit of hindsight and not being the first mover in that respect."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"1a9830c8-f154-4ca8-9573-3e90ee60c6e4","type":"message","text":"<@UMY1LV01G> well, I'm not sure any of these newer libraries really resolve the issue of \"private island\" (unless you mean that different transforms can compose with each other).\n\nI definitely think Julia has a significant advantage here - no disagreements about that :stuck_out_tongue: I think there's a lot of really cool things happening on Julia, which is why I lurk on this slack sometimes.","user":"UN3R6SDDW","ts":"1615513076.325000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bJj4","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UMY1LV01G"},{"type":"text","text":" well, I'm not sure any of these newer libraries really resolve the issue of \"private island\" (unless you mean that different transforms can compose with each other).\n\nI definitely think Julia has a significant advantage here - no disagreements about that "},{"type":"emoji","name":"stuck_out_tongue"},{"type":"text","text":" I think there's a lot of really cool things happening on Julia, which is why I lurk on this slack sometimes."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"5989ddb5-b245-4d08-80b4-b695cfd24083","type":"message","text":"<@UN3R6SDDW> Maybe \"Julia for Torch\" can happen without Jeff Bezanson working at FB? :joy:","user":"UDGT4PM41","ts":"1615517424.326300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JU0S","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UN3R6SDDW"},{"type":"text","text":" Maybe \"Julia for Torch\" can happen without Jeff Bezanson working at FB? "},{"type":"emoji","name":"joy"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"6818f1b0-c6b5-4e11-94a1-ea6ecca25d5a","type":"message","text":"<@UDGT4PM41> what would you want out of \"Julia for Torch\"?","user":"UN3R6SDDW","ts":"1615517646.326500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2Ir","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UDGT4PM41"},{"type":"text","text":" what would you want out of \"Julia for Torch\"?"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"5efa4dfa-0047-48e7-8fa9-519b9e819fe0","type":"message","text":"With where Julia's at now, it would take a tiny tiny fraction of the money Google put into the s4tf ship to get something better. We can get you in touch with Jeremy Howard, Who's  been mulling  Julia instead of swift as a next gen FastAI language (Swift is pretty much dead in that regard now). Yann Was also discussing some interest In Julia on twitter","user":"UDGT4PM41","ts":"1615517675.326700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6b0U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"With where Julia's at now, it would take a tiny tiny fraction of the money Google put into the s4tf ship to get something better. We can get you in touch with Jeremy Howard, Who's  been mulling  Julia instead of swift as a next gen FastAI language (Swift is pretty much dead in that regard now). Yann Was also discussing some interest In Julia on twitter"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"3729b863-70b2-4552-9a2a-03d7ff0f7a6a","type":"message","text":"<@UN3R6SDDW> Facebook support to get Julia to the point where it can be used for internal use and production, which would in turn benefit the Julia community and benefit Facebook by reducing programming overhead","user":"UDGT4PM41","ts":"1615517757.326900","team":"T68168MUP","edited":{"user":"UDGT4PM41","ts":"1615517799.000000"},"blocks":[{"type":"rich_text","block_id":"VSry","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UN3R6SDDW"},{"type":"text","text":" Facebook support to get Julia to the point where it can be used for internal use and production, which would in turn benefit the Julia community and benefit Facebook by reducing programming overhead"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"aa88e2f3-41eb-4434-80c1-ddc85fe9e622","type":"message","text":"Can write vendor agnostic GPU kernels in pure Julia, manipulate large graphs for GNNs, and we're on the verge of small zero runtime static binaries, all done by hacking normal compiler hooks. So can deploy to mobile and WASM","user":"UDGT4PM41","ts":"1615517897.327200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Kj1E+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can write vendor agnostic GPU kernels in pure Julia, manipulate large graphs for GNNs, and we're on the verge of small zero runtime static binaries, all done by hacking normal compiler hooks. So can deploy to mobile and WASM"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"c732c356-803c-4178-88a8-fed5c1a78c25","type":"message","text":"And whatever weird differentiable programming with weird datastructure usecases will pop up in the future. No islands so no need to rewrite scipy or diffeqs, or Pymc3 to work together in and with ML models. One array type system","user":"UDGT4PM41","ts":"1615518001.327400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"o/EQU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And whatever weird differentiable programming with weird datastructure usecases will pop up in the future. No islands so no need to rewrite scipy or diffeqs, or Pymc3 to work together in and with ML models. One array type system"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"cb06fc82-adae-42b5-92e2-e29c15ec68bc","type":"message","text":"<https://github.com/JuliaFolds/FLoops.jl> composable transuders with loop frontend that run distributed, GPU, CPU. Einsum + macro that runs does CPU codegen faster than openblas <https://github.com/mcabbott/Tullio.jl> using Julia's native vectorization capabilities. The possibilities and ecosystem has many many gems of which people are unaware.","user":"UDGT4PM41","ts":"1615518165.327600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Kbcx","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/JuliaFolds/FLoops.jl"},{"type":"text","text":" composable transuders with loop frontend that run distributed, GPU, CPU. Einsum + macro that runs does CPU codegen faster than openblas "},{"type":"link","url":"https://github.com/mcabbott/Tullio.jl"},{"type":"text","text":" using Julia's native vectorization capabilities. The possibilities and ecosystem has many many gems of which people are unaware."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"9bb412fe-2332-4ed8-a85a-f14568521116","type":"message","text":"Re Julia's popularity: It's already doubling every 18 months and I think it's ripe for a phase shift. A FAANG taking notice might trigger that","user":"UDGT4PM41","ts":"1615518253.327800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Rzz5H","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Re Julia's popularity: It's already doubling every 18 months and I think it's ripe for a phase shift. A FAANG taking notice might trigger that"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"a0908c14-36ed-46b2-8773-c82560c2082d","type":"message","text":"The ecosystem is large, so instead of listing every conceivable package you might be interested in (DS, ML, Stats, Backend agnostic 3d plotting etc( and there's even a mature web MVC framework), if you have any specific areas of concern I can help with pointers","user":"UDGT4PM41","ts":"1615518503.328000","team":"T68168MUP","edited":{"user":"UDGT4PM41","ts":"1615518531.000000"},"blocks":[{"type":"rich_text","block_id":"r=BoB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The ecosystem is large, so instead of listing every conceivable package you might be interested in (DS, ML, Stats, Backend agnostic 3d plotting etc( and there's even a mature web MVC framework), if you have any specific areas of concern I can help with pointers"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"21f52ebe-146f-4d74-a8e8-79095a005042","type":"message","text":"Then there's the emergent combinatorial explosion of packages that emerge out of these composable pieces, and so on up the tower of abstraction. Here's some amazing accidental composability. With just a little bit more of a push to accelerate the process , there could be a huge output: <https://twitter.com/KenoFischer/status/1369853531423993858>\n\nAnother example, homomorphic encrypted ML in flux.jl. It's generic so no need for any sort of new package with additional  C++ Tensorflow spaghetti , just reusing code with new types of arrays: <https://juliacomputing.com/blog/2019/11/encrypted-machine-learning/> Possibilities are endless","user":"UDGT4PM41","ts":"1615518879.328300","team":"T68168MUP","edited":{"user":"UDGT4PM41","ts":"1615518935.000000"},"attachments":[{"fallback":"<https://twitter.com/KenoFischer|@KenoFischer>: I should be pretty used to #julialang's extreme composability by now, but c'mon, this just just cool! Literally no code changes required in either package - first thing I tried. <https://pbs.twimg.com/media/EwKyDrfXEAAcDTt.png>","ts":1615433507,"author_name":"Keno Fischer","author_link":"https://twitter.com/KenoFischer/status/1369853531423993858","author_icon":"https://pbs.twimg.com/profile_images/1133949652234047489/_J3gif9e_normal.png","author_subname":"@KenoFischer","text":"I should be pretty used to #julialang's extreme composability by now, but c'mon, this just just cool! Literally no code changes required in either package - first thing I tried. <https://pbs.twimg.com/media/EwKyDrfXEAAcDTt.png>","service_name":"twitter","service_url":"https://twitter.com/","from_url":"https://twitter.com/KenoFischer/status/1369853531423993858","image_url":"https://pbs.twimg.com/media/EwKyDrfXEAAcDTt.png","image_width":339,"image_height":231,"image_bytes":22032,"id":1,"original_url":"https://twitter.com/KenoFischer/status/1369853531423993858","footer":"Twitter","footer_icon":"https://a.slack-edge.com/80588/img/services/twitter_pixel_snapped_32.png"}],"blocks":[{"type":"rich_text","block_id":"fjI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Then there's the emergent combinatorial explosion of packages that emerge out of these composable pieces, and so on up the tower of abstraction. Here's some amazing accidental composability. With just a little bit more of a push to accelerate the process , there could be a huge output: "},{"type":"link","url":"https://twitter.com/KenoFischer/status/1369853531423993858"},{"type":"text","text":"\n\nAnother example, homomorphic encrypted ML in flux.jl. It's generic so no need for any sort of new package with additional  C++ Tensorflow spaghetti , just reusing code with new types of arrays: "},{"type":"link","url":"https://juliacomputing.com/blog/2019/11/encrypted-machine-learning/"},{"type":"text","text":" Possibilities are endless"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"63fc42de-369a-4a61-8c1a-641f804e32a5","type":"message","text":"One last thing. Either a  FB or FastAI adoption of Julia would create a near instant explosion in use and popularity. People are just unfamiliar with just how much better Julia is (It's just like Numba, right??!) but also concerned about  niche status (and admittedly some rough edges for more \"boring\" deep learning usecases\", . Without those  restraints, the community will be huge very fast. In the meantime, without that, will have to wait for the ML stack hit an organic  inflection point, but I think that will eventually  happen, even outside ML. In six months to a year how will people react when they can deploy a fast multithreaded web app using task parallelism and diff programming to a .so?","user":"UDGT4PM41","ts":"1615519955.328900","team":"T68168MUP","edited":{"user":"UDGT4PM41","ts":"1615520151.000000"},"blocks":[{"type":"rich_text","block_id":"fjh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"One last thing. Either a  FB or FastAI adoption of Julia would create a near instant explosion in use and popularity. People are just unfamiliar with just how much better Julia is (It's just like Numba, right??!) but also concerned about  niche status (and admittedly some rough edges for more \"boring\" deep learning usecases\", . Without those  restraints, the community will be huge very fast. In the meantime, without that, will have to wait for the ML stack hit an organic  inflection point, but I think that will eventually  happen, even outside ML. In six months to a year how will people react when they can deploy a fast multithreaded web app using task parallelism and diff programming to a .so?"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41","reactions":[{"name":"juliaheartpulse","users":["U017D621ELC"],"count":1}]},{"client_msg_id":"9a2d1290-7a89-4208-af04-c3431126d329","type":"message","text":"<@UDGT4PM41> I'm with you about Julia being very cool, and have seen most of the stuff you mentioned - although thanks for the sales pitch anyways :stuck_out_tongue: However, most of this just seems like general \"it would be really nice if Facebook put funding into Julia\". I'm not saying that's not possible, but it would probably be an easier transition if there was some nice way for Julia to provide value to the PyTorch ecosystem now.","user":"UN3R6SDDW","ts":"1615599177.359400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"L0sN","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UDGT4PM41"},{"type":"text","text":" I'm with you about Julia being very cool, and have seen most of the stuff you mentioned - although thanks for the sales pitch anyways "},{"type":"emoji","name":"stuck_out_tongue"},{"type":"text","text":" However, most of this just seems like general \"it would be really nice if Facebook put funding into Julia\". I'm not saying that's not possible, but it would probably be an easier transition if there was some nice way for Julia to provide value to the PyTorch ecosystem now."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"7ecad7af-ed4e-445e-8cb9-b0ed9ac24898","type":"message","text":"<@UN3R6SDDW> I get that! By the way, I have absolutely zero affiliation with Julia computing or any direct commercial interest in what I said (except second order career effects). I just love Julia and would love to see more interest / use so I can use it more (packages, collab etc)","user":"UDGT4PM41","ts":"1615681353.421700","team":"T68168MUP","edited":{"user":"UDGT4PM41","ts":"1615681371.000000"},"blocks":[{"type":"rich_text","block_id":"dae","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UN3R6SDDW"},{"type":"text","text":" I get that! By the way, I have absolutely zero affiliation with Julia computing or any direct commercial interest in what I said (except second order career effects). I just love Julia and would love to see more interest / use so I can use it more (packages, collab etc)"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"type":"message","subtype":"thread_broadcast","text":"Regarding \"some nice way for Julia to provide value to the PyTorch ecosystem now.\"\n\nI can see a couple of approaches: 1. Writing custom GPU /TPU kernels in Julia, which can be called from pytorch, with automatic wrapper generation. <@U69BL50BF> might be able to comment on that.\n\n2. Writing custom array or scalar types in Julia, for pytorch\n\n3. Symbolic simplification and optimization of pytorch code using modeling toolkit and <https://discourse.julialang.org/t/ann-symbolics-jl-a-modern-computer-algebra-system-for-a-modern-language/56251/119>\n\n4.  moving aten/ torch  backend from C++ to Julia. More people from the community can hack on that vs c++.\n\nAnyone else have other ideas?","user":"UDGT4PM41","ts":"1615681593.422900","thread_ts":"1614965145.034100","root":{"client_msg_id":"7565e6ef-56ec-426b-9019-2a26b8ceef55","type":"message","text":"PyTorch is getting JAX like code transforms: <https://twitter.com/cHHillee/status/1367621541693857794>","user":"UDGT4PM41","ts":"1614965145.034100","team":"T68168MUP","attachments":[{"fallback":"<https://twitter.com/cHHillee|@cHHillee>: Since your transformation takes in Python code and outputs Python source code, they're\n1. Automatically composable with each other\n2. Automatically composable with anything else (like Torchscript).\n3. Able to be dumped out as Python code. \n\nHere's an example (2/4) <https://pbs.twimg.com/media/EvrDp5TVkAAJt7z.png>","ts":1614901359,"author_name":"Horace He","author_link":"https://twitter.com/cHHillee/status/1367621541693857794","author_icon":"https://pbs.twimg.com/profile_images/918522465478987776/4rqdxVqQ_normal.jpg","author_subname":"@cHHillee","text":"Since your transformation takes in Python code and outputs Python source code, they're\n1. Automatically composable with each other\n2. Automatically composable with anything else (like Torchscript).\n3. Able to be dumped out as Python code. \n\nHere's an example (2/4) <https://pbs.twimg.com/media/EvrDp5TVkAAJt7z.png>","service_name":"twitter","service_url":"https://twitter.com/","from_url":"https://twitter.com/cHHillee/status/1367621541693857794","image_url":"https://pbs.twimg.com/media/EvrDp5TVkAAJt7z.png","image_width":811,"image_height":433,"image_bytes":49251,"id":1,"original_url":"https://twitter.com/cHHillee/status/1367621541693857794","footer":"Twitter","footer_icon":"https://a.slack-edge.com/80588/img/services/twitter_pixel_snapped_32.png"}],"blocks":[{"type":"rich_text","block_id":"=cO9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"PyTorch is getting JAX like code transforms: "},{"type":"link","url":"https://twitter.com/cHHillee/status/1367621541693857794"}]}]}],"thread_ts":"1614965145.034100","reply_count":70,"reply_users_count":6,"latest_reply":"1615730370.451600","reply_users":["UDGT4PM41","U017D621ELC","UMY1LV01G","U011V2YN59N","UN3R6SDDW","U69BL50BF"],"subscribed":false},"attachments":[{"service_name":"JuliaLang","title":"[ANN] Symbolics.jl: A Modern Computer Algebra System for a Modern Language","title_link":"https://discourse.julialang.org/t/ann-symbolics-jl-a-modern-computer-algebra-system-for-a-modern-language/56251/119","text":"Very interesting. Can you guide me to literature about why SCAS need more than real and complex numbers? I also very interested about your experience about bridging the gaps between SCASs and MCASs.","fallback":"JuliaLang: [ANN] Symbolics.jl: A Modern Computer Algebra System for a Modern Language","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","ts":1615663496,"from_url":"https://discourse.julialang.org/t/ann-symbolics-jl-a-modern-computer-algebra-system-for-a-modern-language/56251/119","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/ann-symbolics-jl-a-modern-computer-algebra-system-for-a-modern-language/56251/119"}],"blocks":[{"type":"rich_text","block_id":"cW5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Regarding \"some nice way for Julia to provide value to the PyTorch ecosystem now.\"\n\nI can see a couple of approaches: 1. Writing custom GPU /TPU kernels in Julia, which can be called from pytorch, with automatic wrapper generation. "},{"type":"user","user_id":"U69BL50BF"},{"type":"text","text":" might be able to comment on that.\n\n2. Writing custom array or scalar types in Julia, for pytorch\n\n3. Symbolic simplification and optimization of pytorch code using modeling toolkit and "},{"type":"link","url":"https://discourse.julialang.org/t/ann-symbolics-jl-a-modern-computer-algebra-system-for-a-modern-language/56251/119"},{"type":"text","text":"\n\n4.  moving aten/ torch  backend from C++ to Julia. More people from the community can hack on that vs c++.\n\nAnyone else have other ideas?"}]}]}],"client_msg_id":"e3023b2b-7935-4087-8c54-cc0a51e088b0","edited":{"user":"UDGT4PM41","ts":"1615681666.000000"}},{"client_msg_id":"6640e2c3-1f1c-4125-80ea-c9df49aa7a91","type":"message","text":"For the other way there is: <https://github.com/FluxML/Torch.jl>","user":"UDGT4PM41","ts":"1615681759.423400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iCeWM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"For the other way there is: "},{"type":"link","url":"https://github.com/FluxML/Torch.jl"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"3f7be71e-32f1-4589-90d5-89ca61a90cd9","type":"message","text":"ReversePropogation.jl is the first attempt at the symbolics-based AD system <https://github.com/dpsanders/ReversePropagation.jl>","user":"U69BL50BF","ts":"1615682283.423900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"a7qE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ReversePropogation.jl is the first attempt at the symbolics-based AD system "},{"type":"link","url":"https://github.com/dpsanders/ReversePropagation.jl"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"7798a86f-cbd5-45e8-89e3-0277475513f3","type":"message","text":"I don't think there's much of a competitive advantage to be found in the quasi-static machine learning regime, so I think everyone is fighting for technical scraps and it's not the best game to play.","user":"U69BL50BF","ts":"1615682400.424100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/ME","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I don't think there's much of a competitive advantage to be found in the quasi-static machine learning regime, so I think everyone is fighting for technical scraps and it's not the best game to play."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"01a26091-baff-43aa-b760-c583e9627715","type":"message","text":"Somebody working on PyTorch wrote this at some point haha: <https://github.com/bwasti/torch_julia>","user":"UN3R6SDDW","ts":"1615682482.424300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"m7L","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Somebody working on PyTorch wrote this at some point haha: "},{"type":"link","url":"https://github.com/bwasti/torch_julia"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"d8cca96d-0487-47b7-984c-65c2d5a81439","type":"message","text":"<@U69BL50BF> what's the \"quasi-static\" ML regime?","user":"UN3R6SDDW","ts":"1615682522.424500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qiPw","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U69BL50BF"},{"type":"text","text":" what's the \"quasi-static\" ML regime?"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"057e870d-1dcc-4605-aa36-3ce8243d0e39","type":"message","text":"&gt; Somebody working on PyTorch wrote this at some point haha: <https://github.com/bwasti/torch_julia>\nIf you want to see a fun one <https://github.com/sbi-benchmark/diffeqtorch>","user":"U69BL50BF","ts":"1615682617.424700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Aw2","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"Somebody working on PyTorch wrote this at some point haha: "},{"type":"link","url":"https://github.com/bwasti/torch_julia"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"If you want to see a fun one "},{"type":"link","url":"https://github.com/sbi-benchmark/diffeqtorch"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"b81983dd-ccbc-49e2-939a-e27786b5f8a1","type":"message","text":"&gt; what's the \"quasi-static\" ML regime?\nRead <https://www.stochasticlifestyle.com/generalizing-automatic-differentiation-to-automatic-sparsity-uncertainty-stability-and-parallelism/> on non-standard interpretation","user":"U69BL50BF","ts":"1615682660.424900","team":"T68168MUP","attachments":[{"service_name":"Stochastic Lifestyle","title":"Generalizing Automatic Differentiation to Automatic Sparsity, Uncertainty, Stability, and Parallelism - Stochastic Lifestyle","title_link":"https://www.stochasticlifestyle.com/generalizing-automatic-differentiation-to-automatic-sparsity-uncertainty-stability-and-parallelism/","text":"Automatic differentiation is a \"compiler trick\" whereby a code that calculates f(x) is transformed into a code that calculates f'(x). This trick and its two forms, forward and reverse mode automatic differentiation, have become the pervasive backbone behind all of the machine learning libraries. If you ask what PyTorch or Flux.jl is doing that's special, the answer is really that it's doing automatic differentiation over some functions. What I want to dig into in this blog post is a simple question: what is the trick behind automatic differentiation, why is it always differentiation, and are there other mathematical problems we can be focusing this trick towards? While very technical discussions on this can be found in our recent paper titled \"ModelingToolkit: A Composable Graph Transformation System For Equation-Based Modeling\" and descriptions of methods like intrusive uncertainty quantification, I want ... READ MORE","fallback":"Stochastic Lifestyle: Generalizing Automatic Differentiation to Automatic Sparsity, Uncertainty, Stability, and Parallelism - Stochastic Lifestyle","thumb_url":"https://www.stochasticlifestyle.com/wp-content/themes/chrisrack/style/faviPic2.PNG","fields":[{"title":"Written by","value":"Christopher Rackauckas","short":true},{"title":"Est. reading time","value":"14 minutes","short":true}],"ts":1615383832,"from_url":"https://www.stochasticlifestyle.com/generalizing-automatic-differentiation-to-automatic-sparsity-uncertainty-stability-and-parallelism/","thumb_width":669,"thumb_height":891,"service_icon":"https://i2.wp.com/www.stochasticlifestyle.com/wp-content/uploads/2016/01/cropped-faviPic.png?fit=180%2C180&#038;ssl=1","id":1,"original_url":"https://www.stochasticlifestyle.com/generalizing-automatic-differentiation-to-automatic-sparsity-uncertainty-stability-and-parallelism/"}],"blocks":[{"type":"rich_text","block_id":"0nH","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"what's the \"quasi-static\" ML regime?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Read "},{"type":"link","url":"https://www.stochasticlifestyle.com/generalizing-automatic-differentiation-to-automatic-sparsity-uncertainty-stability-and-parallelism/"},{"type":"text","text":" on non-standard interpretation"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"5ff6278d-3c32-455d-a45f-77e0693bb7ca","type":"message","text":"Now, when can what forms of non-standard interpretation be done?","user":"U69BL50BF","ts":"1615682675.425200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UMMTp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Now, when can what forms of non-standard interpretation be done?"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"de5d3c9b-f33d-41bc-99d8-7728d4f2fdaa","type":"message","text":"For a sparsity pattern with a while loop that is `while x[1] &gt; x[5]` , where inside of the loop you get `x[i+1] = x[i]` for all `i`, the sparsity pattern is directly dependent on the values of `x` , so there is no static program that can capture all of its behavior.","user":"U69BL50BF","ts":"1615682742.425400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fCi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"For a sparsity pattern with a while loop that is "},{"type":"text","text":"while x[1] > x[5]","style":{"code":true}},{"type":"text","text":" , where inside of the loop you get "},{"type":"text","text":"x[i+1] = x[i]","style":{"code":true}},{"type":"text","text":" for all "},{"type":"text","text":"i","style":{"code":true}},{"type":"text","text":", the sparsity pattern is directly dependent on the values of "},{"type":"text","text":"x","style":{"code":true}},{"type":"text","text":" , so there is no static program that can capture all of its behavior."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"0f1772cf-94b2-49c2-a976-0e45f8bef557","type":"message","text":"In that case, Jax's method or MTK's methods are not applicable because the tracing of those approaches make a quasi-static assumption.","user":"U69BL50BF","ts":"1615682783.425600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uiSI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In that case, Jax's method or MTK's methods are not applicable because the tracing of those approaches make a quasi-static assumption."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"c8687ba4-0a66-431c-bdfa-f0a87455fad9","type":"message","text":"PyTorch and Zygote are immune to that, and can work on while loops.","user":"U69BL50BF","ts":"1615682800.425800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dObK5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"PyTorch and Zygote are immune to that, and can work on while loops."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"748a41d4-f125-4b43-9391-5bdcd0379a94","type":"message","text":"I actually already read that, and thought it was quite cool :stuck_out_tongue:","user":"UN3R6SDDW","ts":"1615682815.426000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"U45m","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I actually already read that, and thought it was quite cool "},{"type":"emoji","name":"stuck_out_tongue"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"98fc81a1-bb68-4fbc-98cb-83608e663475","type":"message","text":"On the quasi-static assumption, see <https://www.reddit.com/r/Julia/comments/iblm9g/jax_compared_to_julia/g1xpg15/>","user":"U69BL50BF","ts":"1615682836.426200","team":"T68168MUP","attachments":[{"service_name":"reddit","title":"Jax compared to Julia","title_link":"https://www.reddit.com/r/Julia/comments/iblm9g/jax_compared_to_julia/g1xpg15/","text":"&gt;Also, does XLA have the potential to make numeric python comparable in speed to Julia as a whole? Loaded question, and both a yes and no answer....","fallback":"reddit: Jax compared to Julia","thumb_url":"https://www.redditstatic.com/new-icon.png","from_url":"https://www.reddit.com/r/Julia/comments/iblm9g/jax_compared_to_julia/g1xpg15/","thumb_width":256,"thumb_height":256,"service_icon":"http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png","id":1,"original_url":"https://www.reddit.com/r/Julia/comments/iblm9g/jax_compared_to_julia/g1xpg15/"}],"blocks":[{"type":"rich_text","block_id":"BPH/M","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"On the quasi-static assumption, see "},{"type":"link","url":"https://www.reddit.com/r/Julia/comments/iblm9g/jax_compared_to_julia/g1xpg15/"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"46b73c63-26ee-4c9c-b8a0-2c823cb89a39","type":"message","text":"&gt;  the tracing of those approaches make a quasi-static assumption.\nI see, although those are representable with control flow operators, right? Like `lax.while` or w.e.","user":"UN3R6SDDW","ts":"1615682853.426500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Rn8","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":" the tracing of those approaches make a quasi-static assumption."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I see, although those are representable with control flow operators, right? Like "},{"type":"text","text":"lax.while","style":{"code":true}},{"type":"text","text":" or w.e."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"d973c533-dd46-4e69-9b06-85d6d973e07f","type":"message","text":"or the footnote in <https://arxiv.org/ftp/arxiv/papers/2103/2103.05244.pdf>","user":"U69BL50BF","ts":"1615682853.426700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"W0+l","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"or the footnote in "},{"type":"link","url":"https://arxiv.org/ftp/arxiv/papers/2103/2103.05244.pdf"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"4518f25c-c413-4cee-a99d-347c72565e19","type":"message","text":"&gt; I see, although those are representable with control flow operators, right? Like `lax.while` or w.e.\nCorrect, which is why TensorFlow Eager slows down on them, why Jax doesn't allow JITing them, etc.","user":"U69BL50BF","ts":"1615682882.426900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1n2o","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"I see, although those are representable with control flow operators, right? Like "},{"type":"text","text":"lax.while","style":{"code":true}},{"type":"text","text":" or w.e."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Correct, which is why TensorFlow Eager slows down on them, why Jax doesn't allow JITing them, etc."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"94b50891-5d76-40eb-8a44-4db0f2ab9c19","type":"message","text":"Well, you can JIT a `lax.while`, although it does have certain constraints.","user":"UN3R6SDDW","ts":"1615682910.427100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eK/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Well, you can JIT a "},{"type":"text","text":"lax.while","style":{"code":true}},{"type":"text","text":", although it does have certain constraints."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"7b33f34c-1d41-48e8-99c4-6d0f44e4f292","type":"message","text":"they implement fallback behavior to make people happy, but it's fallback behavior. The true language that they have written requires a static compute graph, and the problem is that some programs cannot be completely static.","user":"U69BL50BF","ts":"1615682930.427300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"H4w","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"they implement fallback behavior to make people happy, but it's fallback behavior. The true language that they have written requires a static compute graph, and the problem is that some programs cannot be completely static."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"00ce679f-457e-4786-b484-2e28a777a5aa","type":"message","text":"It has quite a few constraints :rolling_on_the_floor_laughing:","user":"U69BL50BF","ts":"1615682944.427500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"E62","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It has quite a few constraints "},{"type":"emoji","name":"rolling_on_the_floor_laughing"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41","reactions":[{"name":"laughing","users":["UN3R6SDDW"],"count":1}]},{"client_msg_id":"9bdd6eb4-25db-40db-a91e-b7b07af09187","type":"message","text":"but yeah, and that's the fighting for scraps bit","user":"U69BL50BF","ts":"1615682953.427700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"F7piD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"but yeah, and that's the fighting for scraps bit"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"1a756a83-d98a-4c36-813d-c2bfb3ca9f2a","type":"message","text":"There's a few ways people are fighting for the scraps for tackling that domain, while 99.9% of machine learning programs end of being quasi-static in the first place, which is why TF and Jax end up okay. Zygote solves the harder problem, which is why it's so nice but why sometimes it can be like \"wait, what can Zygote do that the others can't?\". It's source to source on the full imperative programming language. But do you need it?","user":"U69BL50BF","ts":"1615683045.428100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7wxNF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"There's a few ways people are fighting for the scraps for tackling that domain, while 99.9% of machine learning programs end of being quasi-static in the first place, which is why TF and Jax end up okay. Zygote solves the harder problem, which is why it's so nice but why sometimes it can be like \"wait, what can Zygote do that the others can't?\". It's source to source on the full imperative programming language. But do you need it?"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"4b5ed56e-638c-4667-b31e-b8986a3e77ef","type":"message","text":"That's why a branch of my recent research (not shared yet) directly focuses on this question: what are some algorithms that require fully imperative approaches to AD?","user":"U69BL50BF","ts":"1615683083.428300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xSMUf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That's why a branch of my recent research (not shared yet) directly focuses on this question: what are some algorithms that require fully imperative approaches to AD?"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"1243ce8a-a335-43d1-b60c-363593f00a8b","type":"message","text":"I'd be very interested in that.","user":"UN3R6SDDW","ts":"1615683124.428500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Izs+A","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'd be very interested in that."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"75db2384-ce96-4be9-9482-41fe44485ba8","type":"message","text":"One thing I've been wondering about recently is: \"when do people actually need mutation/imperativeness in machine learning code?\"","user":"UN3R6SDDW","ts":"1615683147.428700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"t8cq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"One thing I've been wondering about recently is: \"when do people actually need mutation/imperativeness in machine learning code?\""}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"537a7609-b477-471a-9593-db1a78c6aca7","type":"message","text":"There are the trivial examples, like `A[i] = &lt;x&gt;`, but most of the time that can be rewritten functionally without any change in semantics.","user":"UN3R6SDDW","ts":"1615683182.428900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"n/A","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"There are the trivial examples, like "},{"type":"text","text":"A[i] = <x>","style":{"code":true}},{"type":"text","text":", but most of the time that can be rewritten functionally without any change in semantics."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"6be678b4-f00d-4e61-817a-aff99fa11266","type":"message","text":"mutation, no. You never \"need\" mutation. The question is whether it can be handled better than right now, and yes it can. I would venture to say it's not too hard.","user":"U69BL50BF","ts":"1615683252.429100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8PdLc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"mutation, no. You never \"need\" mutation. The question is whether it can be handled better than right now, and yes it can. I would venture to say it's not too hard."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"617a63fa-2b5f-481e-ba4c-679e1a132ede","type":"message","text":"Imperativeness, yes but it's very subtle :wink:","user":"U69BL50BF","ts":"1615683281.429300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+tn+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Imperativeness, yes but it's very subtle "},{"type":"emoji","name":"wink"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"86af2e48-3c77-4e4a-a979-bac50e3f8f20","type":"message","text":"Can you elaborate? I typically associate imperativeness with mutation.","user":"UN3R6SDDW","ts":"1615683326.430000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6kwK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can you elaborate? I typically associate imperativeness with mutation."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"f97ec3cc-ebdd-4b40-a128-b4bf4e1de155","type":"message","text":"Aside from the cutting edge AD stuff, two things  that have really pushed me to Julia for real work (aside from the general pleasure) are manipulating large custom ontology graphs for NLP (hybrid symbolic/sub-symbolic learning) and working with custom sparse kernels.\n\nBoth currently require C++ to be done in python.\n\n<https://github.com/facebookresearch/SparseConvNet>","user":"UDGT4PM41","ts":"1615683362.430400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iR8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Aside from the cutting edge AD stuff, two things  that have really pushed me to Julia for real work (aside from the general pleasure) are manipulating large custom ontology graphs for NLP (hybrid symbolic/sub-symbolic learning) and working with custom sparse kernels.\n\nBoth currently require C++ to be done in python.\n\n"},{"type":"link","url":"https://github.com/facebookresearch/SparseConvNet"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"509f9512-94dd-4736-8775-6e7fd68d75b8","type":"message","text":"• <https://github.com/facebookresearch/SparseConvNet/search?l=c%2B%2B|C++>\n• <https://github.com/facebookresearch/SparseConvNet/search?l=c%2B%2B|63.3%>\n•  \n• <https://github.com/facebookresearch/SparseConvNet/search?l=python|Python>\n• <https://github.com/facebookresearch/SparseConvNet/search?l=python|21.5%>\n•  \n• <https://github.com/facebookresearch/SparseConvNet/search?l=cuda|Cuda15.1%>\n :disappointed:","user":"UDGT4PM41","ts":"1615683392.430600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"p97","elements":[{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/facebookresearch/SparseConvNet/search?l=c%2B%2B","text":"C++"}]},{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/facebookresearch/SparseConvNet/search?l=c%2B%2B","text":"63.3%"}]},{"type":"rich_text_section","elements":[{"type":"text","text":" "}]},{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/facebookresearch/SparseConvNet/search?l=python","text":"Python"}]},{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/facebookresearch/SparseConvNet/search?l=python","text":"21.5%"}]},{"type":"rich_text_section","elements":[{"type":"text","text":" "}]},{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/facebookresearch/SparseConvNet/search?l=cuda","text":"Cuda15.1%"}]}],"style":"bullet","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":" "},{"type":"emoji","name":"disappointed"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"378feef9-6b91-4aa4-a491-3a9b9968ae2f","type":"message","text":"Ouch","user":"U69BL50BF","ts":"1615683399.430800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pxD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ouch"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"090dded4-f941-4001-95bc-da5a8e74f09b","type":"message","text":"&gt; I typically associate imperativeness with mutation.\nDeclarative languages can mutate, you just don't choose it to.","user":"U69BL50BF","ts":"1615683431.431000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xDK6W","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"I typically associate imperativeness with mutation."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Declarative languages can mutate, you just don't choose it to."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"26741afd-ca96-4578-83ef-f8e86ace3b4b","type":"message","text":"But yeah, I think differentiable programming is cool, but not necessarily from an ML perspective. I think it remains to be proven that for \"standard\" machine learning you need something that does more than the quasi-static set of codes really well, i.e. you really need more than TensorFlow Eager's slow fallback behavior. In scientific machine learning we have a few cases to show that's very clear, in standard ML that's less so but it's what we're currently thinking about.","user":"U69BL50BF","ts":"1615683851.431800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"AN6z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But yeah, I think differentiable programming is cool, but not necessarily from an ML perspective. I think it remains to be proven that for \"standard\" machine learning you need something that does more than the quasi-static set of codes really well, i.e. you really need more than TensorFlow Eager's slow fallback behavior. In scientific machine learning we have a few cases to show that's very clear, in standard ML that's less so but it's what we're currently thinking about."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"9a4a377f-4b2a-42a5-ac63-2c57206e265a","type":"message","text":"Is there a good example of useful self-contained code that's not quasi-static? :stuck_out_tongue:","user":"UN3R6SDDW","ts":"1615683971.432000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RQfO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a good example of useful self-contained code that's not quasi-static? "},{"type":"emoji","name":"stuck_out_tongue"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"b44b89e9-9c87-4be7-aa69-c1fddea4bd4d","type":"message","text":"Newton-Raphson iteration.","user":"U69BL50BF","ts":"1615683994.432200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yRGs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Newton-Raphson iteration."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"6cf66a2f-41f9-4a89-9588-bffa5e2eecc1","type":"message","text":"Bisection","user":"U69BL50BF","ts":"1615684002.432400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RUN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Bisection"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"62849bcc-b68d-4989-a86b-b654789c7f4b","type":"message","text":"Is your claim that newton-raphson iteration is faster in Julia than the equivalent in Jax?","user":"UN3R6SDDW","ts":"1615684097.432700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jHf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is your claim that newton-raphson iteration is faster in Julia than the equivalent in Jax?"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"779bc97b-ae03-4cc4-9128-cb83e0443864","type":"message","text":"It very much depends on how it's done.","user":"U69BL50BF","ts":"1615684132.432900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6dR9N","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It very much depends on how it's done."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"a8cca7bd-b5de-41f0-80eb-8ff60aa758b1","type":"message","text":"What ends up happening really is that in Jax the rootfinders end up making a big assumption about the zero that is \"true\" but not numerically true. How does that effect the derivative process, and when does it matter?","user":"U69BL50BF","ts":"1615684217.433100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5Xtk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What ends up happening really is that in Jax the rootfinders end up making a big assumption about the zero that is \"true\" but not numerically true. How does that effect the derivative process, and when does it matter?"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"95ec8a7c-ee58-4836-af99-8fc2ef91a391","type":"message","text":"They do that to make NR iteration captured in a single op for differentiation","user":"U69BL50BF","ts":"1615684238.433300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jZme","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"They do that to make NR iteration captured in a single op for differentiation"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"70db5939-fcd7-4ee6-8b8b-a7f0765557a2","type":"message","text":"Also, IIUC, in general Julia doesn't really do much like the  domain specific array optimizations that XLA does (which obviously has ease of working with much restricted semantics) ....That's going to be built on top of the new package defineable   composable compiler pass framework  that <@U674T3KB3> is working on. There's also things like escape analysis, dynamic ownership system etc that will be built on this infrastructure.  So there's a lot of room for improvement.\n\n<@U01K2JB9GPJ>’s <https://github.com/0x0f0f0f/Metatheory.jl> which was just generalized to be able to work with arbitrary types, including soon typed SSA IR, might play a role in that. Manipulating  IR while keeping high level semantics","user":"UDGT4PM41","ts":"1615684633.434000","team":"T68168MUP","edited":{"user":"UDGT4PM41","ts":"1615685010.000000"},"blocks":[{"type":"rich_text","block_id":"euHd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Also, IIUC, in general Julia doesn't really do much like the  domain specific array optimizations that XLA does (which obviously has ease of working with much restricted semantics) ....That's going to be built on top of the new package defineable   composable compiler pass framework  that "},{"type":"user","user_id":"U674T3KB3"},{"type":"text","text":" is working on. There's also things like escape analysis, dynamic ownership system etc that will be built on this infrastructure.  So there's a lot of room for improvement.\n\n"},{"type":"user","user_id":"U01K2JB9GPJ"},{"type":"text","text":"’s "},{"type":"link","url":"https://github.com/0x0f0f0f/Metatheory.jl"},{"type":"text","text":" which was just generalized to be able to work with arbitrary types, including soon typed SSA IR, might play a role in that. Manipulating  IR while keeping high level semantics"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"client_msg_id":"d981d55e-dabe-4c83-a67b-cd3c0aeedfe1","type":"message","text":"Yes, so there's two approaches, and Keno and I are actually working together on some projects looking through both at the same time. On one end, there's the whole AbstractInterpreter allowing composable compiler passes directly on imperative Julia. On the other hand there's Symbolics.jl and MTK which can auto-trace Julia into a declarative language, which can now directly use MetaTheory and SU's rewrite system both at the same time. So I think we'll soon see a next generation of tools specializing on the two separate assumptions.","user":"U69BL50BF","ts":"1615684832.434700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"luBvR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes, so there's two approaches, and Keno and I are actually working together on some projects looking through both at the same time. On one end, there's the whole AbstractInterpreter allowing composable compiler passes directly on imperative Julia. On the other hand there's Symbolics.jl and MTK which can auto-trace Julia into a declarative language, which can now directly use MetaTheory and SU's rewrite system both at the same time. So I think we'll soon see a next generation of tools specializing on the two separate assumptions."}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41","reactions":[{"name":"+1","users":["UDGT4PM41","UKG4WF8PJ","UN3R6SDDW"],"count":3}]},{"client_msg_id":"c83c3c84-421e-4ccc-964a-4f9a2cb0cd9d","type":"message","text":"Oh I forgot that there is one of the proof of concepts of fully imperative differentiable programming being very effective: Pumas. It's a deeply domain-specific discussion though. <https://www.biorxiv.org/content/10.1101/2020.11.28.402297v1>","user":"U69BL50BF","ts":"1615685418.435200","team":"T68168MUP","attachments":[{"service_name":"bioRxiv","title":"Accelerated Predictive Healthcare Analytics with Pumas, a High Performance Pharmaceutical Modeling and Simulation Platform","title_link":"https://www.biorxiv.org/content/10.1101/2020.11.28.402297v1","text":"Pharmacometric modeling establishes causal quantitative relationship between administered dose, tissue exposures, desired and undesired effects and patient’s risk factors. These models are employed to de-risk drug development and guide precision medicine decisions. Recent technological advances rendered collecting real-time and detailed data easy. However, the pharmacometric tools have not been designed to handle heterogeneous, big data and complex models. The estimation methods are outdated to solve modern healthcare challenges. We set out to design a platform that facilitates domain specific modeling and its integration with modern analytics to foster innovation and readiness to data deluge in healthcare. New specialized estimation methodologies have been developed that allow dramatic performance advances in areas that have not seen major improvements in decades. New ODE solver algorithms, such as coefficient-optimized higher order integrators and new automatic stiffness detecting algorithms which are robust to frequent discontinuities, give rise to up to 4x performance improvements across a wide range of stiff and non-stiff systems seen in pharmacometric applications. These methods combine with JIT compiler techniques and further specialize the solution process on the individual systems, allowing statically-sized optimizations and discrete sensitivity analysis via forward-mode automatic differentiation, to further enhance the accuracy and performance of the solving and parameter estimation process. We demonstrate that when all of these techniques are combined with a validated clinical trial dosing mechanism and non-compartmental analysis (NCA) suite, real applications like NLME parameter estimation see run times halved while retaining the same accuracy. Meanwhile in areas with less prior optimization of software, like optimal experimental design, we see orders of magnitude performance enhancements. Together we show a fast and modern domain specific modeling framework which lays a platform for innovation via upcoming integrations with modern analytics. ### Competing Interest Statement Pumas is a proprietary software developed by Pumas-AI Inc. Authors Rackauckas, Ma, Noack, Dixit, Mogensen, Byrne, Maddhashiya, Calderon, Nyberg, Gobburu, and Ivaturi all are or have been affiliated with Pumas-AI Inc. in the past 36 months.","fallback":"bioRxiv: Accelerated Predictive Healthcare Analytics with Pumas, a High Performance Pharmaceutical Modeling and Simulation Platform","thumb_url":"https://www.biorxiv.org/sites/default/files/images/biorxiv_logo_homepage7-5-small.png","ts":1606723200,"from_url":"https://www.biorxiv.org/content/10.1101/2020.11.28.402297v1","thumb_width":252,"thumb_height":252,"service_icon":"https://www.biorxiv.org/sites/default/files/images/favicon.ico","id":1,"original_url":"https://www.biorxiv.org/content/10.1101/2020.11.28.402297v1"}],"blocks":[{"type":"rich_text","block_id":"nQn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oh I forgot that there is one of the proof of concepts of fully imperative differentiable programming being very effective: Pumas. It's a deeply domain-specific discussion though. "},{"type":"link","url":"https://www.biorxiv.org/content/10.1101/2020.11.28.402297v1"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"},{"type":"message","subtype":"thread_broadcast","text":"<@UN3R6SDDW> another non sciml oft ignored advantage of source to source autodiff is that  integration with host language and custom types allows one to write use facing apps where the ML is deeply integrated into the meat of the app. The (now defunct) swift for tensorflow example was iirc,  an audio player which  adjusts dynamically to user behavior. Julia's nascent WASM compilation can be leveraged here so people can write ML driven apps for web, mobile and serverside","user":"UDGT4PM41","ts":"1615730060.451300","thread_ts":"1614965145.034100","root":{"client_msg_id":"7565e6ef-56ec-426b-9019-2a26b8ceef55","type":"message","text":"PyTorch is getting JAX like code transforms: <https://twitter.com/cHHillee/status/1367621541693857794>","user":"UDGT4PM41","ts":"1614965145.034100","team":"T68168MUP","attachments":[{"fallback":"<https://twitter.com/cHHillee|@cHHillee>: Since your transformation takes in Python code and outputs Python source code, they're\n1. Automatically composable with each other\n2. Automatically composable with anything else (like Torchscript).\n3. Able to be dumped out as Python code. \n\nHere's an example (2/4) <https://pbs.twimg.com/media/EvrDp5TVkAAJt7z.png>","ts":1614901359,"author_name":"Horace He","author_link":"https://twitter.com/cHHillee/status/1367621541693857794","author_icon":"https://pbs.twimg.com/profile_images/918522465478987776/4rqdxVqQ_normal.jpg","author_subname":"@cHHillee","text":"Since your transformation takes in Python code and outputs Python source code, they're\n1. Automatically composable with each other\n2. Automatically composable with anything else (like Torchscript).\n3. Able to be dumped out as Python code. \n\nHere's an example (2/4) <https://pbs.twimg.com/media/EvrDp5TVkAAJt7z.png>","service_name":"twitter","service_url":"https://twitter.com/","from_url":"https://twitter.com/cHHillee/status/1367621541693857794","image_url":"https://pbs.twimg.com/media/EvrDp5TVkAAJt7z.png","image_width":811,"image_height":433,"image_bytes":49251,"id":1,"original_url":"https://twitter.com/cHHillee/status/1367621541693857794","footer":"Twitter","footer_icon":"https://a.slack-edge.com/80588/img/services/twitter_pixel_snapped_32.png"}],"blocks":[{"type":"rich_text","block_id":"=cO9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"PyTorch is getting JAX like code transforms: "},{"type":"link","url":"https://twitter.com/cHHillee/status/1367621541693857794"}]}]}],"thread_ts":"1614965145.034100","reply_count":70,"reply_users_count":6,"latest_reply":"1615730370.451600","reply_users":["UDGT4PM41","U017D621ELC","UMY1LV01G","U011V2YN59N","UN3R6SDDW","U69BL50BF"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"cNW","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UN3R6SDDW"},{"type":"text","text":" another non sciml oft ignored advantage of source to source autodiff is that  integration with host language and custom types allows one to write use facing apps where the ML is deeply integrated into the meat of the app. The (now defunct) swift for tensorflow example was iirc,  an audio player which  adjusts dynamically to user behavior. Julia's nascent WASM compilation can be leveraged here so people can write ML driven apps for web, mobile and serverside"}]}]}],"client_msg_id":"8cd6214f-978b-43e9-90ab-3facfa1f3f39"},{"client_msg_id":"a89cbfea-53ed-4d00-83e6-22c44f400595","type":"message","text":"Also we don't know where the next breakthrough and path will be.  Yann can be correct that it's self supervised learning, but gary marcus and others and big into hybrid models that incorporate symbolic reasoning. Those would require fast custom datastructure, custom types in symbolic term rewriting  packages etc. In some sense there could be a sapir whorf thing going on where python's ubiquity limits thinking and exploration into these alternatives. Or at least raises the cognitive and runtime cost","user":"UDGT4PM41","ts":"1615730370.451600","team":"T68168MUP","edited":{"user":"UDGT4PM41","ts":"1615730390.000000"},"blocks":[{"type":"rich_text","block_id":"3meoR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Also we don't know where the next breakthrough and path will be.  Yann can be correct that it's self supervised learning, but gary marcus and others and big into hybrid models that incorporate symbolic reasoning. Those would require fast custom datastructure, custom types in symbolic term rewriting  packages etc. In some sense there could be a sapir whorf thing going on where python's ubiquity limits thinking and exploration into these alternatives. Or at least raises the cognitive and runtime cost"}]}]}],"thread_ts":"1614965145.034100","parent_user_id":"UDGT4PM41"}]