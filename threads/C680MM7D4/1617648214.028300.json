[{"client_msg_id":"0de8b010-7d18-4faf-b1ff-d8199d8fd6ec","type":"message","text":"<https://www.lesswrong.com/posts/JZZENevaLzLLeC3zn/predictive-coding-has-been-unified-with-backpropagation|Predictive coding has been unified with backpropagation>","user":"UDGT4PM41","ts":"1617648214.028300","team":"T68168MUP","attachments":[{"title":"Predictive Coding has been Unified with Backpropagation - LessWrong","title_link":"https://www.lesswrong.com/posts/JZZENevaLzLLeC3zn/predictive-coding-has-been-unified-with-backpropagation","text":"Artificial Neural Networks (ANNs) are based around the backpropagation algorithm. The backpropagation algorithm allows you to perform gradient descent on a network of neurons. When we feed training data through an ANNs, we use the backpropagation algorithm to tell us how the weights should change. ANNs are good at inference problems. Biological Neural Networks (BNNs) are good at inference too. ANNs are built out of neurons. BNNs are built out of neurons too. It makes intuitive sense that ANNs and BNNs might be running similar algorithms. There is just one problem: BNNs are physically incapable of running the backpropagation algorithm. We do not know quite enough about biology to say it is impossible for BNNs to run the backpropagation algorithm. However, \"a consensus has emerged that the brain cannot directly implement backprop, since to do so would require biologically implausible connection rules\"[1]. The backpropagation algorithm has three steps. 1. Flow information forward through a network to compute a prediction. 2. Compute an error by comparing the prediction to a target value. 3. Flow the error backward through the network to update the weights. The backpropagation algorithm requires information to flow forward and backward along the network. But biological neurons are one-directional. An action potential goes from the cell body down the axon to the axon terminals to another cell's dendrites. An axon potential never travels backward from a cell's terminals to its body. HEBBIAN THEORY Predictive coding is the idea that BNNs generate a mental model of their environment and then transmit only the information that deviates from this model. Predictive coding considers error and surprise to be the same thing. Hebbian theory is specific mathematical formulation of predictive coding. Predictive coding is biologically plausible. It operates locally. There are no separate prediction and training phases which must be synchronized. Most importantly, it lets","fallback":"Predictive Coding has been Unified with Backpropagation - LessWrong","thumb_url":"https://s3-us-west-2.amazonaws.com/www.lsusr.com/lesswrong/predictive-coding-approximates-backprop/neuron-anatomy.png","from_url":"https://www.lesswrong.com/posts/JZZENevaLzLLeC3zn/predictive-coding-has-been-unified-with-backpropagation","thumb_width":900,"thumb_height":432,"service_icon":"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico","service_name":"lesswrong.com","id":1,"original_url":"https://www.lesswrong.com/posts/JZZENevaLzLLeC3zn/predictive-coding-has-been-unified-with-backpropagation"}],"blocks":[{"type":"rich_text","block_id":"Xv1","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://www.lesswrong.com/posts/JZZENevaLzLLeC3zn/predictive-coding-has-been-unified-with-backpropagation","text":"Predictive coding has been unified with backpropagation"}]}]}],"thread_ts":"1617648214.028300","reply_count":2,"reply_users_count":2,"latest_reply":"1617649789.032900","reply_users":["U01C3624SGJ","U01CQTKB86N"],"is_locked":false,"subscribed":false},{"client_msg_id":"1ef32722-55c4-4f77-bdd8-5e42ce2c7e9e","type":"message","text":"<https://openreview.net/forum?id=PdauS7wZBfC>","user":"U01C3624SGJ","ts":"1617648715.029200","team":"T68168MUP","attachments":[{"service_name":"OpenReview","title":"Predictive Coding Approximates Backprop along Arbitrary Computation...","title_link":"https://openreview.net/forum?id=PdauS7wZBfC","text":"The backpropagation of error (backprop) is a powerful algorithm for training machine learning architectures through end-to-end differentiation. Recently it has been shown that backprop in...","fallback":"OpenReview: Predictive Coding Approximates Backprop along Arbitrary Computation...","thumb_url":"https://openreview.net/images/openreview_logo_512.png","from_url":"https://openreview.net/forum?id=PdauS7wZBfC","thumb_width":512,"thumb_height":512,"service_icon":"https://openreview.net/favicon.ico","id":1,"original_url":"https://openreview.net/forum?id=PdauS7wZBfC"}],"blocks":[{"type":"rich_text","block_id":"jn6wv","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://openreview.net/forum?id=PdauS7wZBfC"}]}]}],"thread_ts":"1617648214.028300","parent_user_id":"UDGT4PM41"},{"client_msg_id":"B7717815-BFEA-4E4C-9BCA-424D9BDEA359","type":"message","text":"I only read the short article in lesswrong, but I do not buy this ’logic’ at all! \n”ANNs are good at inference problems. Biological Neural Networks (BNNs) are good at inference too. ANNs are built out of neurons. BNNs are built out of neurons too. It makes intuitive sense that ANNs and BNNs might be running similar algorithms.”","user":"U01CQTKB86N","ts":"1617649789.032900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kRNc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I only read the short article in lesswrong, but I do not buy this ’logic’ at all! \n"},{"type":"text","text":"”ANNs are good at inference problems. Biological Neural Networks (BNNs) are good at inference too. ANNs are built out of neurons. BNNs are built out of neurons too. It makes intuitive sense that ANNs and BNNs might be running similar algorithms.”"}]}]}],"thread_ts":"1617648214.028300","parent_user_id":"UDGT4PM41"}]