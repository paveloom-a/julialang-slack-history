[{"client_msg_id":"75df26a1-97fa-4555-974d-45daff6f9a76","type":"message","text":"<https://arxiv.org/abs/1903.03129>\nedit: this is the new paper <https://arxiv.org/abs/2103.10891>\ncurrently on the orange website, claims huge speedups for training neural nets on the CPU vs  on GPUs","user":"U011V2YN59N","ts":"1617930530.229000","team":"T68168MUP","edited":{"user":"U011V2YN59N","ts":"1617931303.000000"},"attachments":[{"service_name":"arXiv.org","title":"SLIDE : In Defense of Smart Algorithms over Hardware Acceleration...","title_link":"https://arxiv.org/abs/1903.03129","text":"Deep Learning (DL) algorithms are the central focus of modern machine learning systems. As data volumes keep growing, it has become customary to train large neural networks with hundreds of...","fallback":"arXiv.org: SLIDE : In Defense of Smart Algorithms over Hardware Acceleration...","from_url":"https://arxiv.org/abs/1903.03129","service_icon":"https://static.arxiv.org/static/browse/0.3.2.6/images/icons/favicon.ico","id":1,"original_url":"https://arxiv.org/abs/1903.03129"},{"service_name":"arXiv.org","title":"Accelerating SLIDE Deep Learning on Modern CPUs: Vectorization,...","title_link":"https://arxiv.org/abs/2103.10891","text":"Deep learning implementations on CPUs (Central Processing Units) are gaining more traction. Enhanced AI capabilities on commodity x86 architectures are commercially appealing due to the reuse of...","fallback":"arXiv.org: Accelerating SLIDE Deep Learning on Modern CPUs: Vectorization,...","from_url":"https://arxiv.org/abs/2103.10891","service_icon":"https://static.arxiv.org/static/browse/0.3.2.6/images/icons/favicon.ico","id":2,"original_url":"https://arxiv.org/abs/2103.10891"}],"blocks":[{"type":"rich_text","block_id":"SnJG","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://arxiv.org/abs/1903.03129"},{"type":"text","text":"\nedit: this is the new paper "},{"type":"link","url":"https://arxiv.org/abs/2103.10891"},{"type":"text","text":"\ncurrently on the orange website, claims huge speedups for training neural nets on the CPU vs  on GPUs"}]}]}],"thread_ts":"1617930530.229000","reply_count":15,"reply_users_count":4,"latest_reply":"1617932615.233900","reply_users":["U011V2YN59N","U0179G7FG4F","U9RDM8ZGT","UMY1LV01G"],"is_locked":false,"subscribed":false},{"client_msg_id":"080696fa-41b1-4b07-b51b-bf7bae47f806","type":"message","text":"code <https://github.com/IntelLabs/SLIDE_opt_ia>","user":"U011V2YN59N","ts":"1617930558.229200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0rry","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"code "},{"type":"link","url":"https://github.com/IntelLabs/SLIDE_opt_ia"}]}]}],"thread_ts":"1617930530.229000","parent_user_id":"U011V2YN59N"},{"client_msg_id":"4f3ad55c-11ed-4c4b-9121-cab94f282551","type":"message","text":"I remember seeing this when it was released. TLDR is if you compare heavily optimized code to completely unoptimized code, you can make a CPU look good","user":"U0179G7FG4F","ts":"1617931056.229900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"k8iE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I remember seeing this when it was released. TLDR is if you compare heavily optimized code to completely unoptimized code, you can make a CPU look good"}]}]}],"thread_ts":"1617930530.229000","parent_user_id":"U011V2YN59N"},{"client_msg_id":"fc824add-2931-426b-8786-2ed2e7b85ac6","type":"message","text":"is that really what they're doing? aren't they comparing to tensorflow?","user":"U011V2YN59N","ts":"1617931223.230700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"82oSq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is that really what they're doing? aren't they comparing to tensorflow?"}]}]}],"thread_ts":"1617930530.229000","parent_user_id":"U011V2YN59N"},{"client_msg_id":"71866e6e-2b03-4af7-a76e-e35f998b261a","type":"message","text":"I think this is a new release","user":"U011V2YN59N","ts":"1617931252.230900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SJH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think this is a new release"}]}]}],"thread_ts":"1617930530.229000","parent_user_id":"U011V2YN59N"},{"client_msg_id":"5c03ba1b-be22-4961-993a-b4898cab2bcf","type":"message","text":"it was also released a couple years ago","user":"U011V2YN59N","ts":"1617931260.231100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kC8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it was also released a couple years ago"}]}]}],"thread_ts":"1617930530.229000","parent_user_id":"U011V2YN59N"},{"client_msg_id":"b750c21b-aaf0-492f-ace7-79631d609cea","type":"message","text":"oh, apparently this is an update of the original. I saw the original in the Leela chess discord when it came out, and a bunch of people looked into it.","user":"U0179G7FG4F","ts":"1617931287.231300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bptaC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"oh, apparently this is an update of the original. I saw the original in the Leela chess discord when it came out, and a bunch of people looked into it."}]}]}],"thread_ts":"1617930530.229000","parent_user_id":"U011V2YN59N"},{"client_msg_id":"a94103fa-1369-441e-bcb0-7f61132a39ab","type":"message","text":"woops ya I think I linked to the old one by accident, updated","user":"U011V2YN59N","ts":"1617931318.231700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rxoq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"woops ya I think I linked to the old one by accident, updated"}]}]}],"thread_ts":"1617930530.229000","parent_user_id":"U011V2YN59N"},{"client_msg_id":"9f90a73e-4ade-4a58-9177-08d3b168793b","type":"message","text":"at least for the original, the problem was that the tensorflow they were comparing to was about the most naive implementation possible.","user":"U0179G7FG4F","ts":"1617931399.231900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lx2T","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"at least for the original, the problem was that the tensorflow they were comparing to was about the most naive implementation possible."}]}]}],"thread_ts":"1617930530.229000","parent_user_id":"U011V2YN59N"},{"client_msg_id":"5cc47177-6912-450d-bc24-ee96fdceb813","type":"message","text":"I see.","user":"U011V2YN59N","ts":"1617931415.232100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iz26","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I see."}]}]}],"thread_ts":"1617930530.229000","parent_user_id":"U011V2YN59N"},{"type":"message","subtype":"thread_broadcast","text":"\"sparse hash table based back-propagation\" - maybe this wouldn't be too hard to implement In Julia/Flux/Zygote? (should also post to #machinelearning )","user":"U9RDM8ZGT","ts":"1617931703.232500","thread_ts":"1617930530.229000","root":{"client_msg_id":"75df26a1-97fa-4555-974d-45daff6f9a76","type":"message","text":"<https://arxiv.org/abs/1903.03129>\nedit: this is the new paper <https://arxiv.org/abs/2103.10891>\ncurrently on the orange website, claims huge speedups for training neural nets on the CPU vs  on GPUs","user":"U011V2YN59N","ts":"1617930530.229000","team":"T68168MUP","edited":{"user":"U011V2YN59N","ts":"1617931303.000000"},"attachments":[{"service_name":"arXiv.org","title":"SLIDE : In Defense of Smart Algorithms over Hardware Acceleration...","title_link":"https://arxiv.org/abs/1903.03129","text":"Deep Learning (DL) algorithms are the central focus of modern machine learning systems. As data volumes keep growing, it has become customary to train large neural networks with hundreds of...","fallback":"arXiv.org: SLIDE : In Defense of Smart Algorithms over Hardware Acceleration...","from_url":"https://arxiv.org/abs/1903.03129","service_icon":"https://static.arxiv.org/static/browse/0.3.2.6/images/icons/favicon.ico","id":1,"original_url":"https://arxiv.org/abs/1903.03129"},{"service_name":"arXiv.org","title":"Accelerating SLIDE Deep Learning on Modern CPUs: Vectorization,...","title_link":"https://arxiv.org/abs/2103.10891","text":"Deep learning implementations on CPUs (Central Processing Units) are gaining more traction. Enhanced AI capabilities on commodity x86 architectures are commercially appealing due to the reuse of...","fallback":"arXiv.org: Accelerating SLIDE Deep Learning on Modern CPUs: Vectorization,...","from_url":"https://arxiv.org/abs/2103.10891","service_icon":"https://static.arxiv.org/static/browse/0.3.2.6/images/icons/favicon.ico","id":2,"original_url":"https://arxiv.org/abs/2103.10891"}],"blocks":[{"type":"rich_text","block_id":"SnJG","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://arxiv.org/abs/1903.03129"},{"type":"text","text":"\nedit: this is the new paper "},{"type":"link","url":"https://arxiv.org/abs/2103.10891"},{"type":"text","text":"\ncurrently on the orange website, claims huge speedups for training neural nets on the CPU vs  on GPUs"}]}]}],"thread_ts":"1617930530.229000","reply_count":15,"reply_users_count":4,"latest_reply":"1617932615.233900","reply_users":["U011V2YN59N","U0179G7FG4F","U9RDM8ZGT","UMY1LV01G"],"is_locked":false,"subscribed":false},"blocks":[{"type":"rich_text","block_id":"e5QOg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"\"sparse hash table based back-propagation\" - maybe this wouldn't be too hard to implement In Julia/Flux/Zygote? (should also post to #machinelearning )"}]}]}],"client_msg_id":"5cbdb2b9-7da7-4de3-92e4-6445df9b0970"},{"client_msg_id":"0ef16c3d-6261-4bad-81b7-36735c080d01","type":"message","text":"I remember the original as well. The very specific tasks chosen and that they haven't optimized the TF GPU implementation at all remains a red flag","user":"UMY1LV01G","ts":"1617932378.232800","team":"T68168MUP","edited":{"user":"UMY1LV01G","ts":"1617932392.000000"},"blocks":[{"type":"rich_text","block_id":"oaDSq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I remember the original as well. The very specific tasks chosen and that they haven't optimized the TF GPU implementation at all remains a red flag"}]}]}],"thread_ts":"1617930530.229000","parent_user_id":"U011V2YN59N","reactions":[{"name":"+1","users":["U011V2YN59N"],"count":1}]},{"client_msg_id":"e4f83b68-e094-404a-9a79-7db090a86d78","type":"message","text":"I see. It's always good to post this stuff here, I don't know enough about TF implementations to know that.","user":"U011V2YN59N","ts":"1617932462.233200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"T34G","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I see. It's always good to post this stuff here, I don't know enough about TF implementations to know that."}]}]}],"thread_ts":"1617930530.229000","parent_user_id":"U011V2YN59N"},{"client_msg_id":"9e5de400-5daf-4a89-8080-6ffcbf5c9fea","type":"message","text":"The other thing to notice is that this was funded by Intel, so while they are obviously very smart, they have a result in mind when they start the research.","user":"U0179G7FG4F","ts":"1617932529.233400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"X7T","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The other thing to notice is that this was funded by Intel, so while they are obviously very smart, they have a result in mind when they start the research."}]}]}],"thread_ts":"1617930530.229000","parent_user_id":"U011V2YN59N","reactions":[{"name":"+1","users":["U011V2YN59N"],"count":1}]},{"client_msg_id":"630ff3f5-1a45-4c19-8a28-0176e58ca2c2","type":"message","text":"Moreover, I haven't seen a comparison of the convergence guarantees of their approach over vanilla SGD. If it's like HOGWILD, then they won't be the same and it's kind of apples and oranges","user":"UMY1LV01G","ts":"1617932531.233600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7pJpT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Moreover, I haven't seen a comparison of the convergence guarantees of their approach over vanilla SGD. If it's like HOGWILD, then they won't be the same and it's kind of apples and oranges"}]}]}],"thread_ts":"1617930530.229000","parent_user_id":"U011V2YN59N"},{"client_msg_id":"2d0a645e-44e9-44ab-a326-2d951a8b62e2","type":"message","text":"That's not to say it's not useful (NN-512 and ZeRO-Offload are a good examples of what can be done when we focus on optimizing CPU-side computation), but all the marketing around it is suspect at best","user":"UMY1LV01G","ts":"1617932615.233900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OWFk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That's not to say it's not useful (NN-512 and ZeRO-Offload are a good examples of what can be done when we focus on optimizing CPU-side computation), but all the marketing around it is suspect at best"}]}]}],"thread_ts":"1617930530.229000","parent_user_id":"U011V2YN59N"}]