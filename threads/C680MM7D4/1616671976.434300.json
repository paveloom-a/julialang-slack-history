[{"client_msg_id":"7fad43d5-bd34-4a49-b75b-a6ffa25610ac","type":"message","text":"My new parallel implementation of something is much faster than my original one… but only if you remove the `@threads` macro…","user":"U6C937ENB","ts":"1616671976.434300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FKdQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"My new parallel implementation of something is much faster than my original one… but only if you remove the "},{"type":"text","text":"@threads","style":{"code":true}},{"type":"text","text":" macro…"}]}]}],"thread_ts":"1616671976.434300","reply_count":12,"reply_users_count":3,"latest_reply":"1616674550.437600","reply_users":["U6C937ENB","U7HAYKY9X","U6N6VQE30"],"is_locked":false,"subscribed":false,"reactions":[{"name":"wat","users":["U7HAYKY9X"],"count":1},{"name":"smile","users":["U677NAWV8"],"count":1}]},{"client_msg_id":"b7da7d50-f9b2-4f14-99b9-79b2b2714982","type":"message","text":"It’s a small riddle: I think the “parallel” version is more cache friendly because it works on subarrays, but the live times of task are too short to justify `@spawn`ing","user":"U6C937ENB","ts":"1616672037.434400","team":"T68168MUP","edited":{"user":"U6C937ENB","ts":"1616672248.000000"},"blocks":[{"type":"rich_text","block_id":"OO3Jm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It’s a small riddle: I think the “parallel” version is more cache friendly because it works on subarrays, but the live times of task are too short to justify "},{"type":"text","text":"@spawn","style":{"code":true}},{"type":"text","text":"ing"}]}]}],"thread_ts":"1616671976.434300","parent_user_id":"U6C937ENB"},{"client_msg_id":"f2e88fdf-6362-414b-8790-d980eb200253","type":"message","text":"Do different threads operate on things close together in memory?","user":"U7HAYKY9X","ts":"1616672246.434700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"F35hw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do different threads operate on things close together in memory?"}]}]}],"thread_ts":"1616671976.434300","parent_user_id":"U6C937ENB"},{"client_msg_id":"4153cc4b-68b4-403f-a361-a3805813d7c2","type":"message","text":"yes, is that a problem?","user":"U6C937ENB","ts":"1616672260.435000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"K0w","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yes, is that a problem?"}]}]}],"thread_ts":"1616671976.434300","parent_user_id":"U6C937ENB"},{"client_msg_id":"2b1fd03c-50fa-48e6-a73f-420d9e0b84c9","type":"message","text":"Yes. Because cache syncronizes in chunks, if one thread writes to memory, it will force cache invalidations from all the other threads of all nearby memory","user":"U7HAYKY9X","ts":"1616672309.435200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"W7fK1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes. Because cache syncronizes in chunks, if one thread writes to memory, it will force cache invalidations from all the other threads of all nearby memory"}]}]}],"thread_ts":"1616671976.434300","parent_user_id":"U6C937ENB","reactions":[{"name":"heart","users":["U6C937ENB","U010LT79LKX"],"count":2},{"name":"this","users":["U017J1FHTSA","ULG5V164A"],"count":2}]},{"client_msg_id":"a819b2db-d9d9-47b9-abf1-cbdca8bc256f","type":"message","text":"they are working on different halves on the same array, ok, so I should split that","user":"U6C937ENB","ts":"1616672345.435400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UhJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"they are working on different halves on the same array, ok, so I should split that"}]}]}],"thread_ts":"1616671976.434300","parent_user_id":"U6C937ENB"},{"client_msg_id":"08259734-5e48-4e44-b206-e375ea39ec8c","type":"message","text":"Do we have a wrapper for arrays which can achieve that?","user":"U6C937ENB","ts":"1616672436.435600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"djUM5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do we have a wrapper for arrays which can achieve that?"}]}]}],"thread_ts":"1616671976.434300","parent_user_id":"U6C937ENB"},{"client_msg_id":"09919516-7883-4cff-a6d7-201b2c8d6209","type":"message","text":"(though I think it's usually just one cache line, 64 bytes)","user":"U7HAYKY9X","ts":"1616672908.436000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"e5h3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(though I think it's usually just one cache line, 64 bytes)"}]}]}],"thread_ts":"1616671976.434300","parent_user_id":"U6C937ENB"},{"client_msg_id":"4713c4a1-9bd9-44d2-b37b-51e8618c9cf3","type":"message","text":"So it should not be a problem if one thread writes to the first part of an array and another to the last part of an array. It should only be a problem if .e.g one thread writes to all uneven indices and another to all even indices","user":"U7HAYKY9X","ts":"1616673074.436200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zVR2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So it should not be a problem if one thread writes to the first part of an array and another to the last part of an array. It should only be a problem if .e.g one thread writes to all uneven indices and another to all even indices"}]}]}],"thread_ts":"1616671976.434300","parent_user_id":"U6C937ENB"},{"client_msg_id":"7c2d8beb-fe2f-4255-aee1-fc81c679487b","type":"message","text":"<https://en.wikipedia.org/wiki/False_sharing>","user":"U7HAYKY9X","ts":"1616673270.436400","team":"T68168MUP","attachments":[{"title":"False sharing","title_link":"https://en.wikipedia.org/wiki/False_sharing","from_url":"https://en.wikipedia.org/wiki/False_sharing","author_name":"Wikipedia","author_link":"https://en.wikipedia.org/","text":"In computer science, false sharing is a performance-degrading usage pattern that can arise in systems with distributed, coherent caches at the size of the smallest resource block managed by the caching mechanism.  When a system participant attempts to periodically access data that will never be altered by another party, but those data share a cache block with data that are altered, the caching protocol may force the first participant to reload the whole unit despite a lack of logical necessity.  The caching system is unaware of activity within this block and forces the first participant to bear the caching system overhead required by true shared access of a resource.\nBy far the most common usage of this term is in modern multiprocessor CPU caches, where memory is cached in lines of some small power of two word size (e.g., 64 aligned, contiguous bytes).  If two processors operate on independent data in the same memory address region storable in a single line, the cache coherency mechanisms in the system may force the whole line across the bus or interconnect with every data write, forcing memory stalls in addition to wasting system bandwidth.  False sharing is an inherent artifact of automatically synchronized cache protocols and can also exist in environments such as distributed file systems or databases, but current prevalence is limited to RAM caches.","fallback":"wikipedia: False sharing","service_icon":"https://a.slack-edge.com/80588/img/unfurl_icons/wikipedia.png","id":1,"original_url":"https://en.wikipedia.org/wiki/False_sharing"}],"blocks":[{"type":"rich_text","block_id":"jvU","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://en.wikipedia.org/wiki/False_sharing"}]}]}],"thread_ts":"1616671976.434300","parent_user_id":"U6C937ENB"},{"client_msg_id":"2659059f-1f9a-4d2f-9c89-1c00a8ae2672","type":"message","text":"```(run / runs, runs) = (12258.604519774011, 1770)\n 37.919936 seconds (154.03 M allocations: 4.675 GiB, 8.13% gc time)```\nIndicates that I `@spawn` 2000 threads (in pairs) and each thread does on average 12000 iterations until getting called back for synchronization","user":"U6C937ENB","ts":"1616673861.437100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vUQO","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"(run / runs, runs) = (12258.604519774011, 1770)\n 37.919936 seconds (154.03 M allocations: 4.675 GiB, 8.13% gc time)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Indicates that I "},{"type":"text","text":"@spawn","style":{"code":true}},{"type":"text","text":" 2000 threads (in pairs) and each thread does on average 12000 iterations until getting called back for synchronization"}]}]}],"thread_ts":"1616671976.434300","parent_user_id":"U6C937ENB"},{"client_msg_id":"f49f51a0-864e-478b-9146-e09d126d206f","type":"message","text":"So what is the price tag of a spawn? I didnt think much more than order of milli-seconds?","user":"U6C937ENB","ts":"1616674178.437400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oTOi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So what is the price tag of a spawn? I didnt think much more than order of milli-seconds?"}]}]}],"thread_ts":"1616671976.434300","parent_user_id":"U6C937ENB"},{"client_msg_id":"51381dcd-10f1-4a96-a11e-952bfaec5282","type":"message","text":"in my experience spawning a task is not expensive, but using `@spawn` makes the compiler fail to do inference more often than not","user":"U6N6VQE30","ts":"1616674550.437600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"W0p","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"in my experience spawning a task is not expensive, but using "},{"type":"text","text":"@spawn","style":{"code":true}},{"type":"text","text":" makes the compiler fail to do inference more often than not"}]}]}],"thread_ts":"1616671976.434300","parent_user_id":"U6C937ENB"}]