[{"client_msg_id":"430fdd70-ce0f-4688-a205-cb30fe55c70c","type":"message","text":"This looks interesting\n<https://arxiv.org/pdf/2102.09798.pdf>.\n¿Someone more knowledgeable on the math and theory to comment? :slightly_smiling_face:","user":"U0110B59U0Y","ts":"1614140116.160200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"08t4r","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This looks interesting\n"},{"type":"link","url":"https://arxiv.org/pdf/2102.09798.pdf"},{"type":"text","text":".\n¿Someone more knowledgeable on the math and theory to comment? "},{"type":"emoji","name":"slightly_smiling_face"}]}]}],"thread_ts":"1614140116.160200","reply_count":5,"reply_users_count":3,"latest_reply":"1614171257.166000","reply_users":["U0179G7FG4F","U0110B59U0Y","UBEF50B7C"],"subscribed":false},{"client_msg_id":"c9921e29-2047-4f90-8c20-e7e47835b6d6","type":"message","text":"It seems vaguely interesting from a purely theoretical perspective, but all it really seems to say is that finding global minima of NNs is really hard, which isn't an especially new observation. Also the difference between the previous bound of NP hard, and this bound is not that important given that NNs are typically orders of magnitude too big to solve exactly even if they were only NP hard","user":"U0179G7FG4F","ts":"1614141204.160300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lOj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It seems vaguely interesting from a purely theoretical perspective, but all it really seems to say is that finding global minima of NNs is really hard, which isn't an especially new observation. Also the difference between the previous bound of NP hard, and this bound is not that important given that NNs are typically orders of magnitude too big to solve exactly even if they were only NP hard"}]}]}],"thread_ts":"1614140116.160200","parent_user_id":"U0110B59U0Y","reactions":[{"name":"thinking_face","users":["U0110B59U0Y"],"count":1}]},{"client_msg_id":"e7021bb9-8525-486f-8853-642354497b7c","type":"message","text":"Thanks for the explanation. I'm taking a Machine Learning Class with some PhD student on computer science and the teacher just landed this today to comment. I actually didn't get all that from the reading (I'm a undergraduate student on computer science), so really appreciate the comment.","user":"U0110B59U0Y","ts":"1614141871.160500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Yq5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for the explanation. I'm taking a Machine Learning Class with some PhD student on computer science and the teacher just landed this today to comment. I actually didn't get all that from the reading (I'm a undergraduate student on computer science), so really appreciate the comment."}]}]}],"thread_ts":"1614140116.160200","parent_user_id":"U0110B59U0Y"},{"client_msg_id":"8f638e3f-3328-4085-bbfb-376323f248fa","type":"message","text":"the key is that their definition of \"training\" a neural network is asking \"Do there exist weights and biases of (the NN) such that C(D)≤δ?\", ie given any δ is that loss achievable by the neural network. Since δ can be the global minima, this problem is at least as hard as finding a global minimum","user":"U0179G7FG4F","ts":"1614143468.161000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3vv+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"the key is that their definition of \"training\" a neural network is asking \"Do there exist weights and biases of (the NN) such that C(D)≤δ?\", ie given any δ is that loss achievable by the neural network. Since δ can be the global minima, this problem is at least as hard as finding a global minimum"}]}]}],"thread_ts":"1614140116.160200","parent_user_id":"U0110B59U0Y"},{"client_msg_id":"c87b16ad-e4e1-42b7-af35-5ed26bdfa346","type":"message","text":"Disregard what I just wrote before.\n\nSo my interpretation is, that this means that the weights that minimize the neural network below a  certain threshold are irrational numbers that might be extremely long (i.e the number of non-zeros after the the dot of a suitable zeros is more than any polynomial of the input size)","user":"UBEF50B7C","ts":"1614170809.163500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7C5n","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Disregard what I just wrote before.\n\nSo my interpretation is, that this means that the weights that minimize the neural network below a  certain threshold are irrational numbers that might be extremely long (i.e the number of non-zeros after the the dot of a suitable zeros is more than any polynomial of the input size)"}]}]}],"thread_ts":"1614140116.160200","parent_user_id":"U0110B59U0Y"},{"client_msg_id":"5a24d9b6-2964-4c70-a771-c6af6540539b","type":"message","text":"I think these two paragraphs from the paper are quite interesting:\n\n&gt; By now, strong algorithmic methods and tools are known with which we can find optimal solutions tolarge scale instances ofNP-complete problems. We highlight here FPT algorithms, ILP solvers, and SATsolvers, to name just a few popular approaches. We are completely lacking similarly efficient approaches tosolve∃R-complete problems. Here, it is most common to use some type of gradient descent method, which,in the context of neural networks, includes the backpropagation algorithm. Unfortunately, gradient descentmethods have very weak performance guarantees in general. Specifically, it is difficult to distinguish betweenlocal and global <http://optima.It|optima.It> is interesting to find or disprove the existence of methods that outperform gradient descent methods forthe problemNN-Training. For instance, methods with the convergence speed of ILP solvers would be agreat asset, saving money, energy, and time, and returning solutions of a higher quality. However, our resultsindicate that this is not achievable in general.","user":"UBEF50B7C","ts":"1614171257.166000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xb/0h","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think these two paragraphs from the paper are quite interesting:\n\n"}]},{"type":"rich_text_quote","elements":[{"type":"text","text":"By now, strong algorithmic methods and tools are known with which we can find optimal solutions tolarge scale instances ofNP-complete problems. We highlight here FPT algorithms, ILP solvers, and SATsolvers, to name just a few popular approaches. We are completely lacking similarly efficient approaches tosolve∃R-complete problems. Here, it is most common to use some type of gradient descent method, which,in the context of neural networks, includes the backpropagation algorithm. Unfortunately, gradient descentmethods have very weak performance guarantees in general. Specifically, it is difficult to distinguish betweenlocal and global optima.It is interesting to find or disprove the existence of methods that outperform gradient descent methods forthe problemNN-Training. For instance, methods with the convergence speed of ILP solvers would be agreat asset, saving money, energy, and time, and returning solutions of a higher quality. However, our resultsindicate that this is not achievable in general."}]}]}],"thread_ts":"1614140116.160200","parent_user_id":"U0110B59U0Y"}]