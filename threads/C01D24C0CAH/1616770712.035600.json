[{"client_msg_id":"a99abd34-4cae-4948-8432-ff4cd2fce29c","type":"message","text":"<@UEP056STX> and I have been doing some benchmarking of the `ShallowWaterModel` on `CPU`s vs `GPU`s.  After learning more about benchmarking than we had anticpated, we have the following results to share, which were run on a Tesla V100-SXM2-32GB machine.\n```        Shallow water model CPU -&gt; GPU speedup\n┌─────────────┬───────┬──────────┬─────────┬─────────┐\n│ Float_types │    Ns │  speedup │  memory │  allocs │\n├─────────────┼───────┼──────────┼─────────┼─────────┤\n│     Float64 │    32 │ 0.697116 │ 2.09724 │ 2.97978 │\n│     Float64 │    64 │  1.02927 │ 2.15944 │ 2.97029 │\n│     Float64 │   128 │  2.42711 │ 2.27406 │ 2.97029 │\n│     Float64 │   256 │  7.32025 │ 2.52177 │ 2.97029 │\n│     Float64 │   512 │  28.1914 │ 2.98971 │  2.9174 │\n│     Float64 │  1024 │  112.326 │ 3.93542 │ 2.93495 │\n│     Float64 │  2048 │  175.735 │ 5.81322 │ 2.98141 │\n│     Float64 │  4096 │  183.947 │ 9.58727 │ 2.96283 │\n│     Float64 │  8192 │  189.795 │  17.139 │ 2.96283 │\n│     Float64 │ 16384 │  186.833 │ 32.2437 │ 2.97419 │\n└─────────────┴───────┴──────────┴─────────┴─────────┘```\nWe weren't able to go any higher than this resolution, so we must be close to the 32GB of memory limit.  When we start testing `MultiGPU` (or `MPI_GPU`) we should be able to raise the wall and do even higher resolutions on two or more GPUs.\n\nThe punchline is that starting at `1024^2` we get a speed up of over `100` and it nearly doubles to `190`  at `4096^2`. To put this in context, to get something compariable, we would need `190` CPUs with MPI running at perfect efficiency.  Our eficiency performances of MPI showed that after 32 cores we had an efficiency of just over 80%.  I supsect we would need a lot more than 256 cores on the MPI code to run this quicky, but this is not something that we have checked (yet).  One GPU does a rather amazing job and should probably be used instead.","user":"U01FBLBCP7S","ts":"1616770712.035600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PG5","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UEP056STX"},{"type":"text","text":" and I have been doing some benchmarking of the "},{"type":"text","text":"ShallowWaterModel","style":{"code":true}},{"type":"text","text":" on `CPU`s vs `GPU`s.  After learning more about benchmarking than we had anticpated, we have the following results to share, which were run on a Tesla V100-SXM2-32GB machine.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"        Shallow water model CPU -> GPU speedup\n┌─────────────┬───────┬──────────┬─────────┬─────────┐\n│ Float_types │    Ns │  speedup │  memory │  allocs │\n├─────────────┼───────┼──────────┼─────────┼─────────┤\n│     Float64 │    32 │ 0.697116 │ 2.09724 │ 2.97978 │\n│     Float64 │    64 │  1.02927 │ 2.15944 │ 2.97029 │\n│     Float64 │   128 │  2.42711 │ 2.27406 │ 2.97029 │\n│     Float64 │   256 │  7.32025 │ 2.52177 │ 2.97029 │\n│     Float64 │   512 │  28.1914 │ 2.98971 │  2.9174 │\n│     Float64 │  1024 │  112.326 │ 3.93542 │ 2.93495 │\n│     Float64 │  2048 │  175.735 │ 5.81322 │ 2.98141 │\n│     Float64 │  4096 │  183.947 │ 9.58727 │ 2.96283 │\n│     Float64 │  8192 │  189.795 │  17.139 │ 2.96283 │\n│     Float64 │ 16384 │  186.833 │ 32.2437 │ 2.97419 │\n└─────────────┴───────┴──────────┴─────────┴─────────┘"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"We weren't able to go any higher than this resolution, so we must be close to the 32GB of memory limit.  When we start testing "},{"type":"text","text":"MultiGPU","style":{"code":true}},{"type":"text","text":" (or "},{"type":"text","text":"MPI_GPU","style":{"code":true}},{"type":"text","text":") we should be able to raise the wall and do even higher resolutions on two or more GPUs.\n\nThe punchline is that starting at "},{"type":"text","text":"1024^2","style":{"code":true}},{"type":"text","text":" we get a speed up of over "},{"type":"text","text":"100","style":{"code":true}},{"type":"text","text":" and it nearly doubles to "},{"type":"text","text":"190","style":{"code":true}},{"type":"text","text":"  at "},{"type":"text","text":"4096^2","style":{"code":true}},{"type":"text","text":". To put this in context, to get something compariable, we would need "},{"type":"text","text":"190","style":{"code":true}},{"type":"text","text":" CPUs with MPI running at perfect efficiency.  Our eficiency performances of MPI showed that after 32 cores we had an efficiency of just over 80%.  I supsect we would need a lot more than 256 cores on the MPI code to run this quicky, but this is not something that we have checked (yet).  One GPU does a rather amazing job and should probably be used instead."}]}]}],"thread_ts":"1616770712.035600","reply_count":10,"reply_users_count":3,"latest_reply":"1617109241.040300","reply_users":["U67BJLYCS","U01FBLBCP7S","U010LHCP277"],"is_locked":false,"subscribed":false,"reactions":[{"name":"rocket","users":["UEP056STX","U01E0U2RJJ2"],"count":2},{"name":"+1","users":["U01G39CC63F"],"count":1}]},{"client_msg_id":"60bf303c-0a6f-4f95-b3fa-2219af84d306","type":"message","text":"Nice! I usually keep an eye on `nvidia-smi` to gauge the memory usage of a code","user":"U67BJLYCS","ts":"1616773586.035700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"CVXL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Nice! I usually keep an eye on "},{"type":"text","text":"nvidia-smi","style":{"code":true}},{"type":"text","text":" to gauge the memory usage of a code"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"4f9e3d98-b7ec-4a3a-9d22-c2a1845f9c10","type":"message","text":"Thanks for the suggestion <@U67BJLYCS>, I will try and find that and give it a try.  That would be most helpful","user":"U01FBLBCP7S","ts":"1616777071.036200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xNJEb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for the suggestion "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":", I will try and find that and give it a try.  That would be most helpful"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"ba5d2bad-d451-4338-b167-f08eadccc59f","type":"message","text":"<@U01FBLBCP7S>, <@UEP056STX> and <@U67BJLYCS> it might be interesting to try KA at 32 threads on CPU. we don't have to share the results - but I would interested in how it currently does.","user":"U010LHCP277","ts":"1617040777.038800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iji","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U01FBLBCP7S"},{"type":"text","text":", "},{"type":"user","user_id":"UEP056STX"},{"type":"text","text":" and "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":" it might be interesting to try KA at 32 threads on CPU. we don't have to share the results - but I would interested in how it currently does."}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"9fd3ae5b-6b99-468c-bc63-ae241e61dc0b","type":"message","text":"Thanks for the suggestion <@U010LHCP277>.  In this not so old PR we did some MPI cpu tests going up to 32 cores.  <https://github.com/CliMA/Oceananigans.jl/pull/1505>","user":"U01FBLBCP7S","ts":"1617040932.039100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"T5y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for the suggestion "},{"type":"user","user_id":"U010LHCP277"},{"type":"text","text":".  In this not so old PR we did some MPI cpu tests going up to 32 cores.  "},{"type":"link","url":"https://github.com/CliMA/Oceananigans.jl/pull/1505"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"5d781308-4535-4331-8e30-9e3cc9a8d20f","type":"message","text":"Is this what you had in mind, or did I misunderstand something?","user":"U01FBLBCP7S","ts":"1617040944.039300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"c4T+U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is this what you had in mind, or did I misunderstand something?"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"15c616fc-7841-4729-88eb-ee71ca750ecc","type":"message","text":"No Chris wnats to set `JULIA_NUM_THREADS=32`","user":"U67BJLYCS","ts":"1617040967.039500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sWy60","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"No Chris wnats to set "},{"type":"text","text":"JULIA_NUM_THREADS=32","style":{"code":true}}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"1807d60e-a770-4ed4-b451-0b957a3c75db","type":"message","text":"and see if CPU parallelism with KernelAbstractions scale okay","user":"U67BJLYCS","ts":"1617040987.039700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Thby","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"and see if CPU parallelism with KernelAbstractions scale okay"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"28996f7d-1156-4904-91b1-3f37af7bcbca","type":"message","text":"<@U01FBLBCP7S> thanks, its not urgent - just interested in how KA is doing.","user":"U010LHCP277","ts":"1617042142.039900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PPFyc","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U01FBLBCP7S"},{"type":"text","text":" thanks, its not urgent - just interested in how KA is doing."}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"a5ca2945-4d36-4613-9ece-26a6fa391014","type":"message","text":"Ah, thanks for explaining <@U67BJLYCS>.  I have never actually tried running Oceananigans with threads so that will be a good learning experience.","user":"U01FBLBCP7S","ts":"1617042256.040100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"k1b","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah, thanks for explaining "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":".  I have never actually tried running Oceananigans with threads so that will be a good learning experience."}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"a27070b3-37c1-4bca-a5d6-c508917044c5","type":"message","text":"<@U010LHCP277> I'm having difficulties running the code on threads on the server so I thought I would do a test on my desktop, which has 18 cores, so should be able to go as high as 36 threads.  Also, I am redoing the weak scaling benchmark since I thought that would be easier.  Below are the results from MPI.  The efficiency is not as good as on the server as the number of cores increases but there you have it.\n```                                 Shallow water model weak scaling benchmark\n┌──────────────┬─────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────┬─────────┐\n│         size │   ranks │        min │     median │       mean │        max │     memory │ allocs │ samples │\n├──────────────┼─────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────┼─────────┤\n│  (4096, 256) │  (1, 1) │ 529.446 ms │ 538.296 ms │ 537.873 ms │ 545.570 ms │ 384.94 KiB │   2665 │      10 │\n│  (4096, 512) │  (1, 2) │ 555.490 ms │ 558.281 ms │ 559.896 ms │ 576.204 ms │ 376.92 KiB │   3178 │      18 │\n│ (4096, 1024) │  (1, 4) │ 573.427 ms │ 578.697 ms │ 579.280 ms │ 586.371 ms │ 376.92 KiB │   3178 │      36 │\n│ (4096, 2048) │  (1, 8) │ 620.840 ms │ 626.854 ms │ 626.948 ms │ 633.596 ms │ 376.92 KiB │   3178 │      64 │\n│ (4096, 4096) │ (1, 16) │ 778.615 ms │ 791.477 ms │ 791.290 ms │ 799.397 ms │ 376.92 KiB │   3178 │     112 │\n└──────────────┴─────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────┴─────────┘\n[2021/03/30 07:34:44.154] INFO  Writing Shallow_water_model_weak_scaling_benchmark.html...\n               Shallow water model weak scaling speedup\n┌──────────────┬─────────┬──────────┬────────────┬──────────┬────────┐\n│         size │   ranks │ slowdown │ efficiency │   memory │ allocs │\n├──────────────┼─────────┼──────────┼────────────┼──────────┼────────┤\n│  (4096, 256) │  (1, 1) │      1.0 │        1.0 │      1.0 │    1.0 │\n│  (4096, 512) │  (1, 2) │  1.03713 │   0.964203 │ 0.979177 │ 1.1925 │\n│ (4096, 1024) │  (1, 4) │  1.07505 │   0.930187 │ 0.979177 │ 1.1925 │\n│ (4096, 2048) │  (1, 8) │  1.16451 │   0.858727 │ 0.979177 │ 1.1925 │\n│ (4096, 4096) │ (1, 16) │  1.47034 │   0.680116 │ 0.979177 │ 1.1925 │\n└──────────────┴─────────┴──────────┴────────────┴──────────┴────────┘```\nNext, I specified the number of threads to be 2 times the number of coures, so 1,4,8,16,32.  Below are the results and I must say that I'm a bit confused by the results.  I also have no experience in threading in Julia so am not sure what to expect.  Any reactions are welcome.\n```                                  Shallow water model weak scaling benchmark\n┌──────────────┬─────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────┬─────────┐\n│         size │   ranks │        min │     median │       mean │        max │     memory │ allocs │ samples │\n├──────────────┼─────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────┼─────────┤\n│  (4096, 256) │  (1, 1) │    8.070 s │    8.070 s │    8.070 s │    8.070 s │ 385.63 KiB │   2709 │       1 │\n│  (4096, 512) │  (1, 2) │ 553.727 ms │ 559.750 ms │ 560.117 ms │ 578.846 ms │ 376.92 KiB │   3178 │      18 │\n│ (4096, 1024) │  (1, 4) │ 573.427 ms │ 578.697 ms │ 579.280 ms │ 586.371 ms │ 376.92 KiB │   3178 │      36 │\n│ (4096, 2048) │  (1, 8) │ 620.840 ms │ 626.854 ms │ 626.948 ms │ 633.596 ms │ 376.92 KiB │   3178 │      64 │\n│ (4096, 4096) │ (1, 16) │ 778.615 ms │ 791.477 ms │ 791.290 ms │ 799.397 ms │ 376.92 KiB │   3178 │     112 │\n└──────────────┴─────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────┴─────────┘\n[2021/03/30 08:53:33.687] INFO  Writing Shallow_water_model_weak_scaling_benchmark.html...\n                Shallow water model weak scaling speedup\n┌──────────────┬─────────┬───────────┬────────────┬──────────┬─────────┐\n│         size │   ranks │  slowdown │ efficiency │   memory │  allocs │\n├──────────────┼─────────┼───────────┼────────────┼──────────┼─────────┤\n│  (4096, 256) │  (1, 1) │       1.0 │        1.0 │      1.0 │     1.0 │\n│  (4096, 512) │  (1, 2) │ 0.0693615 │    14.4172 │ 0.977431 │ 1.17313 │\n│ (4096, 1024) │  (1, 4) │ 0.0717093 │    13.9452 │ 0.977431 │ 1.17313 │\n│ (4096, 2048) │  (1, 8) │ 0.0776766 │    12.8739 │ 0.977431 │ 1.17313 │\n│ (4096, 4096) │ (1, 16) │ 0.0980759 │    10.1962 │ 0.977431 │ 1.17313 │\n└──────────────┴─────────┴───────────┴────────────┴──────────┴─────────┘```\nIt seems like there is a huge change from 1 to multi, which I don't believe should be the case.  But the timings actually seem to compare fairly well with what we saw in the MPI case.  I'm happy to share my code in a new branch if someone wanted to look at my first attempt to try and compare MPI vs threading.","user":"U01FBLBCP7S","ts":"1617109241.040300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kVz","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U010LHCP277"},{"type":"text","text":" I'm having difficulties running the code on threads on the server so I thought I would do a test on my desktop, which has 18 cores, so should be able to go as high as 36 threads.  Also, I am redoing the weak scaling benchmark since I thought that would be easier.  Below are the results from MPI.  The efficiency is not as good as on the server as the number of cores increases but there you have it.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"                                 Shallow water model weak scaling benchmark\n┌──────────────┬─────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────┬─────────┐\n│         size │   ranks │        min │     median │       mean │        max │     memory │ allocs │ samples │\n├──────────────┼─────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────┼─────────┤\n│  (4096, 256) │  (1, 1) │ 529.446 ms │ 538.296 ms │ 537.873 ms │ 545.570 ms │ 384.94 KiB │   2665 │      10 │\n│  (4096, 512) │  (1, 2) │ 555.490 ms │ 558.281 ms │ 559.896 ms │ 576.204 ms │ 376.92 KiB │   3178 │      18 │\n│ (4096, 1024) │  (1, 4) │ 573.427 ms │ 578.697 ms │ 579.280 ms │ 586.371 ms │ 376.92 KiB │   3178 │      36 │\n│ (4096, 2048) │  (1, 8) │ 620.840 ms │ 626.854 ms │ 626.948 ms │ 633.596 ms │ 376.92 KiB │   3178 │      64 │\n│ (4096, 4096) │ (1, 16) │ 778.615 ms │ 791.477 ms │ 791.290 ms │ 799.397 ms │ 376.92 KiB │   3178 │     112 │\n└──────────────┴─────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────┴─────────┘\n[2021/03/30 07:34:44.154] INFO  Writing Shallow_water_model_weak_scaling_benchmark.html...\n               Shallow water model weak scaling speedup\n┌──────────────┬─────────┬──────────┬────────────┬──────────┬────────┐\n│         size │   ranks │ slowdown │ efficiency │   memory │ allocs │\n├──────────────┼─────────┼──────────┼────────────┼──────────┼────────┤\n│  (4096, 256) │  (1, 1) │      1.0 │        1.0 │      1.0 │    1.0 │\n│  (4096, 512) │  (1, 2) │  1.03713 │   0.964203 │ 0.979177 │ 1.1925 │\n│ (4096, 1024) │  (1, 4) │  1.07505 │   0.930187 │ 0.979177 │ 1.1925 │\n│ (4096, 2048) │  (1, 8) │  1.16451 │   0.858727 │ 0.979177 │ 1.1925 │\n│ (4096, 4096) │ (1, 16) │  1.47034 │   0.680116 │ 0.979177 │ 1.1925 │\n└──────────────┴─────────┴──────────┴────────────┴──────────┴────────┘"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Next, I specified the number of threads to be 2 times the number of coures, so 1,4,8,16,32.  Below are the results and I must say that I'm a bit confused by the results.  I also have no experience in threading in Julia so am not sure what to expect.  Any reactions are welcome.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"                                  Shallow water model weak scaling benchmark\n┌──────────────┬─────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────┬─────────┐\n│         size │   ranks │        min │     median │       mean │        max │     memory │ allocs │ samples │\n├──────────────┼─────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────┼─────────┤\n│  (4096, 256) │  (1, 1) │    8.070 s │    8.070 s │    8.070 s │    8.070 s │ 385.63 KiB │   2709 │       1 │\n│  (4096, 512) │  (1, 2) │ 553.727 ms │ 559.750 ms │ 560.117 ms │ 578.846 ms │ 376.92 KiB │   3178 │      18 │\n│ (4096, 1024) │  (1, 4) │ 573.427 ms │ 578.697 ms │ 579.280 ms │ 586.371 ms │ 376.92 KiB │   3178 │      36 │\n│ (4096, 2048) │  (1, 8) │ 620.840 ms │ 626.854 ms │ 626.948 ms │ 633.596 ms │ 376.92 KiB │   3178 │      64 │\n│ (4096, 4096) │ (1, 16) │ 778.615 ms │ 791.477 ms │ 791.290 ms │ 799.397 ms │ 376.92 KiB │   3178 │     112 │\n└──────────────┴─────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────┴─────────┘\n[2021/03/30 08:53:33.687] INFO  Writing Shallow_water_model_weak_scaling_benchmark.html...\n                Shallow water model weak scaling speedup\n┌──────────────┬─────────┬───────────┬────────────┬──────────┬─────────┐\n│         size │   ranks │  slowdown │ efficiency │   memory │  allocs │\n├──────────────┼─────────┼───────────┼────────────┼──────────┼─────────┤\n│  (4096, 256) │  (1, 1) │       1.0 │        1.0 │      1.0 │     1.0 │\n│  (4096, 512) │  (1, 2) │ 0.0693615 │    14.4172 │ 0.977431 │ 1.17313 │\n│ (4096, 1024) │  (1, 4) │ 0.0717093 │    13.9452 │ 0.977431 │ 1.17313 │\n│ (4096, 2048) │  (1, 8) │ 0.0776766 │    12.8739 │ 0.977431 │ 1.17313 │\n│ (4096, 4096) │ (1, 16) │ 0.0980759 │    10.1962 │ 0.977431 │ 1.17313 │\n└──────────────┴─────────┴───────────┴────────────┴──────────┴─────────┘"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"It seems like there is a huge change from 1 to multi, which I don't believe should be the case.  But the timings actually seem to compare fairly well with what we saw in the MPI case.  I'm happy to share my code in a new branch if someone wanted to look at my first attempt to try and compare MPI vs threading."}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"}]