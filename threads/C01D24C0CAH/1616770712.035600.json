[{"client_msg_id":"a99abd34-4cae-4948-8432-ff4cd2fce29c","type":"message","text":"<@UEP056STX> and I have been doing some benchmarking of the `ShallowWaterModel` on `CPU`s vs `GPU`s.  After learning more about benchmarking than we had anticpated, we have the following results to share, which were run on a Tesla V100-SXM2-32GB machine.\n```        Shallow water model CPU -&gt; GPU speedup\n┌─────────────┬───────┬──────────┬─────────┬─────────┐\n│ Float_types │    Ns │  speedup │  memory │  allocs │\n├─────────────┼───────┼──────────┼─────────┼─────────┤\n│     Float64 │    32 │ 0.697116 │ 2.09724 │ 2.97978 │\n│     Float64 │    64 │  1.02927 │ 2.15944 │ 2.97029 │\n│     Float64 │   128 │  2.42711 │ 2.27406 │ 2.97029 │\n│     Float64 │   256 │  7.32025 │ 2.52177 │ 2.97029 │\n│     Float64 │   512 │  28.1914 │ 2.98971 │  2.9174 │\n│     Float64 │  1024 │  112.326 │ 3.93542 │ 2.93495 │\n│     Float64 │  2048 │  175.735 │ 5.81322 │ 2.98141 │\n│     Float64 │  4096 │  183.947 │ 9.58727 │ 2.96283 │\n│     Float64 │  8192 │  189.795 │  17.139 │ 2.96283 │\n│     Float64 │ 16384 │  186.833 │ 32.2437 │ 2.97419 │\n└─────────────┴───────┴──────────┴─────────┴─────────┘```\nWe weren't able to go any higher than this resolution, so we must be close to the 32GB of memory limit.  When we start testing `MultiGPU` (or `MPI_GPU`) we should be able to raise the wall and do even higher resolutions on two or more GPUs.\n\nThe punchline is that starting at `1024^2` we get a speed up of over `100` and it nearly doubles to `190`  at `4096^2`. To put this in context, to get something compariable, we would need `190` CPUs with MPI running at perfect efficiency.  Our eficiency performances of MPI showed that after 32 cores we had an efficiency of just over 80%.  I supsect we would need a lot more than 256 cores on the MPI code to run this quicky, but this is not something that we have checked (yet).  One GPU does a rather amazing job and should probably be used instead.","user":"U01FBLBCP7S","ts":"1616770712.035600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PG5","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UEP056STX"},{"type":"text","text":" and I have been doing some benchmarking of the "},{"type":"text","text":"ShallowWaterModel","style":{"code":true}},{"type":"text","text":" on `CPU`s vs `GPU`s.  After learning more about benchmarking than we had anticpated, we have the following results to share, which were run on a Tesla V100-SXM2-32GB machine.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"        Shallow water model CPU -> GPU speedup\n┌─────────────┬───────┬──────────┬─────────┬─────────┐\n│ Float_types │    Ns │  speedup │  memory │  allocs │\n├─────────────┼───────┼──────────┼─────────┼─────────┤\n│     Float64 │    32 │ 0.697116 │ 2.09724 │ 2.97978 │\n│     Float64 │    64 │  1.02927 │ 2.15944 │ 2.97029 │\n│     Float64 │   128 │  2.42711 │ 2.27406 │ 2.97029 │\n│     Float64 │   256 │  7.32025 │ 2.52177 │ 2.97029 │\n│     Float64 │   512 │  28.1914 │ 2.98971 │  2.9174 │\n│     Float64 │  1024 │  112.326 │ 3.93542 │ 2.93495 │\n│     Float64 │  2048 │  175.735 │ 5.81322 │ 2.98141 │\n│     Float64 │  4096 │  183.947 │ 9.58727 │ 2.96283 │\n│     Float64 │  8192 │  189.795 │  17.139 │ 2.96283 │\n│     Float64 │ 16384 │  186.833 │ 32.2437 │ 2.97419 │\n└─────────────┴───────┴──────────┴─────────┴─────────┘"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"We weren't able to go any higher than this resolution, so we must be close to the 32GB of memory limit.  When we start testing "},{"type":"text","text":"MultiGPU","style":{"code":true}},{"type":"text","text":" (or "},{"type":"text","text":"MPI_GPU","style":{"code":true}},{"type":"text","text":") we should be able to raise the wall and do even higher resolutions on two or more GPUs.\n\nThe punchline is that starting at "},{"type":"text","text":"1024^2","style":{"code":true}},{"type":"text","text":" we get a speed up of over "},{"type":"text","text":"100","style":{"code":true}},{"type":"text","text":" and it nearly doubles to "},{"type":"text","text":"190","style":{"code":true}},{"type":"text","text":"  at "},{"type":"text","text":"4096^2","style":{"code":true}},{"type":"text","text":". To put this in context, to get something compariable, we would need "},{"type":"text","text":"190","style":{"code":true}},{"type":"text","text":" CPUs with MPI running at perfect efficiency.  Our eficiency performances of MPI showed that after 32 cores we had an efficiency of just over 80%.  I supsect we would need a lot more than 256 cores on the MPI code to run this quicky, but this is not something that we have checked (yet).  One GPU does a rather amazing job and should probably be used instead."}]}]}],"thread_ts":"1616770712.035600","reply_count":2,"reply_users_count":2,"latest_reply":"1616777071.036200","reply_users":["U67BJLYCS","U01FBLBCP7S"],"is_locked":false,"subscribed":false,"reactions":[{"name":"rocket","users":["UEP056STX"],"count":1},{"name":"+1","users":["U01G39CC63F"],"count":1}]},{"client_msg_id":"60bf303c-0a6f-4f95-b3fa-2219af84d306","type":"message","text":"Nice! I usually keep an eye on `nvidia-smi` to gauge the memory usage of a code","user":"U67BJLYCS","ts":"1616773586.035700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"CVXL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Nice! I usually keep an eye on "},{"type":"text","text":"nvidia-smi","style":{"code":true}},{"type":"text","text":" to gauge the memory usage of a code"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"4f9e3d98-b7ec-4a3a-9d22-c2a1845f9c10","type":"message","text":"Thanks for the suggestion <@U67BJLYCS>, I will try and find that and give it a try.  That would be most helpful","user":"U01FBLBCP7S","ts":"1616777071.036200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xNJEb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for the suggestion "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":", I will try and find that and give it a try.  That would be most helpful"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"}]