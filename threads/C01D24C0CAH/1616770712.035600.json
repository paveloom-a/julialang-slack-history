[{"client_msg_id":"a99abd34-4cae-4948-8432-ff4cd2fce29c","type":"message","text":"<@UEP056STX> and I have been doing some benchmarking of the `ShallowWaterModel` on `CPU`s vs `GPU`s.  After learning more about benchmarking than we had anticpated, we have the following results to share, which were run on a Tesla V100-SXM2-32GB machine.\n```        Shallow water model CPU -&gt; GPU speedup\n┌─────────────┬───────┬──────────┬─────────┬─────────┐\n│ Float_types │    Ns │  speedup │  memory │  allocs │\n├─────────────┼───────┼──────────┼─────────┼─────────┤\n│     Float64 │    32 │ 0.697116 │ 2.09724 │ 2.97978 │\n│     Float64 │    64 │  1.02927 │ 2.15944 │ 2.97029 │\n│     Float64 │   128 │  2.42711 │ 2.27406 │ 2.97029 │\n│     Float64 │   256 │  7.32025 │ 2.52177 │ 2.97029 │\n│     Float64 │   512 │  28.1914 │ 2.98971 │  2.9174 │\n│     Float64 │  1024 │  112.326 │ 3.93542 │ 2.93495 │\n│     Float64 │  2048 │  175.735 │ 5.81322 │ 2.98141 │\n│     Float64 │  4096 │  183.947 │ 9.58727 │ 2.96283 │\n│     Float64 │  8192 │  189.795 │  17.139 │ 2.96283 │\n│     Float64 │ 16384 │  186.833 │ 32.2437 │ 2.97419 │\n└─────────────┴───────┴──────────┴─────────┴─────────┘```\nWe weren't able to go any higher than this resolution, so we must be close to the 32GB of memory limit.  When we start testing `MultiGPU` (or `MPI_GPU`) we should be able to raise the wall and do even higher resolutions on two or more GPUs.\n\nThe punchline is that starting at `1024^2` we get a speed up of over `100` and it nearly doubles to `190`  at `4096^2`. To put this in context, to get something compariable, we would need `190` CPUs with MPI running at perfect efficiency.  Our eficiency performances of MPI showed that after 32 cores we had an efficiency of just over 80%.  I supsect we would need a lot more than 256 cores on the MPI code to run this quicky, but this is not something that we have checked (yet).  One GPU does a rather amazing job and should probably be used instead.","user":"U01FBLBCP7S","ts":"1616770712.035600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PG5","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UEP056STX"},{"type":"text","text":" and I have been doing some benchmarking of the "},{"type":"text","text":"ShallowWaterModel","style":{"code":true}},{"type":"text","text":" on `CPU`s vs `GPU`s.  After learning more about benchmarking than we had anticpated, we have the following results to share, which were run on a Tesla V100-SXM2-32GB machine.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"        Shallow water model CPU -> GPU speedup\n┌─────────────┬───────┬──────────┬─────────┬─────────┐\n│ Float_types │    Ns │  speedup │  memory │  allocs │\n├─────────────┼───────┼──────────┼─────────┼─────────┤\n│     Float64 │    32 │ 0.697116 │ 2.09724 │ 2.97978 │\n│     Float64 │    64 │  1.02927 │ 2.15944 │ 2.97029 │\n│     Float64 │   128 │  2.42711 │ 2.27406 │ 2.97029 │\n│     Float64 │   256 │  7.32025 │ 2.52177 │ 2.97029 │\n│     Float64 │   512 │  28.1914 │ 2.98971 │  2.9174 │\n│     Float64 │  1024 │  112.326 │ 3.93542 │ 2.93495 │\n│     Float64 │  2048 │  175.735 │ 5.81322 │ 2.98141 │\n│     Float64 │  4096 │  183.947 │ 9.58727 │ 2.96283 │\n│     Float64 │  8192 │  189.795 │  17.139 │ 2.96283 │\n│     Float64 │ 16384 │  186.833 │ 32.2437 │ 2.97419 │\n└─────────────┴───────┴──────────┴─────────┴─────────┘"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"We weren't able to go any higher than this resolution, so we must be close to the 32GB of memory limit.  When we start testing "},{"type":"text","text":"MultiGPU","style":{"code":true}},{"type":"text","text":" (or "},{"type":"text","text":"MPI_GPU","style":{"code":true}},{"type":"text","text":") we should be able to raise the wall and do even higher resolutions on two or more GPUs.\n\nThe punchline is that starting at "},{"type":"text","text":"1024^2","style":{"code":true}},{"type":"text","text":" we get a speed up of over "},{"type":"text","text":"100","style":{"code":true}},{"type":"text","text":" and it nearly doubles to "},{"type":"text","text":"190","style":{"code":true}},{"type":"text","text":"  at "},{"type":"text","text":"4096^2","style":{"code":true}},{"type":"text","text":". To put this in context, to get something compariable, we would need "},{"type":"text","text":"190","style":{"code":true}},{"type":"text","text":" CPUs with MPI running at perfect efficiency.  Our eficiency performances of MPI showed that after 32 cores we had an efficiency of just over 80%.  I supsect we would need a lot more than 256 cores on the MPI code to run this quicky, but this is not something that we have checked (yet).  One GPU does a rather amazing job and should probably be used instead."}]}]}],"thread_ts":"1616770712.035600","reply_count":28,"reply_users_count":4,"latest_reply":"1617122270.044500","reply_users":["U67BJLYCS","U01FBLBCP7S","U010LHCP277","UEP056STX"],"is_locked":false,"subscribed":false,"reactions":[{"name":"rocket","users":["UEP056STX","U01E0U2RJJ2"],"count":2},{"name":"+1","users":["U01G39CC63F"],"count":1}]},{"client_msg_id":"60bf303c-0a6f-4f95-b3fa-2219af84d306","type":"message","text":"Nice! I usually keep an eye on `nvidia-smi` to gauge the memory usage of a code","user":"U67BJLYCS","ts":"1616773586.035700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"CVXL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Nice! I usually keep an eye on "},{"type":"text","text":"nvidia-smi","style":{"code":true}},{"type":"text","text":" to gauge the memory usage of a code"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"4f9e3d98-b7ec-4a3a-9d22-c2a1845f9c10","type":"message","text":"Thanks for the suggestion <@U67BJLYCS>, I will try and find that and give it a try.  That would be most helpful","user":"U01FBLBCP7S","ts":"1616777071.036200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xNJEb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for the suggestion "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":", I will try and find that and give it a try.  That would be most helpful"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"ba5d2bad-d451-4338-b167-f08eadccc59f","type":"message","text":"<@U01FBLBCP7S>, <@UEP056STX> and <@U67BJLYCS> it might be interesting to try KA at 32 threads on CPU. we don't have to share the results - but I would interested in how it currently does.","user":"U010LHCP277","ts":"1617040777.038800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iji","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U01FBLBCP7S"},{"type":"text","text":", "},{"type":"user","user_id":"UEP056STX"},{"type":"text","text":" and "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":" it might be interesting to try KA at 32 threads on CPU. we don't have to share the results - but I would interested in how it currently does."}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"9fd3ae5b-6b99-468c-bc63-ae241e61dc0b","type":"message","text":"Thanks for the suggestion <@U010LHCP277>.  In this not so old PR we did some MPI cpu tests going up to 32 cores.  <https://github.com/CliMA/Oceananigans.jl/pull/1505>","user":"U01FBLBCP7S","ts":"1617040932.039100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"T5y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for the suggestion "},{"type":"user","user_id":"U010LHCP277"},{"type":"text","text":".  In this not so old PR we did some MPI cpu tests going up to 32 cores.  "},{"type":"link","url":"https://github.com/CliMA/Oceananigans.jl/pull/1505"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"5d781308-4535-4331-8e30-9e3cc9a8d20f","type":"message","text":"Is this what you had in mind, or did I misunderstand something?","user":"U01FBLBCP7S","ts":"1617040944.039300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"c4T+U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is this what you had in mind, or did I misunderstand something?"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"15c616fc-7841-4729-88eb-ee71ca750ecc","type":"message","text":"No Chris wnats to set `JULIA_NUM_THREADS=32`","user":"U67BJLYCS","ts":"1617040967.039500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sWy60","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"No Chris wnats to set "},{"type":"text","text":"JULIA_NUM_THREADS=32","style":{"code":true}}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"1807d60e-a770-4ed4-b451-0b957a3c75db","type":"message","text":"and see if CPU parallelism with KernelAbstractions scale okay","user":"U67BJLYCS","ts":"1617040987.039700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Thby","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"and see if CPU parallelism with KernelAbstractions scale okay"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"28996f7d-1156-4904-91b1-3f37af7bcbca","type":"message","text":"<@U01FBLBCP7S> thanks, its not urgent - just interested in how KA is doing.","user":"U010LHCP277","ts":"1617042142.039900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PPFyc","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U01FBLBCP7S"},{"type":"text","text":" thanks, its not urgent - just interested in how KA is doing."}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"a5ca2945-4d36-4613-9ece-26a6fa391014","type":"message","text":"Ah, thanks for explaining <@U67BJLYCS>.  I have never actually tried running Oceananigans with threads so that will be a good learning experience.","user":"U01FBLBCP7S","ts":"1617042256.040100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"k1b","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah, thanks for explaining "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":".  I have never actually tried running Oceananigans with threads so that will be a good learning experience."}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"a27070b3-37c1-4bca-a5d6-c508917044c5","type":"message","text":"<@U010LHCP277> I'm having difficulties running the code on threads on the server so I thought I would do a test on my desktop, which has 18 cores, so should be able to go as high as 36 threads.  Also, I am redoing the weak scaling benchmark since I thought that would be easier.  Below are the results from MPI.  The efficiency is not as good as on the server as the number of cores increases but there you have it.\n```                                 Shallow water model weak scaling benchmark\n┌──────────────┬─────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────┬─────────┐\n│         size │   ranks │        min │     median │       mean │        max │     memory │ allocs │ samples │\n├──────────────┼─────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────┼─────────┤\n│  (4096, 256) │  (1, 1) │ 529.446 ms │ 538.296 ms │ 537.873 ms │ 545.570 ms │ 384.94 KiB │   2665 │      10 │\n│  (4096, 512) │  (1, 2) │ 555.490 ms │ 558.281 ms │ 559.896 ms │ 576.204 ms │ 376.92 KiB │   3178 │      18 │\n│ (4096, 1024) │  (1, 4) │ 573.427 ms │ 578.697 ms │ 579.280 ms │ 586.371 ms │ 376.92 KiB │   3178 │      36 │\n│ (4096, 2048) │  (1, 8) │ 620.840 ms │ 626.854 ms │ 626.948 ms │ 633.596 ms │ 376.92 KiB │   3178 │      64 │\n│ (4096, 4096) │ (1, 16) │ 778.615 ms │ 791.477 ms │ 791.290 ms │ 799.397 ms │ 376.92 KiB │   3178 │     112 │\n└──────────────┴─────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────┴─────────┘\n[2021/03/30 07:34:44.154] INFO  Writing Shallow_water_model_weak_scaling_benchmark.html...\n               Shallow water model weak scaling speedup\n┌──────────────┬─────────┬──────────┬────────────┬──────────┬────────┐\n│         size │   ranks │ slowdown │ efficiency │   memory │ allocs │\n├──────────────┼─────────┼──────────┼────────────┼──────────┼────────┤\n│  (4096, 256) │  (1, 1) │      1.0 │        1.0 │      1.0 │    1.0 │\n│  (4096, 512) │  (1, 2) │  1.03713 │   0.964203 │ 0.979177 │ 1.1925 │\n│ (4096, 1024) │  (1, 4) │  1.07505 │   0.930187 │ 0.979177 │ 1.1925 │\n│ (4096, 2048) │  (1, 8) │  1.16451 │   0.858727 │ 0.979177 │ 1.1925 │\n│ (4096, 4096) │ (1, 16) │  1.47034 │   0.680116 │ 0.979177 │ 1.1925 │\n└──────────────┴─────────┴──────────┴────────────┴──────────┴────────┘```\nNext, I specified the number of threads to be 2 times the number of coures, so 1,4,8,16,32.  Below are the results and I must say that I'm a bit confused by the results.  I also have no experience in threading in Julia so am not sure what to expect.  Any reactions are welcome.\n```                                  Shallow water model weak scaling benchmark\n┌──────────────┬─────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────┬─────────┐\n│         size │   ranks │        min │     median │       mean │        max │     memory │ allocs │ samples │\n├──────────────┼─────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────┼─────────┤\n│  (4096, 256) │  (1, 1) │    8.070 s │    8.070 s │    8.070 s │    8.070 s │ 385.63 KiB │   2709 │       1 │\n│  (4096, 512) │  (1, 2) │ 553.727 ms │ 559.750 ms │ 560.117 ms │ 578.846 ms │ 376.92 KiB │   3178 │      18 │\n│ (4096, 1024) │  (1, 4) │ 573.427 ms │ 578.697 ms │ 579.280 ms │ 586.371 ms │ 376.92 KiB │   3178 │      36 │\n│ (4096, 2048) │  (1, 8) │ 620.840 ms │ 626.854 ms │ 626.948 ms │ 633.596 ms │ 376.92 KiB │   3178 │      64 │\n│ (4096, 4096) │ (1, 16) │ 778.615 ms │ 791.477 ms │ 791.290 ms │ 799.397 ms │ 376.92 KiB │   3178 │     112 │\n└──────────────┴─────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────┴─────────┘\n[2021/03/30 08:53:33.687] INFO  Writing Shallow_water_model_weak_scaling_benchmark.html...\n                Shallow water model weak scaling speedup\n┌──────────────┬─────────┬───────────┬────────────┬──────────┬─────────┐\n│         size │   ranks │  slowdown │ efficiency │   memory │  allocs │\n├──────────────┼─────────┼───────────┼────────────┼──────────┼─────────┤\n│  (4096, 256) │  (1, 1) │       1.0 │        1.0 │      1.0 │     1.0 │\n│  (4096, 512) │  (1, 2) │ 0.0693615 │    14.4172 │ 0.977431 │ 1.17313 │\n│ (4096, 1024) │  (1, 4) │ 0.0717093 │    13.9452 │ 0.977431 │ 1.17313 │\n│ (4096, 2048) │  (1, 8) │ 0.0776766 │    12.8739 │ 0.977431 │ 1.17313 │\n│ (4096, 4096) │ (1, 16) │ 0.0980759 │    10.1962 │ 0.977431 │ 1.17313 │\n└──────────────┴─────────┴───────────┴────────────┴──────────┴─────────┘```\nIt seems like there is a huge change from 1 to multi, which I don't believe should be the case.  But the timings actually seem to compare fairly well with what we saw in the MPI case.  I'm happy to share my code in a new branch if someone wanted to look at my first attempt to try and compare MPI vs threading.","user":"U01FBLBCP7S","ts":"1617109241.040300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kVz","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U010LHCP277"},{"type":"text","text":" I'm having difficulties running the code on threads on the server so I thought I would do a test on my desktop, which has 18 cores, so should be able to go as high as 36 threads.  Also, I am redoing the weak scaling benchmark since I thought that would be easier.  Below are the results from MPI.  The efficiency is not as good as on the server as the number of cores increases but there you have it.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"                                 Shallow water model weak scaling benchmark\n┌──────────────┬─────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────┬─────────┐\n│         size │   ranks │        min │     median │       mean │        max │     memory │ allocs │ samples │\n├──────────────┼─────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────┼─────────┤\n│  (4096, 256) │  (1, 1) │ 529.446 ms │ 538.296 ms │ 537.873 ms │ 545.570 ms │ 384.94 KiB │   2665 │      10 │\n│  (4096, 512) │  (1, 2) │ 555.490 ms │ 558.281 ms │ 559.896 ms │ 576.204 ms │ 376.92 KiB │   3178 │      18 │\n│ (4096, 1024) │  (1, 4) │ 573.427 ms │ 578.697 ms │ 579.280 ms │ 586.371 ms │ 376.92 KiB │   3178 │      36 │\n│ (4096, 2048) │  (1, 8) │ 620.840 ms │ 626.854 ms │ 626.948 ms │ 633.596 ms │ 376.92 KiB │   3178 │      64 │\n│ (4096, 4096) │ (1, 16) │ 778.615 ms │ 791.477 ms │ 791.290 ms │ 799.397 ms │ 376.92 KiB │   3178 │     112 │\n└──────────────┴─────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────┴─────────┘\n[2021/03/30 07:34:44.154] INFO  Writing Shallow_water_model_weak_scaling_benchmark.html...\n               Shallow water model weak scaling speedup\n┌──────────────┬─────────┬──────────┬────────────┬──────────┬────────┐\n│         size │   ranks │ slowdown │ efficiency │   memory │ allocs │\n├──────────────┼─────────┼──────────┼────────────┼──────────┼────────┤\n│  (4096, 256) │  (1, 1) │      1.0 │        1.0 │      1.0 │    1.0 │\n│  (4096, 512) │  (1, 2) │  1.03713 │   0.964203 │ 0.979177 │ 1.1925 │\n│ (4096, 1024) │  (1, 4) │  1.07505 │   0.930187 │ 0.979177 │ 1.1925 │\n│ (4096, 2048) │  (1, 8) │  1.16451 │   0.858727 │ 0.979177 │ 1.1925 │\n│ (4096, 4096) │ (1, 16) │  1.47034 │   0.680116 │ 0.979177 │ 1.1925 │\n└──────────────┴─────────┴──────────┴────────────┴──────────┴────────┘"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Next, I specified the number of threads to be 2 times the number of coures, so 1,4,8,16,32.  Below are the results and I must say that I'm a bit confused by the results.  I also have no experience in threading in Julia so am not sure what to expect.  Any reactions are welcome.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"                                  Shallow water model weak scaling benchmark\n┌──────────────┬─────────┬────────────┬────────────┬────────────┬────────────┬────────────┬────────┬─────────┐\n│         size │   ranks │        min │     median │       mean │        max │     memory │ allocs │ samples │\n├──────────────┼─────────┼────────────┼────────────┼────────────┼────────────┼────────────┼────────┼─────────┤\n│  (4096, 256) │  (1, 1) │    8.070 s │    8.070 s │    8.070 s │    8.070 s │ 385.63 KiB │   2709 │       1 │\n│  (4096, 512) │  (1, 2) │ 553.727 ms │ 559.750 ms │ 560.117 ms │ 578.846 ms │ 376.92 KiB │   3178 │      18 │\n│ (4096, 1024) │  (1, 4) │ 573.427 ms │ 578.697 ms │ 579.280 ms │ 586.371 ms │ 376.92 KiB │   3178 │      36 │\n│ (4096, 2048) │  (1, 8) │ 620.840 ms │ 626.854 ms │ 626.948 ms │ 633.596 ms │ 376.92 KiB │   3178 │      64 │\n│ (4096, 4096) │ (1, 16) │ 778.615 ms │ 791.477 ms │ 791.290 ms │ 799.397 ms │ 376.92 KiB │   3178 │     112 │\n└──────────────┴─────────┴────────────┴────────────┴────────────┴────────────┴────────────┴────────┴─────────┘\n[2021/03/30 08:53:33.687] INFO  Writing Shallow_water_model_weak_scaling_benchmark.html...\n                Shallow water model weak scaling speedup\n┌──────────────┬─────────┬───────────┬────────────┬──────────┬─────────┐\n│         size │   ranks │  slowdown │ efficiency │   memory │  allocs │\n├──────────────┼─────────┼───────────┼────────────┼──────────┼─────────┤\n│  (4096, 256) │  (1, 1) │       1.0 │        1.0 │      1.0 │     1.0 │\n│  (4096, 512) │  (1, 2) │ 0.0693615 │    14.4172 │ 0.977431 │ 1.17313 │\n│ (4096, 1024) │  (1, 4) │ 0.0717093 │    13.9452 │ 0.977431 │ 1.17313 │\n│ (4096, 2048) │  (1, 8) │ 0.0776766 │    12.8739 │ 0.977431 │ 1.17313 │\n│ (4096, 4096) │ (1, 16) │ 0.0980759 │    10.1962 │ 0.977431 │ 1.17313 │\n└──────────────┴─────────┴───────────┴────────────┴──────────┴─────────┘"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"It seems like there is a huge change from 1 to multi, which I don't believe should be the case.  But the timings actually seem to compare fairly well with what we saw in the MPI case.  I'm happy to share my code in a new branch if someone wanted to look at my first attempt to try and compare MPI vs threading."}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"6c01bffa-26c1-4d15-b134-8cb3891d6e0a","type":"message","text":"<@U01FBLBCP7S> thats great. I also just came across this - <https://developer.nvidia.com/blog/scaling-scientific-computing-with-nvshmem/>, so the possibilities seem endless!\n\n<@U01FBLBCP7S> <@UEP056STX> , <@U67BJLYCS> etc.... would there be any interest in proposing a 10 minute lighting talk on scaling of this config for Julia Con. Deadline for abstract is tomorrow ( <https://pretalx.com/juliacon2021/cfp> )!!\n\nWe could compare MPI, KA threads, GPU, CPU and maybe for fun revisit the benchmark metric Ali and I tried to get people to take seriously - which was $/solution using various cloud options.\n\nComparing a $10,000 GPU with a $62 CPU core  (which is what we do a lot) always seems a bit incongruous - even if it does give x100+ speed ups! So far everybody has been dismissive of our $/solution metric (and/or CO2e/solution), so it would be fun to try again.","user":"U010LHCP277","ts":"1617112089.040500","team":"T68168MUP","attachments":[{"service_name":"NVIDIA Developer Blog","title":"Scaling Scientific Computing with NVSHMEM | NVIDIA Developer Blog","title_link":"https://developer.nvidia.com/blog/scaling-scientific-computing-with-nvshmem/","text":"Figure 1. In the NVSHMEM memory model, each process (PE) has private memory, as well as symmetric memory that forms a partition of the partitioned global address space. When you double the number of…","fallback":"NVIDIA Developer Blog: Scaling Scientific Computing with NVSHMEM | NVIDIA Developer Blog","image_url":"https://developer-blogs.nvidia.com/wp-content/uploads/2020/07/NVSHMEM-DEVBLOG-FIGURE-1.png","image_width":374,"image_height":250,"ts":1598376180,"from_url":"https://developer.nvidia.com/blog/scaling-scientific-computing-with-nvshmem/","image_bytes":14389,"service_icon":"https://developer-blogs.nvidia.com/wp-content/themes/nvidia/dist/images/favicon.ico","id":1,"original_url":"https://developer.nvidia.com/blog/scaling-scientific-computing-with-nvshmem/"},{"title":"JuliaCon 2021","title_link":"https://pretalx.com/juliacon2021/cfp","text":"Schedule, talks and talk submissions for JuliaCon 2021","fallback":"JuliaCon 2021","from_url":"https://pretalx.com/juliacon2021/cfp","service_icon":"https://pretalx.com/static/common/img/favicon-32x32.38c2aab28d3a.png","service_name":"pretalx","id":2,"original_url":"https://pretalx.com/juliacon2021/cfp"}],"blocks":[{"type":"rich_text","block_id":"wgpT","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U01FBLBCP7S"},{"type":"text","text":" thats great. I also just came across this - "},{"type":"link","url":"https://developer.nvidia.com/blog/scaling-scientific-computing-with-nvshmem/"},{"type":"text","text":", so the possibilities seem endless!\n\n"},{"type":"user","user_id":"U01FBLBCP7S"},{"type":"text","text":" "},{"type":"user","user_id":"UEP056STX"},{"type":"text","text":" , "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":" etc.... would there be any interest in proposing a 10 minute lighting talk on scaling of this config for Julia Con. Deadline for abstract is tomorrow ( "},{"type":"link","url":"https://pretalx.com/juliacon2021/cfp"},{"type":"text","text":" )!!\n\nWe could compare MPI, KA threads, GPU, CPU and maybe for fun revisit the benchmark metric Ali and I tried to get people to take seriously - which was $/solution using various cloud options.\n\nComparing a $10,000 GPU with a $62 CPU core  (which is what we do a lot) always seems a bit incongruous - even if it does give x100+ speed ups! So far everybody has been dismissive of our $/solution metric (and/or CO2e/solution), so it would be fun to try again."}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S","reactions":[{"name":"tada","users":["U01FBLBCP7S","UEP056STX"],"count":2},{"name":"+1","users":["U01FBLBCP7S","UEP056STX"],"count":2}]},{"client_msg_id":"7ec55656-079a-49e1-af2b-480c71fd9654","type":"message","text":"Thanks for sharing this <@U010LHCP277> and I'd be happy to chat today. I should be free from 11am if that works for you?","user":"U01FBLBCP7S","ts":"1617112286.041000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"R2C","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for sharing this "},{"type":"user","user_id":"U010LHCP277"},{"type":"text","text":" and I'd be happy to chat today. I should be free from 11am if that works for you?"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"b5f8204f-f6b3-4d2d-a21b-ea4d38250d2b","type":"message","text":"How about 1:30 Eastern?","user":"U010LHCP277","ts":"1617112361.041200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=MS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How about 1:30 Eastern?"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"eaf6189d-04db-4a97-a991-915a61cf9e91","type":"message","text":"Would be great to revisit doing some proper CPU vs. GPU benchmarking! And happy to help with a submission.","user":"UEP056STX","ts":"1617112660.041600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"X29","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Would be great to revisit doing some proper CPU vs. GPU benchmarking! And happy to help with a submission."}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"43edb183-a3dd-48df-a3cd-1f51f314c7b2","type":"message","text":"I'm free at 1:30 ET.","user":"UEP056STX","ts":"1617112669.041800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"W06","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm free at 1:30 ET."}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"f0b130fe-9373-431b-9026-1d677860d9fb","type":"message","text":"<@U01FBLBCP7S> not sure how to read the results above: What does rank: `(N, M)` mean?","user":"U67BJLYCS","ts":"1617115795.042000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WQkH8","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U01FBLBCP7S"},{"type":"text","text":" not sure how to read the results above: What does rank: "},{"type":"text","text":"(N, M)","style":{"code":true}},{"type":"text","text":" mean?"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"5ec18f9a-24fa-4f5a-a187-c3bf08627208","type":"message","text":"130pm is also good for me","user":"U01FBLBCP7S","ts":"1617116119.042200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hnzo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"130pm is also good for me"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"65f12f93-2aed-4292-8218-16af0d158060","type":"message","text":"Sorry <@U67BJLYCS>, I used the same output but set the mpi rank to 1 and then choose the number of threads. Seemed like an easy thing to try","user":"U01FBLBCP7S","ts":"1617116183.042400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"b3f","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Sorry "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":", I used the same output but set the mpi rank to 1 and then choose the number of threads. Seemed like an easy thing to try"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"432d47fe-92aa-4bb0-bb87-4dd2ef5e1de5","type":"message","text":"So both test runs are with multi-threading","user":"U67BJLYCS","ts":"1617116213.042600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hQg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So both test runs are with multi-threading"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"4d18a910-f5be-42dd-b367-48a8c9a4d383","type":"message","text":"or only the second one?","user":"U67BJLYCS","ts":"1617116217.042800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"n9S7B","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"or only the second one?"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"3d6170d0-8491-4690-88df-d297e035b784","type":"message","text":"I am confused that the inital run is `8s`","user":"U67BJLYCS","ts":"1617116299.043000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MqW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am confused that the inital run is "},{"type":"text","text":"8s","style":{"code":true}}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"cc70b5bc-8bd8-4127-a113-2196629545be","type":"message","text":"other than that the raw times look good","user":"U67BJLYCS","ts":"1617116354.043200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qwj06","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"other than that the raw times look good"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"54753feb-5bd4-4c43-8df9-58790e980359","type":"message","text":"<@U01FBLBCP7S> For benchmarking multithreading are you using something like <https://github.com/CliMA/Oceananigans.jl/blob/master/benchmark/benchmark_multithreading.jl> ?","user":"UEP056STX","ts":"1617116457.043400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"AI2M","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U01FBLBCP7S"},{"type":"text","text":" For benchmarking multithreading are you using something like "},{"type":"link","url":"https://github.com/CliMA/Oceananigans.jl/blob/master/benchmark/benchmark_multithreading.jl"},{"type":"text","text":" ?"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"59a8be66-1b49-457a-99cf-4044cc881404","type":"message","text":"Sorry <@U67BJLYCS>: the first run was MPI and the second run was multi-threaded.  I am also very confused why the first case in the second run is 8s.  Clearly this is a problem but the others look reasonable.","user":"U01FBLBCP7S","ts":"1617117721.043600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8TgJN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Sorry "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":": the first run was MPI and the second run was multi-threaded.  I am also very confused why the first case in the second run is 8s.  Clearly this is a problem but the others look reasonable."}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"1f7cc6ec-3c07-4e43-a2a7-bd02e03e7604","type":"message","text":"Thanks <@UEP056STX> for the suggestion. I will look at this and try this out to get something cleaner working","user":"U01FBLBCP7S","ts":"1617117743.043800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FE2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks "},{"type":"user","user_id":"UEP056STX"},{"type":"text","text":" for the suggestion. I will look at this and try this out to get something cleaner working"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"34d70c91-c2b9-4201-96d5-b0eb637490fc","type":"message","text":"<@UEP056STX> and <@U01FBLBCP7S> (and <@U67BJLYCS>) shall we use <https://mit.zoom.us/my/cnhill> at 1:30 to chat about JuliaCon short talk abstract .","user":"U010LHCP277","ts":"1617121462.044000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YHzz","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UEP056STX"},{"type":"text","text":" and "},{"type":"user","user_id":"U01FBLBCP7S"},{"type":"text","text":" (and "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":") shall we use "},{"type":"link","url":"https://mit.zoom.us/my/cnhill"},{"type":"text","text":" at 1:30 to chat about JuliaCon short talk abstract ."}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S","reactions":[{"name":"+1","users":["UEP056STX"],"count":1}]},{"client_msg_id":"a9c7d947-b7ca-4b5d-a13d-c74a703adb0f","type":"message","text":"Thanks to <@UEP056STX>’s suggestion, I believe I have done the threading more correctly.  I can post this on a branch if people want to look at the code.  Again, something weird is happening in serial but if we believe the 2 thread case, the decrease in efficiency doesn't seem so bad. It actually improves significantly with 16.\n```                       Shallow water model weak scaling with multithreading benchmark\n┌──────────────┬─────────┬────────────┬────────────┬────────────┬────────────┬────────────┬─────────┬─────────┐\n│         size │ threads │        min │     median │       mean │        max │     memory │  allocs │ samples │\n├──────────────┼─────────┼────────────┼────────────┼────────────┼────────────┼────────────┼─────────┼─────────┤\n│  (4096, 256) │       1 │ 494.103 ms │ 494.973 ms │ 495.285 ms │ 498.570 ms │ 342.75 KiB │    2287 │      10 │\n│  (4096, 512) │       2 │    1.016 s │    1.020 s │    1.023 s │    1.038 s │  19.04 MiB │ 1221574 │       5 │\n│ (4096, 1024) │       4 │ 788.844 ms │ 831.427 ms │ 829.394 ms │ 863.276 ms │  14.45 MiB │  915389 │       7 │\n│ (4096, 2048) │       8 │ 744.451 ms │ 770.484 ms │ 761.958 ms │ 776.061 ms │  12.66 MiB │  786863 │       7 │\n│ (4096, 4096) │      16 │ 939.009 ms │ 944.734 ms │ 944.742 ms │ 954.280 ms │  14.63 MiB │  894127 │       6 │\n│ (4096, 8192) │      32 │    1.515 s │    1.541 s │    1.544 s │    1.581 s │  15.82 MiB │  927430 │       4 │\n└──────────────┴─────────┴────────────┴────────────┴────────────┴────────────┴────────────┴─────────┴─────────┘\n[2021/03/30 12:33:31.511] INFO  Writing Shallow_water_model_weak_scaling_with_multithreading_benchmark.html...\n       Shallow water model weak multithreading scaling speedup\n┌──────────────┬─────────┬──────────┬────────────┬─────────┬─────────┐\n│         size │ threads │ slowdown │ efficiency │  memory │  allocs │\n├──────────────┼─────────┼──────────┼────────────┼─────────┼─────────┤\n│  (4096, 256) │       1 │      1.0 │        1.0 │     1.0 │     1.0 │\n│  (4096, 512) │       2 │  2.06149 │   0.485086 │ 56.8822 │ 534.138 │\n│ (4096, 1024) │       4 │  1.67974 │   0.595329 │ 43.1714 │ 400.258 │\n│ (4096, 2048) │       8 │  1.55662 │   0.642418 │ 37.8177 │ 344.059 │\n│ (4096, 4096) │      16 │  1.90866 │   0.523928 │ 43.7185 │ 390.961 │\n│ (4096, 8192) │      32 │  3.11276 │   0.321258 │ 47.2586 │ 405.523 │\n└──────────────┴─────────┴──────────┴────────────┴─────────┴─────────┘```","user":"U01FBLBCP7S","ts":"1617121613.044200","team":"T68168MUP","edited":{"user":"U01FBLBCP7S","ts":"1617122279.000000"},"blocks":[{"type":"rich_text","block_id":"yD=q6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks to "},{"type":"user","user_id":"UEP056STX"},{"type":"text","text":"’s suggestion, I believe I have done the threading more correctly.  I can post this on a branch if people want to look at the code.  Again, something weird is happening in serial but if we believe the 2 thread case, the decrease in efficiency doesn't seem so bad. It actually improves significantly with 16.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"                       Shallow water model weak scaling with multithreading benchmark\n┌──────────────┬─────────┬────────────┬────────────┬────────────┬────────────┬────────────┬─────────┬─────────┐\n│         size │ threads │        min │     median │       mean │        max │     memory │  allocs │ samples │\n├──────────────┼─────────┼────────────┼────────────┼────────────┼────────────┼────────────┼─────────┼─────────┤\n│  (4096, 256) │       1 │ 494.103 ms │ 494.973 ms │ 495.285 ms │ 498.570 ms │ 342.75 KiB │    2287 │      10 │\n│  (4096, 512) │       2 │    1.016 s │    1.020 s │    1.023 s │    1.038 s │  19.04 MiB │ 1221574 │       5 │\n│ (4096, 1024) │       4 │ 788.844 ms │ 831.427 ms │ 829.394 ms │ 863.276 ms │  14.45 MiB │  915389 │       7 │\n│ (4096, 2048) │       8 │ 744.451 ms │ 770.484 ms │ 761.958 ms │ 776.061 ms │  12.66 MiB │  786863 │       7 │\n│ (4096, 4096) │      16 │ 939.009 ms │ 944.734 ms │ 944.742 ms │ 954.280 ms │  14.63 MiB │  894127 │       6 │\n│ (4096, 8192) │      32 │    1.515 s │    1.541 s │    1.544 s │    1.581 s │  15.82 MiB │  927430 │       4 │\n└──────────────┴─────────┴────────────┴────────────┴────────────┴────────────┴────────────┴─────────┴─────────┘\n[2021/03/30 12:33:31.511] INFO  Writing Shallow_water_model_weak_scaling_with_multithreading_benchmark.html...\n       Shallow water model weak multithreading scaling speedup\n┌──────────────┬─────────┬──────────┬────────────┬─────────┬─────────┐\n│         size │ threads │ slowdown │ efficiency │  memory │  allocs │\n├──────────────┼─────────┼──────────┼────────────┼─────────┼─────────┤\n│  (4096, 256) │       1 │      1.0 │        1.0 │     1.0 │     1.0 │\n│  (4096, 512) │       2 │  2.06149 │   0.485086 │ 56.8822 │ 534.138 │\n│ (4096, 1024) │       4 │  1.67974 │   0.595329 │ 43.1714 │ 400.258 │\n│ (4096, 2048) │       8 │  1.55662 │   0.642418 │ 37.8177 │ 344.059 │\n│ (4096, 4096) │      16 │  1.90866 │   0.523928 │ 43.7185 │ 390.961 │\n│ (4096, 8192) │      32 │  3.11276 │   0.321258 │ 47.2586 │ 405.523 │\n└──────────────┴─────────┴──────────┴────────────┴─────────┴─────────┘"}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"},{"client_msg_id":"233811bf-3cc8-4673-ba3c-80ede8381037","type":"message","text":"<@U010LHCP277> Sounds good. <@UMZQBQU67> might join as well.","user":"UEP056STX","ts":"1617122270.044500","team":"T68168MUP","edited":{"user":"UEP056STX","ts":"1617122280.000000"},"blocks":[{"type":"rich_text","block_id":"Y+6v","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U010LHCP277"},{"type":"text","text":" Sounds good. "},{"type":"user","user_id":"UMZQBQU67"},{"type":"text","text":" might join as well."}]}]}],"thread_ts":"1616770712.035600","parent_user_id":"U01FBLBCP7S"}]