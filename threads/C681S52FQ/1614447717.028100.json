[{"client_msg_id":"fd7b9339-2b1f-4a24-8914-b6b784ffdc61","type":"message","text":"L-BFGS is prone to getting stuck on saddle-points (like all Newton-like second-order methods). Are there alternative methods in Optim / NLopt that one can use that avoid this problem?","user":"U7YD3DKL2","ts":"1614447717.028100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jUh8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"L-BFGS is prone to getting stuck on saddle-points (like all Newton-like second-order methods). Are there alternative methods in Optim / NLopt that one can use that avoid this problem?"}]}]}],"thread_ts":"1614447717.028100","reply_count":15,"reply_users_count":4,"latest_reply":"1614532536.032200","reply_users":["U67G3QRJM","U9MD78Z9N","UMDEUKM29","U7YD3DKL2"],"subscribed":false},{"client_msg_id":"d6cf6871-21c2-4e97-8952-f4a496ce5e45","type":"message","text":"Maybe <https://arxiv.org/abs/2006.00719>","user":"U67G3QRJM","ts":"1614448855.028200","team":"T68168MUP","attachments":[{"service_name":"arXiv.org","title":"ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning","title_link":"https://arxiv.org/abs/2006.00719","text":"We introduce ADAHESSIAN, a second order stochastic optimization algorithm which dynamically incorporates the curvature of the loss function via ADAptive estimates of the HESSIAN. Second order...","fallback":"arXiv.org: ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning","from_url":"https://arxiv.org/abs/2006.00719","service_icon":"https://static.arxiv.org/static/browse/0.3.2.6/images/icons/favicon.ico","id":1,"original_url":"https://arxiv.org/abs/2006.00719"}],"blocks":[{"type":"rich_text","block_id":"YaxdW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Maybe "},{"type":"link","url":"https://arxiv.org/abs/2006.00719"}]}]}],"thread_ts":"1614447717.028100","parent_user_id":"U7YD3DKL2"},{"client_msg_id":"5aa2f2ba-a1d8-4905-8b9a-bad16b14fa0a","type":"message","text":"<https://github.com/amirgholami/adahessian>","user":"U67G3QRJM","ts":"1614448885.028500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lEFI","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/amirgholami/adahessian"}]}]}],"thread_ts":"1614447717.028100","parent_user_id":"U7YD3DKL2"},{"type":"message","text":"Momentum based algorithms could also help.","user":"U9MD78Z9N","ts":"1614449448.028700","team":"T68168MUP","thread_ts":"1614447717.028100","parent_user_id":"U7YD3DKL2","reactions":[{"name":"+1","users":["U67G3QRJM","U7YD3DKL2","UKG4WF8PJ"],"count":3}]},{"type":"message","text":"Because a short region with 0 gradient doesn't slow the ball enough.","user":"U9MD78Z9N","ts":"1614450417.029500","team":"T68168MUP","thread_ts":"1614447717.028100","parent_user_id":"U7YD3DKL2","reactions":[{"name":"+1","users":["U67G3QRJM"],"count":1}]},{"client_msg_id":"2b0d9e69-7953-4d14-a164-972da3c9acbc","type":"message","text":"What do you mean getting stuck in saddle points? Why should it be? It's still an optimization method","user":"UMDEUKM29","ts":"1614457702.029900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/I3i","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What do you mean getting stuck in saddle points? Why should it be? It's still an optimization method"}]}]}],"thread_ts":"1614447717.028100","parent_user_id":"U7YD3DKL2","reactions":[{"name":"-1","users":["U9MD78Z9N"],"count":1}]},{"client_msg_id":"108f7543-7625-41bc-88b0-1ec2534e25c3","type":"message","text":"Aren't Newton-like methods designed to find stationary points of the gradient?","user":"U67G3QRJM","ts":"1614463889.030200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"g/8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Aren't Newton-like methods designed to find stationary points of the gradient?"}]}]}],"thread_ts":"1614447717.028100","parent_user_id":"U7YD3DKL2"},{"client_msg_id":"e039f018-fe87-4dbe-b93a-08aec7522d6a","type":"message","text":"<@UMDEUKM29> Newton-like methods are usually attracted to saddle-points, so you can fail to reach (even local) minima.","user":"U7YD3DKL2","ts":"1614464984.030600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PfPL","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UMDEUKM29"},{"type":"text","text":" Newton-like methods are usually attracted to saddle-points, so you can fail to reach (even local) minima."}]}]}],"thread_ts":"1614447717.028100","parent_user_id":"U7YD3DKL2"},{"client_msg_id":"37bf9c7d-b25f-4287-987a-b1d8c9c93b4e","type":"message","text":"You're confusing Newton for root and Newton for optimization","user":"UMDEUKM29","ts":"1614495118.030800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"u8n","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You're confusing Newton for root and Newton for optimization"}]}]}],"thread_ts":"1614447717.028100","parent_user_id":"U7YD3DKL2"},{"client_msg_id":"fedde8c7-5502-43d9-b9a5-55f646d2cf56","type":"message","text":"Newton for optimization uses second order information to make the problem better conditioned, but it's still going downhill","user":"UMDEUKM29","ts":"1614495173.031000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QrE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Newton for optimization uses second order information to make the problem better conditioned, but it's still going downhill"}]}]}],"thread_ts":"1614447717.028100","parent_user_id":"U7YD3DKL2"},{"client_msg_id":"576a9b81-cb47-4914-9240-0f1ba480cb69","type":"message","text":"There's even a linesearch to ensure it does","user":"UMDEUKM29","ts":"1614495195.031200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"q6he","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"There's even a linesearch to ensure it does"}]}]}],"thread_ts":"1614447717.028100","parent_user_id":"U7YD3DKL2"},{"client_msg_id":"a41ba328-1c6e-4dfa-8758-888ce1bd6530","type":"message","text":"I'm not sure I understand. If you take steps of the form H^{-1} * g, where H is the Hessian and 'g' the gradient (this is the step direction of the Newton method), then you are attracted towards saddle-points.","user":"U7YD3DKL2","ts":"1614509113.031400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hddv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm not sure I understand. If you take steps of the form H^{-1} * g, where H is the Hessian and 'g' the gradient (this is the step direction of the Newton method), then you are attracted towards saddle-points."}]}]}],"thread_ts":"1614447717.028100","parent_user_id":"U7YD3DKL2"},{"client_msg_id":"46ad29b7-4d3c-4959-828f-8d25636e3b74","type":"message","text":"Which is why you don't","user":"UMDEUKM29","ts":"1614510327.031600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"BhCH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Which is why you don't"}]}]}],"thread_ts":"1614447717.028100","parent_user_id":"U7YD3DKL2"},{"client_msg_id":"bea3111a-4b30-417d-86af-52395189748a","type":"message","text":"Typically quasi Newton for optimization have ways to keep the approximation of the hessian positive definite","user":"UMDEUKM29","ts":"1614510354.031800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"cmAJ8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Typically quasi Newton for optimization have ways to keep the approximation of the hessian positive definite"}]}]}],"thread_ts":"1614447717.028100","parent_user_id":"U7YD3DKL2"},{"client_msg_id":"5ccceab7-dda2-4e7e-89c6-b27788f362ad","type":"message","text":"A straightforward Newton will indeed go to saddle points but that's not what properly implemented quasi Newton methods do","user":"UMDEUKM29","ts":"1614510403.032000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5O0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"A straightforward Newton will indeed go to saddle points but that's not what properly implemented quasi Newton methods do"}]}]}],"thread_ts":"1614447717.028100","parent_user_id":"U7YD3DKL2"},{"client_msg_id":"273e7f45-2ca7-48c6-b1ce-deeacfca5cb5","type":"message","text":"Do you know if L-BFGS keeps a posdef \"approximation\" of the Hessian?","user":"U7YD3DKL2","ts":"1614532536.032200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DusWL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do you know if L-BFGS keeps a posdef \"approximation\" of the Hessian?"}]}]}],"thread_ts":"1614447717.028100","parent_user_id":"U7YD3DKL2"}]