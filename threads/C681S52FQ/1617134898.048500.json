[{"client_msg_id":"952ab841-c725-4e23-8595-6f92259b1d66","type":"message","text":"I'm trying to double check some (hand written) derivatives for use in optimization, using FiniteDifferences. My objective function has has ~600 dimensional input and real output, and takes about .1 seconds to evaluate. I know the derivatives are not zero, because if I perturb an input dimension by a little (eg .01) the output changes. But with FiniteDifferences.jl, I'm getting all zeros: `grad(forward_fdm(3,1), objective, x_0)` returns all zeros (well, strictly speaking a 1-tuple with first element an array of all zeros).\n\nI thought maybe this has to do with conversions between Float64 and Float32; internally, my objective function (which is a simulation) converts some inputs to F32 and does GPU computation. So I tried `wrapper_obj(x::Vector{Float32}) = obj(Float64.(x))` and took the gradient of `wrapper_obj`, trying to force FiniteDifferences to take bigger (float32 compatible) steps. But while the resulting gradient isn't _all_ zeros now, it's mostly zeros and definitely wrong.\n\nAnything else I can try?","user":"U73ACR3TQ","ts":"1617134898.048500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7/4e5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm trying to double check some (hand written) derivatives for use in optimization, using FiniteDifferences. My objective function has has ~600 dimensional input and real output, and takes about .1 seconds to evaluate. I know the derivatives are not zero, because if I perturb an input dimension by a little (eg .01) the output changes. But with FiniteDifferences.jl, I'm getting all zeros: "},{"type":"text","text":"grad(forward_fdm(3,1), objective, x_0)","style":{"code":true}},{"type":"text","text":" returns all zeros (well, strictly speaking a 1-tuple with first element an array of all zeros).\n\nI thought maybe this has to do with conversions between Float64 and Float32; internally, my objective function (which is a simulation) converts some inputs to F32 and does GPU computation. So I tried "},{"type":"text","text":"wrapper_obj(x::Vector{Float32}) = obj(Float64.(x))","style":{"code":true}},{"type":"text","text":" and took the gradient of "},{"type":"text","text":"wrapper_obj","style":{"code":true}},{"type":"text","text":", trying to force FiniteDifferences to take bigger (float32 compatible) steps. But while the resulting gradient isn't "},{"type":"text","text":"all","style":{"italic":true}},{"type":"text","text":" zeros now, it's mostly zeros and definitely wrong.\n\nAnything else I can try?"}]}]}],"thread_ts":"1617134898.048500","reply_count":4,"reply_users_count":3,"latest_reply":"1617206491.050500","reply_users":["U9MD78Z9N","U6CJRSR63","U73ACR3TQ"],"is_locked":false,"subscribed":false},{"client_msg_id":"e4ebffa5-cc5a-452a-b43c-e042bf097346","type":"message","text":"auto diff?","user":"U9MD78Z9N","ts":"1617135413.048600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"V2/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"auto diff?"}]}]}],"thread_ts":"1617134898.048500","parent_user_id":"U73ACR3TQ"},{"client_msg_id":"eed13949-2af2-49cb-b27e-812eafd4fb70","type":"message","text":"If you say that FiniteDiffernces gives you all zeros like you describe I think you might be onto something. You probably have something like a multivariate stepfunction.  Auto diff might give you the answer. Can you give some more context? Even if you get the gradients through autodiff you're still going to have potential issues with you globalization strategy (line search, trust region) unless you don't use one.","user":"U6CJRSR63","ts":"1617139198.048800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sUa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If you say that FiniteDiffernces gives you all zeros like you describe I think you might be onto something. You probably have something like a multivariate stepfunction.  Auto diff might give you the answer. Can you give some more context? Even if you get the gradients through autodiff you're still going to have potential issues with you globalization strategy (line search, trust region) unless you don't use one."}]}]}],"thread_ts":"1617134898.048500","parent_user_id":"U73ACR3TQ"},{"client_msg_id":"eae1f0f5-3533-48d7-a67f-b1dd58c9dc0e","type":"message","text":"Autodiff is a good idea, thank you for the suggestion. <@U6CJRSR63> a few details. My objective function is something like\n```function obj_and_grad(point, data)\n    objective = simple_function(point, ata)\n    grad_components = derivative_helper.(point, data)\n    gradient = matrix_math(cu(point), cu(data)) # gpu accelerated\n    return objective, gradient\nend```\nSo to actually solve optimization problems no need for autodiff or finite differences, I just want to confirm that the way I set up my helpers `derivative_helper` and `matrix_math` actually produces nearly-correct gradients.","user":"U73ACR3TQ","ts":"1617206341.050300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jPo3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Autodiff is a good idea, thank you for the suggestion. "},{"type":"user","user_id":"U6CJRSR63"},{"type":"text","text":" a few details. My objective function is something like\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function obj_and_grad(point, data)\n    objective = simple_function(point, ata)\n    grad_components = derivative_helper.(point, data)\n    gradient = matrix_math(cu(point), cu(data)) # gpu accelerated\n    return objective, gradient\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"So to actually solve optimization problems no need for autodiff or finite differences, I just want to confirm that the way I set up my helpers "},{"type":"text","text":"derivative_helper","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"matrix_math","style":{"code":true}},{"type":"text","text":" actually produces nearly-correct gradients."}]}]}],"thread_ts":"1617134898.048500","parent_user_id":"U73ACR3TQ"},{"client_msg_id":"c3b8b368-4aba-45d3-b3b3-49999346b6e4","type":"message","text":"I've tested by my own hacky finite difference, ie\n```approx_gradient = map(1:ndims) do i\n    point2 = copy(point)\n    point2[i] += .01\n    return (obj_and_grad(point2, data)[1] - obj_and_grad(point1, data)[1]) / .01\nend```\nwhich I understand is a Bad Idea, ie don't roll your own finite difference. But I am somewhat reassured that `approx_gradient` as defined here is _very_ close to the hand written gradient from `obj_and_grad`. I wanted to do things properly by using FineDifferences, and therein ran into trouble","user":"U73ACR3TQ","ts":"1617206491.050500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Dlb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I've tested by my own hacky finite difference, ie\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"approx_gradient = map(1:ndims) do i\n    point2 = copy(point)\n    point2[i] += .01\n    return (obj_and_grad(point2, data)[1] - obj_and_grad(point1, data)[1]) / .01\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"which I understand is a Bad Idea, ie don't roll your own finite difference. But I am somewhat reassured that "},{"type":"text","text":"approx_gradient","style":{"code":true}},{"type":"text","text":" as defined here is "},{"type":"text","text":"very ","style":{"italic":true}},{"type":"text","text":"close to the hand written gradient from "},{"type":"text","text":"obj_and_grad","style":{"code":true}},{"type":"text","text":". I wanted to do things properly by using FineDifferences, and therein ran into trouble"}]}]}],"thread_ts":"1617134898.048500","parent_user_id":"U73ACR3TQ"}]