[{"client_msg_id":"952ab841-c725-4e23-8595-6f92259b1d66","type":"message","text":"I'm trying to double check some (hand written) derivatives for use in optimization, using FiniteDifferences. My objective function has has ~600 dimensional input and real output, and takes about .1 seconds to evaluate. I know the derivatives are not zero, because if I perturb an input dimension by a little (eg .01) the output changes. But with FiniteDifferences.jl, I'm getting all zeros: `grad(forward_fdm(3,1), objective, x_0)` returns all zeros (well, strictly speaking a 1-tuple with first element an array of all zeros).\n\nI thought maybe this has to do with conversions between Float64 and Float32; internally, my objective function (which is a simulation) converts some inputs to F32 and does GPU computation. So I tried `wrapper_obj(x::Vector{Float32}) = obj(Float64.(x))` and took the gradient of `wrapper_obj`, trying to force FiniteDifferences to take bigger (float32 compatible) steps. But while the resulting gradient isn't _all_ zeros now, it's mostly zeros and definitely wrong.\n\nAnything else I can try?","user":"U73ACR3TQ","ts":"1617134898.048500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7/4e5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm trying to double check some (hand written) derivatives for use in optimization, using FiniteDifferences. My objective function has has ~600 dimensional input and real output, and takes about .1 seconds to evaluate. I know the derivatives are not zero, because if I perturb an input dimension by a little (eg .01) the output changes. But with FiniteDifferences.jl, I'm getting all zeros: "},{"type":"text","text":"grad(forward_fdm(3,1), objective, x_0)","style":{"code":true}},{"type":"text","text":" returns all zeros (well, strictly speaking a 1-tuple with first element an array of all zeros).\n\nI thought maybe this has to do with conversions between Float64 and Float32; internally, my objective function (which is a simulation) converts some inputs to F32 and does GPU computation. So I tried "},{"type":"text","text":"wrapper_obj(x::Vector{Float32}) = obj(Float64.(x))","style":{"code":true}},{"type":"text","text":" and took the gradient of "},{"type":"text","text":"wrapper_obj","style":{"code":true}},{"type":"text","text":", trying to force FiniteDifferences to take bigger (float32 compatible) steps. But while the resulting gradient isn't "},{"type":"text","text":"all","style":{"italic":true}},{"type":"text","text":" zeros now, it's mostly zeros and definitely wrong.\n\nAnything else I can try?"}]}]}],"thread_ts":"1617134898.048500","reply_count":1,"reply_users_count":1,"latest_reply":"1617135413.048600","reply_users":["U9MD78Z9N"],"is_locked":false,"subscribed":false},{"client_msg_id":"e4ebffa5-cc5a-452a-b43c-e042bf097346","type":"message","text":"auto diff?","user":"U9MD78Z9N","ts":"1617135413.048600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"V2/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"auto diff?"}]}]}],"thread_ts":"1617134898.048500","parent_user_id":"U73ACR3TQ"}]