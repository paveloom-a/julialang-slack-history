[{"client_msg_id":"952ab841-c725-4e23-8595-6f92259b1d66","type":"message","text":"I'm trying to double check some (hand written) derivatives for use in optimization, using FiniteDifferences. My objective function has has ~600 dimensional input and real output, and takes about .1 seconds to evaluate. I know the derivatives are not zero, because if I perturb an input dimension by a little (eg .01) the output changes. But with FiniteDifferences.jl, I'm getting all zeros: `grad(forward_fdm(3,1), objective, x_0)` returns all zeros (well, strictly speaking a 1-tuple with first element an array of all zeros).\n\nI thought maybe this has to do with conversions between Float64 and Float32; internally, my objective function (which is a simulation) converts some inputs to F32 and does GPU computation. So I tried `wrapper_obj(x::Vector{Float32}) = obj(Float64.(x))` and took the gradient of `wrapper_obj`, trying to force FiniteDifferences to take bigger (float32 compatible) steps. But while the resulting gradient isn't _all_ zeros now, it's mostly zeros and definitely wrong.\n\nAnything else I can try?","user":"U73ACR3TQ","ts":"1617134898.048500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7/4e5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm trying to double check some (hand written) derivatives for use in optimization, using FiniteDifferences. My objective function has has ~600 dimensional input and real output, and takes about .1 seconds to evaluate. I know the derivatives are not zero, because if I perturb an input dimension by a little (eg .01) the output changes. But with FiniteDifferences.jl, I'm getting all zeros: "},{"type":"text","text":"grad(forward_fdm(3,1), objective, x_0)","style":{"code":true}},{"type":"text","text":" returns all zeros (well, strictly speaking a 1-tuple with first element an array of all zeros).\n\nI thought maybe this has to do with conversions between Float64 and Float32; internally, my objective function (which is a simulation) converts some inputs to F32 and does GPU computation. So I tried "},{"type":"text","text":"wrapper_obj(x::Vector{Float32}) = obj(Float64.(x))","style":{"code":true}},{"type":"text","text":" and took the gradient of "},{"type":"text","text":"wrapper_obj","style":{"code":true}},{"type":"text","text":", trying to force FiniteDifferences to take bigger (float32 compatible) steps. But while the resulting gradient isn't "},{"type":"text","text":"all","style":{"italic":true}},{"type":"text","text":" zeros now, it's mostly zeros and definitely wrong.\n\nAnything else I can try?"}]}]}],"thread_ts":"1617134898.048500","reply_count":2,"reply_users_count":2,"latest_reply":"1617139198.048800","reply_users":["U9MD78Z9N","U6CJRSR63"],"is_locked":false,"subscribed":false},{"client_msg_id":"e4ebffa5-cc5a-452a-b43c-e042bf097346","type":"message","text":"auto diff?","user":"U9MD78Z9N","ts":"1617135413.048600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"V2/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"auto diff?"}]}]}],"thread_ts":"1617134898.048500","parent_user_id":"U73ACR3TQ"},{"client_msg_id":"eed13949-2af2-49cb-b27e-812eafd4fb70","type":"message","text":"If you say that FiniteDiffernces gives you all zeros like you describe I think you might be onto something. You probably have something like a multivariate stepfunction.  Auto diff might give you the answer. Can you give some more context? Even if you get the gradients through autodiff you're still going to have potential issues with you globalization strategy (line search, trust region) unless you don't use one.","user":"U6CJRSR63","ts":"1617139198.048800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sUa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If you say that FiniteDiffernces gives you all zeros like you describe I think you might be onto something. You probably have something like a multivariate stepfunction.  Auto diff might give you the answer. Can you give some more context? Even if you get the gradients through autodiff you're still going to have potential issues with you globalization strategy (line search, trust region) unless you don't use one."}]}]}],"thread_ts":"1617134898.048500","parent_user_id":"U73ACR3TQ"}]