[{"client_msg_id":"72a267b9-6164-46d1-a8fa-95b54beff966","type":"message","text":"Not directly a julia question, but:\nAre methods like SGD and minibatch optimization useful for fitting traditional analytical models? I only see them mentioned in context of neutral networks.\nMaybe someone has experience with this, or pointers...","user":"UGTUKUHLN","ts":"1614554676.035700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=Iar","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not directly a julia question, but:\nAre methods like SGD and minibatch optimization useful for fitting traditional analytical models? I only see them mentioned in context of neutral networks.\nMaybe someone has experience with this, or pointers..."}]}]}],"thread_ts":"1614554676.035700","reply_count":9,"reply_users_count":3,"latest_reply":"1614887382.065300","reply_users":["U9MD78Z9N","UGTUKUHLN","UDDSTBX19"],"subscribed":false},{"type":"message","text":"If you are not very deep into the literature i would recommend watching this talk, if you are a person which can learn from video.","user":"U9MD78Z9N","ts":"1614554778.035900","team":"T68168MUP","thread_ts":"1614554676.035700","parent_user_id":"UGTUKUHLN"},{"client_msg_id":"737b5954-4dc5-4d12-a567-cd353b2ee653","type":"message","text":"what talk? :)","user":"UGTUKUHLN","ts":"1614555096.038900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"EBP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"what talk? :)"}]}]}],"thread_ts":"1614554676.035700","parent_user_id":"UGTUKUHLN"},{"type":"message","text":"Oh, i forgot the link","user":"U9MD78Z9N","ts":"1614555113.039100","team":"T68168MUP","thread_ts":"1614554676.035700","parent_user_id":"UGTUKUHLN"},{"type":"message","text":"<https://www.youtube.com/watch?v=WyKN8CyFZNI>","user":"U9MD78Z9N","ts":"1614555127.039300","team":"T68168MUP","attachments":[{"service_name":"YouTube","service_url":"https://www.youtube.com/","title":"Katya Scheinberg: \"Recent advances in Derivative-Free Optimization and its connection to reinfor...\"","title_link":"https://www.youtube.com/watch?v=WyKN8CyFZNI","author_name":"Institute for Pure & Applied Mathematics (IPAM)","author_link":"https://www.youtube.com/c/IPAMUCLA","thumb_url":"https://i.ytimg.com/vi/WyKN8CyFZNI/hqdefault.jpg","thumb_width":480,"thumb_height":360,"fallback":"YouTube Video: Katya Scheinberg: \"Recent advances in Derivative-Free Optimization and its connection to reinfor...\"","video_html":"<iframe width=\"400\" height=\"225\" src=\"https://www.youtube.com/embed/WyKN8CyFZNI?feature=oembed&autoplay=1&iv_load_policy=3\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","video_html_width":400,"video_html_height":225,"from_url":"https://www.youtube.com/watch?v=WyKN8CyFZNI","service_icon":"https://a.slack-edge.com/80588/img/unfurl_icons/youtube.png","id":1,"original_url":"https://www.youtube.com/watch?v=WyKN8CyFZNI"}],"thread_ts":"1614554676.035700","parent_user_id":"UGTUKUHLN"},{"client_msg_id":"211c3915-b30c-499d-977e-336a7f16715c","type":"message","text":"At least judging from the title I don't see how this talk is relevant - I'm definitely interested in gradient-informed optimization","user":"UGTUKUHLN","ts":"1614555271.039600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jn8x","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"At least judging from the title I don't see how this talk is relevant - I'm definitely interested in gradient-informed optimization"}]}]}],"thread_ts":"1614554676.035700","parent_user_id":"UGTUKUHLN"},{"type":"message","text":"Ok. I jut thought whatever she says about getting the gradient also applies mostly for approximating the hessian from the gradient. If you have hessian available too then there is not sense in watching the talk, i agree.","user":"U9MD78Z9N","ts":"1614555387.039800","team":"T68168MUP","thread_ts":"1614554676.035700","parent_user_id":"UGTUKUHLN"},{"client_msg_id":"3a0d9db6-ff6f-4bf2-940a-7af01fa4f9ae","type":"message","text":"<@UGTUKUHLN> Are you talking about fitting parameters to an analytical model. I think that SGD and minibatch optimization are used. But if you have data of the size that requires batching and all, then that usually suggests a pretty large or complex model. In such cases, estimating things like transition matrices gets difficult if you have a lot of states. There are a couple of good books that I know about. The Gosavi book Simulation-Based Optimization is pretty good. There is a good connection between optimization of these large models and reinforcement learning--as the video indicates. If you want something more model-based, then \"Conditional Monte Carlo-Gradient Estimation and Optimization Applications\" by Fu may be good.","user":"UDDSTBX19","ts":"1614869974.064500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fgDN","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UGTUKUHLN"},{"type":"text","text":" Are you talking about fitting parameters to an analytical model. I think that SGD and minibatch optimization are used. But if you have data of the size that requires batching and all, then that usually suggests a pretty large or complex model. In such cases, estimating things like transition matrices gets difficult if you have a lot of states. There are a couple of good books that I know about. The Gosavi book Simulation-Based Optimization is pretty good. There is a good connection between optimization of these large models and reinforcement learning--as the video indicates. If you want something more model-based, then \"Conditional Monte Carlo-Gradient Estimation and Optimization Applications\" by Fu may be good."}]}]}],"thread_ts":"1614554676.035700","parent_user_id":"UGTUKUHLN"},{"client_msg_id":"7e6a91ad-636d-4c2a-a93e-b2617349ec94","type":"message","text":"<@UDDSTBX19> my interested in SGD is not caused by big data. I read that these stochastic descent methods have a tendency to escape local minima, compared to regular steepest descent.","user":"UGTUKUHLN","ts":"1614886552.065100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"q9bP","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UDDSTBX19"},{"type":"text","text":" my interested in SGD is not caused by big data. I read that these stochastic descent methods have a tendency to escape local minima, compared to regular steepest descent."}]}]}],"thread_ts":"1614554676.035700","parent_user_id":"UGTUKUHLN"},{"client_msg_id":"31daf356-dc7d-47e6-a188-c243330f9c2d","type":"message","text":"Yes, that makes sense <@UGTUKUHLN>. I guess it depends on what kind of surface you are optimizing over. You can use methods like simulated annealing or something like Nelder-Mead as well, depending on the purpose. There are versions of these algorithms to avoid getting caught in local minima or maxima. SGD is just another method in the arsenal, and you are correct that I don't usually see a lot of mentions of SGD outside of deep learning lately. But also keep in mind that there are just like tons and tons of deep learning papers coming out every day, so the sample is a bit biased. Seems like SGD or Adam or other variants make more sense for deep learning problems because we don't have much knowledge about the optimization surface, since it is like 1,000,000  dimensional or whatever the number of parameters are.","user":"UDDSTBX19","ts":"1614887382.065300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LEHOM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes, that makes sense "},{"type":"user","user_id":"UGTUKUHLN"},{"type":"text","text":". I guess it depends on what kind of surface you are optimizing over. You can use methods like simulated annealing or something like Nelder-Mead as well, depending on the purpose. There are versions of these algorithms to avoid getting caught in local minima or maxima. SGD is just another method in the arsenal, and you are correct that I don't usually see a lot of mentions of SGD outside of deep learning lately. But also keep in mind that there are just like tons and tons of deep learning papers coming out every day, so the sample is a bit biased. Seems like SGD or Adam or other variants make more sense for deep learning problems because we don't have much knowledge about the optimization surface, since it is like 1,000,000  dimensional or whatever the number of parameters are."}]}]}],"thread_ts":"1614554676.035700","parent_user_id":"UGTUKUHLN"}]