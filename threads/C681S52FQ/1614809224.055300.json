[{"client_msg_id":"a87b6708-1e31-4bbb-ae5e-969dface2fe3","type":"message","text":"Is there a Julia package that allows exploiting the function space structure of the problem (using the right inner products, instead of Euclidian l^2)?","user":"UFMH09DAR","ts":"1614809224.055300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"r4ts","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a Julia package that allows exploiting the function space structure of the problem (using the right inner products, instead of Euclidian l^2)?"}]}]}],"thread_ts":"1614809224.055300","reply_count":2,"reply_users_count":2,"latest_reply":"1614811887.055600","reply_users":["UCT7E536E","UFMH09DAR"],"subscribed":false},{"client_msg_id":"69236648-2351-4cd1-a264-c4f95eb7ab45","type":"message","text":"What do you mean by that?","user":"UCT7E536E","ts":"1614809944.055400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xj=Vc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What do you mean by that?"}]}]}],"thread_ts":"1614809224.055300","parent_user_id":"UFMH09DAR"},{"client_msg_id":"79dd01dd-9b15-478f-bab7-435a033b0fb6","type":"message","text":"If we want to minimize the functional J : ℝⁿ -&gt; ℝ, we can use steepest descent x_new = x_old - stepsize * ∇J(x_old). ∇J(x_old) ∈ ℝⁿ is the gradient of J.\nNow if we have minimization of J: V -&gt; ℝ, where V is a Hilbert Space, the analogue of steepest descent is Fréchet derivative J' that lives in the dual of V and we need a Riesz map R: V* -&gt; V, then the minimization algorithm is x_new = x_old - stepsize*R ( J' (x_old) ). Without the Riesz map number of iterations required for convergence blows up. That's usually considered in PDE-constrained optimization or preconditioning of PDEs for Krylov methods.","user":"UFMH09DAR","ts":"1614811887.055600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1MTWy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If we want to minimize the functional J : ℝⁿ -> ℝ, we can use steepest descent x_new = x_old - stepsize * ∇J(x_old). ∇J(x_old) ∈ ℝⁿ is the gradient of J.\nNow if we have minimization of J: V -> ℝ, where V is a Hilbert Space, the analogue of steepest descent is Fréchet derivative J' that lives in the dual of V and we need a Riesz map R: V* -> V, then the minimization algorithm is x_new = x_old - stepsize*R ( J' (x_old) ). Without the Riesz map number of iterations required for convergence blows up. That's usually considered in PDE-constrained optimization or preconditioning of PDEs for Krylov methods."}]}]}],"thread_ts":"1614809224.055300","parent_user_id":"UFMH09DAR","reactions":[{"name":"today-i-learned","users":["U0138UTB7A4"],"count":1},{"name":"astonished","users":["U0138UTB7A4"],"count":1}]}]