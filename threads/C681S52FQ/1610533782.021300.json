[{"type":"message","text":"Okay I think I have narrowed it down to the MWE below.\n\nFor context (feel free to skip), this occurs as part of my (still woefully incomplete) SynthControl package, which implements synthetic control estimation. The core of the method is to take a number of potential \"donor\" time series and weight them in such a way that they get as close as possible to a target time series. The full code is here: <https://github.com/nilshg/SynthControl.jl/blob/master/src/SynthControl.jl>, but I've extracted out the relevant bits below:\n\n```using Optim, DelimitedFiles, LinearAlgebra\n\nfunction get_weights(y, xs)\n\n    obj(w) = norm(y .- xs*w) + 1e6*abs(1.0 - sum(w))\n\n    # Initial condition and bounds\n    initial = [1e-5 for _ in 1:size(xs, 2)]\n    lower = zeros(size(xs, 2))\n    upper = ones(size(xs, 2))\n\n    # Get weights through optimization\n    res = optimize(obj, lower, upper, initial)\n\n    return res.minimizer\nend\n\nyᵂ = [100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.7, 100.7, 100.7, 100.7, \n      100.2, 100.2, 100.2, 100.1, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, \n      99.4, 99.4, 99.4, 99.4, 99.3, 99.2, 99.2, 99.2, 99.2, 98.9, 98.9, 98.9, 98.9, 98.9, \n      98.9, 98.9, 98.9, 98.9, 98.9, 98.8, 98.8, 98.8, 98.7, 98.7, 98.6, 98.6, 98.6, 98.3, \n      97.9, 97.9, 97.9, 97.9, 97.8, 97.7, 97.7, 97.7]\n\nxs = readdlm(\"xs.txt\")\n\nget_weights(yᵂ, xs) # Fails - some weights are NaN\nw₁ = get_weights(yᵂ, xs[:, [1:4; 6:end]])\n\nsum(w₁) # ≈ 1.00\nnorm(yᵂ .- xs[:, [1:4; 6:end]]*w₁) # ≈ 0.897```\n","files":[{"id":"F01JZDH0YP3","created":1610533648,"timestamp":1610533648,"name":"xs.txt","title":"xs.txt","mimetype":"text/plain","filetype":"text","pretty_type":"Plain Text","user":"U7JQGPGCQ","editable":true,"size":5899,"mode":"snippet","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01JZDH0YP3/xs.txt","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01JZDH0YP3/download/xs.txt","permalink":"https://julialang.slack.com/files/U7JQGPGCQ/F01JZDH0YP3/xs.txt","permalink_public":"https://slack-files.com/T68168MUP-F01JZDH0YP3-9eb35dde6d","edit_link":"https://julialang.slack.com/files/U7JQGPGCQ/F01JZDH0YP3/xs.txt/edit","preview":"100.0\t100.7\t101.7\t102.1\t96.2\t100.8\t100.3\t99.6\t97.6\t99.8\t99.5\t100.9\t99.0\t101.4\t101.2\t100.9\t102.5\t102.4\n100.0\t101.3\t101.7\t102.1\t95.7\t101.0\t100.3\t99.6\t97.4\t99.7\t99.5\t101.0\t98.2\t101.4\t101.2\t100.9\t102.5\t102.4\n99.9\t101.1\t101.7\t102.1\t97.1\t101.1\t100.3\t99.6\t97.4\t99.8\t99.5\t100.7\t97.9\t101.4\t101.2\t100.9\t102.5\t102.4\n100.0\t101.1\t101.7\t101.9\t97.4\t101.1\t100.7\t99.6\t97.6\t99.8\t99.6\t100.9\t98.0\t101.3\t101.2\t100.6\t102.5\t102.3\n100.0\t101.1\t101.7\t101.9\t97.6\t101.1\t100.7\t99.5\t97.7\t99.9\t99.6\t100.8\t97.9\t100.2\t101.3\t100.6\t102.5\t102.3","preview_highlight":"<div class=\"CodeMirror cm-s-default CodeMirrorServer\" oncopy=\"if(event.clipboardData){event.clipboardData.setData('text/plain',window.getSelection().toString().replace(/\\u200b/g,''));event.preventDefault();event.stopPropagation();}\">\n<div class=\"CodeMirror-code\">\n<div><pre>100.0   100.7   101.7   102.1   96.2    100.8   100.3   99.6    97.6    99.8    99.5    100.9   99.0    101.4   101.2   100.9   102.5   102.4</pre></div>\n<div><pre>100.0   101.3   101.7   102.1   95.7    101.0   100.3   99.6    97.4    99.7    99.5    101.0   98.2    101.4   101.2   100.9   102.5   102.4</pre></div>\n<div><pre>99.9    101.1   101.7   102.1   97.1    101.1   100.3   99.6    97.4    99.8    99.5    100.7   97.9    101.4   101.2   100.9   102.5   102.4</pre></div>\n<div><pre>100.0   101.1   101.7   101.9   97.4    101.1   100.7   99.6    97.6    99.8    99.6    100.9   98.0    101.3   101.2   100.6   102.5   102.3</pre></div>\n<div><pre>100.0   101.1   101.7   101.9   97.6    101.1   100.7   99.5    97.7    99.9    99.6    100.8   97.9    100.2   101.3   100.6   102.5   102.3</pre></div>\n</div>\n</div>\n","lines":61,"lines_more":56,"preview_is_truncated":true,"is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"pMLF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Okay I think I have narrowed it down to the MWE below.\n\nFor context (feel free to skip), this occurs as part of my (still woefully incomplete) SynthControl package, which implements synthetic control estimation. The core of the method is to take a number of potential \"donor\" time series and weight them in such a way that they get as close as possible to a target time series. The full code is here: "},{"type":"link","url":"https://github.com/nilshg/SynthControl.jl/blob/master/src/SynthControl.jl"},{"type":"text","text":", but I've extracted out the relevant bits below:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Optim, DelimitedFiles, LinearAlgebra\n\nfunction get_weights(y, xs)\n\n    obj(w) = norm(y .- xs*w) + 1e6*abs(1.0 - sum(w))\n\n    # Initial condition and bounds\n    initial = [1e-5 for _ in 1:size(xs, 2)]\n    lower = zeros(size(xs, 2))\n    upper = ones(size(xs, 2))\n\n    # Get weights through optimization\n    res = optimize(obj, lower, upper, initial)\n\n    return res.minimizer\nend\n\nyᵂ = [100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.7, 100.7, 100.7, 100.7, \n      100.2, 100.2, 100.2, 100.1, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, \n      99.4, 99.4, 99.4, 99.4, 99.3, 99.2, 99.2, 99.2, 99.2, 98.9, 98.9, 98.9, 98.9, 98.9, \n      98.9, 98.9, 98.9, 98.9, 98.9, 98.8, 98.8, 98.8, 98.7, 98.7, 98.6, 98.6, 98.6, 98.3, \n      97.9, 97.9, 97.9, 97.9, 97.8, 97.7, 97.7, 97.7]\n\nxs = readdlm(\"xs.txt\")\n\nget_weights(yᵂ, xs) # Fails - some weights are NaN\nw₁ = get_weights(yᵂ, xs[:, [1:4; 6:end]])\n\nsum(w₁) # ≈ 1.00\nnorm(yᵂ .- xs[:, [1:4; 6:end]]*w₁) # ≈ 0.897"}]},{"type":"rich_text_section","elements":[]}]}],"user":"U7JQGPGCQ","display_as_bot":false,"ts":"1610533782.021300","thread_ts":"1610533782.021300","reply_count":5,"reply_users_count":2,"latest_reply":"1610555690.024600","reply_users":["UCT7E536E","U7JQGPGCQ"],"subscribed":false},{"client_msg_id":"5d925770-a154-423c-8a4f-6628ea60b92e","type":"message","text":"(don't know the inner workings of Optim, so I can only comment on the math part)\nYour objective is not differentiable everywhere: `norm` is not differentiable at `0`, nor is `abs` ---&gt; I'd bet this is why you're getting some NaN values.\nYou can square the objective &amp; penalty, which will make everything strongly convex:\n``` obj(w) = dot(y .- xs*w, y .- xs*w) + 1e6*(1.0 - sum(w))^2```\nPlus, it may make the optimizer's life easier if your initial point satisfies the `sum(w) == 1` constraint. I'd suggest starting with\n```n = size(xs, 2)\ninitial = [(1/n) for _ in 1:n]```","user":"UCT7E536E","ts":"1610547253.023800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IN=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(don't know the inner workings of Optim, so I can only comment on the math part)\nYour objective is not differentiable everywhere: "},{"type":"text","text":"norm","style":{"code":true}},{"type":"text","text":" is not differentiable at "},{"type":"text","text":"0","style":{"code":true}},{"type":"text","text":", nor is "},{"type":"text","text":"abs","style":{"code":true}},{"type":"text","text":" ---> I'd bet this is why you're getting some NaN values.\nYou can square the objective & penalty, which will make everything strongly convex:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":" obj(w) = dot(y .- xs*w, y .- xs*w) + 1e6*(1.0 - sum(w))^2"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Plus, it may make the optimizer's life easier if your initial point satisfies the "},{"type":"text","text":"sum(w) == 1","style":{"code":true}},{"type":"text","text":" constraint. I'd suggest starting with\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"n = size(xs, 2)\ninitial = [(1/n) for _ in 1:n]"}]}]}],"thread_ts":"1610533782.021300","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"04c1373d-e19d-42c7-b9b2-2b40a80f0a73","type":"message","text":"BTW, this problem is a simple quadratic optimization problem, for which you can also use a number of solvers through, e.g., JuMP.\nFor instance:\n```using JuMP, ECOS\n\nfunction get_weights(y, xs)\n    m, n = size(xs)\n    \n    model = Model(ECOS.Optimizer)\n    \n    @variable(model, w[1:n] &gt;= 0)  # your weights\n    \n    @constraint(model, sum(w) == 1)  # this automatically implies w &lt;= 1\n    \n    @variable(model, z[1:m])       # this will hold y - Xw\n    @constraint(model, z .== y .- xs * w)\n    @objective(model, Min, z'z)\n\n    optimize!(model)\n    return value.(w)\nend```","user":"UCT7E536E","ts":"1610547750.024000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hcG/f","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"BTW, this problem is a simple quadratic optimization problem, for which you can also use a number of solvers through, e.g., JuMP.\nFor instance:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using JuMP, ECOS\n\nfunction get_weights(y, xs)\n    m, n = size(xs)\n    \n    model = Model(ECOS.Optimizer)\n    \n    @variable(model, w[1:n] >= 0)  # your weights\n    \n    @constraint(model, sum(w) == 1)  # this automatically implies w <= 1\n    \n    @variable(model, z[1:m])       # this will hold y - Xw\n    @constraint(model, z .== y .- xs * w)\n    @objective(model, Min, z'z)\n\n    optimize!(model)\n    return value.(w)\nend"}]}]}],"thread_ts":"1610533782.021300","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"1590f39c-66e6-4260-908d-5c3235318b1a","type":"message","text":"Thanks for the suggestions!","user":"U7JQGPGCQ","ts":"1610554320.024200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SAQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for the suggestions!"}]}]}],"thread_ts":"1610533782.021300","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"09de1845-f949-4a67-80eb-5ed4ad176ae8","type":"message","text":"Okay so switching the objective to quadratic definitely helps, starting from 1/n as initial guess is not so clear cut. I'm getting:\n\n```Starting guess 0, 18 columns\n1.360420 seconds (4.88 M allocations: 2.421 GiB, 17.64% gc time)\nStarting guess 0, 17 columns\n  0.781552 seconds (2.63 M allocations: 1.294 GiB, 17.52% gc time)\nStarting guess 1/n, 18 columns\n┌ Warning: Failed to achieve finite new evaluation point, using alpha=0\n└ @ LineSearches C:\\Users\\ngudat\\.julia\\packages\\LineSearches\\Ki4c5\\src\\hagerzhang.jl:148\n┌ Warning: Failed to achieve finite new evaluation point, using alpha=0\n└ @ LineSearches C:\\Users\\ngudat\\.julia\\packages\\LineSearches\\Ki4c5\\src\\hagerzhang.jl:148\n  1.793122 seconds (6.33 M allocations: 3.154 GiB, 17.62% gc time)\nStarting guess 1/n, 17 columns\n  0.528031 seconds (1.96 M allocations: 981.625 MiB, 17.13% gc time)```\nSo in the \"nicer\" case (where I remove the column that seems to complicate the solution), it helps to speed up the solution if I start from 1/n, while in the pathological case I get warnings and (presumable because of that) the runtime goes up. The overall solution is unaffected though, i.e. both starting guesses yield the same final optimizer.","user":"U7JQGPGCQ","ts":"1610555366.024400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OO9a","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Okay so switching the objective to quadratic definitely helps, starting from 1/n as initial guess is not so clear cut. I'm getting:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Starting guess 0, 18 columns\n1.360420 seconds (4.88 M allocations: 2.421 GiB, 17.64% gc time)\nStarting guess 0, 17 columns\n  0.781552 seconds (2.63 M allocations: 1.294 GiB, 17.52% gc time)\nStarting guess 1/n, 18 columns\n┌ Warning: Failed to achieve finite new evaluation point, using alpha=0\n└ @ LineSearches C:\\Users\\ngudat\\.julia\\packages\\LineSearches\\Ki4c5\\src\\hagerzhang.jl:148\n┌ Warning: Failed to achieve finite new evaluation point, using alpha=0\n└ @ LineSearches C:\\Users\\ngudat\\.julia\\packages\\LineSearches\\Ki4c5\\src\\hagerzhang.jl:148\n  1.793122 seconds (6.33 M allocations: 3.154 GiB, 17.62% gc time)\nStarting guess 1/n, 17 columns\n  0.528031 seconds (1.96 M allocations: 981.625 MiB, 17.13% gc time)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nSo in the \"nicer\" case (where I remove the column that seems to complicate the solution), it helps to speed up the solution if I start from 1/n, while in the pathological case I get warnings and (presumable because of that) the runtime goes up. The overall solution is unaffected though, i.e. both starting guesses yield the same final optimizer."}]}]}],"thread_ts":"1610533782.021300","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"a711e8fb-9377-4f34-945c-5ef2b63bae84","type":"message","text":"And my god why is the JuMP solution so fast? The runtime is on the order of 0.01 seconds, 50 times faster than going through Optim??","user":"U7JQGPGCQ","ts":"1610555690.024600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WJh6H","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And my god why is the JuMP solution so fast? The runtime is on the order of 0.01 seconds, 50 times faster than going through Optim??"}]}]}],"thread_ts":"1610533782.021300","parent_user_id":"U7JQGPGCQ"}]