[{"client_msg_id":"d88ad074-0687-4257-add7-09c865d7f67f","type":"message","text":"Probably a stupid question but, since the markov chain created by the Metropolis-Hastings algorithm is ergodic, why can't we sample many chains in parallel for just few steps and then join the results together?","user":"UGB3MK8MC","ts":"1611745393.009200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MZwVn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Probably a stupid question but, since the markov chain created by the Metropolis-Hastings algorithm is ergodic, why can't we sample many chains in parallel for just few steps and then join the results together?"}]}]}],"thread_ts":"1611745393.009200","reply_count":21,"reply_users_count":5,"latest_reply":"1611953570.027800","reply_users":["UJ7DVTVQ8","U81PB6N77","UGB3MK8MC","U7THT3TM3","UAUPJLBQX"],"subscribed":false},{"client_msg_id":"8c3ec36c-e889-49ea-934f-77b9b49be5c6","type":"message","text":"Most frameworks have tools for making this easy. You need to discard the initial samples from all chains though, so there is a constant overhead per chain.","user":"UJ7DVTVQ8","ts":"1611749774.011100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sxKHa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Most frameworks have tools for making this easy. You need to discard the initial samples from all chains though, so there is a constant overhead per chain."}]}]}],"thread_ts":"1611745393.009200","parent_user_id":"UGB3MK8MC","reactions":[{"name":"+1","users":["U81PB6N77","U7THT3TM3","UAUPJLBQX"],"count":3}]},{"client_msg_id":"0ef4b95d-660a-461c-9652-79ff30517f8d","type":"message","text":"I think there's value in something like\n1. Approximate using variational inference\n2. Sample from result\n3. A few MCMC steps for each sample","user":"U81PB6N77","ts":"1611754547.011500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"y9/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think there's value in something like\n1. Approximate using variational inference\n2. Sample from result\n3. A few MCMC steps for each sample"}]}]}],"thread_ts":"1611745393.009200","parent_user_id":"UGB3MK8MC","reactions":[{"name":"+1","users":["U7THT3TM3","UAUPJLBQX"],"count":2}]},{"client_msg_id":"d686d8f3-b096-4701-9eb7-c1acc782ca91","type":"message","text":"Thanks!","user":"UGB3MK8MC","ts":"1611756005.011700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Z/0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks!"}]}]}],"thread_ts":"1611745393.009200","parent_user_id":"UGB3MK8MC"},{"client_msg_id":"25bf1290-db01-4659-9457-830577b58472","type":"message","text":"I have seen that most tools allow you to run one chain per core and then plot each chain separately. They don't actually seem to merge two chains and plot only the resulting one, is there a particular reason why?","user":"UGB3MK8MC","ts":"1611756104.011900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iyTki","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have seen that most tools allow you to run one chain per core and then plot each chain separately. They don't actually seem to merge two chains and plot only the resulting one, is there a particular reason why?"}]}]}],"thread_ts":"1611745393.009200","parent_user_id":"UGB3MK8MC"},{"client_msg_id":"91d1d097-82b2-4306-9369-a90cca72eb19","type":"message","text":"I was thinking more about 100-1000 chains run on a GPU or something like that. I know of replica exchange but if I understood it correctly it's more about using multiple chains to explore multimodal distributions rather than using all of them to speed up the convergence","user":"UGB3MK8MC","ts":"1611756194.012100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LrrHW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I was thinking more about 100-1000 chains run on a GPU or something like that. I know of replica exchange but if I understood it correctly it's more about using multiple chains to explore multimodal distributions rather than using all of them to speed up the convergence"}]}]}],"thread_ts":"1611745393.009200","parent_user_id":"UGB3MK8MC"},{"client_msg_id":"27d87c0d-37f1-4413-af5f-6663b17a2b13","type":"message","text":"Keeping them separate lets you track autocorrelations, and also things like the R-hat statistic, which compares within-chain variance to between-chain variance","user":"U81PB6N77","ts":"1611767079.012900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ragd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Keeping them separate lets you track autocorrelations, and also things like the R-hat statistic, which compares within-chain variance to between-chain variance"}]}]}],"thread_ts":"1611745393.009200","parent_user_id":"UGB3MK8MC","reactions":[{"name":"+1","users":["U7THT3TM3"],"count":1}]},{"client_msg_id":"1017A39B-EE9A-487F-ABB7-D53671D78410","type":"message","text":"Is it useful to have 100 chains just for r hat?","user":"UGB3MK8MC","ts":"1611780372.013800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WV6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is it useful to have 100 chains just for r hat?"}]}]}],"thread_ts":"1611745393.009200","parent_user_id":"UGB3MK8MC"},{"client_msg_id":"3bea63b0-1a95-4525-90fa-0469e8b6835c","type":"message","text":"I wouldn't think so, but maybe if there are lots of local regions it can get stuck in. I'd say warmup cost is the biggest trouble with lots of chains","user":"U81PB6N77","ts":"1611781528.014000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0SF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I wouldn't think so, but maybe if there are lots of local regions it can get stuck in. I'd say warmup cost is the biggest trouble with lots of chains"}]}]}],"thread_ts":"1611745393.009200","parent_user_id":"UGB3MK8MC"},{"client_msg_id":"5315dbad-f581-4e31-b5eb-811b253c2195","type":"message","text":"All in all, my biggest question is, does sampling using many cores actually provide a linear speed up for probabilistic programming? And if yes, why is nobody doing it?\n\nWould it open the possibility of training a Bayesian NN using MCMC? Or having a turing model converge in milliseconds?","user":"UGB3MK8MC","ts":"1611781923.014200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"12Qa0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"All in all, my biggest question is, does sampling using many cores actually provide a linear speed up for probabilistic programming? And if yes, why is nobody doing it?\n\nWould it open the possibility of training a Bayesian NN using MCMC? Or having a turing model converge in milliseconds?"}]}]}],"thread_ts":"1611745393.009200","parent_user_id":"UGB3MK8MC"},{"client_msg_id":"74A6CE60-DB70-402B-B52C-FA26F6BBB4BB","type":"message","text":"Well if you are doing a NN, your matmul is probably multithreaded, right? So you need multiple cores for a single chain.\n\nSo e.g. if you are doing NUTS for a Bayesian NN, and you have a 16 core machine, maybe you sample 4 chains at a time? Divide your 16 cores up into four groups? 4 cores per chain. So then your matmul for each chain gets 4 cores.\n\nI think <@UAUPJLBQX> had some thoughts on this recently?","user":"U7THT3TM3","ts":"1611802907.018600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mpxbp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Well if you are doing a NN, your matmul is probably multithreaded, right? So you need multiple cores for a single chain.\n"},{"type":"text","text":"\n"},{"type":"text","text":"So e.g. if you are doing NUTS for a Bayesian NN, and you have a 16 core machine, maybe you sample 4 chains at a time? Divide your 16 cores up into four groups? 4 cores per chain. So then your matmul for each chain gets 4 cores.\n"},{"type":"text","text":"\n"},{"type":"text","text":"I think "},{"type":"user","user_id":"UAUPJLBQX"},{"type":"text","text":" had some thoughts on this recently?"}]}]}],"thread_ts":"1611745393.009200","parent_user_id":"UGB3MK8MC","reactions":[{"name":"+1","users":["UAUPJLBQX"],"count":1}]},{"client_msg_id":"5E9BF947-9902-486A-B4DB-1426E21FE451","type":"message","text":"That sounds like evaluating the likelihood in parallel, which I agree is useful. I think Stan is working on gpu support atm.","user":"UGB3MK8MC","ts":"1611828037.020300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vhp3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That sounds like evaluating the likelihood in parallel, which I agree is useful. I think Stan is working on gpu support atm."}]}]}],"thread_ts":"1611745393.009200","parent_user_id":"UGB3MK8MC"},{"client_msg_id":"CDD1AFA2-337D-4529-AB7F-FC68467E6134","type":"message","text":"I was wondering about evaluating many chains in parallel in order to speed up convergence. Especially for simple models like linear/logistic regressions/glms","user":"UGB3MK8MC","ts":"1611828107.022100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Xpu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I was wondering about evaluating many chains in parallel in order to speed up convergence. Especially for simple models like linear/logistic regressions/glms"}]}]}],"thread_ts":"1611745393.009200","parent_user_id":"UGB3MK8MC"},{"client_msg_id":"bf13db5f-b306-4ced-a33a-3694b8c6f1fe","type":"message","text":"For simple models I think the communication overhead to the GPU will dominate the computation","user":"U81PB6N77","ts":"1611855952.022700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uenU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"For simple models I think the communication overhead to the GPU will dominate the computation"}]}]}],"thread_ts":"1611745393.009200","parent_user_id":"UGB3MK8MC"},{"client_msg_id":"f9216469-7144-44ff-b293-4edfad836809","type":"message","text":"We have a linear speedup with more cores -- after the warmup.\nLets say you have a 16-core system and want to warmup for 900 iterations (e.g. 50 initial to tune step sizes, 50-&gt;100-&gt;200-&gt;400 to update mass matrix estimates, 100 final to refine step sizes) and want 10_000 samples.\nA few possibilities:\n1. 1 chain, evaluate the likelihood in parallel. You need 10_900 iterations total. Not a good choice, because you want multiple independent chains for convergence diagnostics.\n2. 4 chains, evaluate the likelihood in parallel with 4 threads on each. Each chain needs 900 + 2_500 = 3_400 samples, or 13_600 total.\n3. 16 chains, single threaded likelihood. A nice thing here is that there is very little overhead from synchronization. However, each chain needs 900 + 625 = 1_525 samples, for 24_400 total.","user":"UAUPJLBQX","ts":"1611856995.023200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tqFam","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We have a linear speedup with more cores -- after the warmup.\nLets say you have a 16-core system and want to warmup for 900 iterations (e.g. 50 initial to tune step sizes, 50->100->200->400 to update mass matrix estimates, 100 final to refine step sizes) and want 10_000 samples.\nA few possibilities:\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"1 chain, evaluate the likelihood in parallel. You need 10_900 iterations total. Not a good choice, because you want multiple independent chains for convergence diagnostics."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"4 chains, evaluate the likelihood in parallel with 4 threads on each. Each chain needs 900 + 2_500 = 3_400 samples, or 13_600 total."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"16 chains, single threaded likelihood. A nice thing here is that there is very little overhead from synchronization. However, each chain needs 900 + 625 = 1_525 samples, for 24_400 total."}]}],"style":"ordered","indent":0}]}],"thread_ts":"1611745393.009200","parent_user_id":"UGB3MK8MC","reactions":[{"name":"+1","users":["U7THT3TM3","U81PB6N77","UJ7DVTVQ8","UGB3MK8MC"],"count":4}]},{"client_msg_id":"4278ec70-57d6-4c6e-85ab-424dcf98b48f","type":"message","text":"That's why <@U81PB6N77>’s idea to use something like Variational Inference to skip the warmup is so nifty.\nWe should be able to use the Variational Inference results to help with tuning HMC as well, right? E.g. use its estimated covariance as an initial mass matrix.","user":"UAUPJLBQX","ts":"1611857102.023600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rN++","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That's why "},{"type":"user","user_id":"U81PB6N77"},{"type":"text","text":"’s idea to use something like Variational Inference to skip the warmup is so nifty.\nWe should be able to use the Variational Inference results to help with tuning HMC as well, right? E.g. use its estimated covariance as an initial mass matrix."}]}]}],"thread_ts":"1611745393.009200","parent_user_id":"UGB3MK8MC","reactions":[{"name":"+1","users":["U7THT3TM3","U81PB6N77"],"count":2}]},{"client_msg_id":"1a382ef9-81a4-4751-9643-f5a42ff8f4cb","type":"message","text":"I think that's right. Also, the model gives us the inverse covariance structure for free, though I don't of anyone using this. But in principle that's much better than mean field VI or full rank","user":"U81PB6N77","ts":"1611857247.024100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ejqs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think that's right. Also, the model gives us the inverse covariance structure for free, though I don't of anyone using this. But in principle that's much better than mean field VI or full rank"}]}]}],"thread_ts":"1611745393.009200","parent_user_id":"UGB3MK8MC"},{"client_msg_id":"420d40bf-cfab-4313-a6cc-f26d941e5a4e","type":"message","text":"Doesn't that gives the covariance structure of HMC's momentum parameters (the inverse of the covariance of the model parameters, unless I'm mistaken)?\nWould be interesting to explore accelerating HMC convergence.\n\nI haven't looked into it too much, but surely there's work on basing MCMC off approximations like VI? E.g., use it as a StaticProposal in AdvancedMH.jl?","user":"UAUPJLBQX","ts":"1611857793.024300","team":"T68168MUP","edited":{"user":"UAUPJLBQX","ts":"1611857854.000000"},"blocks":[{"type":"rich_text","block_id":"Kyl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Doesn't that gives the covariance structure of HMC's momentum parameters (the inverse of the covariance of the model parameters, unless I'm mistaken)?\nWould be interesting to explore accelerating HMC convergence.\n\nI haven't looked into it too much, but surely there's work on basing MCMC off approximations like VI? E.g., use it as a StaticProposal in AdvancedMH.jl?"}]}]}],"thread_ts":"1611745393.009200","parent_user_id":"UGB3MK8MC"},{"client_msg_id":"8f22cdaf-780a-4af0-a4a5-821ea1347f17","type":"message","text":"I think that's right. As for the VI-&gt;MCMC thing, I think others have looked at it but I don't have details offhand. This whole area moves pretty quickly I think","user":"U81PB6N77","ts":"1611858204.024600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nJwzA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think that's right. As for the VI->MCMC thing, I think others have looked at it but I don't have details offhand. This whole area moves pretty quickly I think"}]}]}],"thread_ts":"1611745393.009200","parent_user_id":"UGB3MK8MC"},{"client_msg_id":"653567B0-7A45-497A-8076-7607861B2888","type":"message","text":"Thanks a lot, that’s so interesting","user":"UGB3MK8MC","ts":"1611911588.025300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vXG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks a lot, that’s so interesting"}]}]}],"thread_ts":"1611745393.009200","parent_user_id":"UGB3MK8MC"},{"client_msg_id":"97A816E7-74A6-44B1-9C40-2B4D0D3E2942","type":"message","text":"Also, do you know any research groups that study this topic in particular? Or is it too applied and I should be looking more towards research centers/companies?","user":"UGB3MK8MC","ts":"1611912336.027600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rGSA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Also, do you know any research groups that study this topic in particular? Or is it too applied and I should be looking more towards research centers/companies?"}]}]}],"thread_ts":"1611745393.009200","parent_user_id":"UGB3MK8MC"},{"client_msg_id":"ad3de92f-288e-4aa9-a817-bd1776d148ef","type":"message","text":"There are a few with this or nearby interests. Offhand, I'd say maybe David Blei, David Duvenaud, Matt Hoffman.","user":"U81PB6N77","ts":"1611953570.027800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MsSWD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"There are a few with this or nearby interests. Offhand, I'd say maybe David Blei, David Duvenaud, Matt Hoffman."}]}]}],"thread_ts":"1611745393.009200","parent_user_id":"UGB3MK8MC","reactions":[{"name":"+1","users":["UGB3MK8MC"],"count":1}]}]