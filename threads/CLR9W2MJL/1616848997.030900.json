[{"client_msg_id":"cbeae3a1-c8ec-457d-9e38-4fdb0fa3222f","type":"message","text":"What's the status of using directional statistics in Turing? In particular, I want to estimate a parameter in `SO(3)`, like a rotation matrix or a unit quaternion. I've tried\n<https://github.com/JuliaStats/Distributions.jl/blob/master/src/multivariate/vonmisesfisher.jl>\nbut no bijector was defined for that distribution. I've also read this thread\n<https://discourse.julialang.org/t/rfc-taking-directional-orientational-statistics-seriously/31951>\nbut didn't know what to make of it. I'm looking at <https://juliamanifolds.github.io/Manifolds.jl/latest/features/distributions.html> but can honestly say I have no idea if it's applicable to my example or not :flushed:","user":"UJ7DVTVQ8","ts":"1616848997.030900","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"[RFC] Taking directional/orientational statistics seriously","title_link":"https://discourse.julialang.org/t/rfc-taking-directional-orientational-statistics-seriously/31951","text":"To my knowledge, there are no Julia packages with good support for directional and orientational statistics. Directional statistics is used in biology (especially structural biology), crystallography, astronomy, and various other physics applications including geophysics. Distributions.jl implements VonMises for circular variables (angles) and VonMisesFisher for spherical variables (unit vectors), but that‚Äôs it. No orientational distributions are implemented. From the discussion on slack, it s...","fallback":"JuliaLang: [RFC] Taking directional/orientational statistics seriously","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","fields":[{"title":"Reading time","value":"3 mins :clock2:","short":true},{"title":"Likes","value":"35 :heart:","short":true}],"ts":1575675866,"from_url":"https://discourse.julialang.org/t/rfc-taking-directional-orientational-statistics-seriously/31951","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/rfc-taking-directional-orientational-statistics-seriously/31951"}],"blocks":[{"type":"rich_text","block_id":"gq6IJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What's the status of using directional statistics in Turing? In particular, I want to estimate a parameter in "},{"type":"text","text":"SO(3)","style":{"code":true}},{"type":"text","text":", like a rotation matrix or a unit quaternion. I've tried\n"},{"type":"link","url":"https://github.com/JuliaStats/Distributions.jl/blob/master/src/multivariate/vonmisesfisher.jl"},{"type":"text","text":"\nbut no bijector was defined for that distribution. I've also read this thread\n"},{"type":"link","url":"https://discourse.julialang.org/t/rfc-taking-directional-orientational-statistics-seriously/31951"},{"type":"text","text":"\nbut didn't know what to make of it. I'm looking at "},{"type":"link","url":"https://juliamanifolds.github.io/Manifolds.jl/latest/features/distributions.html"},{"type":"text","text":" but can honestly say I have no idea if it's applicable to my example or not "},{"type":"emoji","name":"flushed"}]}]}],"thread_ts":"1616848997.030900","reply_count":11,"reply_users_count":3,"latest_reply":"1617002363.033500","reply_users":["U85JBUGGP","UHDQQ4GN6","UJ7DVTVQ8"],"is_locked":false,"subscribed":false},{"client_msg_id":"d7bda83b-88a1-468f-8b4c-70ac35ee4d6f","type":"message","text":"Last I checked it was a special function blocking it because it won‚Äôt take in duals. See <https://discourse.julialang.org/t/stackoverflowerror-when-fitting-data-to-a-von-mises-distribution/45406>.","user":"U85JBUGGP","ts":"1616852077.031100","team":"T68168MUP","edited":{"user":"U85JBUGGP","ts":"1616852085.000000"},"attachments":[{"service_name":"JuliaLang","title":"StackOverflowError when fitting data to a von Mises distribution","title_link":"https://discourse.julialang.org/t/stackoverflowerror-when-fitting-data-to-a-von-mises-distribution/45406","text":"Hi everyone. I‚Äôm trying to use Turing.jl to fit data to a von Mises circular distribution, and I am having some ‚Äústack overflow‚Äù issues during the fitting. This is my (very simple) code (coin flip adaptation): using Turing using Distributions Œº = 0; Œ∫ = 517 vm = VonMises(Œº, Œ∫) data = rand(vm, 100) @model model(x) = begin Œº ~ Normal(0, 10) Œ∫ ~ Uniform(450, 550) N = length(x) for n in 1:N x[n] ~ VonMises(Œº, Œ∫) end end œµ = 0.05 œÑ = 10 iterations = 1000 chain = sam...","fallback":"JuliaLang: StackOverflowError when fitting data to a von Mises distribution","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","fields":[{"title":"Reading time","value":"1 mins :clock2:","short":true},{"title":"Likes","value":"3 :heart:","short":true}],"ts":1598171181,"from_url":"https://discourse.julialang.org/t/stackoverflowerror-when-fitting-data-to-a-von-mises-distribution/45406","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/stackoverflowerror-when-fitting-data-to-a-von-mises-distribution/45406"}],"blocks":[{"type":"rich_text","block_id":"fYL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Last I checked it was a special function blocking it because it won‚Äôt take in duals. See "},{"type":"link","url":"https://discourse.julialang.org/t/stackoverflowerror-when-fitting-data-to-a-von-mises-distribution/45406"},{"type":"text","text":"."}]}]}],"thread_ts":"1616848997.030900","parent_user_id":"UJ7DVTVQ8"},{"client_msg_id":"4c69d582-8552-41df-86cc-3834b36d1cbc","type":"message","text":"I haven't begun work on the proposal for Manifold statistics yet due to time constraints. And when I do, I might use MeasureTheory.jl for it instead of Distributions,jl. You can easily define a distribution on SO(3) or S^3 using the distribution types defined for Manifolds. But to use it in a PPL, more work is needed; the Manifolds types are a bit overkill here.\n\nIn particular, to define directional variables in Turing, one would need a map from the latent (unconstrained) space to parameter space. For the sphere in particular, every bijective map will have singularities that induce numerical instability; this is fine if you know your variables will never take on values near the singularity. An alternative is to use the trick Stan uses, where an unconstrained 4-vector is sampled and projected to the sphere via normalization, but this requires a non-bijective map (see <https://github.com/TuringLang/Bijectors.jl/issues/58> for ongoing discussion)\n\nThe quick-and-dirty way to do this now is something like this (untested):\n\n```Turing.@model function vMF_model(Œº, Œ∫, y)\n    x ~ MvNormal(4, 1)\n    q = x / norm(x)  # uniform base measure on ùïä¬≥\n    Turing.@addlogprob! logpdf_vMF(q, Œº, Œ∫) # vMF density wrt uniform measure on S¬≥\n    y ~ ... # some likelihood\nend\n\n# constant included for completeness\nlogpdf_vMF(x, Œº, Œ∫) = Distributions.vmflck(length(Œº), Œ∫) + Œ∫ * dot(Œº, x)```","user":"UHDQQ4GN6","ts":"1616876358.031700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yOqxN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I haven't begun work on the proposal for Manifold statistics yet due to time constraints. And when I do, I might use MeasureTheory.jl for it instead of Distributions,jl. You can easily define a distribution on SO(3) or S^3 using the distribution types defined for Manifolds. But to use it in a PPL, more work is needed; the Manifolds types are a bit overkill here.\n\nIn particular, to define directional variables in Turing, one would need a map from the latent (unconstrained) space to parameter space. For the sphere in particular, every bijective map will have singularities that induce numerical instability; this is fine if you know your variables will never take on values near the singularity. An alternative is to use the trick Stan uses, where an unconstrained 4-vector is sampled and projected to the sphere via normalization, but this requires a non-bijective map (see "},{"type":"link","url":"https://github.com/TuringLang/Bijectors.jl/issues/58"},{"type":"text","text":" for ongoing discussion)\n\nThe quick-and-dirty way to do this now is something like this (untested):\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Turing.@model function vMF_model(Œº, Œ∫, y)\n    x ~ MvNormal(4, 1)\n    q = x / norm(x)  # uniform base measure on ùïä¬≥\n    Turing.@addlogprob! logpdf_vMF(q, Œº, Œ∫) # vMF density wrt uniform measure on S¬≥\n    y ~ ... # some likelihood\nend\n\n# constant included for completeness\nlogpdf_vMF(x, Œº, Œ∫) = Distributions.vmflck(length(Œº), Œ∫) + Œ∫ * dot(Œº, x)"}]}]}],"thread_ts":"1616848997.030900","parent_user_id":"UJ7DVTVQ8"},{"client_msg_id":"d5d35f51-4af1-4ce1-9ca3-01e4ae89b926","type":"message","text":"Note that for a unit quaternion, where antipodal points correspond to the same rotation, you probably want an axial distribution, not a directional one. Also, because the posterior will be bimodal wrt the quaternion, expect diagnostics like rhat to fail. You can post-process the sampled quaternions to standardize them using Manifolds' projective space implementation (see <https://juliamanifolds.github.io/Manifolds.jl/stable/manifolds/projectivespace.html>), in which case the diagnostics should then work. Let me know if interested and I can give more details.","user":"UHDQQ4GN6","ts":"1616876990.031900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ge2fT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Note that for a unit quaternion, where antipodal points correspond to the same rotation, you probably want an axial distribution, not a directional one. Also, because the posterior will be bimodal wrt the quaternion, expect diagnostics like rhat to fail. You can post-process the sampled quaternions to standardize them using Manifolds' projective space implementation (see "},{"type":"link","url":"https://juliamanifolds.github.io/Manifolds.jl/stable/manifolds/projectivespace.html"},{"type":"text","text":"), in which case the diagnostics should then work. Let me know if interested and I can give more details."}]}]}],"thread_ts":"1616848997.030900","parent_user_id":"UJ7DVTVQ8"},{"client_msg_id":"322229b7-a7cd-49c0-9e9c-15fefb85b97c","type":"message","text":"Thanks for your comments! I managed to use `VonMisesFischer` with the `MH` sampler in Turing, but I couldn't get the sampler to work very well even with fake data coming from the model so I must be doing something wrong. If I do end up with a bimodal posterior, I guess I could convert all quaternions to, e.g., an axis-angle representation and make sure all axes have positive inner products with each other, is this similar in spirit to the standardization you suggested <@UHDQQ4GN6>?","user":"UJ7DVTVQ8","ts":"1616996569.032100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nz3B","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for your comments! I managed to use "},{"type":"text","text":"VonMisesFischer","style":{"code":true}},{"type":"text","text":" with the "},{"type":"text","text":"MH","style":{"code":true}},{"type":"text","text":" sampler in Turing, but I couldn't get the sampler to work very well even with fake data coming from the model so I must be doing something wrong. If I do end up with a bimodal posterior, I guess I could convert all quaternions to, e.g., an axis-angle representation and make sure all axes have positive inner products with each other, is this similar in spirit to the standardization you suggested "},{"type":"user","user_id":"UHDQQ4GN6"},{"type":"text","text":"?"}]}]}],"thread_ts":"1616848997.030900","parent_user_id":"UJ7DVTVQ8"},{"client_msg_id":"4498300a-8d35-42dc-95f7-2dc21ee139fc","type":"message","text":"I'd be happy to take a look at your MH model if you like. I'm curious how you got it to work at all.","user":"UHDQQ4GN6","ts":"1616999883.032300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Vr2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'd be happy to take a look at your MH model if you like. I'm curious how you got it to work at all."}]}]}],"thread_ts":"1616848997.030900","parent_user_id":"UJ7DVTVQ8"},{"client_msg_id":"a197e68f-e9a4-4952-a807-b92d6fc7439b","type":"message","text":"I think it would be similar in spirit, yes. And it depends how you intend to use the quaternions downstream. My suggestion would have been to find the intrinsic mean quaternion using Manifolds and then standardize by projecting all quaternions to that same hemisphere, which would have the benefit of creating a distribution of quaternions that is unimodal in the embedding. Which is easy and fast to do with Manifolds, but perhaps the dependency is not worth it for you. Here's an example anyways.\n\n``````julia\nusing Manifolds, Distributions\n\n# generate random bimodal sample of quaternions\nŒº = [0.0, 0.0, 0.0, 1.0]\nŒ∫ = 10.0\nd = VonMisesFisher(Œº, Œ∫)\nxs_chain1 = [rand(d) for _ in 1:1000]\nxs_chain2 = [-rand(d) for _ in 1:1000]\n\n# compute intrinsic mean quaternion\nM = ProjectiveSpace(3)\nxs_combined = [xs_chain1; xs_chain2]\nm = mean(M, xs_combined) # intrinsic mean\n\n# standardize to hemisphere centered at mean\nxs_stand = [x * sign(dot(m, x)) for x in xs_combined]\n``````\n","user":"UHDQQ4GN6","ts":"1616999911.032500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"z6rY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think it would be similar in spirit, yes. And it depends how you intend to use the quaternions downstream. My suggestion would have been to find the intrinsic mean quaternion using Manifolds and then standardize by projecting all quaternions to that same hemisphere, which would have the benefit of creating a distribution of quaternions that is unimodal in the embedding. Which is easy and fast to do with Manifolds, but perhaps the dependency is not worth it for you. Here's an example anyways.\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"```julia\nusing Manifolds, Distributions\n\n# generate random bimodal sample of quaternions\nŒº = [0.0, 0.0, 0.0, 1.0]\nŒ∫ = 10.0\nd = VonMisesFisher(Œº, Œ∫)\nxs_chain1 = [rand(d) for _ in 1:1000]\nxs_chain2 = [-rand(d) for _ in 1:1000]\n\n# compute intrinsic mean quaternion\nM = ProjectiveSpace(3)\nxs_combined = [xs_chain1; xs_chain2]\nm = mean(M, xs_combined) # intrinsic mean\n\n# standardize to hemisphere centered at mean\nxs_stand = [x * sign(dot(m, x)) for x in xs_combined]\n```"}]},{"type":"rich_text_section","elements":[]}]}],"thread_ts":"1616848997.030900","parent_user_id":"UJ7DVTVQ8"},{"client_msg_id":"2ae4f167-bdad-47a2-9a43-c14f26a817f2","type":"message","text":"Here's a MWE. The problem is to estimate two matrices in SE(3) in a robotics context, there is one transformation between a camera and the robot `T_RB_T`, and one transformation between a trackable frame the camera can track and the hand of the robot `T_TF_S`. The trackable frame consists of three LED:s the coordinates of which the camera measures.\n```# using Pkg; pkg\"add Robotlib#master\"\nusing Turing, Robotlib\nusing Robotlib: T2t, T2R, Rt2T\nBase.vec(q::Quaternion) = normalize([q.s, q.v1, q.v2, q.v2])\n\n# Gnerate fake data, the matrices T_RB_T0 and T_TF_S0 are the true values we want to estimate. \nT_RB_T0, T_TF_S0, POSESt, MEASURED, LEDs = Robotlib.simulateCalibration_AXYB(30, œÉt = 10, œÉT=1000, œÉy=0.2)\n\nfunction runturing(POSES, L, T_RB_T, T_TF_S)\n    e_RB_T0 = Quaternion(T_RB_T)\n    t_RB_T0 = T2t(T_RB_T)\n    e_TF_S0 = Quaternion(T_TF_S)\n    t_TF_S0 = T2t(T_TF_S)\n    Œ£e = 20 # Float64[5,5,5] .|&gt; deg2rad\n    Œ£t = Float64[20,20,20]\n    Turing.@model function model(POSES, L)\n        # e_RB_T ~ MvNormal(e_RB_T0, Œ£e)\n        # e_TF_S ~ MvNormal(e_TF_S0, Œ£e)\n        # e_RB_T ~ uniform_distribution(Sphere(3), vec(e_RB_T0))# , Œ£e)\n        # e_TF_S ~ uniform_distribution(Sphere(3), vec(e_TF_S0))# , Œ£e)\n        e_RB_T ~ VonMisesFisher(vec(e_RB_T0), Œ£e)\n        e_TF_S ~ VonMisesFisher(vec(e_TF_S0), Œ£e)\n        t_RB_T ~ MvNormal(t_RB_T0, Œ£t)\n        T_RB_T = Rt2T(rotationmatrix(Quaternion(e_RB_T...)), t_RB_T)\n\n        R_TF_S = rotationmatrix(Quaternion(e_TF_S...))# rpy2R(e_TF_S)\n        t_TF_S ~ arraydist([MvNormal(t_TF_S0, Œ£t) for l in eachindex(L)]) # TODO: there is room for improvement in the initialization of the different t_TF_S since how the TF is formed is known\n\n        œÉy ~ truncated(Normal(0, 0.2), 0.05, Inf)\n        # e = similar.(L)\n\n        for l in eachindex(L)\n            T_TF_Sj = Rt2T(R_TF_S, t_TF_S[:,l])\n            for i in eachindex(POSES)\n                yh = T2t(trinv(T_RB_T) * POSES[i] * T_TF_Sj)\n                for j = 1:3\n                    L[l][j,i] ~ Normal(yh[j], œÉy)\n                    # e[l][j,i] ~ Normal(0,1)\n                    # L[l][j,i] ~ Normal(yh[j] + œÉy*e[l][j,i], 0.001)\n                    \n                end\n            end\n        end\n    end\n\n    iterations = 2000\n    œµ = 0.05\n    œÑ = 10\n    # sampler = HMC(œµ, œÑ)\n    # sampler = NUTS()\n    # sampler = SMC()    # this segfaults!!!!!!!!!!!!\n    # sampler = PG(10)\n    sampler = MH()\n    # sampler = Gibbs(NUTS(:e), MH([:e_RB_T, :e_TF_S]))\n    chain = sample(model(POSES, L), sampler, iterations, progress=true)\n    # mle_estimate = optimize(model(POSES, L), MLE(), ParticleSwarm())\nend\n\n# We send in the true parameters in this MWE to simulate that we have initial estimates obtained from solving a linear-least squares problem\nchain = runturing(POSESt, LEDs, T_RB_T0, T_TF_S0)```","user":"UJ7DVTVQ8","ts":"1617000472.032700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YjK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Here's a MWE. The problem is to estimate two matrices in SE(3) in a robotics context, there is one transformation between a camera and the robot "},{"type":"text","text":"T_RB_T","style":{"code":true}},{"type":"text","text":", and one transformation between a trackable frame the camera can track and the hand of the robot "},{"type":"text","text":"T_TF_S","style":{"code":true}},{"type":"text","text":". The trackable frame consists of three LED:s the coordinates of which the camera measures.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"# using Pkg; pkg\"add Robotlib#master\"\nusing Turing, Robotlib\nusing Robotlib: T2t, T2R, Rt2T\nBase.vec(q::Quaternion) = normalize([q.s, q.v1, q.v2, q.v2])\n\n# Gnerate fake data, the matrices T_RB_T0 and T_TF_S0 are the true values we want to estimate. \nT_RB_T0, T_TF_S0, POSESt, MEASURED, LEDs = Robotlib.simulateCalibration_AXYB(30, œÉt = 10, œÉT=1000, œÉy=0.2)\n\nfunction runturing(POSES, L, T_RB_T, T_TF_S)\n    e_RB_T0 = Quaternion(T_RB_T)\n    t_RB_T0 = T2t(T_RB_T)\n    e_TF_S0 = Quaternion(T_TF_S)\n    t_TF_S0 = T2t(T_TF_S)\n    Œ£e = 20 # Float64[5,5,5] .|> deg2rad\n    Œ£t = Float64[20,20,20]\n    Turing.@model function model(POSES, L)\n        # e_RB_T ~ MvNormal(e_RB_T0, Œ£e)\n        # e_TF_S ~ MvNormal(e_TF_S0, Œ£e)\n        # e_RB_T ~ uniform_distribution(Sphere(3), vec(e_RB_T0))# , Œ£e)\n        # e_TF_S ~ uniform_distribution(Sphere(3), vec(e_TF_S0))# , Œ£e)\n        e_RB_T ~ VonMisesFisher(vec(e_RB_T0), Œ£e)\n        e_TF_S ~ VonMisesFisher(vec(e_TF_S0), Œ£e)\n        t_RB_T ~ MvNormal(t_RB_T0, Œ£t)\n        T_RB_T = Rt2T(rotationmatrix(Quaternion(e_RB_T...)), t_RB_T)\n\n        R_TF_S = rotationmatrix(Quaternion(e_TF_S...))# rpy2R(e_TF_S)\n        t_TF_S ~ arraydist([MvNormal(t_TF_S0, Œ£t) for l in eachindex(L)]) # TODO: there is room for improvement in the initialization of the different t_TF_S since how the TF is formed is known\n\n        œÉy ~ truncated(Normal(0, 0.2), 0.05, Inf)\n        # e = similar.(L)\n\n        for l in eachindex(L)\n            T_TF_Sj = Rt2T(R_TF_S, t_TF_S[:,l])\n            for i in eachindex(POSES)\n                yh = T2t(trinv(T_RB_T) * POSES[i] * T_TF_Sj)\n                for j = 1:3\n                    L[l][j,i] ~ Normal(yh[j], œÉy)\n                    # e[l][j,i] ~ Normal(0,1)\n                    # L[l][j,i] ~ Normal(yh[j] + œÉy*e[l][j,i], 0.001)\n                    \n                end\n            end\n        end\n    end\n\n    iterations = 2000\n    œµ = 0.05\n    œÑ = 10\n    # sampler = HMC(œµ, œÑ)\n    # sampler = NUTS()\n    # sampler = SMC()    # this segfaults!!!!!!!!!!!!\n    # sampler = PG(10)\n    sampler = MH()\n    # sampler = Gibbs(NUTS(:e), MH([:e_RB_T, :e_TF_S]))\n    chain = sample(model(POSES, L), sampler, iterations, progress=true)\n    # mle_estimate = optimize(model(POSES, L), MLE(), ParticleSwarm())\nend\n\n# We send in the true parameters in this MWE to simulate that we have initial estimates obtained from solving a linear-least squares problem\nchain = runturing(POSESt, LEDs, T_RB_T0, T_TF_S0)"}]}]}],"thread_ts":"1616848997.030900","parent_user_id":"UJ7DVTVQ8"},{"client_msg_id":"cb8a8238-bcc4-46ff-a162-73fbb66760fd","type":"message","text":"Cool! How did you work around the issue with Turing not having a bijector for `VonMisesFisher`?","user":"UHDQQ4GN6","ts":"1617001369.032900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FtAX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Cool! How did you work around the issue with Turing not having a bijector for "},{"type":"text","text":"VonMisesFisher","style":{"code":true}},{"type":"text","text":"?"}]}]}],"thread_ts":"1616848997.030900","parent_user_id":"UJ7DVTVQ8"},{"client_msg_id":"893f81f0-fbff-4542-aec5-abd992055603","type":"message","text":"It just doesn't complain about it when using `MH()` as sampler, I didn't do anything :stuck_out_tongue:","user":"UJ7DVTVQ8","ts":"1617001402.033100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HfRXA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It just doesn't complain about it when using "},{"type":"text","text":"MH()","style":{"code":true}},{"type":"text","text":" as sampler, I didn't do anything "},{"type":"emoji","name":"stuck_out_tongue"}]}]}],"thread_ts":"1616848997.030900","parent_user_id":"UJ7DVTVQ8"},{"client_msg_id":"f8eed0f9-66d7-4c88-bf5a-eed4bf741db7","type":"message","text":"I guess since MH is just drawing samples it works without a bijector","user":"UJ7DVTVQ8","ts":"1617001460.033300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zDSd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I guess since MH is just drawing samples it works without a bijector"}]}]}],"thread_ts":"1616848997.030900","parent_user_id":"UJ7DVTVQ8"},{"client_msg_id":"4806175f-2de7-4a45-92d7-308ee253dffd","type":"message","text":"How does Turing select the initial state of the sampler? I have a feeling part of why it's not working very well is that the initial state is way off. I used your trick with `Turing.@addlogprob! logpdf_vMF(e_RB_T, Œºr, Œ∫)` and tried the NUTS sampler, it starts waaaay off, nowhere near, e.g., the mean of each prior distribution.","user":"UJ7DVTVQ8","ts":"1617002363.033500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ge1Rg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How does Turing select the initial state of the sampler? I have a feeling part of why it's not working very well is that the initial state is way off. I used your trick with "},{"type":"text","text":"Turing.@addlogprob! logpdf_vMF(e_RB_T, Œºr, Œ∫)","style":{"code":true}},{"type":"text","text":" and tried the NUTS sampler, it starts waaaay off, nowhere near, e.g., the mean of each prior distribution."}]}]}],"thread_ts":"1616848997.030900","parent_user_id":"UJ7DVTVQ8"}]