[{"client_msg_id":"2550201f-9348-4bea-998c-0609040c2d8d","type":"message","text":"I have a bit of an audit sampling conundrum, which isn't really my area of expertise, but one of those problems where I think I should definitely be a Bayesian so maybe someone here has ideas or pointers. I have a population of (let's say) 100,000 invoices, and want to answer the standard audit question \"which fraction of these invoices is incorrect\", for which a sample has to be audited. Now the standard approach would (probably) be to just specify some prior Beta distribution with \"success\" denoting an error found, and then work out how many samples I need (assuming a certain chance of drawing a success on each draw) to get to a posterior that gives the required confidence interval.\n\nNow the twist is that I have a small (let's say 5%) subpopulation for which I know that the error rate is higher, and I wonder how (if at all) I should incorporate this knowledge:\n\n* Just ignore it, the higher expected error rate in the subpopulation changes my expected whole population error rate and so the info is already in the sample calculation above;\n* Treat them as two separate populations, which would likely lead to a higher overall sample size (as the whole population error rate is close to zero, removing the subpopulation will not really decrease the required sample size there, but a relatively large sample size will be required for the more volatile subpopulation)\n* Something in between (I hear a lot about \"pooling\" estimators in Bayesian analysis, but it's one of these things I've never really got my traditional-econometrics-trained head around\n\nI could imagine amending point (2) above in some way to basically calculate the sample size with reference to the whole population posterior, i.e. taking into account that the posterior for the subpopulation will be much less important when estimating the population posterior (which I guess would be a mixture of the two posteriors, weighted by the whole population share of both subpopulations?)\n\nIf anyone has any ideas on how to conceptually best approach this I'd be most grateful!","user":"U7JQGPGCQ","ts":"1616583154.028200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5T3/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a bit of an audit sampling conundrum, which isn't really my area of expertise, but one of those problems where I think I should definitely be a Bayesian so maybe someone here has ideas or pointers. I have a population of (let's say) 100,000 invoices, and want to answer the standard audit question \"which fraction of these invoices is incorrect\", for which a sample has to be audited. Now the standard approach would (probably) be to just specify some prior Beta distribution with \"success\" denoting an error found, and then work out how many samples I need (assuming a certain chance of drawing a success on each draw) to get to a posterior that gives the required confidence interval.\n\nNow the twist is that I have a small (let's say 5%) subpopulation for which I know that the error rate is higher, and I wonder how (if at all) I should incorporate this knowledge:\n\n* Just ignore it, the higher expected error rate in the subpopulation changes my expected whole population error rate and so the info is already in the sample calculation above;\n* Treat them as two separate populations, which would likely lead to a higher overall sample size (as the whole population error rate is close to zero, removing the subpopulation will not really decrease the required sample size there, but a relatively large sample size will be required for the more volatile subpopulation)\n* Something in between (I hear a lot about \"pooling\" estimators in Bayesian analysis, but it's one of these things I've never really got my traditional-econometrics-trained head around\n\nI could imagine amending point (2) above in some way to basically calculate the sample size with reference to the whole population posterior, i.e. taking into account that the posterior for the subpopulation will be much less important when estimating the population posterior (which I guess would be a mixture of the two posteriors, weighted by the whole population share of both subpopulations?)\n\nIf anyone has any ideas on how to conceptually best approach this I'd be most grateful!"}]}]}],"thread_ts":"1616583154.028200","reply_count":10,"reply_users_count":4,"latest_reply":"1616678000.030600","reply_users":["U6C937ENB","U7HAYKY9X","U7JQGPGCQ","U7LNECWEA"],"is_locked":false,"subscribed":false},{"client_msg_id":"859c46d0-de50-4ebd-a3ef-7e30e5175d38","type":"message","text":"If you have to subpopulations with different error rates, the model which assumes one error rate for everybody is not correct* and the uncertainty in the error rate of the subpopulation is real.","user":"U6C937ENB","ts":"1616583758.028300","team":"T68168MUP","edited":{"user":"U6C937ENB","ts":"1616583825.000000"},"blocks":[{"type":"rich_text","block_id":"uQf8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If you have to subpopulations with different error rates, the model which assumes one error rate for everybody is not correct* and the uncertainty in the error rate of the subpopulation is real."}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"11d95abe-0739-47b9-9f1a-35ce8a7a448f","type":"message","text":"* mildy, because it is difficult to tell a mixture of Bernoullis apart from homogeneous Bernoullis based on  data without observing the labels. Might be impossible","user":"U6C937ENB","ts":"1616583920.028600","team":"T68168MUP","edited":{"user":"U6C937ENB","ts":"1616583972.000000"},"blocks":[{"type":"rich_text","block_id":"UcV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"* mildy, because it is difficult to tell a mixture of Bernoullis apart from homogeneous Bernoullis based on  data without observing the labels. Might be impossible"}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"94979625-cf09-4690-86c3-8a555646c60d","type":"message","text":"I think now you can take option “just ignore it” if this is good news","user":"U6C937ENB","ts":"1616584262.028900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"X0F","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think now you can take option “just ignore it” if this is good news"}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"efc210f5-5f51-4fd2-84e8-0a07fd551b54","type":"message","text":"IIRC, you can do this with Markov chain monte carlo sampling, but it'll be overkill. Specify two models, run them through Turing, and evaluate their free parameters versus their observed likelihood, and have a look at AIC.","user":"U7HAYKY9X","ts":"1616586781.029100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VuZhW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"IIRC, you can do this with Markov chain monte carlo sampling, but it'll be overkill. Specify two models, run them through Turing, and evaluate their free parameters versus their observed likelihood, and have a look at AIC."}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"2472a9cc-6af2-4fcd-98e8-398afd77da8c","type":"message","text":"I'm not sure what's good news here :smile:","user":"U7JQGPGCQ","ts":"1616588234.029300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"c=KiR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm not sure what's good news here "},{"type":"emoji","name":"smile"}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"3e1e694e-7adb-4513-8bbc-ae51d5e39a90","type":"message","text":"Just genuinely interested","user":"U7JQGPGCQ","ts":"1616588239.029500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"J6S","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Just genuinely interested"}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"f23e426f-8968-4757-b645-ace5db983525","type":"message","text":"I guess from an effort perspective it's whatever gives the lowest sample size that's good news...","user":"U7JQGPGCQ","ts":"1616588264.029700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"BMY9n","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I guess from an effort perspective it's whatever gives the lowest sample size that's good news..."}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"1a7b3ef3-413f-42b4-9b88-0d10fa079672","type":"message","text":"&gt;  for which I know that the error rate is higher\nKnowledge (informative priors) reduces required sample size :slightly_smiling_face:","user":"U6C937ENB","ts":"1616588353.029900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dDh","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":" for which I know that the error rate is higher"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Knowledge (informative priors) reduces required sample size "},{"type":"emoji","name":"slightly_smiling_face"}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"d40f58d2-8d8c-4ebe-88d3-940d86f85c9f","type":"message","text":"(if you are asking for credible intervals and not confidence intervals which are worst case bounds assuming your prior knowledge is worthless)","user":"U6C937ENB","ts":"1616588415.030100","team":"T68168MUP","edited":{"user":"U6C937ENB","ts":"1616588444.000000"},"blocks":[{"type":"rich_text","block_id":"7BU+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(if you are asking for credible intervals and not confidence intervals which are worst case bounds assuming your prior knowledge is worthless)"}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"11667bab-f089-4a09-819b-59ccc280ee3d","type":"message","text":"One option, if you’re doing an estimate based on “total sampling”  (including both error rates) is to assume the ratio is fixed and equally select from both “biased” distributions.  This requires you can select the sub-population beforehand and removes the variance due to the variable amount of the subpop. If out of 100k, you wanted to select 100, assuming 5% identifiable sub-population, then you select 95 from the original and 5 from the subpop. This is a constrained realization, and will underestimate the variation if the ~5% — but it’ll also remove the difference in error rates as a primary driver of the audit fraction.","user":"U7LNECWEA","ts":"1616678000.030600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kIa8=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"One option, if you’re doing an estimate based on “total sampling”  (including both error rates) is to assume the ratio is fixed and equally select from both “biased” distributions.  This requires you can select the sub-population beforehand and removes the variance due to the variable amount of the subpop. If out of 100k, you wanted to select 100, assuming 5% identifiable sub-population, then you select 95 from the original and 5 from the subpop. This is a constrained realization, and will underestimate the variation if the ~5% — but it’ll also remove the difference in error rates as a primary driver of the audit fraction."}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"}]