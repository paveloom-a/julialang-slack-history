[{"client_msg_id":"2550201f-9348-4bea-998c-0609040c2d8d","type":"message","text":"I have a bit of an audit sampling conundrum, which isn't really my area of expertise, but one of those problems where I think I should definitely be a Bayesian so maybe someone here has ideas or pointers. I have a population of (let's say) 100,000 invoices, and want to answer the standard audit question \"which fraction of these invoices is incorrect\", for which a sample has to be audited. Now the standard approach would (probably) be to just specify some prior Beta distribution with \"success\" denoting an error found, and then work out how many samples I need (assuming a certain chance of drawing a success on each draw) to get to a posterior that gives the required confidence interval.\n\nNow the twist is that I have a small (let's say 5%) subpopulation for which I know that the error rate is higher, and I wonder how (if at all) I should incorporate this knowledge:\n\n* Just ignore it, the higher expected error rate in the subpopulation changes my expected whole population error rate and so the info is already in the sample calculation above;\n* Treat them as two separate populations, which would likely lead to a higher overall sample size (as the whole population error rate is close to zero, removing the subpopulation will not really decrease the required sample size there, but a relatively large sample size will be required for the more volatile subpopulation)\n* Something in between (I hear a lot about \"pooling\" estimators in Bayesian analysis, but it's one of these things I've never really got my traditional-econometrics-trained head around\n\nI could imagine amending point (2) above in some way to basically calculate the sample size with reference to the whole population posterior, i.e. taking into account that the posterior for the subpopulation will be much less important when estimating the population posterior (which I guess would be a mixture of the two posteriors, weighted by the whole population share of both subpopulations?)\n\nIf anyone has any ideas on how to conceptually best approach this I'd be most grateful!","user":"U7JQGPGCQ","ts":"1616583154.028200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5T3/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a bit of an audit sampling conundrum, which isn't really my area of expertise, but one of those problems where I think I should definitely be a Bayesian so maybe someone here has ideas or pointers. I have a population of (let's say) 100,000 invoices, and want to answer the standard audit question \"which fraction of these invoices is incorrect\", for which a sample has to be audited. Now the standard approach would (probably) be to just specify some prior Beta distribution with \"success\" denoting an error found, and then work out how many samples I need (assuming a certain chance of drawing a success on each draw) to get to a posterior that gives the required confidence interval.\n\nNow the twist is that I have a small (let's say 5%) subpopulation for which I know that the error rate is higher, and I wonder how (if at all) I should incorporate this knowledge:\n\n* Just ignore it, the higher expected error rate in the subpopulation changes my expected whole population error rate and so the info is already in the sample calculation above;\n* Treat them as two separate populations, which would likely lead to a higher overall sample size (as the whole population error rate is close to zero, removing the subpopulation will not really decrease the required sample size there, but a relatively large sample size will be required for the more volatile subpopulation)\n* Something in between (I hear a lot about \"pooling\" estimators in Bayesian analysis, but it's one of these things I've never really got my traditional-econometrics-trained head around\n\nI could imagine amending point (2) above in some way to basically calculate the sample size with reference to the whole population posterior, i.e. taking into account that the posterior for the subpopulation will be much less important when estimating the population posterior (which I guess would be a mixture of the two posteriors, weighted by the whole population share of both subpopulations?)\n\nIf anyone has any ideas on how to conceptually best approach this I'd be most grateful!"}]}]}],"thread_ts":"1616583154.028200","reply_count":14,"reply_users_count":4,"latest_reply":"1616769497.038800","reply_users":["U6C937ENB","U7HAYKY9X","U7JQGPGCQ","U7LNECWEA"],"is_locked":false,"subscribed":false},{"client_msg_id":"859c46d0-de50-4ebd-a3ef-7e30e5175d38","type":"message","text":"If you have to subpopulations with different error rates, the model which assumes one error rate for everybody is not correct* and the uncertainty in the error rate of the subpopulation is real.","user":"U6C937ENB","ts":"1616583758.028300","team":"T68168MUP","edited":{"user":"U6C937ENB","ts":"1616583825.000000"},"blocks":[{"type":"rich_text","block_id":"uQf8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If you have to subpopulations with different error rates, the model which assumes one error rate for everybody is not correct* and the uncertainty in the error rate of the subpopulation is real."}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"11d95abe-0739-47b9-9f1a-35ce8a7a448f","type":"message","text":"* mildy, because it is difficult to tell a mixture of Bernoullis apart from homogeneous Bernoullis based on  data without observing the labels. Might be impossible","user":"U6C937ENB","ts":"1616583920.028600","team":"T68168MUP","edited":{"user":"U6C937ENB","ts":"1616583972.000000"},"blocks":[{"type":"rich_text","block_id":"UcV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"* mildy, because it is difficult to tell a mixture of Bernoullis apart from homogeneous Bernoullis based on  data without observing the labels. Might be impossible"}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"94979625-cf09-4690-86c3-8a555646c60d","type":"message","text":"I think now you can take option “just ignore it” if this is good news","user":"U6C937ENB","ts":"1616584262.028900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"X0F","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think now you can take option “just ignore it” if this is good news"}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"efc210f5-5f51-4fd2-84e8-0a07fd551b54","type":"message","text":"IIRC, you can do this with Markov chain monte carlo sampling, but it'll be overkill. Specify two models, run them through Turing, and evaluate their free parameters versus their observed likelihood, and have a look at AIC.","user":"U7HAYKY9X","ts":"1616586781.029100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VuZhW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"IIRC, you can do this with Markov chain monte carlo sampling, but it'll be overkill. Specify two models, run them through Turing, and evaluate their free parameters versus their observed likelihood, and have a look at AIC."}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"2472a9cc-6af2-4fcd-98e8-398afd77da8c","type":"message","text":"I'm not sure what's good news here :smile:","user":"U7JQGPGCQ","ts":"1616588234.029300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"c=KiR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm not sure what's good news here "},{"type":"emoji","name":"smile"}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"3e1e694e-7adb-4513-8bbc-ae51d5e39a90","type":"message","text":"Just genuinely interested","user":"U7JQGPGCQ","ts":"1616588239.029500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"J6S","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Just genuinely interested"}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"f23e426f-8968-4757-b645-ace5db983525","type":"message","text":"I guess from an effort perspective it's whatever gives the lowest sample size that's good news...","user":"U7JQGPGCQ","ts":"1616588264.029700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"BMY9n","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I guess from an effort perspective it's whatever gives the lowest sample size that's good news..."}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"1a7b3ef3-413f-42b4-9b88-0d10fa079672","type":"message","text":"&gt;  for which I know that the error rate is higher\nKnowledge (informative priors) reduces required sample size :slightly_smiling_face:","user":"U6C937ENB","ts":"1616588353.029900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dDh","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":" for which I know that the error rate is higher"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Knowledge (informative priors) reduces required sample size "},{"type":"emoji","name":"slightly_smiling_face"}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"d40f58d2-8d8c-4ebe-88d3-940d86f85c9f","type":"message","text":"(if you are asking for credible intervals and not confidence intervals which are worst case bounds assuming your prior knowledge is worthless)","user":"U6C937ENB","ts":"1616588415.030100","team":"T68168MUP","edited":{"user":"U6C937ENB","ts":"1616588444.000000"},"blocks":[{"type":"rich_text","block_id":"7BU+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(if you are asking for credible intervals and not confidence intervals which are worst case bounds assuming your prior knowledge is worthless)"}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"11667bab-f089-4a09-819b-59ccc280ee3d","type":"message","text":"One option, if you’re doing an estimate based on “total sampling”  (including both error rates) is to assume the ratio is fixed and equally select from both “biased” distributions.  This requires you can select the sub-population beforehand and removes the variance due to the variable amount of the subpop. If out of 100k, you wanted to select 100, assuming 5% identifiable sub-population, then you select 95 from the original and 5 from the subpop. This is a constrained realization, and will underestimate the variation if the ~5% — but it’ll also remove the difference in error rates as a primary driver of the audit fraction.","user":"U7LNECWEA","ts":"1616678000.030600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kIa8=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"One option, if you’re doing an estimate based on “total sampling”  (including both error rates) is to assume the ratio is fixed and equally select from both “biased” distributions.  This requires you can select the sub-population beforehand and removes the variance due to the variable amount of the subpop. If out of 100k, you wanted to select 100, assuming 5% identifiable sub-population, then you select 95 from the original and 5 from the subpop. This is a constrained realization, and will underestimate the variation if the ~5% — but it’ll also remove the difference in error rates as a primary driver of the audit fraction."}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"83cdff40-ffe7-45ce-b9dd-21bddce99aef","type":"message","text":"Thanks for the hints so far, although I'll admit that I haven't been able to really come up with an actionable plan based on the suggestions so far :slightly_smiling_face:\n\nI have now had some further information that turns the question on it's head, so let me rephrase: Assume we have 100,000 invoices, out of which 90,000 are likely \"ok\" (defined as fewer than 1% of invoices having &gt;5% deviation between recorded amount and true value that would be determined by an audit), and 10,000 are \"problematic\", with potentially 20% being erroneous. Now I want to draw inferences about the 100,000 invoices based on a sample of 100 invoices (assume this number is fixed). The question is then: what is the optimal sampling strategy to produce the smallest credible sets or confidence interval (would the optimal strategy differ between them?), and would that optimal strategy differ depending on whether I want to estimate the error rate (i.e. a binary outcome) or the average deviation (i.e. make inference about the total value of all invoices)?","user":"U7JQGPGCQ","ts":"1616705058.030900","team":"T68168MUP","edited":{"user":"U7JQGPGCQ","ts":"1616705093.000000"},"blocks":[{"type":"rich_text","block_id":"+rUN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for the hints so far, although I'll admit that I haven't been able to really come up with an actionable plan based on the suggestions so far "},{"type":"emoji","name":"slightly_smiling_face"},{"type":"text","text":"\n\nI have now had some further information that turns the question on it's head, so let me rephrase: Assume we have 100,000 invoices, out of which 90,000 are likely \"ok\" (defined as fewer than 1% of invoices having >5% deviation between recorded amount and true value that would be determined by an audit), and 10,000 are \"problematic\", with potentially 20% being erroneous. Now I want to draw inferences about the 100,000 invoices based on a sample of 100 invoices (assume this number is fixed). The question is then: what is the optimal sampling strategy to produce the smallest credible sets or confidence interval (would the optimal strategy differ between them?), and would that optimal strategy differ depending on whether I want to estimate the error rate (i.e. a binary outcome) or the average deviation (i.e. make inference about the total value of all invoices)?"}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"662240af-a694-4372-9cc8-38f5df266da1","type":"message","text":"<@U7LNECWEA> to your point, I know exactly which subpopulation every item in the full population belongs to (if that's what you meant by \"assume the ratio is fixed\")","user":"U7JQGPGCQ","ts":"1616705185.031200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Lby","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U7LNECWEA"},{"type":"text","text":" to your point, I know exactly which subpopulation every item in the full population belongs to (if that's what you meant by \"assume the ratio is fixed\")"}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"28522e48-6ac6-468a-bb41-7a0ee0001bee","type":"message","text":"<@U7JQGPGCQ> Yeah, if you can identify the sub-population that likely has a larger error rate ahead of time, that is what I mean by knowing “ahead of time”. What I mean is that you can “fix” the ratio of subpopulations in your sampling if you can classify them from the full sample.  If the subpop was 5% with big error rate and 95% of the smaller. You can then estimate what you want (credible sets) for a 100 sampled audits by choosing 5 from the “larger error” and 95 from the regular subpop.  And then do what you want with the 100 sampled set like it’s one distribution.","user":"U7LNECWEA","ts":"1616769321.038600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sjrs","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U7JQGPGCQ"},{"type":"text","text":" Yeah, if you can identify the sub-population that likely has a larger error rate ahead of time, that is what I mean by knowing “ahead of time”. What I mean is that you can “fix” the ratio of subpopulations in your sampling if you can classify them from the full sample.  If the subpop was 5% with big error rate and 95% of the smaller. You can then estimate what you want (credible sets) for a 100 sampled audits by choosing 5 from the “larger error” and 95 from the regular subpop.  And then do what you want with the 100 sampled set like it’s one distribution."}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"},{"client_msg_id":"968cef88-5b3b-43df-b679-8f025e2034ce","type":"message","text":"I’m just using “100” for illustration. There are some formal points of concern here, but those aren’t going to just go away regardless of the sampled technique. (e.g. Since you have a smaller subpop of higher error, you are actually amplifying  the noise of this contribution since you’re only realizing 5% of your sample to estimate this).","user":"U7LNECWEA","ts":"1616769497.038800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"U/H","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’m just using “100” for illustration. There are some formal points of concern here, but those aren’t going to just go away regardless of the sampled technique. (e.g. Since you have a smaller subpop of higher error, you are actually amplifying  the noise of this contribution since you’re only realizing 5% of your sample to estimate this)."}]}]}],"thread_ts":"1616583154.028200","parent_user_id":"U7JQGPGCQ"}]