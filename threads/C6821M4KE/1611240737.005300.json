[{"client_msg_id":"0a2f33bd-ebe9-4ec4-943e-20ad5e9bb02e","type":"message","text":"Following up from <@U680T6770>’s question, here's a little example of what we can do with MeasureTheory.jl + Soss.jl:\n```julia&gt; using Soss\n\njulia&gt; m = @model x, α begin\n           σ ~ Normal(0,α)\n           β ~ Normal(0,1) \n           y ~ For(1:10) do j\n               Normal(x[j] * β, σ)\n           end\n           return y\n       end;\n\njulia&gt; x = randn(10);\n\njulia&gt; tr = simulate(m(x=x, α=1.0)).trace\n(y = [3.8746216264672046, 4.070841165904433, 0.5861945092982052, 0.04991832730055079, 2.6193000436341736, 1.8970453478810443, 1.1333786298392112, -0.9652291603688541, 1.2851465121616734, -2.43239474159502], β = -0.72005026769353, σ = -1.4926901301729563)\n\njulia&gt; y = tr.y;\n\njulia&gt; symlogdensity(m(x=x, α=1.0) | (;y))\n-10log(σ) + -0.5((β^2) + (σ^2)) + -0.5(52.174430346203465 + 58.127152240288424β + 24.529854231286606(β^2))*(σ^-2)\n\njulia&gt; symlogdensity(m(x=x, α=1.0) | (;y); noinline=(:α,))\n-1log(α) + -10log(σ) + -0.5(β^2) + -0.5(52.174430346203465 + 58.12715224028842β + 24.5298542312866(β^2))*(σ^-2) + -0.5(α^-2)*(σ^2)```\nThe symbolic part relies on SymbolicUtils.jl. There are still lots of cases it doesn't handle, but it's getting there. But given that, out codegen is pretty reliable:\n```julia&gt; codegen(m(x=x, α=1.0) | (;y)).f\nfunction = (_args, _data, _pars;) -&gt; begin\n    begin\n        β = (Main).getproperty(_pars, :β)\n        σ = (Main).getproperty(_pars, :σ)\n        y = (Main).getproperty(_data, :y)\n        α = (Main).getproperty(_args, :α)\n        x = (Main).getproperty(_args, :x)\n        var\"##1733\" = (log)(σ)\n        var\"##1734\" = (*)(-10, var\"##1733\")\n        var\"##1735\" = (^)(β, 2)\n        var\"##1736\" = (^)(σ, 2)\n        var\"##1737\" = (+)(var\"##1735\", var\"##1736\")\n        var\"##1738\" = (*)(-0.5, var\"##1737\")\n        var\"##1739\" = (*)(58.127152240288424, β)\n        var\"##1740\" = (*)(24.529854231286606, var\"##1735\")\n        var\"##1741\" = (+)(52.174430346203465, var\"##1739\", var\"##1740\")\n        var\"##1742\" = (^)(σ, -2)\n        var\"##1743\" = (*)(-0.5, var\"##1741\", var\"##1742\")\n        var\"##1744\" = (+)(var\"##1734\", var\"##1738\", var\"##1743\")\n    end\nend```\nSo we can use this as a very fast way to evaluate the log-density. I expect that for exponential family likelihood functions, this will blow Stan out of the water :slightly_smiling_face:","user":"U81PB6N77","ts":"1611240737.005300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=pALL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Following up from "},{"type":"user","user_id":"U680T6770"},{"type":"text","text":"’s question, here's a little example of what we can do with MeasureTheory.jl + Soss.jl:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> using Soss\n\njulia> m = @model x, α begin\n           σ ~ Normal(0,α)\n           β ~ Normal(0,1) \n           y ~ For(1:10) do j\n               Normal(x[j] * β, σ)\n           end\n           return y\n       end;\n\njulia> x = randn(10);\n\njulia> tr = simulate(m(x=x, α=1.0)).trace\n(y = [3.8746216264672046, 4.070841165904433, 0.5861945092982052, 0.04991832730055079, 2.6193000436341736, 1.8970453478810443, 1.1333786298392112, -0.9652291603688541, 1.2851465121616734, -2.43239474159502], β = -0.72005026769353, σ = -1.4926901301729563)\n\njulia> y = tr.y;\n\njulia> symlogdensity(m(x=x, α=1.0) | (;y))\n-10log(σ) + -0.5((β^2) + (σ^2)) + -0.5(52.174430346203465 + 58.127152240288424β + 24.529854231286606(β^2))*(σ^-2)\n\njulia> symlogdensity(m(x=x, α=1.0) | (;y); noinline=(:α,))\n-1log(α) + -10log(σ) + -0.5(β^2) + -0.5(52.174430346203465 + 58.12715224028842β + 24.5298542312866(β^2))*(σ^-2) + -0.5(α^-2)*(σ^2)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nThe symbolic part relies on SymbolicUtils.jl. There are still lots of cases it doesn't handle, but it's getting there. But given that, out codegen is pretty reliable:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> codegen(m(x=x, α=1.0) | (;y)).f\nfunction = (_args, _data, _pars;) -> begin\n    begin\n        β = (Main).getproperty(_pars, :β)\n        σ = (Main).getproperty(_pars, :σ)\n        y = (Main).getproperty(_data, :y)\n        α = (Main).getproperty(_args, :α)\n        x = (Main).getproperty(_args, :x)\n        var\"##1733\" = (log)(σ)\n        var\"##1734\" = (*)(-10, var\"##1733\")\n        var\"##1735\" = (^)(β, 2)\n        var\"##1736\" = (^)(σ, 2)\n        var\"##1737\" = (+)(var\"##1735\", var\"##1736\")\n        var\"##1738\" = (*)(-0.5, var\"##1737\")\n        var\"##1739\" = (*)(58.127152240288424, β)\n        var\"##1740\" = (*)(24.529854231286606, var\"##1735\")\n        var\"##1741\" = (+)(52.174430346203465, var\"##1739\", var\"##1740\")\n        var\"##1742\" = (^)(σ, -2)\n        var\"##1743\" = (*)(-0.5, var\"##1741\", var\"##1742\")\n        var\"##1744\" = (+)(var\"##1734\", var\"##1738\", var\"##1743\")\n    end\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nSo we can use this as a very fast way to evaluate the log-density. I expect that for exponential family likelihood functions, this will blow Stan out of the water "},{"type":"emoji","name":"slightly_smiling_face"}]}]}],"thread_ts":"1611240737.005300","reply_count":14,"reply_users_count":2,"latest_reply":"1611257949.010800","reply_users":["UN97XTLCV","U81PB6N77"],"subscribed":false,"reactions":[{"name":"+1","users":["U680T6770","U7THT3TM3","UE91V9CUC","UKLKS1WC8","U82LX4ACB","UB197FRCL","UJB9LTG5V","U85JBUGGP","U6LMK53QC"],"count":9},{"name":"fire","users":["UF6T1632L"],"count":1}]},{"client_msg_id":"fcb88fb9-050d-43c9-9467-55f9deda699b","type":"message","text":"when making these models how much control do you have over pre-processing certain values? For example, If my likelihood uses a dense covariance matrix which is not an RV and I specify the likelihood as (for example using Distributions) `y ~ Normal(Sig)` normally the cholesky decomposition of `Sig` gets calculated when the `Normal` distribution is constructed, but since this is slow it would be nice if the object (`Normal(Sig)`) only gets evaluated, say, the first time the model is called instead of every instance","user":"UN97XTLCV","ts":"1611255029.006100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Wfl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"when making these models how much control do you have over pre-processing certain values? For example, If my likelihood uses a dense covariance matrix which is not an RV and I specify the likelihood as (for example using Distributions) "},{"type":"text","text":"y ~ Normal(Sig)","style":{"code":true}},{"type":"text","text":" normally the cholesky decomposition of "},{"type":"text","text":"Sig","style":{"code":true}},{"type":"text","text":" gets calculated when the "},{"type":"text","text":"Normal","style":{"code":true}},{"type":"text","text":" distribution is constructed, but since this is slow it would be nice if the object ("},{"type":"text","text":"Normal(Sig)","style":{"code":true}},{"type":"text","text":") only gets evaluated, say, the first time the model is called instead of every instance"}]}]}],"thread_ts":"1611240737.005300","parent_user_id":"U81PB6N77"},{"client_msg_id":"60348b8e-52ac-4ca3-8342-dec4ef4c0af5","type":"message","text":"There are a few things that come into play here...\n\nFirst, in terms of the symbolic stuff, the rule I'm using is \"any subexpression that evaluates to `&lt;:Real` and has a statically known value should be evaluated at compile time\".\n\nSecond, we're setting up MeasureTheory.jl to easily allow multiple parameterizations. So for a multivariate normal, we'll have one in terms of the Cholesky factor.\n\nThird, Soss is based on codegen, so really the sky's the limit. If there's a rule for rewriting a model, we should be able to do it.","user":"U81PB6N77","ts":"1611255479.006300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eNag","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"There are a few things that come into play here...\n\nFirst, in terms of the symbolic stuff, the rule I'm using is \"any subexpression that evaluates to "},{"type":"text","text":"<:Real","style":{"code":true}},{"type":"text","text":" and has a statically known value should be evaluated at compile time\".\n\nSecond, we're setting up MeasureTheory.jl to easily allow multiple parameterizations. So for a multivariate normal, we'll have one in terms of the Cholesky factor.\n\nThird, Soss is based on codegen, so really the sky's the limit. If there's a rule for rewriting a model, we should be able to do it."}]}]}],"thread_ts":"1611240737.005300","parent_user_id":"U81PB6N77"},{"client_msg_id":"a7c30dd7-deda-499a-98a9-7f83b2a4559a","type":"message","text":"The first part is just to keep the code from getting too big. My first PPL back in 2010 took the approach of just inlining everything, then the C compiler did its thing. Really fast to run, but compilation was very slow. So we need to balance these","user":"U81PB6N77","ts":"1611255645.006500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"cm/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The first part is just to keep the code from getting too big. My first PPL back in 2010 took the approach of just inlining everything, then the C compiler did its thing. Really fast to run, but compilation was very slow. So we need to balance these"}]}]}],"thread_ts":"1611240737.005300","parent_user_id":"U81PB6N77"},{"client_msg_id":"2252468e-a592-4d3f-a1c7-29f96b8c24bb","type":"message","text":"great, that sounds really exciting! For reference I have plenty of examples of problems where I take parameters in, generate a model, then compute usually a Guassian likelihood. This model will often cache a lot of intermediate values, its the kind of thing that you want to evaluate ahead of time, and create essentially a function for transforming parameters to data. Having a simple but efficient interface to specify priors and evaluate the likelihood would take a lot of the “statistical burden” out of a package for the “transformer” part","user":"UN97XTLCV","ts":"1611256148.006700","team":"T68168MUP","edited":{"user":"UN97XTLCV","ts":"1611256209.000000"},"blocks":[{"type":"rich_text","block_id":"Bb9E","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"great, that sounds really exciting! For reference I have plenty of examples of problems where I take parameters in, generate a model, then compute usually a Guassian likelihood. This model will often cache a lot of intermediate values, its the kind of thing that you want to evaluate ahead of time, and create essentially a function for transforming parameters to data. Having a simple but efficient interface to specify priors and evaluate the likelihood would take a lot of the “statistical burden” out of a package for the “transformer” part"}]}]}],"thread_ts":"1611240737.005300","parent_user_id":"U81PB6N77","reactions":[{"name":"heart","users":["U81PB6N77"],"count":1}]},{"client_msg_id":"05bc03b1-bf43-4a4e-bc2d-a8d5487ae18b","type":"message","text":"Great! Can you point me to an example?","user":"U81PB6N77","ts":"1611256527.007200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FnyQt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Great! Can you point me to an example?"}]}]}],"thread_ts":"1611240737.005300","parent_user_id":"U81PB6N77"},{"client_msg_id":"1222d729-3fa8-4f46-ad89-9e298379b8ea","type":"message","text":"Here’s an example in Julia, it’s a little rough (still work in progress) but it shows kind of what I’m looking at. <https://github.com/JuliaHCI/Firefly.jl/blob/master/src/joint.jl> Another example is this python project, where the majority of the code is just facilitating the likelihood calculation <https://github.com/iancze/Starfish/blob/master/Starfish/models/spectrum_model.py> . This is what a lot of astro code looks like- a whole package dedicated to both the modelling _and_ the statistical inference (many of them come with built-in MCMC methods, too, hard coding the entire statistical stack, so to speak). If I can tell a colleague that they could write a package that is just as useful but they can cut half the code in their package and potentially get a faster likelihood, I think that would be really enticing","user":"UN97XTLCV","ts":"1611256944.007800","team":"T68168MUP","edited":{"user":"UN97XTLCV","ts":"1611256990.000000"},"blocks":[{"type":"rich_text","block_id":"hAI9=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Here’s an example in Julia, it’s a little rough (still work in progress) but it shows kind of what I’m looking at. "},{"type":"link","url":"https://github.com/JuliaHCI/Firefly.jl/blob/master/src/joint.jl"},{"type":"text","text":" Another example is this python project, where the majority of the code is just facilitating the likelihood calculation "},{"type":"link","url":"https://github.com/iancze/Starfish/blob/master/Starfish/models/spectrum_model.py"},{"type":"text","text":" . This is what a lot of astro code looks like- a whole package dedicated to both the modelling "},{"type":"text","text":"and","style":{"italic":true}},{"type":"text","text":" the statistical inference (many of them come with built-in MCMC methods, too, hard coding the entire statistical stack, so to speak). If I can tell a colleague that they could write a package that is just as useful but they can cut half the code in their package and potentially get a faster likelihood, I think that would be really enticing"}]}]}],"thread_ts":"1611240737.005300","parent_user_id":"U81PB6N77"},{"client_msg_id":"7be36b1f-9182-478f-917b-5cec9a5960f5","type":"message","text":"In the Firefly pcakage you can see the `JointDistribution` type I had tried to use to interface with Distributions.jl along with Turing, but I’ve somewhat abandoned that since there are autodiff errors in Turing I haven’t yet addressed","user":"UN97XTLCV","ts":"1611257090.008500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KwARx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In the Firefly pcakage you can see the "},{"type":"text","text":"JointDistribution","style":{"code":true}},{"type":"text","text":" type I had tried to use to interface with Distributions.jl along with Turing, but I’ve somewhat abandoned that since there are autodiff errors in Turing I haven’t yet addressed"}]}]}],"thread_ts":"1611240737.005300","parent_user_id":"U81PB6N77"},{"client_msg_id":"2b35352a-2201-4a1b-bbab-f2fe60667965","type":"message","text":"Ok, so everything in `JointModel` is constant at inference time?","user":"U81PB6N77","ts":"1611257170.008700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qbA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ok, so everything in "},{"type":"text","text":"JointModel","style":{"code":true}},{"type":"text","text":" is constant at inference time?"}]}]}],"thread_ts":"1611240737.005300","parent_user_id":"U81PB6N77"},{"client_msg_id":"dd53238a-9aa6-4119-ad79-71b53d218a68","type":"message","text":"Oh wait now I see `JointDistribution`","user":"U81PB6N77","ts":"1611257218.008900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rjX=z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oh wait now I see "},{"type":"text","text":"JointDistribution","style":{"code":true}}]}]}],"thread_ts":"1611240737.005300","parent_user_id":"U81PB6N77"},{"client_msg_id":"087c32e1-fd79-4df5-a505-ebedb710f0c2","type":"message","text":"yeah everything stored in the `JointModel` struct- the `loglikelihood` method shows what is calculated for each step","user":"UN97XTLCV","ts":"1611257257.009100","team":"T68168MUP","edited":{"user":"UN97XTLCV","ts":"1611257363.000000"},"blocks":[{"type":"rich_text","block_id":"oNY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yeah everything stored in the "},{"type":"text","text":"JointModel","style":{"code":true}},{"type":"text","text":" struct- the "},{"type":"text","text":"loglikelihood","style":{"code":true}},{"type":"text","text":" method shows what is calculated for each step"}]}]}],"thread_ts":"1611240737.005300","parent_user_id":"U81PB6N77"},{"client_msg_id":"70691250-1056-44e7-ab12-675c645b70e2","type":"message","text":"Ok, so this is a multivariate normal with a woodbury matrix for the covariance, right?","user":"U81PB6N77","ts":"1611257499.009900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ecrdl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ok, so this is a multivariate normal with a woodbury matrix for the covariance, right?"}]}]}],"thread_ts":"1611240737.005300","parent_user_id":"U81PB6N77"},{"client_msg_id":"08837eca-3ed2-4c2e-98f9-ee219421bc49","type":"message","text":"yep, exactly","user":"UN97XTLCV","ts":"1611257531.010400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5ycn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yep, exactly"}]}]}],"thread_ts":"1611240737.005300","parent_user_id":"U81PB6N77"},{"client_msg_id":"2775fc43-83d2-43e0-8c6c-6c3d2b37d422","type":"message","text":"Ok so yeah we can get to the sufficient stats instead, should be much quicker","user":"U81PB6N77","ts":"1611257566.010600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GZhb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ok so yeah we can get to the sufficient stats instead, should be much quicker"}]}]}],"thread_ts":"1611240737.005300","parent_user_id":"U81PB6N77"},{"client_msg_id":"7c0ee992-6e01-463d-886d-76b25b4d3294","type":"message","text":"And you shouldn't need to add a new `JointDistribution`, in MeasureTheory it would just be a multivariate normal","user":"U81PB6N77","ts":"1611257949.010800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VTW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And you shouldn't need to add a new "},{"type":"text","text":"JointDistribution","style":{"code":true}},{"type":"text","text":", in MeasureTheory it would just be a multivariate normal"}]}]}],"thread_ts":"1611240737.005300","parent_user_id":"U81PB6N77"}]