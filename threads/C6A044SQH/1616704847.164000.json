[{"client_msg_id":"5089b3be-287a-4724-a0df-4b18fd3c43fa","type":"message","text":"Looking around for an optimizer that doesn't use gradients. Any suggestions?","user":"U01G3BG7AFR","ts":"1616704847.164000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ZLL8x","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Looking around for an optimizer that doesn't use gradients. Any suggestions?"}]}]}],"thread_ts":"1616704847.164000","reply_count":4,"reply_users_count":3,"latest_reply":"1616714538.171800","reply_users":["U01FAHWCMFF","U0123HBMZ7Z","UCZ7VBGUD"],"is_locked":false,"subscribed":false},{"client_msg_id":"34c368f5-86b1-420d-a59d-5a432a595cd8","type":"message","text":"Not an optimization expert but i think the EM algorithem is a common one that doesnt use gradients.\n<https://en.m.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm|https://en.m.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm>","user":"U01FAHWCMFF","ts":"1616705825.164100","team":"T68168MUP","attachments":[{"image_url":"https://upload.wikimedia.org/wikipedia/commons/6/69/EM_Clustering_of_Old_Faithful_data.gif","image_width":360,"image_height":309,"image_bytes":116206,"title":"Expectation–maximization algorithm","title_link":"https://en.m.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm","from_url":"https://en.m.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm","author_name":"Wikipedia","author_link":"https://en.wikipedia.org/","text":"In statistics, an expectation–maximization (EM) algorithm is an iterative method to find (local) maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.","fallback":"wikipedia: Expectation–maximization algorithm","service_icon":"https://a.slack-edge.com/80588/img/unfurl_icons/wikipedia.png","id":1,"original_url":"https://en.m.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm"}],"blocks":[{"type":"rich_text","block_id":"MIZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not an optimization expert but i think the EM algorithem is a common one that doesnt use gradients.\n"},{"type":"link","url":"https://en.m.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm","text":"https://en.m.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm"}]}]}],"thread_ts":"1616704847.164000","parent_user_id":"U01G3BG7AFR"},{"client_msg_id":"d4aa5f3a-cdad-42e4-a515-c3c001fbd8b6","type":"message","text":"Many gradient-based algorithms can be done in a zero-order manner that stochastically approximates the gradient by evaluating random points in the neighborhood of the previous step","user":"U0123HBMZ7Z","ts":"1616706074.164400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"aht3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Many gradient-based algorithms can be done in a zero-order manner that stochastically approximates the gradient by evaluating random points in the neighborhood of the previous step"}]}]}],"thread_ts":"1616704847.164000","parent_user_id":"U01G3BG7AFR"},{"client_msg_id":"e642fe57-a8cd-4433-91f6-11ac23e8ca7f","type":"message","text":"Here's an article about that style of approach: <https://arxiv.org/abs/2006.06224>","user":"U0123HBMZ7Z","ts":"1616706175.164600","team":"T68168MUP","attachments":[{"service_name":"arXiv.org","title":"A Primer on Zeroth-Order Optimization in Signal Processing and...","title_link":"https://arxiv.org/abs/2006.06224","text":"Zeroth-order (ZO) optimization is a subset of gradient-free optimization that emerges in many signal processing and machine learning applications. It is used for solving optimization problems...","fallback":"arXiv.org: A Primer on Zeroth-Order Optimization in Signal Processing and...","from_url":"https://arxiv.org/abs/2006.06224","service_icon":"https://static.arxiv.org/static/browse/0.3.2.6/images/icons/favicon.ico","id":1,"original_url":"https://arxiv.org/abs/2006.06224"}],"blocks":[{"type":"rich_text","block_id":"7B67","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Here's an article about that style of approach: "},{"type":"link","url":"https://arxiv.org/abs/2006.06224"}]}]}],"thread_ts":"1616704847.164000","parent_user_id":"U01G3BG7AFR"},{"client_msg_id":"a6ce9a1f-765d-44cf-8912-be6f21c3c104","type":"message","text":"BlackBoxOptim has some methods and Optim has particle swarm","user":"UCZ7VBGUD","ts":"1616714538.171800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4w18p","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"BlackBoxOptim has some methods and Optim has particle swarm"}]}]}],"thread_ts":"1616704847.164000","parent_user_id":"U01G3BG7AFR"}]