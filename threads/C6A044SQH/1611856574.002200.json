[{"client_msg_id":"8ed90bdc-1ba9-4d0c-99fe-02dd6921235a","type":"message","text":"I want to write a function that tokenizes text by merging frequent pairs of tokens together. This means I have to search and replace a lot. My naive approach would be to store the corpus as a big string of tokens separated by some character and use Julia's replace function with some regex.\nNow I'm wondering whether this is an efficient way to go about things. Or would it make more sense to assign a number to each token and store tokens in arrays?\nThanks","user":"UBGC95BDJ","ts":"1611856574.002200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VrCIq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I want to write a function that tokenizes text by merging frequent pairs of tokens together. This means I have to search and replace a lot. My naive approach would be to store the corpus as a big string of tokens separated by some character and use Julia's replace function with some regex.\nNow I'm wondering whether this is an efficient way to go about things. Or would it make more sense to assign a number to each token and store tokens in arrays?\nThanks"}]}]}],"thread_ts":"1611856574.002200","reply_count":1,"reply_users_count":1,"latest_reply":"1611875141.028000","reply_users":["U68QW0PUZ"],"subscribed":false},{"client_msg_id":"4687d909-76cb-4d0b-ac85-7fc070855193","type":"message","text":"<@UB7JS9CHF> ?","user":"U68QW0PUZ","ts":"1611875141.028000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"v7o","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UB7JS9CHF"},{"type":"text","text":" ?"}]}]}],"thread_ts":"1611856574.002200","parent_user_id":"UBGC95BDJ"}]