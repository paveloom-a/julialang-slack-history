[{"client_msg_id":"8ed90bdc-1ba9-4d0c-99fe-02dd6921235a","type":"message","text":"I want to write a function that tokenizes text by merging frequent pairs of tokens together. This means I have to search and replace a lot. My naive approach would be to store the corpus as a big string of tokens separated by some character and use Julia's replace function with some regex.\nNow I'm wondering whether this is an efficient way to go about things. Or would it make more sense to assign a number to each token and store tokens in arrays?\nThanks","user":"UBGC95BDJ","ts":"1611856574.002200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VrCIq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I want to write a function that tokenizes text by merging frequent pairs of tokens together. This means I have to search and replace a lot. My naive approach would be to store the corpus as a big string of tokens separated by some character and use Julia's replace function with some regex.\nNow I'm wondering whether this is an efficient way to go about things. Or would it make more sense to assign a number to each token and store tokens in arrays?\nThanks"}]}]}],"thread_ts":"1611856574.002200","reply_count":9,"reply_users_count":3,"latest_reply":"1611919606.082500","reply_users":["U68QW0PUZ","UB7JS9CHF","UBGC95BDJ"],"subscribed":false},{"client_msg_id":"4687d909-76cb-4d0b-ac85-7fc070855193","type":"message","text":"<@UB7JS9CHF> ?","user":"U68QW0PUZ","ts":"1611875141.028000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"v7o","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UB7JS9CHF"},{"type":"text","text":" ?"}]}]}],"thread_ts":"1611856574.002200","parent_user_id":"UBGC95BDJ"},{"client_msg_id":"d1e2b072-866a-49ee-aa9c-3a3770f0a9e9","type":"message","text":"Can you describe the problem more in depth?  Also, maybe better to have this discussing elsewhere, not on Slack (where questions / answers may disappear into the dreaded “Slack hole” after just a few days.","user":"UB7JS9CHF","ts":"1611880865.042900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QcE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can you describe the problem more in depth?  Also, maybe better to have this discussing elsewhere, not on Slack (where questions / answers may disappear into the dreaded “Slack hole” after just a few days."}]}]}],"thread_ts":"1611856574.002200","parent_user_id":"UBGC95BDJ"},{"client_msg_id":"439ebe08-a438-4f72-8871-2464cd991d61","type":"message","text":"I definitely would not be using anything that operates via Regex and replace, if you are concerned about performance.","user":"UB7JS9CHF","ts":"1611880913.043100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xl3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I definitely would not be using anything that operates via Regex and replace, if you are concerned about performance."}]}]}],"thread_ts":"1611856574.002200","parent_user_id":"UBGC95BDJ","reactions":[{"name":"+1::skin-tone-3","users":["U68QW0PUZ"],"count":1}]},{"client_msg_id":"5637c4a8-ad41-49b5-a9ee-e0e1c87fe51d","type":"message","text":"One, if I understand what you are trying to do, you have some large corpus (all as one large string?) that you wish to split into tokens, and then find which pairs are more frequent?","user":"UB7JS9CHF","ts":"1611880997.043600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"STg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"One, if I understand what you are trying to do, you have some large corpus (all as one large string?) that you wish to split into tokens, and then find which pairs are more frequent?"}]}]}],"thread_ts":"1611856574.002200","parent_user_id":"UBGC95BDJ"},{"client_msg_id":"0ba21e7a-4a22-4fb5-a7b4-f1e6913fbb45","type":"message","text":"you can convert a string to a symbol `sym = Symbol(str)` ie `Symbol(\"this\") == :this` and vice versa `str = String(sym)`. The same symbols have the same `objectid(sym)`  eg `objectid(:this) == 0x512f325362df6056` ; that  is not true of strings.    Working with symbols makes for more performance, especially where there are many \"text words\" involved.","user":"U68QW0PUZ","ts":"1611881324.043800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"crzCS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you can convert a string to a symbol "},{"type":"text","text":"sym = Symbol(str)","style":{"code":true}},{"type":"text","text":" ie "},{"type":"text","text":"Symbol(\"this\") == :this","style":{"code":true}},{"type":"text","text":" and vice versa `str = String(sym)`. The same symbols have the same "},{"type":"text","text":"objectid(sym)","style":{"code":true}},{"type":"text","text":"  eg "},{"type":"text","text":"objectid(:this) == 0x512f325362df6056","style":{"code":true}},{"type":"text","text":" ; that  is not true of strings.    Working with symbols makes for more performance, especially where there are many \"text words\" involved."}]}]}],"thread_ts":"1611856574.002200","parent_user_id":"UBGC95BDJ"},{"client_msg_id":"4ce57735-844d-47f9-a50a-2072be7184a4","type":"message","text":"Or do you already have a list of the tokens you are looking for?  It seems like you’d want some sort of fast dictionary, to go from the text of the tokens to a small number.   Another pass could make a dictionary with pairs of the tokens, and the value of each entry could be the frequency.  What you do next depends on whether you wish to merge tokens that have over a certain # of pairs, or the top N.  For the latter, you’d want to make a vector, sorted by the frequency (but you could just have N entries, scan the dictionary, keep track of the lowest one)","user":"UB7JS9CHF","ts":"1611881333.044000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"knfok","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Or do you already have a list of the tokens you are looking for?  It seems like you’d want some sort of fast dictionary, to go from the text of the tokens to a small number.   Another pass could make a dictionary with pairs of the tokens, and the value of each entry could be the frequency.  What you do next depends on whether you wish to merge tokens that have over a certain # of pairs, or the top N.  For the latter, you’d want to make a vector, sorted by the frequency (but you could just have N entries, scan the dictionary, keep track of the lowest one)"}]}]}],"thread_ts":"1611856574.002200","parent_user_id":"UBGC95BDJ","reactions":[{"name":"heart","users":["UBGC95BDJ"],"count":1}]},{"client_msg_id":"977ad6fa-a421-4687-be75-531767c48d46","type":"message","text":"I’d be careful about using Symbols.  You cannot garbage collect them.","user":"UB7JS9CHF","ts":"1611881356.044200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"T7Od","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’d be careful about using Symbols.  You cannot garbage collect them."}]}]}],"thread_ts":"1611856574.002200","parent_user_id":"UBGC95BDJ"},{"client_msg_id":"4bed5339-c49b-484f-ac8f-aab4cba3b8d2","type":"message","text":"news to me -- good to know","user":"U68QW0PUZ","ts":"1611881417.044400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"D/0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"news to me -- good to know"}]}]}],"thread_ts":"1611856574.002200","parent_user_id":"UBGC95BDJ"},{"client_msg_id":"f3437ce4-1e00-4e41-b8ea-f88191dabacb","type":"message","text":"Thanks a lot for the responses! I've created a discourse post <https://discourse.julialang.org/t/writing-a-fast-nlp-tokenizer-in-julia/54176> where I explain my question in a bit more detail.","user":"UBGC95BDJ","ts":"1611919606.082500","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"Writing a fast nlp tokenizer in Julia","title_link":"https://discourse.julialang.org/t/writing-a-fast-nlp-tokenizer-in-julia/54176","text":"Hello everyone, As a learning exercise, I’m interested in writing a byte-pair-encoding (BPE) tokenizer in Julia. The meat of this tokenizer would be to find the most occuring pairs of tokens (initially these are just the individual characters) in a big text file and merge them into a single token. I’ve looked at SentencePiece and I’d like the tokenizer to operate on raw text where the spaces are replaced by _ and treated as a regular character of the stream, like they do. Now I’m wondering h...","fallback":"JuliaLang: Writing a fast nlp tokenizer in Julia","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","ts":1611919421,"from_url":"https://discourse.julialang.org/t/writing-a-fast-nlp-tokenizer-in-julia/54176","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/writing-a-fast-nlp-tokenizer-in-julia/54176"}],"blocks":[{"type":"rich_text","block_id":"S0Oan","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks a lot for the responses! I've created a discourse post "},{"type":"link","url":"https://discourse.julialang.org/t/writing-a-fast-nlp-tokenizer-in-julia/54176"},{"type":"text","text":" where I explain my question in a bit more detail."}]}]}],"thread_ts":"1611856574.002200","parent_user_id":"UBGC95BDJ"}]