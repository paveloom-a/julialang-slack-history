[{"client_msg_id":"0c179a62-66a1-47da-b463-d6857f038179","type":"message","text":"Hello, I am really new with Julia and I wanted to get your advice on backtesting here. I am currently working only with python and is slowly moving part of more computationally intensive work to Julia.\n\n*Problem:*\n```Let's say I have a dataset from a csv file of 40million rows of timestamps and share price by ticks (the interval could be by seconds, or even up to minutes during less liquid hours). I would like to backtest with a sliding window of fixed 1 hour for instance. \nShould I be slicing each window from the 40 million dataset, or is there a more efficient way of doing so?```\nThat was one way I used in Python but it was really slow. Other ideas I could thought of was splitting the dataset into smaller set of files, but that would require writing another function to communicate between the files.\nWithout trying, I also wouldn’t be able to know the optimal size to split into for improvement in speed. I wanted to know if you have similar experience or advice that could help me?\n\nThanks!","user":"U01QAH035AB","ts":"1616410932.369200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Cwd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello, I am really new with Julia and I wanted to get your advice on backtesting here. I am currently working only with python and is slowly moving part of more computationally intensive work to Julia.\n\n"},{"type":"text","text":"Problem:","style":{"bold":true}},{"type":"text","text":"\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Let's say I have a dataset from a csv file of 40million rows of timestamps and share price by ticks (the interval could be by seconds, or even up to minutes during less liquid hours). I would like to backtest with a sliding window of fixed 1 hour for instance. \nShould I be slicing each window from the 40 million dataset, or is there a more efficient way of doing so?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nThat was one way I used in Python but it was really slow. Other ideas I could thought of was splitting the dataset into smaller set of files, but that would require writing another function to communicate between the files.\nWithout trying, I also wouldn’t be able to know the optimal size to split into for improvement in speed. I wanted to know if you have similar experience or advice that could help me?\n\nThanks!"}]}]}],"thread_ts":"1616410932.369200","reply_count":1,"reply_users_count":1,"latest_reply":"1616411322.370000","reply_users":["U7HAYKY9X"],"subscribed":false},{"client_msg_id":"7f88d517-5907-477b-af4b-0a18b7ce16dd","type":"message","text":"This sounds like a usecase for `view` . You should probably load your data into large `Vector` s with timepoints and share prices. Then a window of 1 hour is simply a slice of that vector. By using `view` for the slice, you don't copy any data, and it will be slow if you use julia v1.5 or newer","user":"U7HAYKY9X","ts":"1616411322.370000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3tYkw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This sounds like a usecase for "},{"type":"text","text":"view","style":{"code":true}},{"type":"text","text":" . You should probably load your data into large "},{"type":"text","text":"Vector","style":{"code":true}},{"type":"text","text":" s with timepoints and share prices. Then a window of 1 hour is simply a slice of that vector. By using "},{"type":"text","text":"view","style":{"code":true}},{"type":"text","text":" for the slice, you don't copy any data, and it will be slow if you use julia v1.5 or newer"}]}]}],"thread_ts":"1616410932.369200","parent_user_id":"U01QAH035AB"}]