[{"client_msg_id":"0c179a62-66a1-47da-b463-d6857f038179","type":"message","text":"Hello, I am really new with Julia and I wanted to get your advice on backtesting here. I am currently working only with python and is slowly moving part of more computationally intensive work to Julia.\n\n*Problem:*\n```Let's say I have a dataset from a csv file of 40million rows of timestamps and share price by ticks (the interval could be by seconds, or even up to minutes during less liquid hours). I would like to backtest with a sliding window of fixed 1 hour for instance. \nShould I be slicing each window from the 40 million dataset, or is there a more efficient way of doing so?```\nThat was one way I used in Python but it was really slow. Other ideas I could thought of was splitting the dataset into smaller set of files, but that would require writing another function to communicate between the files.\nWithout trying, I also wouldn’t be able to know the optimal size to split into for improvement in speed. I wanted to know if you have similar experience or advice that could help me?\n\nThanks!","user":"U01QAH035AB","ts":"1616410932.369200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Cwd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello, I am really new with Julia and I wanted to get your advice on backtesting here. I am currently working only with python and is slowly moving part of more computationally intensive work to Julia.\n\n"},{"type":"text","text":"Problem:","style":{"bold":true}},{"type":"text","text":"\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Let's say I have a dataset from a csv file of 40million rows of timestamps and share price by ticks (the interval could be by seconds, or even up to minutes during less liquid hours). I would like to backtest with a sliding window of fixed 1 hour for instance. \nShould I be slicing each window from the 40 million dataset, or is there a more efficient way of doing so?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nThat was one way I used in Python but it was really slow. Other ideas I could thought of was splitting the dataset into smaller set of files, but that would require writing another function to communicate between the files.\nWithout trying, I also wouldn’t be able to know the optimal size to split into for improvement in speed. I wanted to know if you have similar experience or advice that could help me?\n\nThanks!"}]}]}],"thread_ts":"1616410932.369200","reply_count":3,"reply_users_count":2,"latest_reply":"1616412480.372000","reply_users":["U7HAYKY9X","U01QAH035AB"],"is_locked":false,"subscribed":false},{"client_msg_id":"7f88d517-5907-477b-af4b-0a18b7ce16dd","type":"message","text":"This sounds like a usecase for `view` . You should probably load your data into large `Vector` s with timepoints and share prices. Then a window of 1 hour is simply a slice of that vector. By using `view` for the slice, you don't copy any data, and it will be slow if you use julia v1.5 or newer","user":"U7HAYKY9X","ts":"1616411322.370000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3tYkw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This sounds like a usecase for "},{"type":"text","text":"view","style":{"code":true}},{"type":"text","text":" . You should probably load your data into large "},{"type":"text","text":"Vector","style":{"code":true}},{"type":"text","text":" s with timepoints and share prices. Then a window of 1 hour is simply a slice of that vector. By using "},{"type":"text","text":"view","style":{"code":true}},{"type":"text","text":" for the slice, you don't copy any data, and it will be slow if you use julia v1.5 or newer"}]}]}],"thread_ts":"1616410932.369200","parent_user_id":"U01QAH035AB"},{"client_msg_id":"43272c3a-54ac-4ef2-abba-e264fa076b60","type":"message","text":"yup, initial thought is to load them with CSV pkg and then slice from there. May I know what's the reason why `view` will be slower with v1.5 onwards?","user":"U01QAH035AB","ts":"1616412292.371800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"F2oL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yup, initial thought is to load them with CSV pkg and then slice from there. May I know what's the reason why "},{"type":"text","text":"view","style":{"code":true}},{"type":"text","text":" will be slower with v1.5 onwards?"}]}]}],"thread_ts":"1616410932.369200","parent_user_id":"U01QAH035AB"},{"client_msg_id":"e61f087b-c2f7-4484-8f1b-43af0b0e9d7e","type":"message","text":"Ah sorry, I meant it's FAST in v1.5 or newer. There were some optimizations for views (more specifically, immutable structs can now be stack allocated even if it contains pointers)","user":"U7HAYKY9X","ts":"1616412480.372000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mumY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah sorry, I meant it's FAST in v1.5 or newer. There were some optimizations for views (more specifically, immutable structs can now be stack allocated even if it contains pointers)"}]}]}],"thread_ts":"1616410932.369200","parent_user_id":"U01QAH035AB","reactions":[{"name":"bow","users":["U01QAH035AB"],"count":1}]}]