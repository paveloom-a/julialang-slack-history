[{"client_msg_id":"d8087d16-4624-487a-96ed-61a47dddc312","type":"message","text":"Theoretically, is it possible to fill Dict/Set in such a way that the worst-case scenario for time complexity is filled (all entries are inserted into the same bucket)? Let's say I want to make a set with 10 carefully chosen integers such a way that checking the existence of some value v is done in O(n) time?","user":"UAGBT2X1A","ts":"1615292750.013900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"v=iv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Theoretically, is it possible to fill Dict/Set in such a way that the worst-case scenario for time complexity is filled (all entries are inserted into the same bucket)? Let's say I want to make a set with 10 carefully chosen integers such a way that checking the existence of some value v is done in O(n) time?"}]}]}],"thread_ts":"1615292750.013900","reply_count":11,"reply_users_count":5,"latest_reply":"1615314279.022500","reply_users":["U7HAYKY9X","U6A936746","UAGBT2X1A","UDHCV0BHD","U01M655G9AR"],"subscribed":false},{"client_msg_id":"03ad13e4-dc3d-4312-9b91-5b399372541a","type":"message","text":"That's hard to do, I think. It's easy to find a sequence of intergers with O(n) insert time, but once you've put in a few of those, the dict will re-hash, and then there are no more slots with O(n) insert time.\nIf you have a very large dict, it can take long before it re-hashes, though","user":"U7HAYKY9X","ts":"1615293807.014300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GZKZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That's hard to do, I think. It's easy to find a sequence of intergers with O(n) insert time, but once you've put in a few of those, the dict will re-hash, and then there are no more slots with O(n) insert time.\nIf you have a very large dict, it can take long before it re-hashes, though"}]}]}],"thread_ts":"1615292750.013900","parent_user_id":"UAGBT2X1A"},{"client_msg_id":"0c344060-d53e-48c6-a319-1906396f299d","type":"message","text":"If you are allowed to use your own type it is easy.\n```Base.hash(::MyType, seed::UInt) = seed```\n","user":"U6A936746","ts":"1615298649.014700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"e5t","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If you are allowed to use your own type it is easy.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Base.hash(::MyType, seed::UInt) = seed"}]},{"type":"rich_text_section","elements":[]}]}],"thread_ts":"1615292750.013900","parent_user_id":"UAGBT2X1A","reactions":[{"name":"+1","users":["U7HAYKY9X"],"count":1}]},{"client_msg_id":"eb05a111-2f52-4e27-8661-97efeb1b05fd","type":"message","text":"For any type that has 10x more possible values than an `UInt` , e.g. `NTuple{10, UInt}`  it is certain to be possible to find 10 collisions that apply no matter the size of the Dict.\nand in general it is probably possible much earlier, because many `hash` functions are not perfect.\n*and nor should they be*, it is cheaper to have them be fast and then add a few `isequal` checks when collisions occur, than to make them slow but perfect.\n`OrderedCollections.LittleDict`  is in effect a dictionary with 1 bucket (i.e. that always collides) and it is faster for a lot of types than a `Dict` if you don’t have more than a few dozen elements, because `hash` is expensive","user":"U6A936746","ts":"1615299073.015100","team":"T68168MUP","edited":{"user":"U6A936746","ts":"1615299098.000000"},"blocks":[{"type":"rich_text","block_id":"KVvaa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"For any type that has 10x more possible values than an "},{"type":"text","text":"UInt","style":{"code":true}},{"type":"text","text":" , e.g. "},{"type":"text","text":"NTuple{10, UInt}","style":{"code":true}},{"type":"text","text":"  it is certain to be possible to find 10 collisions that apply no matter the size of the Dict.\nand in general it is probably possible much earlier, because many "},{"type":"text","text":"hash","style":{"code":true}},{"type":"text","text":" functions are not perfect.\n"},{"type":"text","text":"and nor should they be","style":{"bold":true}},{"type":"text","text":", it is cheaper to have them be fast and then add a few "},{"type":"text","text":"isequal","style":{"code":true}},{"type":"text","text":" checks when collisions occur, than to make them slow but perfect.\n"},{"type":"text","text":"OrderedCollections.LittleDict","style":{"code":true}},{"type":"text","text":"  is in effect a dictionary with 1 bucket (i.e. that always collides) and it is faster for a lot of types than a "},{"type":"text","text":"Dict","style":{"code":true}},{"type":"text","text":" if you don’t have more than a few dozen elements, because "},{"type":"text","text":"hash","style":{"code":true}},{"type":"text","text":" is expensive"}]}]}],"thread_ts":"1615292750.013900","parent_user_id":"UAGBT2X1A"},{"client_msg_id":"6a8bb082-2eb0-43f5-8459-c155504ed029","type":"message","text":"The reason why I'm wondering is that I was asked an algorithm with no worse complexity than O(n log n) of finding a number from a list that also has its double in that same list. So if there's e.g. list `[3, 22, 7, 5, 11, 5]`, the correct number is 11 because there's also 22. My initial solution is using a set:\n\n```function ratk5(T)\n    V = Set(T)\n    for t ∈ V\n        2*t ∈ V &amp;&amp; return t\n    end\n    return -1\nend```\nBut the initial comments was that checking if `2t in V` can be O(n) in the worst case, making this O(n^2) in the worst-case scenario. Yes, it's true that if all numbers are in one bucket due to some clever choice of numbers in T, it could be O(n^2) but I still think could this be possible if Set is implemented in the right way.","user":"UAGBT2X1A","ts":"1615308403.020100","team":"T68168MUP","edited":{"user":"UAGBT2X1A","ts":"1615308435.000000"},"blocks":[{"type":"rich_text","block_id":"BXh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The reason why I'm wondering is that I was asked an algorithm with no worse complexity than O(n log n) of finding a number from a list that also has its double in that same list. So if there's e.g. list "},{"type":"text","text":"[3, 22, 7, 5, 11, 5]","style":{"code":true}},{"type":"text","text":", the correct number is 11 because there's also 22. My initial solution is using a set:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function ratk5(T)\n    V = Set(T)\n    for t ∈ V\n        2*t ∈ V && return t\n    end\n    return -1\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"But the initial comments was that checking if "},{"type":"text","text":"2t in V","style":{"code":true}},{"type":"text","text":" can be O(n) in the worst case, making this O(n^2) in the worst-case scenario. Yes, it's true that if all numbers are in one bucket due to some clever choice of numbers in T, it could be O(n^2) but I still think could this be possible if Set is implemented in the right way."}]}]}],"thread_ts":"1615292750.013900","parent_user_id":"UAGBT2X1A"},{"client_msg_id":"982d5a83-a47c-4d01-9871-909cb54c4c0c","type":"message","text":"In general people often lie and say that big O for dictionaries is `O(1)`","user":"U6A936746","ts":"1615309063.020400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"h95xS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In general people often lie and say that big O for dictionaries is "},{"type":"text","text":"O(1)","style":{"code":true}}]}]}],"thread_ts":"1615292750.013900","parent_user_id":"UAGBT2X1A"},{"client_msg_id":"033c72e6-565b-45d2-bcf2-58de7ecc4460","type":"message","text":"Yes, but do they equally lie when telling that the worst case is O(n) if there's no theoretical proof that when properly implemented, it's impossible to make is O(n)?","user":"UAGBT2X1A","ts":"1615309137.020600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nL6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes, but do they equally lie when telling that the worst case is O(n) if there's no theoretical proof that when properly implemented, it's impossible to make is O(n)?"}]}]}],"thread_ts":"1615292750.013900","parent_user_id":"UAGBT2X1A"},{"client_msg_id":"fff3e62f-98b8-41de-adbf-9a34abb8dd67","type":"message","text":"If you know bounds on the range of values that can occur in the list. then from a theoretical perspective you can just have a bit set.\nWhich is O(1) check and set for certain.","user":"U6A936746","ts":"1615309211.020800","team":"T68168MUP","edited":{"user":"U6A936746","ts":"1615309223.000000"},"blocks":[{"type":"rich_text","block_id":"0Ow","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If you know bounds on the range of values that can occur in the list. then from a theoretical perspective you can just have a bit set.\nWhich is O(1) check and set for certain."}]}]}],"thread_ts":"1615292750.013900","parent_user_id":"UAGBT2X1A"},{"client_msg_id":"6ecafbbd-e794-41a0-adc5-d0eab8bda4bd","type":"message","text":"If you take a binary tree for `V` your code should do. See for example <https://juliacollections.github.io/DataStructures.jl/latest/red_black_tree/>","user":"UDHCV0BHD","ts":"1615312330.021700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mqi8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If you take a binary tree for "},{"type":"text","text":"V","style":{"code":true}},{"type":"text","text":" your code should do. See for example "},{"type":"link","url":"https://juliacollections.github.io/DataStructures.jl/latest/red_black_tree/"}]}]}],"thread_ts":"1615292750.013900","parent_user_id":"UAGBT2X1A"},{"client_msg_id":"3c45aa62-8b67-44bb-b706-db5c911c1bb0","type":"message","text":"Yes, binary trees are indeed the right solution (and there's one more based on merge sort). Theoretical judgment would be easier. But in reality, I'm quite confident that my solution is in real life faster as checking existence from a set is by average O(1). I'm talking about practice. I think my solution is the fastest one. Could we find a counterexample where it's actually slower?","user":"UAGBT2X1A","ts":"1615313518.022000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"T8s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes, binary trees are indeed the right solution (and there's one more based on merge sort). Theoretical judgment would be easier. But in reality, I'm quite confident that my solution is in real life faster as checking existence from a set is by average O(1). I'm talking about practice. I think my solution is the fastest one. Could we find a counterexample where it's actually slower?"}]}]}],"thread_ts":"1615292750.013900","parent_user_id":"UAGBT2X1A"},{"client_msg_id":"c8cc2de9-d83d-4d71-aa9c-cfc1d3eda0dc","type":"message","text":"you could just sort all elements and scan the list with two pointers. Could probably be written more elegantly, but you get the idea.:\n```function finddouble(x)\n   x = sort(x)\n   j = 1\n   n = length(x)\n   for i=1:n\n       while x[j] &lt; 2x[i] &amp;&amp; j &lt; n\n           j += 1\n       end\n       if x[j] == 2x[i]\n           return x[i],x[j]\n       end\n   end\nend```\n","user":"U01M655G9AR","ts":"1615314116.022300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"G57","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you could just sort all elements and scan the list with two pointers. Could probably be written more elegantly, but you get the idea.:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function finddouble(x)\n   x = sort(x)\n   j = 1\n   n = length(x)\n   for i=1:n\n       while x[j] < 2x[i] && j < n\n           j += 1\n       end\n       if x[j] == 2x[i]\n           return x[i],x[j]\n       end\n   end\nend"}]},{"type":"rich_text_section","elements":[]}]}],"thread_ts":"1615292750.013900","parent_user_id":"UAGBT2X1A"},{"client_msg_id":"d5eeb5ec-2e7d-4b9f-b93f-b8c4350647a6","type":"message","text":"Because of that sort, it's definitely O(n log n). What is left is to make experiments with bigger data.","user":"UAGBT2X1A","ts":"1615314279.022500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fkZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Because of that sort, it's definitely O(n log n). What is left is to make experiments with bigger data."}]}]}],"thread_ts":"1615292750.013900","parent_user_id":"UAGBT2X1A"}]