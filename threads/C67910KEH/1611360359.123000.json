[{"client_msg_id":"0a486351-8390-4e4b-9965-24ada081e6a6","type":"message","text":"How does pre-allocated memory work with `Distributed`? Should the pre-allocation be within the worker processes? If not, will there be a race condition?","user":"UT9SK8EF4","ts":"1611360359.123000","team":"T68168MUP","edited":{"user":"UT9SK8EF4","ts":"1611360377.000000"},"blocks":[{"type":"rich_text","block_id":"NY6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How does pre-allocated memory work with "},{"type":"text","text":"Distributed","style":{"code":true}},{"type":"text","text":"? Should the pre-allocation be within the worker processes? If not, will there be a race condition?"}]}]}],"thread_ts":"1611360359.123000","reply_count":9,"reply_users_count":3,"latest_reply":"1611383171.126100","reply_users":["UH8A351DJ","UT9SK8EF4","U7JQGPGCQ"],"subscribed":false},{"client_msg_id":"a8b03940-30d6-4e98-b77a-704f9ca602a9","type":"message","text":"you can pre-allocate a `SharedArray` but yeah most likely you would want to pre-allocate on each worker process for the sub task","user":"UH8A351DJ","ts":"1611360451.123200","team":"T68168MUP","edited":{"user":"UH8A351DJ","ts":"1611360477.000000"},"blocks":[{"type":"rich_text","block_id":"Lrd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you can pre-allocate a "},{"type":"text","text":"SharedArray","style":{"code":true}},{"type":"text","text":" but yeah most likely you would want to pre-allocate on each worker process for the sub task"}]}]}],"thread_ts":"1611360359.123000","parent_user_id":"UT9SK8EF4"},{"client_msg_id":"60b299db-9f94-4190-ba49-dc93dab31b77","type":"message","text":"I assume that if I go the `SharedArray` route I'm responsible for locking and unlocking to ensure that if I have multiple workers that need the Shared memory to be accessed in a sequence (like imagine foo1() and then foo2() on the same array) that one worker can't come in and overwrite with their foo1() while another is waiting to its foo2().?","user":"UT9SK8EF4","ts":"1611360541.123600","team":"T68168MUP","edited":{"user":"UT9SK8EF4","ts":"1611360575.000000"},"blocks":[{"type":"rich_text","block_id":"oat","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I assume that if I go the "},{"type":"text","text":"SharedArray","style":{"code":true}},{"type":"text","text":" route I'm responsible for locking and unlocking to ensure that if I have multiple workers that need the Shared memory to be accessed in a sequence (like imagine foo1() and then foo2() on the same array) that one worker can't come in and overwrite with their foo1() while another is waiting to its foo2().?"}]}]}],"thread_ts":"1611360359.123000","parent_user_id":"UT9SK8EF4"},{"client_msg_id":"6cabd3d8-b611-42c8-ad50-fb6aa539de5a","type":"message","text":"(Hopefully that made sense lol)","user":"UT9SK8EF4","ts":"1611360561.123800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JaXWb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(Hopefully that made sense lol)"}]}]}],"thread_ts":"1611360359.123000","parent_user_id":"UT9SK8EF4"},{"client_msg_id":"84f5980f-892d-4acb-bc42-733695485d3e","type":"message","text":"I think as long as you're accessing independent part of the array it should be fine, not 100% sure on this","user":"UH8A351DJ","ts":"1611360592.124100","team":"T68168MUP","edited":{"user":"UH8A351DJ","ts":"1611360610.000000"},"blocks":[{"type":"rich_text","block_id":"KymzI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think as long as you're accessing independent part of the array it should be fine, not 100% sure on this"}]}]}],"thread_ts":"1611360359.123000","parent_user_id":"UT9SK8EF4"},{"client_msg_id":"c62a2d18-f848-469b-9371-4bff7628b27f","type":"message","text":"Understood. I think I have enough space to do the pre-allocation per processor even if that doesn't lead to the best utilization","user":"UT9SK8EF4","ts":"1611360632.124500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bhdh4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Understood. I think I have enough space to do the pre-allocation per processor even if that doesn't lead to the best utilization"}]}]}],"thread_ts":"1611360359.123000","parent_user_id":"UT9SK8EF4"},{"client_msg_id":"40908b93-5da2-4686-9422-057128600835","type":"message","text":"Thanks!","user":"UT9SK8EF4","ts":"1611360641.124700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nM+l","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks!"}]}]}],"thread_ts":"1611360359.123000","parent_user_id":"UT9SK8EF4"},{"client_msg_id":"eb590593-0010-4510-a989-6b2161ae9eb2","type":"message","text":"if you're just on a local machine, why not multi-thread?","user":"UH8A351DJ","ts":"1611360663.124900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"AUnV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"if you're just on a local machine, why not multi-thread?"}]}]}],"thread_ts":"1611360359.123000","parent_user_id":"UT9SK8EF4"},{"client_msg_id":"c6502d9f-1598-4fad-abd8-2e9c379713aa","type":"message","text":"yeah. I could go that route --- just seeing what is the best time investment vs. performance payoff right now","user":"UT9SK8EF4","ts":"1611360732.125100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FMt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yeah. I could go that route --- just seeing what is the best time investment vs. performance payoff right now"}]}]}],"thread_ts":"1611360359.123000","parent_user_id":"UT9SK8EF4"},{"client_msg_id":"915f8852-1bf2-451f-8fcd-ea549341d767","type":"message","text":"Here's an example from my thesis a few years ago where I used Distributed and ShardeArrays (written before threads were a thing). I have two big multidimensional arrays, and compute one slice of them at a time. For that slice I use a Shared Array, from which I copy out into the large array after every (outer) iteration and then reuse it for the next step. This scaled quite well on my university cluster at the time. <https://github.com/nilshg/LearningModels/blob/master/NHL/NHL_6_Bellman.jl|https://github.com/nilshg/LearningModels/blob/master/NHL/NHL_6_Bellman.jl>","user":"U7JQGPGCQ","ts":"1611383171.126100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xw1qG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Here's an example from my thesis a few years ago where I used Distributed and ShardeArrays (written before threads were a thing). I have two big multidimensional arrays, and compute one slice of them at a time. For that slice I use a Shared Array, from which I copy out into the large array after every (outer) iteration and then reuse it for the next step. This scaled quite well on my university cluster at the time. "},{"type":"link","url":"https://github.com/nilshg/LearningModels/blob/master/NHL/NHL_6_Bellman.jl","text":"https://github.com/nilshg/LearningModels/blob/master/NHL/NHL_6_Bellman.jl"}]}]}],"thread_ts":"1611360359.123000","parent_user_id":"UT9SK8EF4"}]