[{"client_msg_id":"7159666d-cb7e-4d7c-b238-e7eeb0c53d87","type":"message","text":"Hey everybody, I'd like to announce that I've just released *<https://github.com/dzhang314/MultiFloats.jl|MultiFloats.jl>*<https://github.com/dzhang314/MultiFloats.jl| >*<https://github.com/dzhang314/MultiFloats.jl|v1.0>*, my library for fast extended-precision arithmetic. It provides the types Float64x2, Float64x3, ..., Float64x8 for doing math with approximately 100, 150, ..., 400 accurate bits. It currently supports arithmetic (`+-*/`), `sqrt`, `exp`, and `log`. I'm still working on trig functions, but as a temporary workaround, you can call `MultiFloats.use_bigfloat_transcendentals()` to patch in BigFloat trig/hyperbolic functions for use with Float64xN.\n\nUnder the hood, a Float64xN is just an `NTuple{N,Float64}` manipulated using error-free transformations. The transformations we need in order to go up to Float64x8 are too large to practically write by hand, so they are generated with metaprogramming at library load time. I've taken care to ensure that the generated code compiles down to a straight line of native, vectorizable Float64 operations, making *<https://github.com/dzhang314/MultiFloats.jl|MultiFloats.jl>* the fastest extended-precision library I know of in the 100-400 bit precision range.\n\nA huge kudos to the Julia designers for providing this unique high-performance JIT architecture. I don't think a library like *<https://github.com/dzhang314/MultiFloats.jl|MultiFloats.jl>* could exist in any other language!","user":"UPKUR1KHB","ts":"1609908459.404700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"G9qCU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey everybody, I'd like to announce that I've just released "},{"type":"link","url":"https://github.com/dzhang314/MultiFloats.jl","text":"MultiFloats.jl","style":{"bold":true}},{"type":"link","url":"https://github.com/dzhang314/MultiFloats.jl","text":" "},{"type":"link","url":"https://github.com/dzhang314/MultiFloats.jl","text":"v1.0","style":{"bold":true}},{"type":"text","text":", my library for fast extended-precision arithmetic. It provides the types Float64x2, Float64x3, ..., Float64x8 for doing math with approximately 100, 150, ..., 400 accurate bits. It currently supports arithmetic ("},{"type":"text","text":"+-*/","style":{"code":true}},{"type":"text","text":"), "},{"type":"text","text":"sqrt","style":{"code":true}},{"type":"text","text":", "},{"type":"text","text":"exp","style":{"code":true}},{"type":"text","text":", and "},{"type":"text","text":"log","style":{"code":true}},{"type":"text","text":". I'm still working on trig functions, but as a temporary workaround, you can call "},{"type":"text","text":"MultiFloats.use_bigfloat_transcendentals()","style":{"code":true}},{"type":"text","text":" to patch in BigFloat trig/hyperbolic functions for use with Float64xN.\n\nUnder the hood, a Float64xN is just an "},{"type":"text","text":"NTuple{N,Float64}","style":{"code":true}},{"type":"text","text":" manipulated using error-free transformations. The transformations we need in order to go up to Float64x8 are too large to practically write by hand, so they are generated with metaprogramming at library load time. I've taken care to ensure that the generated code compiles down to a straight line of native, vectorizable Float64 operations, making "},{"type":"link","url":"https://github.com/dzhang314/MultiFloats.jl","text":"MultiFloats.jl","style":{"bold":true}},{"type":"text","text":" the fastest extended-precision library I know of in the 100-400 bit precision range.\n\nA huge kudos to the Julia designers for providing this unique high-performance JIT architecture. I don't think a library like "},{"type":"link","url":"https://github.com/dzhang314/MultiFloats.jl","text":"MultiFloats.jl","style":{"bold":true}},{"type":"text","text":" could exist in any other language!"}]}]}],"thread_ts":"1609908459.404700","reply_count":50,"reply_users_count":5,"latest_reply":"1609934837.425200","reply_users":["U0179G7FG4F","UPKUR1KHB","U6N6VQE30","U018FJVBXPD","UPRKNS675"],"subscribed":false,"reactions":[{"name":"heart","users":["UM30MT6RF","UPRKNS675","UAUPJLBQX","U01C15GH58B","U6795JH6H"],"count":5},{"name":"smile","users":["U6N6VQE30","U6795JH6H"],"count":2}]},{"client_msg_id":"3827a68a-bba2-446d-ad31-891be1151110","type":"message","text":"Any thoughts on how this is able to outperform `DoubleFloats`?","user":"U0179G7FG4F","ts":"1609909702.406800","team":"T68168MUP","edited":{"user":"U0179G7FG4F","ts":"1609909711.000000"},"blocks":[{"type":"rich_text","block_id":"BZeIn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Any thoughts on how this is able to outperform "},{"type":"text","text":"DoubleFloats","style":{"code":true}},{"type":"text","text":"?"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"9b0e3676-4f40-4f60-a8e7-0209e95262ab","type":"message","text":"To be honest, I don't actually understand why Float64x2 is faster than Double64. In principle, we should be doing the same thing","user":"UPKUR1KHB","ts":"1609910390.407100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JbpA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"To be honest, I don't actually understand why Float64x2 is faster than Double64. In principle, we should be doing the same thing"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"a2d0dc49-fe72-4519-bfbc-dc7b0393293d","type":"message","text":"I just had a look at `code_native(+, (Double64, Double64))`, and it seems like DoubleFloats is doing a NaN check in every arithmetic operation. I make no attempt in MultiFloats to propagate Inf/NaN values, so that probably accounts for the difference","user":"UPKUR1KHB","ts":"1609910669.407400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6QDz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I just had a look at "},{"type":"text","text":"code_native(+, (Double64, Double64))","style":{"code":true}},{"type":"text","text":", and it seems like DoubleFloats is doing a NaN check in every arithmetic operation. I make no attempt in MultiFloats to propagate Inf/NaN values, so that probably accounts for the difference"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"8c23c6f0-1956-4f2c-a059-bdd34790d00a","type":"message","text":"ah. That makes sense","user":"U0179G7FG4F","ts":"1609911115.407600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LbuZM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ah. That makes sense"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"d23d935c-545e-41a6-9a49-099b1c2e2d8c","type":"message","text":"Let's keep it that way, no nan checks in multifloats :)","user":"U6N6VQE30","ts":"1609919870.409000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7yn21","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Let's keep it that way, no nan checks in multifloats :)"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB","reactions":[{"name":"+1","users":["UPKUR1KHB","U6795JH6H"],"count":2}]},{"client_msg_id":"d87fb15b-9ab4-4b2a-8e05-04714457198a","type":"message","text":"Wow this is awesome!","user":"U018FJVBXPD","ts":"1609921241.409200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/DW7w","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Wow this is awesome!"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"f553cdb3-22a1-4789-86df-616669f2062f","type":"message","text":"Kudos!","user":"UPRKNS675","ts":"1609922202.409500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kHWy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Kudos!"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"4c186a6f-b6ef-432a-8d57-b4e054b64cd8","type":"message","text":"What kind of EFTs are you using? e.g. for the two-operand addition: is it something like `two_sum` or rather `fast_two_sum`? I've found the performance of branch-free vs branch-ful EFTs to vary significantly between CPU architectures (esp. AVX512 is a game changer there).","user":"UPRKNS675","ts":"1609922205.409700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mp=y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What kind of EFTs are you using? e.g. for the two-operand addition: is it something like "},{"type":"text","text":"two_sum","style":{"code":true}},{"type":"text","text":" or rather "},{"type":"text","text":"fast_two_sum","style":{"code":true}},{"type":"text","text":"? I've found the performance of branch-free vs branch-ful EFTs to vary significantly between CPU architectures (esp. AVX512 is a game changer there)."}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"673e3615-2cd6-4493-a6ed-6fd68f5aa1da","type":"message","text":"for Float64x2 for instance:\n```julia&gt; MultiFloats.MultiFloatsCodeGen.multifloat_add_func(2)\n:(function multifloat_add(a::MultiFloat{T, 2}, b::MultiFloat{T, 2}) where T\n      $(Expr(:meta, :inline))\n      (t0, e1) = two_sum(a._limbs[1], b._limbs[1])\n      (t1, e2) = two_sum(a._limbs[2], b._limbs[2])\n      s0 = t0\n      (s1, m1_2) = two_sum(t1, e1)\n      s2 = e2 + m1_2\n      MultiFloat{T, 2}(renorm_2(s0, s1, s2))\n  end)```","user":"U6N6VQE30","ts":"1609922484.409900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JRg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"for Float64x2 for instance:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> MultiFloats.MultiFloatsCodeGen.multifloat_add_func(2)\n:(function multifloat_add(a::MultiFloat{T, 2}, b::MultiFloat{T, 2}) where T\n      $(Expr(:meta, :inline))\n      (t0, e1) = two_sum(a._limbs[1], b._limbs[1])\n      (t1, e2) = two_sum(a._limbs[2], b._limbs[2])\n      s0 = t0\n      (s1, m1_2) = two_sum(t1, e1)\n      s2 = e2 + m1_2\n      MultiFloat{T, 2}(renorm_2(s0, s1, s2))\n  end)"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB","reactions":[{"name":"+1","users":["UPRKNS675"],"count":1}]},{"client_msg_id":"542d4f64-7601-4b32-901d-e3b86f35832a","type":"message","text":"Also, I think we now have a lot of EFT implementations scattered all over the place in the Julia ecosystem: to my knowledge, MultiFloats.jl, DoubleFloats.jl, AccurateArithmetic.jl, StochasticArithmetic.jl (and quite possibly others) all embed some sort of EFT implementations. Do you think there would be a way to unify all this? Possibly using MultiFloats.jl's EFTs? They seem to have the best of all worlds: provide multi-argument variants, and support SIMD vectorization.","user":"UPRKNS675","ts":"1609922562.410100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3f7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Also, I think we now have a lot of EFT implementations scattered all over the place in the Julia ecosystem: to my knowledge, MultiFloats.jl, DoubleFloats.jl, AccurateArithmetic.jl, StochasticArithmetic.jl (and quite possibly others) all embed some sort of EFT implementations. Do you think there would be a way to unify all this? Possibly using MultiFloats.jl's EFTs? They seem to have the best of all worlds: provide multi-argument variants, and support SIMD vectorization."}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"9122f14d-439c-4784-a0c9-946ef9b534c8","type":"message","text":"Thanks <@U6N6VQE30> I was actually looking for the `two_sum` implementation, and found it there: <https://github.com/dzhang314/MultiFloats.jl/blob/master/src/MultiFloats.jl#L322-L326>","user":"UPRKNS675","ts":"1609922788.410300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"d66n","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks "},{"type":"user","user_id":"U6N6VQE30"},{"type":"text","text":" I was actually looking for the "},{"type":"text","text":"two_sum","style":{"code":true}},{"type":"text","text":" implementation, and found it there: "},{"type":"link","url":"https://github.com/dzhang314/MultiFloats.jl/blob/master/src/MultiFloats.jl#L322-L326"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"6e5cd09c-6ea6-47ab-b562-2b18749c62ac","type":"message","text":"So it seems to be a standard, branch-free two sum algorithm (by Knuth).","user":"UPRKNS675","ts":"1609922992.410500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"w6hi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So it seems to be a standard, branch-free two sum algorithm (by Knuth)."}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"6ab061ac-9b00-47f1-a82a-8be0a7900538","type":"message","text":"Btw <@UPRKNS675>: isn't it so that AccurateArithmetic.jl can in principle still be used for summing an array of say Float64x4's, and it would give the roughly same accuracy as summing Float64x8's?","user":"U6N6VQE30","ts":"1609923179.410800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SxXM0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Btw "},{"type":"user","user_id":"UPRKNS675"},{"type":"text","text":": isn't it so that AccurateArithmetic.jl can in principle still be used for summing an array of say Float64x4's, and it would give the roughly same accuracy as summing Float64x8's?"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"f0a309d9-52bf-4f29-bd51-7450b0c7c622","type":"message","text":"I haven't really looked into this, but as I understand it for AccurateArithmetic you keep 1 accumulator value with higher precision, correct? Which is way cheaper than converting the full array to double its precision and do a normal running sum?","user":"U6N6VQE30","ts":"1609923330.411000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bP5t9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I haven't really looked into this, but as I understand it for AccurateArithmetic you keep 1 accumulator value with higher precision, correct? Which is way cheaper than converting the full array to double its precision and do a normal running sum?"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"1634135f-d17c-4b69-bf9a-a3a31caba9d0","type":"message","text":"&gt; isn't it so that AccurateArithmetic.jl can in principle still be used for summing an array of say Float64x4's, and it would give the roughly same accuracy as summing Float64x8's?\nThe short answer would be no, if only because the implementation of compensated algorithms in AccurateArithmetic.jl are only specialized for Float32 and Float64.","user":"UPRKNS675","ts":"1609923335.411200","team":"T68168MUP","edited":{"user":"UPRKNS675","ts":"1609925590.000000"},"blocks":[{"type":"rich_text","block_id":"f=kZD","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"isn't it so that AccurateArithmetic.jl can in principle still be used for summing an array of say Float64x4's, and it would give the roughly same accuracy as summing Float64x8's?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"The short answer would be no, if only because the implementation of compensated algorithms in AccurateArithmetic.jl are only specialized for Float32 and Float64."}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"266f6d92-e0e2-4b99-a00f-f9b95790b824","type":"message","text":"&gt; for AccurateArithmetic you keep 1 accumulator value with higher precision, correct?\nSummation (and dot product) algorithms in AA.jl indeed \"simply\" use an accumulator of a different type, which is able to store more information. For mixed precision algorithms, this accumulator is simply a Float64 (when operands are single precision), and operands are converted to double-precision on the fly just before being accumulated. Forr compensated algorithms (Ogita-Rump-Oishi and Kahan-Babuska-Neumaier), the accumulator is essentially akin to a double-double (a tuple of a high-order part and a low-order part) with one notable distinction: we don't normalize anything until the end.","user":"UPRKNS675","ts":"1609923643.412600","team":"T68168MUP","edited":{"user":"UPRKNS675","ts":"1609925703.000000"},"blocks":[{"type":"rich_text","block_id":"9cQdu","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"for AccurateArithmetic you keep 1 accumulator value with higher precision, correct?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Summation (and dot product) algorithms in AA.jl indeed \"simply\" use an accumulator of a different type, which is able to store more information. For mixed precision algorithms, this accumulator is simply a Float64 (when operands are single precision), and operands are converted to double-precision on the fly just before being accumulated. Forr compensated algorithms (Ogita-Rump-Oishi and Kahan-Babuska-Neumaier), the accumulator is essentially akin to a double-double (a tuple of a high-order part and a low-order part) with one notable distinction: we don't normalize anything until the end."}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB","reactions":[{"name":"heavy_check_mark","users":["U6N6VQE30"],"count":1}]},{"client_msg_id":"d77af0e5-7de5-4597-8f61-a80f1f83afc0","type":"message","text":"Ok, I'm not so knowledgeable about the theory, but it seems that means AA.jl's vector sum algorithm is still different from / more efficient than implementing a vector sum with Float64x{N} with an accumulator in Float64x{2N}, because MultiFloats.jl will renormalize after every intermediate addition. (this is a bit hypothetical, since right now there's no such thing as `+(::Float64x{N}, ::Float64x{2N}`)","user":"U6N6VQE30","ts":"1609923947.413000","team":"T68168MUP","edited":{"user":"U6N6VQE30","ts":"1609923972.000000"},"blocks":[{"type":"rich_text","block_id":"=B/s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ok, I'm not so knowledgeable about the theory, but it seems that means AA.jl's vector sum algorithm is still different from / more efficient than implementing a vector sum with Float64x{N} with an accumulator in Float64x{2N}, because MultiFloats.jl will renormalize after every intermediate addition. (this is a bit hypothetical, since right now there's no such thing as "},{"type":"text","text":"+(::Float64x{N}, ::Float64x{2N}","style":{"code":true}},{"type":"text","text":")"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"678fe5a3-4d77-4204-a844-695dcc2ec9f7","type":"message","text":"&gt; isn't it so that AccurateArithmetic.jl can in principle still be used for summing an array of say Float64x4's, and it would give the roughly same accuracy as summing Float64x8's?\nCompensated algorithms in AA.jl rely on the availability of EFTs for the type of elements to be summed. So if we wanted to use compensated summation algorithms for Float64x4 numbers, we'd need to have an EFT that takes two Float64x4 inputs and returns two Float64x4 outputs. There probably is something like this internally available in MultiFloats.jl, but I doubt it's in the public API. So this is another reason why using compensated algorithms from AccurateArithmetic.jl in order to sum arrays of Float64x4 would not be straightforward (but you're right that it should give roughly Float64x8 accuracy in the end). That could be an interesting option for someone wanting to run a calculation if Float64x4, and only have extra precision in a summation/dot product somewhere in the code. (But I don't know if anyone does that in practice)","user":"UPRKNS675","ts":"1609923974.413400","team":"T68168MUP","edited":{"user":"UPRKNS675","ts":"1609925745.000000"},"blocks":[{"type":"rich_text","block_id":"bq53y","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"isn't it so that AccurateArithmetic.jl can in principle still be used for summing an array of say Float64x4's, and it would give the roughly same accuracy as summing Float64x8's?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Compensated algorithms in AA.jl rely on the availability of EFTs for the type of elements to be summed. So if we wanted to use compensated summation algorithms for Float64x4 numbers, we'd need to have an EFT that takes two Float64x4 inputs and returns two Float64x4 outputs. There probably is something like this internally available in MultiFloats.jl, but I doubt it's in the public API. So this is another reason why using compensated algorithms from AccurateArithmetic.jl in order to sum arrays of Float64x4 would not be straightforward (but you're right that it should give roughly Float64x8 accuracy in the end). That could be an interesting option for someone wanting to run a calculation if Float64x4, and only have extra precision in a summation/dot product somewhere in the code. (But I don't know if anyone does that in practice)"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"a55b0b1f-16df-4cbb-bdfa-6c56d4cdfac7","type":"message","text":"&gt; But I don't know if anyone does that in practice\nIt could definitely be interesting, because + scales as O(3N²) flops for Float64xN, and if you care about high precision, it would be nice to get many significant digits in the chosen number type for most linalg operations.","user":"U6N6VQE30","ts":"1609924256.413600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dd0","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"But I don't know if anyone does that in practice"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"It could definitely be interesting, because + scales as O(3N²) flops for Float64xN, and if you care about high precision, it would be nice to get many significant digits in the chosen number type for most linalg operations."}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"e9fe472b-dc4a-42b8-bbe4-7bef47ff802e","type":"message","text":"&gt; AA.jl's vector sum algorithm is still different from / more efficient than implementing a vector sum with Float64x{N} with an accumulator in Float64x{2N}\nYes I would expect that you're right. And this question is not so theoretical. An example implementation that could be built right away would be to perform the mixed-precision sum of an array of Float64 elements using a Float64x2 accumulator (I expect there already exists a `+(Float64x{2}, Float64)` method). Such an implementation would perform more or less the same EFTs as the ORO algorithm in AA.jl, but would probably renormalize numbers after each step (which I expect would be less efficient but perhaps give extra bits of precision, in the same way as a mixed-precision Float32/Float64 summation is usually a bit more accurate than ORO on Float32 numbers)","user":"UPRKNS675","ts":"1609924308.413800","team":"T68168MUP","edited":{"user":"UPRKNS675","ts":"1609925542.000000"},"blocks":[{"type":"rich_text","block_id":"fMQ","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"AA.jl's vector sum algorithm is still different from / more efficient than implementing a vector sum with Float64x{N} with an accumulator in Float64x{2N}"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Yes I would expect that you're right. And this question is not so theoretical. An example implementation that could be built right away would be to perform the mixed-precision sum of an array of Float64 elements using a Float64x2 accumulator (I expect there already exists a "},{"type":"text","text":"+(Float64x{2}, Float64)","style":{"code":true}},{"type":"text","text":" method). Such an implementation would perform more or less the same EFTs as the ORO algorithm in AA.jl, but would probably renormalize numbers after each step (which I expect would be less efficient but perhaps give extra bits of precision, in the same way as a mixed-precision Float32/Float64 summation is usually a bit more accurate than ORO on Float32 numbers)"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"cdee792b-e300-49ca-8699-d8e5432d42e9","type":"message","text":"There's currently no type promotion from Float64x{N} to Float64x{M} with M &gt; N, but probably what's needed is a specialized implementation for arithmetic ops that exploits the trailing zeros in the less significant number. Don't know how many flops that would save effectively. And yes, renormalization is currently built into `+`.","user":"U6N6VQE30","ts":"1609924573.414000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TWS75","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"There's currently no type promotion from Float64x{N} to Float64x{M} with M > N, but probably what's needed is a specialized implementation for arithmetic ops that exploits the trailing zeros in the less significant number. Don't know how many flops that would save effectively. And yes, renormalization is currently built into "},{"type":"text","text":"+","style":{"code":true}},{"type":"text","text":"."}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"b446b672-17ac-404a-83e7-fa0f4422b8fc","type":"message","text":"&gt;  It could definitely be interesting,\nYet another reason why it might beneficial to unify the ecosystem. I haven't thought about this much, but we might even have an EFTBase package that defines the API for EFTs (and perhaps implements it for standard floats). Then we could perhaps have algorithms in AA.jl rely only on the element types to fulfill the EFTBase interface, while MultiFloats.jl could itself implement the interface to provide EFTs for Float64x{N}. Food for thought...","user":"UPRKNS675","ts":"1609924631.414200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oIIY9","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":" It could definitely be interesting,"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Yet another reason why it might beneficial to unify the ecosystem. I haven't thought about this much, but we might even have an EFTBase package that defines the API for EFTs (and perhaps implements it for standard floats). Then we could perhaps have algorithms in AA.jl rely only on the element types to fulfill the EFTBase interface, while MultiFloats.jl could itself implement the interface to provide EFTs for Float64x{N}. Food for thought..."}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"cce35abe-eafa-40ac-9c92-41493fee26dd","type":"message","text":"In principle the MultiFloats.MultiFloatsCodeGen module can just work on tuples of some unspecified number type and be moved to a separate package. But I'm not very knowledgeable about the algorithm internals. Just superficially it looks like AA.jl is different from MultiFloats.jl in the sense that it has `ifelse` in two_sum. I leave this to <@UPKUR1KHB> :stuck_out_tongue:","user":"U6N6VQE30","ts":"1609924818.414400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qNlM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In principle the MultiFloats.MultiFloatsCodeGen module can just work on tuples of some unspecified number type and be moved to a separate package. But I'm not very knowledgeable about the algorithm internals. Just superficially it looks like AA.jl is different from MultiFloats.jl in the sense that it has "},{"type":"text","text":"ifelse","style":{"code":true}},{"type":"text","text":" in two_sum. I leave this to "},{"type":"user","user_id":"UPKUR1KHB"},{"type":"text","text":" "},{"type":"emoji","name":"stuck_out_tongue"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"932186bb-efa8-46b3-94be-de7356c05e39","type":"message","text":"&gt;  Just superficially it looks like AA.jl is different from MultiFloats.jl in the sense that it has `ifelse` in two_sum.\nAA.jl implements both Knuth's `two_sum` (without branch, exactly like in MultiFloats.jl; it is used in Ogita-Rump-Oishi compensated algorithms), and Dekker's `fast_two_sum` (with a branch; although it wasn't initially formulated in this way, the Kahan-Babuska-Neumaier algorithm can be re-interpreted as using this EFT)\n<https://github.com/JuliaMath/AccurateArithmetic.jl/blob/master/src/errorfree.jl#L10-L15>\n<https://github.com/JuliaMath/AccurateArithmetic.jl/blob/master/src/errorfree.jl#L28-L41>","user":"UPRKNS675","ts":"1609925138.414800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=WY","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":" Just superficially it looks like AA.jl is different from MultiFloats.jl in the sense that it has "},{"type":"text","text":"ifelse","style":{"code":true}},{"type":"text","text":" in two_sum."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"AA.jl implements both Knuth's "},{"type":"text","text":"two_sum","style":{"code":true}},{"type":"text","text":" (without branch, exactly like in MultiFloats.jl; it is used in Ogita-Rump-Oishi compensated algorithms), and Dekker's "},{"type":"text","text":"fast_two_sum","style":{"code":true}},{"type":"text","text":" (with a branch; although it wasn't initially formulated in this way, the Kahan-Babuska-Neumaier algorithm can be re-interpreted as using this EFT)\n"},{"type":"link","url":"https://github.com/JuliaMath/AccurateArithmetic.jl/blob/master/src/errorfree.jl#L10-L15"},{"type":"text","text":"\n"},{"type":"link","url":"https://github.com/JuliaMath/AccurateArithmetic.jl/blob/master/src/errorfree.jl#L28-L41"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB","reactions":[{"name":"white_check_mark","users":["U6N6VQE30"],"count":1}]},{"client_msg_id":"9bed3517-602b-46b4-87e7-f32e00ebe48f","type":"message","text":"hey guys! I'm just catching up on the conversation, but to answer <@UPRKNS675>’s first question: all MultiFloat arithmetic (+-*/), exp, and log are absolutely branch-free, to keep things as SIMD-friendly as possible. (sqrt has a single branch to detect zero)","user":"UPKUR1KHB","ts":"1609931549.418400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Cvwb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hey guys! I'm just catching up on the conversation, but to answer "},{"type":"user","user_id":"UPRKNS675"},{"type":"text","text":"’s first question: all MultiFloat arithmetic (+-*/), exp, and log are absolutely branch-free, to keep things as SIMD-friendly as possible. (sqrt has a single branch to detect zero)"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"b3727ba2-0c35-4b81-a293-3cf4c1c51524","type":"message","text":"I would be interested in an effort to unify EFTs in Julia as well. I wrote my own for MultiFloats.jl, mostly for the sake of learning about them; but I think the implementation I have produced is more flexible than what is available elsewhere","user":"UPKUR1KHB","ts":"1609931646.418600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DmJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I would be interested in an effort to unify EFTs in Julia as well. I wrote my own for MultiFloats.jl, mostly for the sake of learning about them; but I think the implementation I have produced is more flexible than what is available elsewhere"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"15b840ea-4a6a-4275-b67c-d23f97d74920","type":"message","text":"For example, I can very easily implement <@U6N6VQE30> suggestion of `+(::Float64x{N}, ::Float64x{2N})`, or indeed any Float64xN by Float64xM addition/subtraction/multiplication. The MultiFloatsCodeGen submodule can generate accumulation networks for any number of terms, each at any magnitude","user":"UPKUR1KHB","ts":"1609931747.418800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qJc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"For example, I can very easily implement "},{"type":"user","user_id":"U6N6VQE30"},{"type":"text","text":" suggestion of "},{"type":"text","text":"+(::Float64x{N}, ::Float64x{2N})","style":{"code":true}},{"type":"text","text":", or indeed any Float64xN by Float64xM addition/subtraction/multiplication. The MultiFloatsCodeGen submodule can generate accumulation networks for any number of terms, each at any magnitude"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"06169bae-3017-4bc9-9659-731b143bb426","type":"message","text":"Say I want to sum a Float64x3 (a, b, c) with a Float64x2 (d, e) and also a Float64 f. Then I can write\n```code = Expr[]\nMultiFloatsCodeGen.generate_accumulation_code!(code,\n    [(:a, 0), (:b, 1), (:c, 2), (:d, 0), (:e, 1), (:f, 0)], 3)\nprint(Expr(:block, code...))```\nto get\n```begin\n    (s0, m0_1, m0_2) = mpadd_3_3(a, d, f)\n    (s1, m1_2, m1_3) = mpadd_3_3(b, e, m0_1)\n    (s2, m2_3) = mpadd_3_2(c, m0_2, m1_2)\n    s3 = m1_3 + m2_3\n    MultiFloat{T, 3}(renorm_3(s0, s1, s2, s3))\nend```\nEach of the `mpadd_?_?` subroutines is stored in `MultiFloatsCodeGen.MPADD_CACHE`:\n```print(MultiFloatsCodeGen.MPADD_CACHE[:mpadd_3_3])```\n```function mpadd_3_3(a1::T, a2::T, a3::T) where T\n    $(Expr(:meta, :inline))\n    (m0_0, m1_1) = two_sum(a1, a2)\n    (m2_0, m3_1) = two_sum(a3, m0_0)\n    s0 = m2_0\n    (m4_1, m5_2) = two_sum(m1_1, m3_1)\n    s1 = m4_1\n    s2 = m5_2\n    (s0, s1, s2)\nend```","user":"UPKUR1KHB","ts":"1609932097.419800","team":"T68168MUP","edited":{"user":"UPKUR1KHB","ts":"1609932108.000000"},"blocks":[{"type":"rich_text","block_id":"cwRIZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Say I want to sum a Float64x3 (a, b, c) with a Float64x2 (d, e) and also a Float64 f. Then I can write\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"code = Expr[]\nMultiFloatsCodeGen.generate_accumulation_code!(code,\n    [(:a, 0), (:b, 1), (:c, 2), (:d, 0), (:e, 1), (:f, 0)], 3)\nprint(Expr(:block, code...))"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"to get\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"begin\n    (s0, m0_1, m0_2) = mpadd_3_3(a, d, f)\n    (s1, m1_2, m1_3) = mpadd_3_3(b, e, m0_1)\n    (s2, m2_3) = mpadd_3_2(c, m0_2, m1_2)\n    s3 = m1_3 + m2_3\n    MultiFloat{T, 3}(renorm_3(s0, s1, s2, s3))\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Each of the "},{"type":"text","text":"mpadd_?_?","style":{"code":true}},{"type":"text","text":" subroutines is stored in "},{"type":"text","text":"MultiFloatsCodeGen.MPADD_CACHE","style":{"code":true}},{"type":"text","text":":\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"print(MultiFloatsCodeGen.MPADD_CACHE[:mpadd_3_3])"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function mpadd_3_3(a1::T, a2::T, a3::T) where T\n    $(Expr(:meta, :inline))\n    (m0_0, m1_1) = two_sum(a1, a2)\n    (m2_0, m3_1) = two_sum(a3, m0_0)\n    s0 = m2_0\n    (m4_1, m5_2) = two_sum(m1_1, m3_1)\n    s1 = m4_1\n    s2 = m5_2\n    (s0, s1, s2)\nend"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB","reactions":[{"name":"open_mouth","users":["U6N6VQE30"],"count":1}]},{"client_msg_id":"d6d424a5-9918-4c58-b9c6-ee255623335a","type":"message","text":"so you can see this is a quite general framework for producing large EFTs. You can take a look at `MultiFloatsCodeGen.multifloat_mul_func(8)` for a monstrosity that I would never want to write by hand","user":"UPKUR1KHB","ts":"1609932265.420100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YU+p","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"so you can see this is a quite general framework for producing large EFTs. You can take a look at "},{"type":"text","text":"MultiFloatsCodeGen.multifloat_mul_func(8)","style":{"code":true}},{"type":"text","text":" for a monstrosity that I would never want to write by hand"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"c713f20d-3034-4440-a263-d4ac014b47b7","type":"message","text":"For now this should all be considered private implementation details, as I'm planning to move things around soon. You can see on `master` I've renamed MultiFloatsCodeGen to MultiFloats.Arithmetic, in anticipation of MultiFloats.Exponential, MultiFloats.Trigonometric, etc.","user":"UPKUR1KHB","ts":"1609932348.420300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4MW0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"For now this should all be considered private implementation details, as I'm planning to move things around soon. You can see on "},{"type":"text","text":"master","style":{"code":true}},{"type":"text","text":" I've renamed MultiFloatsCodeGen to MultiFloats.Arithmetic, in anticipation of MultiFloats.Exponential, MultiFloats.Trigonometric, etc."}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"57b6d045-b64d-464a-b625-4523c8b959ea","type":"message","text":"This is nice! Indeed, your generated `mpadd_3_3` is almost the same as the `three_sum` function in AccurateArithmetic.jl (which has been hand-written)","user":"UPRKNS675","ts":"1609932393.420500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RMpBX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This is nice! Indeed, your generated "},{"type":"text","text":"mpadd_3_3","style":{"code":true}},{"type":"text","text":" is almost the same as the "},{"type":"text","text":"three_sum","style":{"code":true}},{"type":"text","text":" function in AccurateArithmetic.jl (which has been hand-written)"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"a5b93f16-9562-40e6-af89-f1037f2e694a","type":"message","text":"I also have to move things around a bit in AA.jl to update our vector-friendly EFT implementations following the deprecation of SIMDPirates.jl. Once this is done, and when things will be more stable on your end too, then it might be a good time to see whether we can factor out an independent &amp; unified EFT package that could be used as a dependency at least in MultiFloats.jl, AccurateArithmetic.jl (and I suspect StochasticArithmetic.jl will immediately be able to benefit from it as well). This might be a good start!","user":"UPRKNS675","ts":"1609932674.420700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sqk+n","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I also have to move things around a bit in AA.jl to update our vector-friendly EFT implementations following the deprecation of SIMDPirates.jl. Once this is done, and when things will be more stable on your end too, then it might be a good time to see whether we can factor out an independent & unified EFT package that could be used as a dependency at least in MultiFloats.jl, AccurateArithmetic.jl (and I suspect StochasticArithmetic.jl will immediately be able to benefit from it as well). This might be a good start!"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"85979179-ab3c-4472-835e-1dc2f7741046","type":"message","text":"Thanks! I will need to carefully consider how to factor this stuff out of MultiFloats. There is currently a bit of global state in MPADD_CACHE, because I don't know which n-m-sum functions I will need ahead of time. For example, `multifloat_mul_func(8)` generates calls to 3-3-sum, 6-6-sum, 9-6-sum, 11-5-sum, 14-4-sum, 17-3-sum, and 20-2-sum","user":"UPKUR1KHB","ts":"1609932690.420900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+MniG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks! I will need to carefully consider how to factor this stuff out of MultiFloats. There is currently a bit of global state in MPADD_CACHE, because I don't know which n-m-sum functions I will need ahead of time. For example, "},{"type":"text","text":"multifloat_mul_func(8)","style":{"code":true}},{"type":"text","text":" generates calls to 3-3-sum, 6-6-sum, 9-6-sum, 11-5-sum, 14-4-sum, 17-3-sum, and 20-2-sum"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"ff25ddb5-230f-480c-88d7-55c0f82279ff","type":"message","text":"Makes sense. This might be less straightforward than I anticipated, then.","user":"UPRKNS675","ts":"1609932745.421100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"80MRs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Makes sense. This might be less straightforward than I anticipated, then."}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"e52872a5-9d30-4864-b5a7-9ceb7647404f","type":"message","text":"<@UPRKNS675> you might also be interested in my attempts <https://github.com/dzhang314/MultiFloats.jl/issues/10>. The TL;DR is that reductions do not auto-vectorize, so they need some help, the solution had been to use `MultiFloat{Vec&lt;W,Float64&gt;,M}`, and patch VectorizationBase.jl to not use fastmath (with the exception of contractions to fma, even though that's already done explicitly in David's code).","user":"U6N6VQE30","ts":"1609932925.421400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sk3","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UPRKNS675"},{"type":"text","text":" you might also be interested in my attempts "},{"type":"link","url":"https://github.com/dzhang314/MultiFloats.jl/issues/10"},{"type":"text","text":". The TL;DR is that reductions do not auto-vectorize, so they need some help, the solution had been to use "},{"type":"text","text":"MultiFloat{Vec<W,Float64>,M}","style":{"code":true}},{"type":"text","text":", and patch VectorizationBase.jl to not use fastmath (with the exception of contractions to fma, even though that's already done explicitly in David's code)."}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"50ef9b96-1a83-47eb-bb5e-86a396079179","type":"message","text":"well, that, and potentially combined with an array type that effectively does `y = permutedims(reinterpret(Float64, x), circshift(1:ndims(x)+1, -1))` so that the x[I...] as a multifloat is effectively stored in (y[I.., 1], .., y[I..., M]); that way vectorized routines can just do a bunch of vmovs instead of shuffling registers. This generates slightly better code than using StructArrays.jl","user":"U6N6VQE30","ts":"1609933204.421600","team":"T68168MUP","edited":{"user":"U6N6VQE30","ts":"1609933282.000000"},"blocks":[{"type":"rich_text","block_id":"3a+cO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"well, that, and potentially combined with an array type that effectively does "},{"type":"text","text":"y = permutedims(reinterpret(Float64, x), circshift(1:ndims(x)+1, -1))","style":{"code":true}},{"type":"text","text":" so that the x[I...] as a multifloat is effectively stored in (y[I.., 1], .., y[I..., M]); that way vectorized routines can just do a bunch of vmovs instead of shuffling registers. This generates slightly better code than using StructArrays.jl"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"dfe6493f-5388-4e80-9a09-9c204bfa79f3","type":"message","text":"indeed, <@U6N6VQE30>' comments have been very instructive! This is my first exposure to manual vectorization in Julia; up til now, I'd just been writing code that is (in principle) easily vectorizable, and praying to the LLVM gods that the auto-vectorizer figures it out","user":"UPKUR1KHB","ts":"1609933356.421900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bWuxj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"indeed, "},{"type":"user","user_id":"U6N6VQE30"},{"type":"text","text":"' comments have been very instructive! This is my first exposure to manual vectorization in Julia; up til now, I'd just been writing code that is (in principle) easily vectorizable, and praying to the LLVM gods that the auto-vectorizer figures it out"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"8a1b8708-1086-4f8f-affa-79c34c775eee","type":"message","text":"&gt; the solution had been to use `MultiFloat{Vec&lt;W,Float64&gt;,M}`, and patch VectorizationBase.jl to not use fastmath\nYes, the vectorization of reductions is something we had to worry a lot about with Chris when we initially implemented the first PoC compensated summation in AA.jl. We have some SIMDPirates.jl-based code that allows to du just that: make EFTs work with `Vec` types and mark which instructions are allowed to fuse and which are not. This is the part that needs updating following the deprecation of SIMDPirates in favor of VectorizationBase, but I think we might be able to reuse some of the AA.jl techniques in order to make MultiFloats.jl directly generate EFTs compatible with explicit `Vec` types.","user":"UPRKNS675","ts":"1609933362.422100","team":"T68168MUP","edited":{"user":"UPRKNS675","ts":"1609933418.000000"},"blocks":[{"type":"rich_text","block_id":"ZIo","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"the solution had been to use "},{"type":"text","text":"MultiFloat{Vec<W,Float64>,M}","style":{"code":true}},{"type":"text","text":", and patch VectorizationBase.jl to not use fastmath"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Yes, the vectorization of reductions is something we had to worry a lot about with Chris when we initially implemented the first PoC compensated summation in AA.jl. We have some SIMDPirates.jl-based code that allows to du just that: make EFTs work with "},{"type":"text","text":"Vec","style":{"code":true}},{"type":"text","text":" types and mark which instructions are allowed to fuse and which are not. This is the part that needs updating following the deprecation of SIMDPirates in favor of VectorizationBase, but I think we might be able to reuse some of the AA.jl techniques in order to make MultiFloats.jl directly generate EFTs compatible with explicit "},{"type":"text","text":"Vec","style":{"code":true}},{"type":"text","text":" types."}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"a4d953c8-a20d-4c83-a148-5336eb2a648d","type":"message","text":"interesting! I have a lot to learn here, as I don't yet understand the Julia SIMD ecosystem. What's with all these packages (SIMD.jl, SIMDPirates.jl, VectorizationBase.jl), and why can't I just call `_mm256_add_pd`*...*","user":"UPKUR1KHB","ts":"1609933566.422400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sLo1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"interesting! I have a lot to learn here, as I don't yet understand the Julia SIMD ecosystem. What's with all these packages (SIMD.jl, SIMDPirates.jl, VectorizationBase.jl), and why can't I just call "},{"type":"text","text":"_mm256_add_pd","style":{"code":true}},{"type":"text","text":"...","style":{"bold":true}}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"06ebb468-9a65-4dc0-a832-48da6c75ab4d","type":"message","text":"Tell you what, we should continue this conversation elsewhere. It seems that old messages are deleted very quickly in Julia Slack. I think there is a lot to be gained from a well-designed and SIMD-friendly EFTBase.jl","user":"UPKUR1KHB","ts":"1609933645.422600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vZIuR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Tell you what, we should continue this conversation elsewhere. It seems that old messages are deleted very quickly in Julia Slack. I think there is a lot to be gained from a well-designed and SIMD-friendly EFTBase.jl"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"0387a19e-8f60-421a-9293-9c1f05aac260","type":"message","text":"I think <@UAUPJLBQX> would be happy if we'd make VectorizationBase.jl not use fastmath flags by default, and instead implement Base.FastMath.add_fast etc for Vec types. That way there's not really anything to be done to make EFT-codes vector-friendly or VectorizationBase.jl-aware.","user":"U6N6VQE30","ts":"1609933695.422800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FkrL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think "},{"type":"user","user_id":"UAUPJLBQX"},{"type":"text","text":" would be happy if we'd make VectorizationBase.jl not use fastmath flags by default, and instead implement Base.FastMath.add_fast etc for Vec types. That way there's not really anything to be done to make EFT-codes vector-friendly or VectorizationBase.jl-aware."}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"988e8700-9cbd-4d83-9d57-04af58f7acbd","type":"message","text":"I just didn't get to that","user":"U6N6VQE30","ts":"1609933722.423000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QyJd2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I just didn't get to that"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"6583b93b-5a76-4665-983a-eb70acfca22e","type":"message","text":"oh wow, I didn't even know `Base.FastMath` existed. Clearly I've got quite a bit to learn about optimization in Julia","user":"UPKUR1KHB","ts":"1609933917.423200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HfoE0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"oh wow, I didn't even know "},{"type":"text","text":"Base.FastMath","style":{"code":true}},{"type":"text","text":" existed. Clearly I've got quite a bit to learn about optimization in Julia"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"aaabee20-05f1-44bf-a204-956bd222d230","type":"message","text":"&gt; Tell you what, we should continue this conversation elsewhere.\nAgreed! Do you have something to suggest? All I can think of would be a Github issue in MultiFloats.jl, but I'm not sure whether it would be convenient...","user":"UPRKNS675","ts":"1609934092.423500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fHb","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"Tell you what, we should continue this conversation elsewhere."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Agreed! Do you have something to suggest? All I can think of would be a Github issue in MultiFloats.jl, but I'm not sure whether it would be convenient..."}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"1928753c-915b-42fd-8558-24e41df7105f","type":"message","text":"am I correct to assume that `Base.FastMath.add_fast` is an equivalent of `+` that the Julia compiler is allowed to freely reassociate outside of a `@fastmath` context? if so, that would be really useful for MultiFloats","user":"UPKUR1KHB","ts":"1609934095.423700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bdG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"am I correct to assume that "},{"type":"text","text":"Base.FastMath.add_fast","style":{"code":true}},{"type":"text","text":" is an equivalent of "},{"type":"text","text":"+","style":{"code":true}},{"type":"text","text":" that the Julia compiler is allowed to freely reassociate outside of a "},{"type":"text","text":"@fastmath","style":{"code":true}},{"type":"text","text":" context? if so, that would be really useful for MultiFloats"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"e6560206-e628-4158-8939-d2827e8f8b6c","type":"message","text":"Well, fastmath support in julia is either through `julia --math-mode=fast` globally of `@fastmath [code]` locally, but the latter is awkward because it doesn't propagate into function calls inside of `[code]`","user":"U6N6VQE30","ts":"1609934100.423900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TXlB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Well, fastmath support in julia is either through "},{"type":"text","text":"julia --math-mode=fast","style":{"code":true}},{"type":"text","text":" globally of "},{"type":"text","text":"@fastmath [code]","style":{"code":true}},{"type":"text","text":" locally, but the latter is awkward because it doesn't propagate into function calls inside of "},{"type":"text","text":"[code]","style":{"code":true}}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"78dd6e96-8865-4456-910f-175c2fe01b82","type":"message","text":"&gt; I think <@UAUPJLBQX> would be happy if we'd make VectorizationBase.jl not use fastmath flags by default, and instead implement Base.FastMath.add_fast etc for Vec types. That way there's not really anything to be done to make EFT-codes vector-friendly or VectorizationBase.jl-aware.\nYes, that's also more or less what I understood from <https://github.com/chriselrod/SIMDPirates.jl/issues/15#issuecomment-753922309>","user":"UPRKNS675","ts":"1609934261.424100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MGDl","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"I think "},{"type":"user","user_id":"UAUPJLBQX"},{"type":"text","text":" would be happy if we'd make VectorizationBase.jl not use fastmath flags by default, and instead implement Base.FastMath.add_fast etc for Vec types. That way there's not really anything to be done to make EFT-codes vector-friendly or VectorizationBase.jl-aware."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Yes, that's also more or less what I understood from "},{"type":"link","url":"https://github.com/chriselrod/SIMDPirates.jl/issues/15#issuecomment-753922309"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"da908f16-4253-490d-abfa-28e6c8b7e2bc","type":"message","text":"Yeah, I agree with all points made there :slightly_smiling_face: FloatVec would be neat too","user":"U6N6VQE30","ts":"1609934334.424400","team":"T68168MUP","edited":{"user":"U6N6VQE30","ts":"1609934337.000000"},"blocks":[{"type":"rich_text","block_id":"LHQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah, I agree with all points made there "},{"type":"emoji","name":"slightly_smiling_face"},{"type":"text","text":" FloatVec would be neat too"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"86348d38-4844-413b-b07b-a6da10de4266","type":"message","text":"<@UPRKNS675> I have just created an empty EFTBase.jl repository, along with an initial issue: <https://github.com/dzhang314/EFTBase.jl/issues/1> perhaps we can take the discussion there?","user":"UPKUR1KHB","ts":"1609934524.425000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WZHJ","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UPRKNS675"},{"type":"text","text":" I have just created an empty EFTBase.jl repository, along with an initial issue: "},{"type":"link","url":"https://github.com/dzhang314/EFTBase.jl/issues/1"},{"type":"text","text":" perhaps we can take the discussion there?"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"2099f420-6e46-4376-a5f1-cdef9e285f3c","type":"message","text":"btw, this is my first time seeing <@UAUPJLBQX>’s name spelled out; this whole time, I had been reading his GitHub handle as \"chisel rod\"","user":"UPKUR1KHB","ts":"1609934837.425200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"39h","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"btw, this is my first time seeing "},{"type":"user","user_id":"UAUPJLBQX"},{"type":"text","text":"’s name spelled out; this whole time, I had been reading his GitHub handle as \"chisel rod\""}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB","reactions":[{"name":"laughing","users":["UAUPJLBQX","U6795JH6H"],"count":2}]}]