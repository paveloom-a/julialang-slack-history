[{"client_msg_id":"7159666d-cb7e-4d7c-b238-e7eeb0c53d87","type":"message","text":"Hey everybody, I'd like to announce that I've just released *<https://github.com/dzhang314/MultiFloats.jl|MultiFloats.jl>*<https://github.com/dzhang314/MultiFloats.jl| >*<https://github.com/dzhang314/MultiFloats.jl|v1.0>*, my library for fast extended-precision arithmetic. It provides the types Float64x2, Float64x3, ..., Float64x8 for doing math with approximately 100, 150, ..., 400 accurate bits. It currently supports arithmetic (`+-*/`), `sqrt`, `exp`, and `log`. I'm still working on trig functions, but as a temporary workaround, you can call `MultiFloats.use_bigfloat_transcendentals()` to patch in BigFloat trig/hyperbolic functions for use with Float64xN.\n\nUnder the hood, a Float64xN is just an `NTuple{N,Float64}` manipulated using error-free transformations. The transformations we need in order to go up to Float64x8 are too large to practically write by hand, so they are generated with metaprogramming at library load time. I've taken care to ensure that the generated code compiles down to a straight line of native, vectorizable Float64 operations, making *<https://github.com/dzhang314/MultiFloats.jl|MultiFloats.jl>* the fastest extended-precision library I know of in the 100-400 bit precision range.\n\nA huge kudos to the Julia designers for providing this unique high-performance JIT architecture. I don't think a library like *<https://github.com/dzhang314/MultiFloats.jl|MultiFloats.jl>* could exist in any other language!","user":"UPKUR1KHB","ts":"1609908459.404700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"G9qCU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey everybody, I'd like to announce that I've just released "},{"type":"link","url":"https://github.com/dzhang314/MultiFloats.jl","text":"MultiFloats.jl","style":{"bold":true}},{"type":"link","url":"https://github.com/dzhang314/MultiFloats.jl","text":" "},{"type":"link","url":"https://github.com/dzhang314/MultiFloats.jl","text":"v1.0","style":{"bold":true}},{"type":"text","text":", my library for fast extended-precision arithmetic. It provides the types Float64x2, Float64x3, ..., Float64x8 for doing math with approximately 100, 150, ..., 400 accurate bits. It currently supports arithmetic ("},{"type":"text","text":"+-*/","style":{"code":true}},{"type":"text","text":"), "},{"type":"text","text":"sqrt","style":{"code":true}},{"type":"text","text":", "},{"type":"text","text":"exp","style":{"code":true}},{"type":"text","text":", and "},{"type":"text","text":"log","style":{"code":true}},{"type":"text","text":". I'm still working on trig functions, but as a temporary workaround, you can call "},{"type":"text","text":"MultiFloats.use_bigfloat_transcendentals()","style":{"code":true}},{"type":"text","text":" to patch in BigFloat trig/hyperbolic functions for use with Float64xN.\n\nUnder the hood, a Float64xN is just an "},{"type":"text","text":"NTuple{N,Float64}","style":{"code":true}},{"type":"text","text":" manipulated using error-free transformations. The transformations we need in order to go up to Float64x8 are too large to practically write by hand, so they are generated with metaprogramming at library load time. I've taken care to ensure that the generated code compiles down to a straight line of native, vectorizable Float64 operations, making "},{"type":"link","url":"https://github.com/dzhang314/MultiFloats.jl","text":"MultiFloats.jl","style":{"bold":true}},{"type":"text","text":" the fastest extended-precision library I know of in the 100-400 bit precision range.\n\nA huge kudos to the Julia designers for providing this unique high-performance JIT architecture. I don't think a library like "},{"type":"link","url":"https://github.com/dzhang314/MultiFloats.jl","text":"MultiFloats.jl","style":{"bold":true}},{"type":"text","text":" could exist in any other language!"}]}]}],"thread_ts":"1609908459.404700","reply_count":6,"reply_users_count":4,"latest_reply":"1609921241.409200","reply_users":["U0179G7FG4F","UPKUR1KHB","U6N6VQE30","U018FJVBXPD"],"subscribed":false},{"client_msg_id":"3827a68a-bba2-446d-ad31-891be1151110","type":"message","text":"Any thoughts on how this is able to outperform `DoubleFloats`?","user":"U0179G7FG4F","ts":"1609909702.406800","team":"T68168MUP","edited":{"user":"U0179G7FG4F","ts":"1609909711.000000"},"blocks":[{"type":"rich_text","block_id":"BZeIn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Any thoughts on how this is able to outperform "},{"type":"text","text":"DoubleFloats","style":{"code":true}},{"type":"text","text":"?"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"9b0e3676-4f40-4f60-a8e7-0209e95262ab","type":"message","text":"To be honest, I don't actually understand why Float64x2 is faster than Double64. In principle, we should be doing the same thing","user":"UPKUR1KHB","ts":"1609910390.407100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JbpA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"To be honest, I don't actually understand why Float64x2 is faster than Double64. In principle, we should be doing the same thing"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"a2d0dc49-fe72-4519-bfbc-dc7b0393293d","type":"message","text":"I just had a look at `code_native(+, (Double64, Double64))`, and it seems like DoubleFloats is doing a NaN check in every arithmetic operation. I make no attempt in MultiFloats to propagate Inf/NaN values, so that probably accounts for the difference","user":"UPKUR1KHB","ts":"1609910669.407400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6QDz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I just had a look at "},{"type":"text","text":"code_native(+, (Double64, Double64))","style":{"code":true}},{"type":"text","text":", and it seems like DoubleFloats is doing a NaN check in every arithmetic operation. I make no attempt in MultiFloats to propagate Inf/NaN values, so that probably accounts for the difference"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"8c23c6f0-1956-4f2c-a059-bdd34790d00a","type":"message","text":"ah. That makes sense","user":"U0179G7FG4F","ts":"1609911115.407600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LbuZM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ah. That makes sense"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"d23d935c-545e-41a6-9a49-099b1c2e2d8c","type":"message","text":"Let's keep it that way, no nan checks in multifloats :)","user":"U6N6VQE30","ts":"1609919870.409000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7yn21","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Let's keep it that way, no nan checks in multifloats :)"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"},{"client_msg_id":"d87fb15b-9ab4-4b2a-8e05-04714457198a","type":"message","text":"Wow this is awesome!","user":"U018FJVBXPD","ts":"1609921241.409200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/DW7w","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Wow this is awesome!"}]}]}],"thread_ts":"1609908459.404700","parent_user_id":"UPKUR1KHB"}]