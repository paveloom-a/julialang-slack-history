[{"client_msg_id":"ccd5fdf1-c45a-4b77-a86f-8bd2caa2cb0b","type":"message","text":"Hi I'm trying to train a LSTM to predict a value based upon last 5 records for obs`i`.  I just use the sum of mse as my loss function and `trX`  is my records, `trY` is my labels in training sets.\n```function LSTMTest()\nlt = Chain(LSTM(5,1),Dense(1,1)) |&gt;gpu\n    function trial(i)\n        pre = lt(trX[i])\n        return pre\n    end  \nend\nm = LSTMBW()\ntx = ty =  [i for i=1:10];\ntrainData = Flux.Data.DataLoader(tx,ty,shuffle=true,batchsize=10)\nps = Flux.params(m.lt)\nloss(x,y) = sum(mse.(m.(x),trY[y]))\n@time Flux.train!(loss,ps,trainData,ADAM())```\nHowever, when training this very simple LSTM, I got this warning which confuse me a lot. I was wondering whether I missed anything? Many thanks.\n```┌ Warning: calls to Base intrinsics might be GPU incompatible\n│   exception = (GPUCompiler.MethodSubstitutionWarning(log(x::Float64) in Base.Math at special/log.jl:253, log(x::Float64) in CUDA at /home/zq/.julia/packages/CUDA/wTQsK/src/device/intrinsics/math.jl:72), Base.StackTraces.StackFrame[log at log.jl:253, broadcast_kernel at broadcast.jl:59])\n└ @ GPUCompiler /home/zq/.julia/packages/GPUCompiler/uTpNx/src/irgen.jl:68\nGPU compilation of kernel broadcast_kernel(CUDA.CuKernelContext, CuDeviceArray{ForwardDiff.Dual{Nothing,Float32,2},1,1}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},Zygote.var\"#1079#1082\"{typeof(mse)},Tuple{Base.Broadcast.Extruded{Array{CuArray{Float32,1},1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}}}}, Int64) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},Zygote.var\"#1079#1082\"{typeof(mse)},Tuple{Base.Broadcast.Extruded{Array{CuArray{Float32,1},1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}}}}, which is not isbits:\n  .args is of type Tuple{Base.Broadcast.Extruded{Array{CuArray{Float32,1},1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}}} which is not isbits.\n    .1 is of type Base.Broadcast.Extruded{Array{CuArray{Float32,1},1},Tuple{Bool},Tuple{Int64}} which is not isbits.\n      .x is of type Array{CuArray{Float32,1},1} which is not isbits.\n\n\n\nStacktrace:\n [1] check_invocation(::GPUCompiler.CompilerJob, ::LLVM.Function) at /home/zq/.julia/packages/GPUCompiler/uTpNx/src/validation.jl:68\n [2] macro expansion at /home/zq/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:238 [inlined]\n [3] macro expansion at /home/zq/.julia/packages/TimerOutputs/4QAIk/src/TimerOutput.jl:206 [inlined]\n [4] codegen(::Symbol, ::GPUCompiler.CompilerJob; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/zq/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:237\n [5] compile(::Symbol, ::GPUCompiler.CompilerJob; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/zq/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:39\n [6] compile at /home/zq/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:35 [inlined]\n [7] cufunction_compile(::GPUCompiler.FunctionSpec; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/zq/.julia/packages/CUDA/wTQsK/src/compiler/execution.jl:302\n [8] cufunction_compile(::GPUCompiler.FunctionSpec) at /home/zq/.julia/packages/CUDA/wTQsK/src/compiler/execution.jl:297\n [9] check_cache(::Dict{UInt64,Any}, ::Any, ::Any, ::GPUCompiler.FunctionSpec{GPUArrays.var\"#broadcast_kernel#12\",Tuple{CUDA.CuKernelContext,CuDeviceArray{ForwardDiff.Dual{Nothing,Float32,2},1,1},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},Zygote.var\"#1079#1082\"{typeof(mse)},Tuple{Base.Broadcast.Extruded{Array{CuArray{Float32,1},1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}}}},Int64}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/zq/.julia/packages/GPUCompiler/uTpNx/src/cache.jl:40\n [10] broadcast_kernel at /home/zq/.julia/packages/GPUArrays/WV76E/src/host/broadcast.jl:60 [inlined]\n [11] cached_compilation at /home/zq/.julia/packages/GPUCompiler/uTpNx/src/cache.jl:65 [inlined]\n [12] cufunction(::GPUArrays.var\"#broadcast_kernel#12\", ::Type{Tuple{CUDA.CuKernelContext,CuDeviceArray{ForwardDiff.Dual{Nothing,Float32,2},1,1},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},Zygote.var\"#1079#1082\"{typeof(mse)},Tuple{Base.Broadcast.Extruded{Array{CuArray{Float32,1},1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}}}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/zq/.julia/packages/CUDA/wTQsK/src/compiler/execution.jl:289\n [13] cufunction at /home/zq/.julia/packages/CUDA/wTQsK/src/compiler/execution.jl:286 [inlined]\n [14] macro expansion at /home/zq/.julia/packages/CUDA/wTQsK/src/compiler/execution.jl:100 [inlined]\n [15] #launch_heuristic#857 at /home/zq/.julia/packages/CUDA/wTQsK/src/gpuarrays.jl:17 [inlined]\n [16] launch_heuristic at /home/zq/.julia/packages/CUDA/wTQsK/src/gpuarrays.jl:17 [inlined]\n [17] copyto! at /home/zq/.julia/packages/GPUArrays/WV76E/src/host/broadcast.jl:66 [inlined]\n [18] copyto! at ./broadcast.jl:886 [inlined]\n [19] copy(::Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},Zygote.var\"#1079#1082\"{typeof(mse)},Tuple{Array{CuArray{Float32,1},1},CuArray{Float32,1}}}) at ./broadcast.jl:862\n [20] materialize at ./broadcast.jl:837 [inlined]\n [21] broadcast_forward at /home/zq/.julia/packages/Zygote/9X8lu/src/lib/broadcast.jl:212 [inlined]\n [22] adjoint at /home/zq/.julia/packages/Zygote/9X8lu/src/lib/broadcast.jl:224 [inlined]\n [23] _pullback at /home/zq/.julia/packages/ZygoteRules/OjfTt/src/adjoint.jl:57 [inlined]\n [24] adjoint at /home/zq/.julia/packages/Zygote/9X8lu/src/lib/lib.jl:191 [inlined]\n [25] _pullback at /home/zq/.julia/packages/ZygoteRules/OjfTt/src/adjoint.jl:57 [inlined]\n [26] broadcasted at ./broadcast.jl:1263 [inlined]\n [27] _pullback(::Zygote.Context, ::typeof(Base.Broadcast.broadcasted), ::typeof(mse), ::Array{CuArray{Float32,1},1}, ::CuArray{Float32,1}) at /home/zq/.julia/packages/Zygote/9X8lu/src/compiler/interface2.jl:0\n [28] loss at ./In[255]:1 [inlined]\n [29] _pullback(::Zygote.Context, ::typeof(loss), ::Array{Int64,1}, ::Array{Int64,1}) at /home/zq/.julia/packages/Zygote/9X8lu/src/compiler/interface2.jl:0\n [30] adjoint at /home/zq/.julia/packages/Zygote/9X8lu/src/lib/lib.jl:191 [inlined]\n [31] _pullback at /home/zq/.julia/packages/ZygoteRules/OjfTt/src/adjoint.jl:57 [inlined]\n [32] #15 at /home/zq/.julia/packages/Flux/goUGu/src/optimise/train.jl:103 [inlined]\n [33] _pullback(::Zygote.Context, ::Flux.Optimise.var\"#15#21\"{typeof(loss),Tuple{Array{Int64,1},Array{Int64,1}}}) at /home/zq/.julia/packages/Zygote/9X8lu/src/compiler/interface2.jl:0\n [34] pullback(::Function, ::Params) at /home/zq/.julia/packages/Zygote/9X8lu/src/compiler/interface.jl:247\n [35] gradient(::Function, ::Params) at /home/zq/.julia/packages/Zygote/9X8lu/src/compiler/interface.jl:58\n [36] macro expansion at /home/zq/.julia/packages/Flux/goUGu/src/optimise/train.jl:102 [inlined]\n [37] macro expansion at /home/zq/.julia/packages/Juno/n6wyj/src/progress.jl:134 [inlined]\n [38] train!(::Function, ::Params, ::DataLoader{Tuple{Array{Int64,1},Array{Int64,1}}}, ::ADAM; cb::Flux.Optimise.var\"#16#22\") at /home/zq/.julia/packages/Flux/goUGu/src/optimise/train.jl:100\n [39] train!(::Function, ::Params, ::DataLoader{Tuple{Array{Int64,1},Array{Int64,1}}}, ::ADAM) at /home/zq/.julia/packages/Flux/goUGu/src/optimise/train.jl:98\n [40] top-level scope at ./timing.jl:174 [inlined]\n [41] top-level scope at ./In[256]:0\n [42] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091```","user":"U01977X150R","ts":"1616734521.056300","team":"T68168MUP","edited":{"user":"U01977X150R","ts":"1616736727.000000"},"blocks":[{"type":"rich_text","block_id":"ccc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi I'm trying to train a LSTM to predict a value based upon last 5 records for obs"},{"type":"text","text":"i","style":{"code":true}},{"type":"text","text":".  I just use the sum of mse as my loss function and "},{"type":"text","text":"trX","style":{"code":true}},{"type":"text","text":"  is my records, "},{"type":"text","text":"trY","style":{"code":true}},{"type":"text","text":" is my labels in training sets.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function LSTMTest()\nlt = Chain(LSTM(5,1),Dense(1,1)) |>gpu\n    function trial(i)\n        pre = lt(trX[i])\n        return pre\n    end  \nend\nm = LSTMBW()\ntx = ty =  [i for i=1:10];\ntrainData = Flux.Data.DataLoader(tx,ty,shuffle=true,batchsize=10)\nps = Flux.params(m.lt)\nloss(x,y) = sum(mse.(m.(x),trY[y]))\n@time Flux.train!(loss,ps,trainData,ADAM())"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"However, when training this very simple LSTM, I got this warning which confuse me a lot. I was wondering whether I missed anything? Many thanks.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"┌ Warning: calls to Base intrinsics might be GPU incompatible\n│   exception = (GPUCompiler.MethodSubstitutionWarning(log(x::Float64) in Base.Math at special/log.jl:253, log(x::Float64) in CUDA at /home/zq/.julia/packages/CUDA/wTQsK/src/device/intrinsics/math.jl:72), Base.StackTraces.StackFrame[log at log.jl:253, broadcast_kernel at broadcast.jl:59])\n└ @ GPUCompiler /home/zq/.julia/packages/GPUCompiler/uTpNx/src/irgen.jl:68\nGPU compilation of kernel broadcast_kernel(CUDA.CuKernelContext, CuDeviceArray{ForwardDiff.Dual{Nothing,Float32,2},1,1}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},Zygote.var\"#1079#1082\"{typeof(mse)},Tuple{Base.Broadcast.Extruded{Array{CuArray{Float32,1},1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}}}}, Int64) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},Zygote.var\"#1079#1082\"{typeof(mse)},Tuple{Base.Broadcast.Extruded{Array{CuArray{Float32,1},1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}}}}, which is not isbits:\n  .args is of type Tuple{Base.Broadcast.Extruded{Array{CuArray{Float32,1},1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}}} which is not isbits.\n    .1 is of type Base.Broadcast.Extruded{Array{CuArray{Float32,1},1},Tuple{Bool},Tuple{Int64}} which is not isbits.\n      .x is of type Array{CuArray{Float32,1},1} which is not isbits.\n\n\n\nStacktrace:\n [1] check_invocation(::GPUCompiler.CompilerJob, ::LLVM.Function) at /home/zq/.julia/packages/GPUCompiler/uTpNx/src/validation.jl:68\n [2] macro expansion at /home/zq/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:238 [inlined]\n [3] macro expansion at /home/zq/.julia/packages/TimerOutputs/4QAIk/src/TimerOutput.jl:206 [inlined]\n [4] codegen(::Symbol, ::GPUCompiler.CompilerJob; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/zq/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:237\n [5] compile(::Symbol, ::GPUCompiler.CompilerJob; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/zq/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:39\n [6] compile at /home/zq/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:35 [inlined]\n [7] cufunction_compile(::GPUCompiler.FunctionSpec; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/zq/.julia/packages/CUDA/wTQsK/src/compiler/execution.jl:302\n [8] cufunction_compile(::GPUCompiler.FunctionSpec) at /home/zq/.julia/packages/CUDA/wTQsK/src/compiler/execution.jl:297\n [9] check_cache(::Dict{UInt64,Any}, ::Any, ::Any, ::GPUCompiler.FunctionSpec{GPUArrays.var\"#broadcast_kernel#12\",Tuple{CUDA.CuKernelContext,CuDeviceArray{ForwardDiff.Dual{Nothing,Float32,2},1,1},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},Zygote.var\"#1079#1082\"{typeof(mse)},Tuple{Base.Broadcast.Extruded{Array{CuArray{Float32,1},1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}}}},Int64}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/zq/.julia/packages/GPUCompiler/uTpNx/src/cache.jl:40\n [10] broadcast_kernel at /home/zq/.julia/packages/GPUArrays/WV76E/src/host/broadcast.jl:60 [inlined]\n [11] cached_compilation at /home/zq/.julia/packages/GPUCompiler/uTpNx/src/cache.jl:65 [inlined]\n [12] cufunction(::GPUArrays.var\"#broadcast_kernel#12\", ::Type{Tuple{CUDA.CuKernelContext,CuDeviceArray{ForwardDiff.Dual{Nothing,Float32,2},1,1},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},Zygote.var\"#1079#1082\"{typeof(mse)},Tuple{Base.Broadcast.Extruded{Array{CuArray{Float32,1},1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}}}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/zq/.julia/packages/CUDA/wTQsK/src/compiler/execution.jl:289\n [13] cufunction at /home/zq/.julia/packages/CUDA/wTQsK/src/compiler/execution.jl:286 [inlined]\n [14] macro expansion at /home/zq/.julia/packages/CUDA/wTQsK/src/compiler/execution.jl:100 [inlined]\n [15] #launch_heuristic#857 at /home/zq/.julia/packages/CUDA/wTQsK/src/gpuarrays.jl:17 [inlined]\n [16] launch_heuristic at /home/zq/.julia/packages/CUDA/wTQsK/src/gpuarrays.jl:17 [inlined]\n [17] copyto! at /home/zq/.julia/packages/GPUArrays/WV76E/src/host/broadcast.jl:66 [inlined]\n [18] copyto! at ./broadcast.jl:886 [inlined]\n [19] copy(::Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{1},Tuple{Base.OneTo{Int64}},Zygote.var\"#1079#1082\"{typeof(mse)},Tuple{Array{CuArray{Float32,1},1},CuArray{Float32,1}}}) at ./broadcast.jl:862\n [20] materialize at ./broadcast.jl:837 [inlined]\n [21] broadcast_forward at /home/zq/.julia/packages/Zygote/9X8lu/src/lib/broadcast.jl:212 [inlined]\n [22] adjoint at /home/zq/.julia/packages/Zygote/9X8lu/src/lib/broadcast.jl:224 [inlined]\n [23] _pullback at /home/zq/.julia/packages/ZygoteRules/OjfTt/src/adjoint.jl:57 [inlined]\n [24] adjoint at /home/zq/.julia/packages/Zygote/9X8lu/src/lib/lib.jl:191 [inlined]\n [25] _pullback at /home/zq/.julia/packages/ZygoteRules/OjfTt/src/adjoint.jl:57 [inlined]\n [26] broadcasted at ./broadcast.jl:1263 [inlined]\n [27] _pullback(::Zygote.Context, ::typeof(Base.Broadcast.broadcasted), ::typeof(mse), ::Array{CuArray{Float32,1},1}, ::CuArray{Float32,1}) at /home/zq/.julia/packages/Zygote/9X8lu/src/compiler/interface2.jl:0\n [28] loss at ./In[255]:1 [inlined]\n [29] _pullback(::Zygote.Context, ::typeof(loss), ::Array{Int64,1}, ::Array{Int64,1}) at /home/zq/.julia/packages/Zygote/9X8lu/src/compiler/interface2.jl:0\n [30] adjoint at /home/zq/.julia/packages/Zygote/9X8lu/src/lib/lib.jl:191 [inlined]\n [31] _pullback at /home/zq/.julia/packages/ZygoteRules/OjfTt/src/adjoint.jl:57 [inlined]\n [32] #15 at /home/zq/.julia/packages/Flux/goUGu/src/optimise/train.jl:103 [inlined]\n [33] _pullback(::Zygote.Context, ::Flux.Optimise.var\"#15#21\"{typeof(loss),Tuple{Array{Int64,1},Array{Int64,1}}}) at /home/zq/.julia/packages/Zygote/9X8lu/src/compiler/interface2.jl:0\n [34] pullback(::Function, ::Params) at /home/zq/.julia/packages/Zygote/9X8lu/src/compiler/interface.jl:247\n [35] gradient(::Function, ::Params) at /home/zq/.julia/packages/Zygote/9X8lu/src/compiler/interface.jl:58\n [36] macro expansion at /home/zq/.julia/packages/Flux/goUGu/src/optimise/train.jl:102 [inlined]\n [37] macro expansion at /home/zq/.julia/packages/Juno/n6wyj/src/progress.jl:134 [inlined]\n [38] train!(::Function, ::Params, ::DataLoader{Tuple{Array{Int64,1},Array{Int64,1}}}, ::ADAM; cb::Flux.Optimise.var\"#16#22\") at /home/zq/.julia/packages/Flux/goUGu/src/optimise/train.jl:100\n [39] train!(::Function, ::Params, ::DataLoader{Tuple{Array{Int64,1},Array{Int64,1}}}, ::ADAM) at /home/zq/.julia/packages/Flux/goUGu/src/optimise/train.jl:98\n [40] top-level scope at ./timing.jl:174 [inlined]\n [41] top-level scope at ./In[256]:0\n [42] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"}]}]}],"thread_ts":"1616734521.056300","reply_count":2,"reply_users_count":2,"latest_reply":"1616736786.056800","reply_users":["UMY1LV01G","U01977X150R"],"is_locked":false,"subscribed":false},{"client_msg_id":"d3d250af-4b68-47f7-a3ea-15917e59c076","type":"message","text":"Can you provide an MWE that actually has the code that triggers the error, as well as a full stack trace? This isn't enough to go on.","user":"UMY1LV01G","ts":"1616735982.056500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"R8Gu2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can you provide an MWE that actually has the code that triggers the error, as well as a full stack trace? This isn't enough to go on."}]}]}],"thread_ts":"1616734521.056300","parent_user_id":"U01977X150R"},{"client_msg_id":"72357170-3033-4a71-a7df-79e6c4f3bd6d","type":"message","text":"Many thanks , Brian, I just updated the trace and  hope it's clear enough this time.","user":"U01977X150R","ts":"1616736786.056800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0cW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Many thanks , Brian, I just updated the trace and  hope it's clear enough this time."}]}]}],"thread_ts":"1616734521.056300","parent_user_id":"U01977X150R"}]