[{"type":"message","text":"What would be the recommended approach to add a new optimizer to Flux?","user":"U9MD78Z9N","ts":"1612555966.067500","team":"T68168MUP","thread_ts":"1612555966.067500","reply_count":12,"reply_users_count":4,"latest_reply":"1612559419.070400","reply_users":["UH9KWTTD3","U9MD78Z9N","UMY1LV01G","U01724Q3PGW"],"subscribed":false},{"client_msg_id":"DF252317-1B1D-4D84-9BAC-E7DA1F89BA49","type":"message","text":"As a PR or do you mean you want to use a custom optimizer in your code?","user":"UH9KWTTD3","ts":"1612556039.068100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hYlP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"As a PR or do you mean you want to use a custom optimizer in your code?"}]}]}],"thread_ts":"1612555966.067500","parent_user_id":"U9MD78Z9N"},{"type":"message","text":"I would want to share the optimizer but it might depend on existing Julia packaes","user":"U9MD78Z9N","ts":"1612556097.068300","team":"T68168MUP","thread_ts":"1612555966.067500","parent_user_id":"U9MD78Z9N"},{"client_msg_id":"15b878b9-5f2e-4ece-9679-cad019f127df","type":"message","text":"If you can remove all non-Base + stdlib dependencies, <https://github.com/FluxML/Optimisers.jl> is probably the best place going forwards","user":"UMY1LV01G","ts":"1612558309.068500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"z4K","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If you can remove all non-Base + stdlib dependencies, "},{"type":"link","url":"https://github.com/FluxML/Optimisers.jl"},{"type":"text","text":" is probably the best place going forwards"}]}]}],"thread_ts":"1612555966.067500","parent_user_id":"U9MD78Z9N"},{"type":"message","text":"Is the interface an optimzer takes documented?","user":"U9MD78Z9N","ts":"1612558493.068700","team":"T68168MUP","thread_ts":"1612555966.067500","parent_user_id":"U9MD78Z9N"},{"client_msg_id":"01823c25-d9b9-402c-bacc-78229b3381f5","type":"message","text":"It's not documented, but it's pretty simple","user":"UMY1LV01G","ts":"1612558561.068900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nGb1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It's not documented, but it's pretty simple"}]}]}],"thread_ts":"1612555966.067500","parent_user_id":"U9MD78Z9N"},{"client_msg_id":"7a06061f-5007-433b-a300-450e73682f9a","type":"message","text":"e.g. here's plain ol gradient descent: <https://github.com/FluxML/Optimisers.jl/blob/master/src/rules.jl#L11-L16>","user":"UMY1LV01G","ts":"1612558572.069100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Y6/o7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"e.g. here's plain ol gradient descent: "},{"type":"link","url":"https://github.com/FluxML/Optimisers.jl/blob/master/src/rules.jl#L11-L16"}]}]}],"thread_ts":"1612555966.067500","parent_user_id":"U9MD78Z9N"},{"client_msg_id":"305aaa6e-f09a-481c-b6b0-0f6ff1080f82","type":"message","text":"Docs are coming soon (TM)","user":"UMY1LV01G","ts":"1612558580.069300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gPg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Docs are coming soon (TM)"}]}]}],"thread_ts":"1612555966.067500","parent_user_id":"U9MD78Z9N"},{"client_msg_id":"63e44bc7-3b10-474f-b493-460c0321553f","type":"message","text":"Yeah it isn’t documented but it boils down to implementing `init(o::MyOpt, x::AbstractArray)` and `apply(o::MyOpt, x, dx, state) -&gt; dx', state'`.","user":"UH9KWTTD3","ts":"1612558694.069500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jXUKF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah it isn’t documented but it boils down to implementing "},{"type":"text","text":"init(o::MyOpt, x::AbstractArray)","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"apply(o::MyOpt, x, dx, state) -> dx', state'","style":{"code":true}},{"type":"text","text":"."}]}]}],"thread_ts":"1612555966.067500","parent_user_id":"U9MD78Z9N","reactions":[{"name":"+1","users":["UMY1LV01G"],"count":1}]},{"client_msg_id":"1aa0f0b0-d174-4a43-8fba-d04d688be7ca","type":"message","text":"`init` will initialize the state of the optimizer. This includes stuff that changes after each application of the optimizer (e.g. velocity in a `Momentum` optimizer). It won’t include static parameters like learning rate or momentum.","user":"UH9KWTTD3","ts":"1612558770.069700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xgT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"init","style":{"code":true}},{"type":"text","text":" will initialize the state of the optimizer. This includes stuff that changes after each application of the optimizer (e.g. velocity in a "},{"type":"text","text":"Momentum","style":{"code":true}},{"type":"text","text":" optimizer). It won’t include static parameters like learning rate or momentum."}]}]}],"thread_ts":"1612555966.067500","parent_user_id":"U9MD78Z9N"},{"client_msg_id":"ed00472d-b63c-414c-bbad-07cdac6f6967","type":"message","text":"`apply` actually transforms the gradient according to whatever your optimizer rule is. It shouldn’t be a mutating operation. It also updates the state.","user":"UH9KWTTD3","ts":"1612558823.070000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"F3O2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"apply","style":{"code":true}},{"type":"text","text":" actually transforms the gradient according to whatever your optimizer rule is. It shouldn’t be a mutating operation. It also updates the state."}]}]}],"thread_ts":"1612555966.067500","parent_user_id":"U9MD78Z9N"},{"client_msg_id":"4ffb715c-494d-4e02-8516-866e1d78ef9a","type":"message","text":"<@UH9KWTTD3> by non-mutating, you mean that you produce a new optimizer (with new state) on every application?","user":"U01724Q3PGW","ts":"1612559294.070200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"d=R","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UH9KWTTD3"},{"type":"text","text":" by non-mutating, you mean that you produce a new optimizer (with new state) on every application?"}]}]}],"thread_ts":"1612555966.067500","parent_user_id":"U9MD78Z9N"},{"type":"message","text":"dx is the gradient with regard to the loss function?","user":"U9MD78Z9N","ts":"1612559419.070400","team":"T68168MUP","thread_ts":"1612555966.067500","parent_user_id":"U9MD78Z9N"}]