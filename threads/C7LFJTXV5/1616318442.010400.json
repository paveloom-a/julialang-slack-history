[{"client_msg_id":"b18150cf-b551-4fad-9891-b393ab1a82cf","type":"message","text":"Good morning! I wanted to ask, how can I compute the second derivative of the loss(x,y) with respect each parameter of my neural net ?","user":"U01FTFACYJ0","ts":"1616318442.010400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hsJaw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Good morning! I wanted to ask, how can I compute the second derivative of the loss(x,y) with respect each parameter of my neural net ?"}]}]}],"thread_ts":"1616318442.010400","reply_count":6,"reply_users_count":2,"latest_reply":"1616431226.028300","reply_users":["U6YRZ18GZ","U01FTFACYJ0"],"is_locked":false,"subscribed":false},{"client_msg_id":"4e0b8fa7-cccf-4796-9e1c-9dd181cc448f","type":"message","text":"Hi Marcello,\n\nas have been written above, you should be able to do\n`gradient(x -&gt; sum(gradient(x -&gt; loss(x,y), x)[1]), x)`\n\nbut unfortunately the logitcrossentropy is not written to be twice differentiable (techinically it is because of the gradient of softmax), so the above will fail.","user":"U6YRZ18GZ","ts":"1616403056.022600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OKwgC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi Marcello,\n\nas have been written above, you should be able to do\n"},{"type":"text","text":"gradient(x -> sum(gradient(x -> loss(x,y), x)[1]), x)","style":{"code":true}},{"type":"text","text":"\n\nbut unfortunately the logitcrossentropy is not written to be twice differentiable (techinically it is because of the gradient of softmax), so the above will fail."}]}]}],"thread_ts":"1616318442.010400","parent_user_id":"U01FTFACYJ0"},{"client_msg_id":"e0b97491-6d46-4992-8136-5b9889108370","type":"message","text":"But Zygote can handle 2nd order gradient","user":"U6YRZ18GZ","ts":"1616403067.022800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PnQlD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But Zygote can handle 2nd order gradient"}]}]}],"thread_ts":"1616318442.010400","parent_user_id":"U01FTFACYJ0"},{"client_msg_id":"a72885ca-8173-4801-94c3-5e212e77fbd0","type":"message","text":"I'll give a try! thanks a lot!","user":"U01FTFACYJ0","ts":"1616406085.023000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uCay","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'll give a try! thanks a lot!"}]}]}],"thread_ts":"1616318442.010400","parent_user_id":"U01FTFACYJ0"},{"client_msg_id":"228cb96c-9769-4847-aa07-0634b92a0385","type":"message","text":"`ERROR: MethodError: no method matching iterate(::Nothing)`\n\nmy code:\n\n`for x in current_weights #Flux.params(current_layer)[1]`\n      `gradient(x -&gt; sum(gradient(x -&gt; loss(test_X,test_Y), x)[1]), x)`\n `end`","user":"U01FTFACYJ0","ts":"1616411091.027200","team":"T68168MUP","edited":{"user":"U01FTFACYJ0","ts":"1616412379.000000"},"blocks":[{"type":"rich_text","block_id":"IT3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ERROR: MethodError: no method matching iterate(::Nothing)","style":{"code":true}},{"type":"text","text":"\n\nmy code:\n\n"},{"type":"text","text":"for x in current_weights #Flux.params(current_layer)[1]","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"      gradient(x -> sum(gradient(x -> loss(test_X,test_Y), x)[1]), x)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":" end","style":{"code":true}}]}]}],"thread_ts":"1616318442.010400","parent_user_id":"U01FTFACYJ0"},{"client_msg_id":"c51bc477-3eae-4b7e-88c6-eb15e5feb3c2","type":"message","text":"I think you are not indexing properly. It should be more like\n```gradient(x -&gt; sum(gradient(() -&gt; loss(test_X,test_Y), Flux.params([x]))[x]), x)```","user":"U6YRZ18GZ","ts":"1616419844.027900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6gv5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think you are not indexing properly. It should be more like\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"gradient(x -> sum(gradient(() -> loss(test_X,test_Y), Flux.params([x]))[x]), x)"}]}]}],"thread_ts":"1616318442.010400","parent_user_id":"U01FTFACYJ0"},{"client_msg_id":"037fe3eb-d674-4ec9-ab47-9a2a0ebc5e53","type":"message","text":"mmm.. still not working\n\n`ERROR: Only reference types can be differentiated with `Params`.`","user":"U01FTFACYJ0","ts":"1616431226.028300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ZJR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"mmm.. still not working\n\n"},{"type":"text","text":"ERROR: Only reference types can be differentiated with `Params`.","style":{"code":true}}]}]}],"thread_ts":"1616318442.010400","parent_user_id":"U01FTFACYJ0"}]