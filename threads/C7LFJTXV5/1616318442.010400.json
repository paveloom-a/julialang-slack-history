[{"client_msg_id":"b18150cf-b551-4fad-9891-b393ab1a82cf","type":"message","text":"Good morning! I wanted to ask, how can I compute the second derivative of the loss(x,y) with respect each parameter of my neural net ?","user":"U01FTFACYJ0","ts":"1616318442.010400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hsJaw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Good morning! I wanted to ask, how can I compute the second derivative of the loss(x,y) with respect each parameter of my neural net ?"}]}]}],"thread_ts":"1616318442.010400","reply_count":2,"reply_users_count":1,"latest_reply":"1616403067.022800","reply_users":["U6YRZ18GZ"],"subscribed":false},{"client_msg_id":"4e0b8fa7-cccf-4796-9e1c-9dd181cc448f","type":"message","text":"Hi Marcello,\n\nas have been written above, you should be able to do\n`gradient(x -&gt; sum(gradient(x -&gt; loss(x,y), x)[1]), x)`\n\nbut unfortunately the logitcrossentropy is not written to be twice differentiable (techinically it is because of the gradient of softmax), so the above will fail.","user":"U6YRZ18GZ","ts":"1616403056.022600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OKwgC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi Marcello,\n\nas have been written above, you should be able to do\n"},{"type":"text","text":"gradient(x -> sum(gradient(x -> loss(x,y), x)[1]), x)","style":{"code":true}},{"type":"text","text":"\n\nbut unfortunately the logitcrossentropy is not written to be twice differentiable (techinically it is because of the gradient of softmax), so the above will fail."}]}]}],"thread_ts":"1616318442.010400","parent_user_id":"U01FTFACYJ0"},{"client_msg_id":"e0b97491-6d46-4992-8136-5b9889108370","type":"message","text":"But Zygote can handle 2nd order gradient","user":"U6YRZ18GZ","ts":"1616403067.022800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PnQlD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But Zygote can handle 2nd order gradient"}]}]}],"thread_ts":"1616318442.010400","parent_user_id":"U01FTFACYJ0"}]