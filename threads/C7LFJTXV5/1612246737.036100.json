[{"client_msg_id":"39ca300e-7dcb-4812-97ab-93c5b75ec6f1","type":"message","text":"<https://julialang.slack.com/archives/C7120PCUQ/p1612217592004700>","user":"UM70NJEER","ts":"1612246737.036100","team":"T68168MUP","attachments":[{"from_url":"https://julialang.slack.com/archives/C7120PCUQ/p1612217592004700","fallback":"[February 1st, 2021 2:13 PM] lazarus.alon: Hello, it seems that the new LSTM, doesn't allow to put the activation function within the call, as the source code says :`LSTM(in::Integer, out::Integer, σ = tanh),`  any idea how could I achieve the same result. `Chain(LSTM(3,10), Flux.relu, Dense(10,1))` gives problems also because of broadcasting.","ts":"1612217592.004700","author_id":"UM70NJEER","author_subname":"Lazaro","channel_id":"C7120PCUQ","channel_name":"flux","is_msg_unfurl":true,"text":"Hello, it seems that the new LSTM, doesn't allow to put the activation function within the call, as the source code says :`LSTM(in::Integer, out::Integer, σ = tanh),`  any idea how could I achieve the same result. `Chain(LSTM(3,10), Flux.relu, Dense(10,1))` gives problems also because of broadcasting.","author_name":"Lazaro","author_link":"https://julialang.slack.com/team/UM70NJEER","author_icon":"https://avatars.slack-edge.com/2019-10-29/801496800483_d1d644dfbd61c7e4a7f8_48.png","mrkdwn_in":["text"],"id":1,"original_url":"https://julialang.slack.com/archives/C7120PCUQ/p1612217592004700","footer":"Posted in #flux"}],"blocks":[{"type":"rich_text","block_id":"wES","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://julialang.slack.com/archives/C7120PCUQ/p1612217592004700"}]}]}],"thread_ts":"1612246737.036100","reply_count":14,"reply_users_count":2,"latest_reply":"1612370238.050100","reply_users":["UMY1LV01G","UM70NJEER"],"subscribed":false},{"client_msg_id":"1e5b5331-fea0-4812-84e7-bcec4d08cf69","type":"message","text":"Use a closure?\n`Chain(LSTM(3,10), x -&gt; Flux.relu.(x), Dense(10,1))`","user":"UMY1LV01G","ts":"1612248843.036300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pK0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Use a closure?\n"},{"type":"text","text":"Chain(LSTM(3,10), x -> Flux.relu.(x), Dense(10,1))","style":{"code":true}}]}]}],"thread_ts":"1612246737.036100","parent_user_id":"UM70NJEER"},{"client_msg_id":"53995e26-a6f2-4e44-a13d-8ce8f7fee122","type":"message","text":"yes, this works. Although, it looks ugly. The previous syntax looks better.   Thanks.","user":"UM70NJEER","ts":"1612251183.036500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IHkF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yes, this works. Although, it looks ugly. The previous syntax looks better.   Thanks."}]}]}],"thread_ts":"1612246737.036100","parent_user_id":"UM70NJEER"},{"client_msg_id":"98519c0a-fa4f-41f4-b89f-54b6741c42e2","type":"message","text":"Where did you get this \"previous syntax\" from? I looked back and Flux 0.9 through master don't support passing a custom activation function (presumably because it doesn't make sense for an LSTM)","user":"UMY1LV01G","ts":"1612253643.036700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"43u","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Where did you get this \"previous syntax\" from? I looked back and Flux 0.9 through master don't support passing a custom activation function (presumably because it doesn't make sense for an LSTM)"}]}]}],"thread_ts":"1612246737.036100","parent_user_id":"UM70NJEER"},{"client_msg_id":"0d1035e4-4db7-4d41-8e49-406ed45056d0","type":"message","text":"<https://github.com/FluxML/Flux.jl/blob/a8ccc79f61e81d38d3235e53650fe9466693cbf9/src/layers/recurrent.jl#L151-L159>","user":"UM70NJEER","ts":"1612253718.036900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4N5","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/FluxML/Flux.jl/blob/a8ccc79f61e81d38d3235e53650fe9466693cbf9/src/layers/recurrent.jl#L151-L159"}]}]}],"thread_ts":"1612246737.036100","parent_user_id":"UM70NJEER"},{"client_msg_id":"cd3fe3d3-1c05-4cce-800c-12b9852005df","type":"message","text":"Looks like that hasn't been updated since _v0.3.3_...\nEdit: it's also long since been removed from the source, not sure how you got to it in the first place?","user":"UMY1LV01G","ts":"1612253775.037100","team":"T68168MUP","edited":{"user":"UMY1LV01G","ts":"1612254134.000000"},"blocks":[{"type":"rich_text","block_id":"iFC+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Looks like that hasn't been updated since "},{"type":"text","text":"v0.3.3","style":{"italic":true}},{"type":"text","text":"...\nEdit: it's also long since been removed from the source, not sure how you got to it in the first place?"}]}]}],"thread_ts":"1612246737.036100","parent_user_id":"UM70NJEER"},{"client_msg_id":"3fa35980-e2e7-4e3c-bf1c-efe9dce8aae0","type":"message","text":"The least ugly way to do things would be to define a custom RNN cell like <https://github.com/fluxml/Flux.jl/blob/master/src/layers/recurrent.jl#L120-L165>. The built-in implementations are relatively brief","user":"UMY1LV01G","ts":"1612253829.037300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"V6lyI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The least ugly way to do things would be to define a custom RNN cell like "},{"type":"link","url":"https://github.com/fluxml/Flux.jl/blob/master/src/layers/recurrent.jl#L120-L165"},{"type":"text","text":". The built-in implementations are relatively brief"}]}]}],"thread_ts":"1612246737.036100","parent_user_id":"UM70NJEER"},{"client_msg_id":"d197589c-40e4-47b5-84de-ea83e13540fc","type":"message","text":"ref. <https://github.com/FluxML/Flux.jl/issues/411>","user":"UMY1LV01G","ts":"1612254220.037600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"m9bC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ref. "},{"type":"link","url":"https://github.com/FluxML/Flux.jl/issues/411"}]}]}],"thread_ts":"1612246737.036100","parent_user_id":"UM70NJEER"},{"client_msg_id":"3c1a6325-d4c3-4e99-89a3-ac6538b6b3ca","type":"message","text":"mmm.... then doing... `Chain(LSTM(3,10), x -&gt; Flux.relu.(x), Dense(10,1))`  is like applying two activations... because LSTM already has one, given by default. :thinking_face:","user":"UM70NJEER","ts":"1612254399.037800","team":"T68168MUP","edited":{"user":"UM70NJEER","ts":"1612254440.000000"},"blocks":[{"type":"rich_text","block_id":"HKbxC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"mmm.... then doing... "},{"type":"text","text":"Chain(LSTM(3,10), x -> Flux.relu.(x), Dense(10,1))","style":{"code":true}},{"type":"text","text":"  is like applying two activations... because LSTM already has one, given by default. "},{"type":"emoji","name":"thinking_face"}]}]}],"thread_ts":"1612246737.036100","parent_user_id":"UM70NJEER"},{"client_msg_id":"50238cbe-627d-4d62-850a-624660da8e25","type":"message","text":"I suspect the limited choice is because of cuDNN. For example, tf.keras.LSTM only supports the cuDNN path with that exact combination of activations: <https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM>","user":"UMY1LV01G","ts":"1612254730.038200","team":"T68168MUP","attachments":[{"service_name":"TensorFlow","title":"tf.keras.layers.LSTM  |  TensorFlow Core v2.4.1","title_link":"https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM","text":"Long Short-Term Memory layer - Hochreiter 1997.","fallback":"TensorFlow: tf.keras.layers.LSTM  |  TensorFlow Core v2.4.1","from_url":"https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM","service_icon":"https://www.gstatic.com/devrel-devsite/prod/vc5f5097f7e98f45082257ed44f785e23f8176f944afb30dfad7aee218957f132/tensorflow/images/apple-touch-icon-180x180.png","id":1,"original_url":"https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM"}],"blocks":[{"type":"rich_text","block_id":"mGG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I suspect the limited choice is because of cuDNN. For example, tf.keras.LSTM only supports the cuDNN path with that exact combination of activations: "},{"type":"link","url":"https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM"}]}]}],"thread_ts":"1612246737.036100","parent_user_id":"UM70NJEER"},{"client_msg_id":"4c24745c-0a1c-45d6-9096-e4fad90c27d0","type":"message","text":"Now some recent changes have removed the cuDNN dependency, it may be worth revisiting custom activation functions. That's part of a bigger conversation around RNN API design though, so I'll leave said discussion to GitHub","user":"UMY1LV01G","ts":"1612254819.038500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"69uB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Now some recent changes have removed the cuDNN dependency, it may be worth revisiting custom activation functions. That's part of a bigger conversation around RNN API design though, so I'll leave said discussion to GitHub"}]}]}],"thread_ts":"1612246737.036100","parent_user_id":"UM70NJEER"},{"client_msg_id":"8e683f85-853f-4aa5-a00a-9f1972068fbf","type":"message","text":"back to TensorFlow then, again :smile:. Flux is nice, the syntax when possible is really clean, but simple things are still hard or really confusing to get them working fast. Thanks.","user":"UM70NJEER","ts":"1612255176.038700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LuxX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"back to TensorFlow then, again "},{"type":"emoji","name":"smile"},{"type":"text","text":". Flux is nice, the syntax when possible is really clean, but simple things are still hard or really confusing to get them working fast. Thanks."}]}]}],"thread_ts":"1612246737.036100","parent_user_id":"UM70NJEER"},{"client_msg_id":"377f4dac-eadb-4b91-82e3-f12eafec4c12","type":"message","text":"In fairness, PyTorch's LSTM doesn't let you tweak activation functions either.  Though I think tf.Keras does a much better job than Flux here, RNN API design (and anything dealing with variable-length sequences) is still very much an unsolved problem with no satisfying solutions. Maybe that's why people are using transformers nowadays :stuck_out_tongue:","user":"UMY1LV01G","ts":"1612315112.039200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IQ7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In fairness, PyTorch's LSTM doesn't let you tweak activation functions either.  Though I think tf.Keras does a much better job than Flux here, RNN API design (and anything dealing with variable-length sequences) is still very much an unsolved problem with no satisfying solutions. Maybe that's why people are using transformers nowadays "},{"type":"emoji","name":"stuck_out_tongue"}]}]}],"thread_ts":"1612246737.036100","parent_user_id":"UM70NJEER"},{"client_msg_id":"0221ad72-352e-40db-99aa-6fa9664c63bf","type":"message","text":"fair enough, do we have transformers in Flux?","user":"UM70NJEER","ts":"1612368821.048600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UA=s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"fair enough, do we have transformers in Flux?"}]}]}],"thread_ts":"1612246737.036100","parent_user_id":"UM70NJEER"},{"client_msg_id":"54d163d1-c48e-408a-a9f2-1c5f96961059","type":"message","text":"<https://github.com/chengchingwen/Transformers.jl|https://github.com/chengchingwen/Transformers.jl>","user":"UMY1LV01G","ts":"1612370238.050100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"h4hBM","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/chengchingwen/Transformers.jl","text":"https://github.com/chengchingwen/Transformers.jl"}]}]}],"thread_ts":"1612246737.036100","parent_user_id":"UM70NJEER","reactions":[{"name":"100","users":["UM70NJEER"],"count":1}]}]