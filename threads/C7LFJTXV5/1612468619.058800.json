[{"client_msg_id":"7d4f0c16-788d-4e51-b3f4-41e852d6fd9a","type":"message","text":"Hi, I stumbled across some posts on Discourse comparing the speed of Flux with Pytorch. I know there’s been intense development done to Flux, both extending it’s API and accelerating specific operations, but I couldn’t find any recent speed comparisons.","user":"UPM0H43C7","ts":"1612468619.058800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2sEs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I stumbled across some posts on Discourse comparing the speed of Flux with Pytorch. I know there’s been intense development done to Flux, both extending it’s API and accelerating specific operations, but I couldn’t find any recent speed comparisons."}]}]}],"thread_ts":"1612468619.058800","reply_count":39,"reply_users_count":4,"latest_reply":"1612815470.084100","reply_users":["ULG5V164A","UPM0H43C7","UH9KWTTD3","U69BL50BF"],"subscribed":false},{"client_msg_id":"ea24437e-7ee8-445e-80ec-2ba5528839da","type":"message","text":"PyTorch is almost certainly faster on average for the subset of things PyTorch has implemented.","user":"ULG5V164A","ts":"1612471560.058900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7I0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"PyTorch is almost certainly faster on average for the subset of things PyTorch has implemented."}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"9a591b45-99a1-470e-be14-c0bc0c75335b","type":"message","text":"Sure, but how big is the difference? And is there any way to leverage `LoopVectorization` without significantly reworking the code to get a speed-up for CUDA?","user":"UPM0H43C7","ts":"1612471790.059100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"EHDM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Sure, but how big is the difference? And is there any way to leverage "},{"type":"text","text":"LoopVectorization","style":{"code":true}},{"type":"text","text":" without significantly reworking the code to get a speed-up for CUDA?"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"e21acb79-7a0d-43b4-85a6-4109060ff8ed","type":"message","text":"Feel worth testing with <https://github.com/FluxML/Torch.jl|Torch.jl>....","user":"ULG5V164A","ts":"1612472048.059300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Sti2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Feel worth testing with "},{"type":"link","url":"https://github.com/FluxML/Torch.jl","text":"Torch.jl"},{"type":"text","text":"...."}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"5256d697-2c7d-4a77-81f3-143893ed5ab4","type":"message","text":"I’d need to check how I can make it play nice with <#C7T968HRU|diffeq-bridged> eco system","user":"UPM0H43C7","ts":"1612472095.059500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8z7Vy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’d need to check how I can make it play nice with "},{"type":"channel","channel_id":"C7T968HRU"},{"type":"text","text":" eco system"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7","reactions":[{"name":"+1","users":["ULG5V164A"],"count":1}]},{"client_msg_id":"2f9539d7-b0fa-40df-bfc0-520d50a2c78c","type":"message","text":"<https://fluxml.ai/blog/2020/06/29/acclerating-flux-torch.html>","user":"ULG5V164A","ts":"1612472095.059700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"NdkA","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://fluxml.ai/blog/2020/06/29/acclerating-flux-torch.html"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"8d6b4047-ba0f-48fd-af4f-1adba3232f51","type":"message","text":"knew I'd seen that","user":"ULG5V164A","ts":"1612472111.059900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WGbx5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"knew I'd seen that"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"a7d3bdb5-5a9f-4066-8a6b-2643f78c7d9c","type":"message","text":"&gt; knew I’d seen that\nyou mean Torch.jl with DiffEqFlux ?","user":"UPM0H43C7","ts":"1612472152.060200","team":"T68168MUP","edited":{"user":"UPM0H43C7","ts":"1612472172.000000"},"blocks":[{"type":"rich_text","block_id":"euoHs","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"knew I’d seen that"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"you mean Torch.jl with DiffEqFlux ?"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"55ee7340-a950-477f-a546-de3068334f96","type":"message","text":"No, sorry, the benchmarks in the blog post","user":"ULG5V164A","ts":"1612472187.060500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JCu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"No, sorry, the benchmarks in the blog post"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"1fda2e25-c483-471f-8e52-3aa30df8c4d0","type":"message","text":"You want to accelerate a model from DiffEqFlux with LoopVectorization?","user":"UH9KWTTD3","ts":"1612472203.060700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"NtGX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You want to accelerate a model from DiffEqFlux with LoopVectorization?"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"4aec229d-3b4a-42df-abfe-5cd241e9967d","type":"message","text":"I have large neural ODE that’s obscenely slow, and I want to gauge how much more speed I squeeze out","user":"UPM0H43C7","ts":"1612472254.060900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ByS1Z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have large neural ODE that’s obscenely slow, and I want to gauge how much more speed I squeeze out"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"e6e1d3e7-69c6-4715-ba59-b3bb9e0e83d9","type":"message","text":"I’m not at all familiar with profiling with Julia but I see that my volatile GPU utilization hovers at 30% (but the memory is allocated up to 70-80%)","user":"UPM0H43C7","ts":"1612472316.061100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bx2e","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’m not at all familiar with profiling with Julia but I see that my volatile GPU utilization hovers at 30% (but the memory is allocated up to 70-80%)"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"06434be4-9037-4312-bf29-61cff846beac","type":"message","text":"LoopVectorization + Flux is something that’s in the works, but AFAIK, not ready for users yet. <@UAUPJLBQX> can comment on that.","user":"UH9KWTTD3","ts":"1612472327.061300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YFzs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"LoopVectorization + Flux is something that’s in the works, but AFAIK, not ready for users yet. "},{"type":"user","user_id":"UAUPJLBQX"},{"type":"text","text":" can comment on that."}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"6a53b9a6-4b01-4ecf-aa25-65aa6ec7b737","type":"message","text":"btw. any updates on CUDA compatible OneHot?","user":"UPM0H43C7","ts":"1612472485.061500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tgIvj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"btw. any updates on CUDA compatible OneHot?"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"077cca1f-afa6-44a2-ac81-59b57d26a3b3","type":"message","text":"Though I suspect LoopVectorization will not give you the speed bump you are seeking for your problem size. The large memory allocation is something that CUDA + Flux tends to do. Unfortunately I don’t have a good answer for you on how to address that. This issue might help <https://github.com/SciML/DiffEqFlux.jl/pull/387>","user":"UH9KWTTD3","ts":"1612472522.061700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FCAx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Though I suspect LoopVectorization will not give you the speed bump you are seeking for your problem size. The large memory allocation is something that CUDA + Flux tends to do. Unfortunately I don’t have a good answer for you on how to address that. This issue might help "},{"type":"link","url":"https://github.com/SciML/DiffEqFlux.jl/pull/387"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"686f0558-8d2f-4b90-a07b-856ca51fc35d","type":"message","text":"CUDA + OneHot should be compatible on Flux#master (I feel like in general it should have always been compatible, so do you have a specific issue that you faced?).","user":"UH9KWTTD3","ts":"1612472586.061900","team":"T68168MUP","edited":{"user":"UH9KWTTD3","ts":"1612472594.000000"},"blocks":[{"type":"rich_text","block_id":"buPnr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CUDA + OneHot should be compatible on Flux#master (I feel like in general it should have always been compatible, so do you have a specific issue that you faced?)."}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"c964d674-9c70-422d-af8d-4192935f8da2","type":"message","text":"trying to hack an embedding layer so I need a GPU compatible gradient of `A*onehotbatch(x)`. The magic trick using indexing to do that product generally gives rise to race conditions and incorrect gradients. Last I checked there was talk of using gather/scatter with CUDA dispatches to do it correctly and efficiently","user":"UPM0H43C7","ts":"1612472724.062200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TsQU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"trying to hack an embedding layer so I need a GPU compatible gradient of "},{"type":"text","text":"A*onehotbatch(x)","style":{"code":true}},{"type":"text","text":". The magic trick using indexing to do that product generally gives rise to race conditions and incorrect gradients. Last I checked there was talk of using gather/scatter with CUDA dispatches to do it correctly and efficiently"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"1f0b8605-65b1-432e-a021-f3aaabbc566f","type":"message","text":"The gather/scatter stuff is not in NNlib/CUDA yet. I was going to link the PR, but I see you already commented yesterday. But as a consequence, it isn’t in Flux yet either.","user":"UH9KWTTD3","ts":"1612473270.062400","team":"T68168MUP","edited":{"user":"UH9KWTTD3","ts":"1612473317.000000"},"blocks":[{"type":"rich_text","block_id":"ga2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The gather/scatter stuff is not in NNlib/CUDA yet. I was going to link the PR, but I see you already commented yesterday. But as a consequence, it isn’t in Flux yet either."}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"491c0841-f728-4421-86bf-5513ebea82b8","type":"message","text":"yeah. I feel silly asking, but will play nice with CUDA ? I don’t see how that implementation works for CUDA.","user":"UPM0H43C7","ts":"1612473381.062700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7aC4T","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yeah. I feel silly asking, but will play nice with CUDA ? I don’t see how that implementation works for CUDA."}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"f589453d-12be-4b2d-b3c6-4027c02864bc","type":"message","text":"I’m sorry I’m asking so many questions","user":"UPM0H43C7","ts":"1612473391.062900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ARDlE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’m sorry I’m asking so many questions"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"6fc6b836-b6ce-4c12-bc9b-c4b21ee01bfa","type":"message","text":"The gather/scatter operations? I thought that implementation already worked with CUDA.jl, it was just a question of making the PR to integrate it into the repo. Or do you mean the `*(::AbstractMatrix, ::OneHotArray)` implementation?","user":"UH9KWTTD3","ts":"1612473475.063100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1fN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The gather/scatter operations? I thought that implementation already worked with CUDA.jl, it was just a question of making the PR to integrate it into the repo. Or do you mean the "},{"type":"text","text":"*(::AbstractMatrix, ::OneHotArray)","style":{"code":true}},{"type":"text","text":" implementation?"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"dd4ef341-e590-4eb0-8185-871976e7971d","type":"message","text":"&gt; I’m sorry I’m asking so many questions\nNo worries!","user":"UH9KWTTD3","ts":"1612473496.063300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rUo","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"I’m sorry I’m asking so many questions"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nNo worries!"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"43f3a736-5dd4-439a-aa02-4d8eb85b189a","type":"message","text":"gather/scatter. The PR author had special CUDA kernels in their `ScatterNNlib` repo that’s why I was confused","user":"UPM0H43C7","ts":"1612473535.063500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RCZj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"gather/scatter. The PR author had special CUDA kernels in their "},{"type":"text","text":"ScatterNNlib","style":{"code":true}},{"type":"text","text":" repo that’s why I was confused"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"3e9b21e6-2a00-4396-8e2d-2b6b670a02b9","type":"message","text":"Isn’t that what we want? Those CUDA kernels are built using CUDA.jl. The missing piece is moving those specialized kernels to CUDA.jl. Sorry if I’m just missing the question.","user":"UH9KWTTD3","ts":"1612473755.063700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bR/1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Isn’t that what we want? Those CUDA kernels are built using CUDA.jl. The missing piece is moving those specialized kernels to CUDA.jl. Sorry if I’m just missing the question."}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"414c9ba3-40d5-4ba3-afdb-34077ef299ae","type":"message","text":"Oh wait I think I understand. You mean the PR to NNlib isn’t CUDA compatible?","user":"UH9KWTTD3","ts":"1612473910.063900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Zfy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oh wait I think I understand. You mean the PR to NNlib isn’t CUDA compatible?"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"d5bd6613-a095-4b55-b84f-aa2add9a9fb5","type":"message","text":"yes","user":"UPM0H43C7","ts":"1612473932.064100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bL8r","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yes"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"9036ca8b-3c13-4478-8713-7f18602c1000","type":"message","text":"Yeah those kernels will move to CUDA.jl. So the order of PRs is 1) NNlib supports gather/scatter for CPUs, 2) CUDA provides GPU implementations of the NNlib ops (these GPU implementations will the specialized kernels in the ScatterNNlib.jl repo).","user":"UH9KWTTD3","ts":"1612474029.064300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"cg=b=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah those kernels will move to CUDA.jl. So the order of PRs is 1) NNlib supports gather/scatter for CPUs, 2) CUDA provides GPU implementations of the NNlib ops (these GPU implementations will the specialized kernels in the ScatterNNlib.jl repo)."}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"d626ca32-46e9-4254-80b7-04203d786d2a","type":"message","text":"have you played with GPUs, the ODE solver method, and tolerances?","user":"U69BL50BF","ts":"1612476472.064600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sVv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"have you played with GPUs, the ODE solver method, and tolerances?"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"eb3f3b9b-53f2-4bea-925d-cca67ff969a5","type":"message","text":"I played a bit with the tolerances (1e-4 to 1e-8)","user":"UPM0H43C7","ts":"1612476524.064800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1K3uk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I played a bit with the tolerances (1e-4 to 1e-8)"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"97eaae2b-5246-453e-b180-23910972bd53","type":"message","text":"Currently running a medium size RNN model with 250 units. I tried different mini-batch sizes using Tsit5 and forward and adjoint calls take about as long for a batch of 250 and 500","user":"UPM0H43C7","ts":"1612476658.065000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MF5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Currently running a medium size RNN model with 250 units. I tried different mini-batch sizes using Tsit5 and forward and adjoint calls take about as long for a batch of 250 and 500"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"5580236c-937c-4b52-8369-1f4ac57ad451","type":"message","text":"Tsit5 isn't great at 1e-8 tolerances","user":"U69BL50BF","ts":"1612476692.065200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8/GaE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Tsit5 isn't great at 1e-8 tolerances"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"8c573f6a-bf53-4f10-a565-36486e68f418","type":"message","text":"when you say RNN model, what does your RHS look like?","user":"U69BL50BF","ts":"1612476708.065400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Rsi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"when you say RNN model, what does your RHS look like?"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"1c2c4def-8551-423f-9f5f-2b4a7553aec9","type":"message","text":"The model is ‘unusual’  by neural ODE standards, dynamics are\n\n```du_dt = \\sigma( Wi*x(t) +Wr*u + b)```\nwhere `x(t)` is an interpolation that runs on the gpu","user":"UPM0H43C7","ts":"1612476852.065600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"NYTg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The model is ‘unusual’  by neural ODE standards, dynamics are\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"du_dt = \\sigma( Wi*x(t) +Wr*u + b)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"where "},{"type":"text","text":"x(t)","style":{"code":true}},{"type":"text","text":" is an interpolation that runs on the gpu"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"448c23a7-0913-4752-90a2-3ed63554b7d6","type":"message","text":"looks like a neural CDE","user":"U69BL50BF","ts":"1612476891.065800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xun","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"looks like a neural CDE"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"7546a886-84e1-4ada-a2ab-3934b4ac4095","type":"message","text":"when you profile where is all of the time?","user":"U69BL50BF","ts":"1612476897.066000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LCgT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"when you profile where is all of the time?"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"5dcf5595-cd27-4a63-bd07-3d3012e61cb8","type":"message","text":"in a way, yes they’re very similar","user":"UPM0H43C7","ts":"1612476908.066200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Acwvt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"in a way, yes they’re very similar"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"ab27fcdb-dcad-4c67-9abe-93b408760d68","type":"message","text":":disappointed: I haven’t figured out how to use the profiler in Julia. I looked at Tim Holy’s packages and nope’d out","user":"UPM0H43C7","ts":"1612476939.066400","team":"T68168MUP","edited":{"user":"UPM0H43C7","ts":"1612476979.000000"},"blocks":[{"type":"rich_text","block_id":"sKEoA","elements":[{"type":"rich_text_section","elements":[{"type":"emoji","name":"disappointed"},{"type":"text","text":" I haven’t figured out how to use the profiler in Julia. I looked at Tim Holy’s packages and nope’d out"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"a7f32679-9e7e-4388-a3bb-09e92be128a8","type":"message","text":"but I’ll have to give it a closer look now that I failed to submit to ICML","user":"UPM0H43C7","ts":"1612476963.066600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QdJd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"but I’ll have to give it a closer look now that I failed to submit to ICML"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"cb00a719-b5f0-4c69-9542-62438a26b19f","type":"message","text":"<https://www.youtube.com/watch?v=h-xVBD2Pk9o>","user":"U69BL50BF","ts":"1612477052.066900","team":"T68168MUP","attachments":[{"service_name":"YouTube","service_url":"https://www.youtube.com/","title":"Code Profiling and Optimization (in Julia)","title_link":"https://www.youtube.com/watch?v=h-xVBD2Pk9o","author_name":"Parallel Computing and Scientific Machine Learning","author_link":"https://www.youtube.com/channel/UCDtsHjkOEMHYPGgpKX8VOPg","thumb_url":"https://i.ytimg.com/vi/h-xVBD2Pk9o/hqdefault.jpg","thumb_width":480,"thumb_height":360,"fallback":"YouTube Video: Code Profiling and Optimization (in Julia)","video_html":"<iframe width=\"400\" height=\"225\" src=\"https://www.youtube.com/embed/h-xVBD2Pk9o?feature=oembed&autoplay=1&iv_load_policy=3\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","video_html_width":400,"video_html_height":225,"from_url":"https://www.youtube.com/watch?v=h-xVBD2Pk9o","service_icon":"https://a.slack-edge.com/80588/img/unfurl_icons/youtube.png","id":1,"original_url":"https://www.youtube.com/watch?v=h-xVBD2Pk9o"}],"blocks":[{"type":"rich_text","block_id":"4IOj","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://www.youtube.com/watch?v=h-xVBD2Pk9o"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7","reactions":[{"name":"+1","users":["UPM0H43C7"],"count":1}]},{"client_msg_id":"86492d5c-9658-45f7-b643-08c4aaabe98c","type":"message","text":"this has been extremely helpful! thank you very much for the great tutorial","user":"UPM0H43C7","ts":"1612815470.084100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wg7sT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"this has been extremely helpful! thank you very much for the great tutorial"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"}]