[{"client_msg_id":"7d4f0c16-788d-4e51-b3f4-41e852d6fd9a","type":"message","text":"Hi, I stumbled across some posts on Discourse comparing the speed of Flux with Pytorch. I know there’s been intense development done to Flux, both extending it’s API and accelerating specific operations, but I couldn’t find any recent speed comparisons.","user":"UPM0H43C7","ts":"1612468619.058800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2sEs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I stumbled across some posts on Discourse comparing the speed of Flux with Pytorch. I know there’s been intense development done to Flux, both extending it’s API and accelerating specific operations, but I couldn’t find any recent speed comparisons."}]}]}],"thread_ts":"1612468619.058800","reply_count":16,"reply_users_count":3,"latest_reply":"1612472724.062200","reply_users":["ULG5V164A","UPM0H43C7","UH9KWTTD3"],"subscribed":false},{"client_msg_id":"ea24437e-7ee8-445e-80ec-2ba5528839da","type":"message","text":"PyTorch is almost certainly faster on average for the subset of things PyTorch has implemented.","user":"ULG5V164A","ts":"1612471560.058900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7I0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"PyTorch is almost certainly faster on average for the subset of things PyTorch has implemented."}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"9a591b45-99a1-470e-be14-c0bc0c75335b","type":"message","text":"Sure, but how big is the difference? And is there any way to leverage `LoopVectorization` without significantly reworking the code to get a speed-up for CUDA?","user":"UPM0H43C7","ts":"1612471790.059100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"EHDM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Sure, but how big is the difference? And is there any way to leverage "},{"type":"text","text":"LoopVectorization","style":{"code":true}},{"type":"text","text":" without significantly reworking the code to get a speed-up for CUDA?"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"e21acb79-7a0d-43b4-85a6-4109060ff8ed","type":"message","text":"Feel worth testing with <https://github.com/FluxML/Torch.jl|Torch.jl>....","user":"ULG5V164A","ts":"1612472048.059300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Sti2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Feel worth testing with "},{"type":"link","url":"https://github.com/FluxML/Torch.jl","text":"Torch.jl"},{"type":"text","text":"...."}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"5256d697-2c7d-4a77-81f3-143893ed5ab4","type":"message","text":"I’d need to check how I can make it play nice with <#C7T968HRU|diffeq-bridged> eco system","user":"UPM0H43C7","ts":"1612472095.059500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8z7Vy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’d need to check how I can make it play nice with "},{"type":"channel","channel_id":"C7T968HRU"},{"type":"text","text":" eco system"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7","reactions":[{"name":"+1","users":["ULG5V164A"],"count":1}]},{"client_msg_id":"2f9539d7-b0fa-40df-bfc0-520d50a2c78c","type":"message","text":"<https://fluxml.ai/blog/2020/06/29/acclerating-flux-torch.html>","user":"ULG5V164A","ts":"1612472095.059700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"NdkA","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://fluxml.ai/blog/2020/06/29/acclerating-flux-torch.html"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"8d6b4047-ba0f-48fd-af4f-1adba3232f51","type":"message","text":"knew I'd seen that","user":"ULG5V164A","ts":"1612472111.059900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WGbx5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"knew I'd seen that"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"a7d3bdb5-5a9f-4066-8a6b-2643f78c7d9c","type":"message","text":"&gt; knew I’d seen that\nyou mean Torch.jl with DiffEqFlux ?","user":"UPM0H43C7","ts":"1612472152.060200","team":"T68168MUP","edited":{"user":"UPM0H43C7","ts":"1612472172.000000"},"blocks":[{"type":"rich_text","block_id":"euoHs","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"knew I’d seen that"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"you mean Torch.jl with DiffEqFlux ?"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"55ee7340-a950-477f-a546-de3068334f96","type":"message","text":"No, sorry, the benchmarks in the blog post","user":"ULG5V164A","ts":"1612472187.060500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JCu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"No, sorry, the benchmarks in the blog post"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"1fda2e25-c483-471f-8e52-3aa30df8c4d0","type":"message","text":"You want to accelerate a model from DiffEqFlux with LoopVectorization?","user":"UH9KWTTD3","ts":"1612472203.060700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"NtGX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You want to accelerate a model from DiffEqFlux with LoopVectorization?"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"4aec229d-3b4a-42df-abfe-5cd241e9967d","type":"message","text":"I have large neural ODE that’s obscenely slow, and I want to gauge how much more speed I squeeze out","user":"UPM0H43C7","ts":"1612472254.060900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ByS1Z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have large neural ODE that’s obscenely slow, and I want to gauge how much more speed I squeeze out"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"e6e1d3e7-69c6-4715-ba59-b3bb9e0e83d9","type":"message","text":"I’m not at all familiar with profiling with Julia but I see that my volatile GPU utilization hovers at 30% (but the memory is allocated up to 70-80%)","user":"UPM0H43C7","ts":"1612472316.061100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bx2e","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’m not at all familiar with profiling with Julia but I see that my volatile GPU utilization hovers at 30% (but the memory is allocated up to 70-80%)"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"06434be4-9037-4312-bf29-61cff846beac","type":"message","text":"LoopVectorization + Flux is something that’s in the works, but AFAIK, not ready for users yet. <@UAUPJLBQX> can comment on that.","user":"UH9KWTTD3","ts":"1612472327.061300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YFzs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"LoopVectorization + Flux is something that’s in the works, but AFAIK, not ready for users yet. "},{"type":"user","user_id":"UAUPJLBQX"},{"type":"text","text":" can comment on that."}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"6a53b9a6-4b01-4ecf-aa25-65aa6ec7b737","type":"message","text":"btw. any updates on CUDA compatible OneHot?","user":"UPM0H43C7","ts":"1612472485.061500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tgIvj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"btw. any updates on CUDA compatible OneHot?"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"077cca1f-afa6-44a2-ac81-59b57d26a3b3","type":"message","text":"Though I suspect LoopVectorization will not give you the speed bump you are seeking for your problem size. The large memory allocation is something that CUDA + Flux tends to do. Unfortunately I don’t have a good answer for you on how to address that. This issue might help <https://github.com/SciML/DiffEqFlux.jl/pull/387>","user":"UH9KWTTD3","ts":"1612472522.061700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FCAx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Though I suspect LoopVectorization will not give you the speed bump you are seeking for your problem size. The large memory allocation is something that CUDA + Flux tends to do. Unfortunately I don’t have a good answer for you on how to address that. This issue might help "},{"type":"link","url":"https://github.com/SciML/DiffEqFlux.jl/pull/387"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"686f0558-8d2f-4b90-a07b-856ca51fc35d","type":"message","text":"CUDA + OneHot should be compatible on Flux#master (I feel like in general it should have always been compatible, so do you have a specific issue that you faced?).","user":"UH9KWTTD3","ts":"1612472586.061900","team":"T68168MUP","edited":{"user":"UH9KWTTD3","ts":"1612472594.000000"},"blocks":[{"type":"rich_text","block_id":"buPnr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CUDA + OneHot should be compatible on Flux#master (I feel like in general it should have always been compatible, so do you have a specific issue that you faced?)."}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"},{"client_msg_id":"c964d674-9c70-422d-af8d-4192935f8da2","type":"message","text":"trying to hack an embedding layer so I need a GPU compatible gradient of `A*onehotbatch(x)`. The magic trick using indexing to do that product generally gives rise to race conditions and incorrect gradients. Last I checked there was talk of using gather/scatter with CUDA dispatches to do it correctly and efficiently","user":"UPM0H43C7","ts":"1612472724.062200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TsQU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"trying to hack an embedding layer so I need a GPU compatible gradient of "},{"type":"text","text":"A*onehotbatch(x)","style":{"code":true}},{"type":"text","text":". The magic trick using indexing to do that product generally gives rise to race conditions and incorrect gradients. Last I checked there was talk of using gather/scatter with CUDA dispatches to do it correctly and efficiently"}]}]}],"thread_ts":"1612468619.058800","parent_user_id":"UPM0H43C7"}]