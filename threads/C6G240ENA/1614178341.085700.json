[{"client_msg_id":"1929b266-aa1e-41db-ac44-3fc4fbaec7f6","type":"message","text":"Is this (below) an accurate view of one of the the differences with Julia's autodiff ecosystem and Python's autodiff frameworks?\n\nIt seems to me likes long as your code is written in normal Julia code, the Julia autodiff packages will probably work(?) at doing the autodiff in ways you want! Which leads to things like being able to backprop through other Julia packages like powerful differential equation solvers. \n\nWhereas in Python, with Tensorflow and Pytorch, you have to build the computational graph using the objects made for that purpose with knowing in mind in advance that you want to autodiff.\n\nIf anyone has some some other significant differences, feel free to suggest! I would love to look into them","user":"U0138UTB7A4","ts":"1614178341.085700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"v3=Rt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is this (below) an accurate view of one of the the differences with Julia's autodiff ecosystem and Python's autodiff frameworks?\n\nIt seems to me likes long as your code is written in normal Julia code, the Julia autodiff packages will probably work(?) at doing the autodiff in ways you want! Which leads to things like being able to backprop through other Julia packages like powerful differential equation solvers. \n\nWhereas in Python, with Tensorflow and Pytorch, you have to build the computational graph using the objects made for that purpose with knowing in mind in advance that you want to autodiff.\n\nIf anyone has some some other significant differences, feel free to suggest! I would love to look into them"}]}]}],"thread_ts":"1614178341.085700","reply_count":2,"reply_users_count":1,"latest_reply":"1614178816.086100","reply_users":["U6A936746"],"subscribed":false},{"client_msg_id":"d466506d-83fc-4061-b8bb-d129046312c2","type":"message","text":"Don’t talk about TensorFlow 2.0 or Pytorch using the word Computational Graph.\nWhile technically applicable to dynmaic, it does have strong connections to the explict graph structure TensorFlow. 1.0 used","user":"U6A936746","ts":"1614178528.085900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vsmW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Don’t talk about TensorFlow 2.0 or Pytorch using the word Computational Graph.\nWhile technically applicable to dynmaic, it does have strong connections to the explict graph structure TensorFlow. 1.0 used"}]}]}],"thread_ts":"1614178341.085700","parent_user_id":"U0138UTB7A4"},{"client_msg_id":"6de03741-368d-4d5a-a981-88a62462846a","type":"message","text":"I would say yes, in Pytorch and TensorFlow the code does need to be written with AD specifically in mind, useful functions from those or made for those frameworks.\nRather than just what ever libraries you want.\nSome might work through duck-typing. Those that that have a C backend (etc) won’t. Which is most serious computational packages in python.\n\nOn the upside: as long as you stay inside that walled garden most things are well tested and used by a tone of people, so  you won’t run into many sharp edges.\n\nOn the significant downside, many things are outside of that walled garden.\nIncluding efficient ways to solve small systems.\nLike things other than traditional neural networks.\n(E.g. compare torchdiffeq vs DiffEqFlux. On LokaVoltra, DIffEqFlux has solved the problem in less time that it took to even run a single forward pass in torchdiffeq)","user":"U6A936746","ts":"1614178816.086100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DEE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I would say yes, in Pytorch and TensorFlow the code does need to be written with AD specifically in mind, useful functions from those or made for those frameworks.\nRather than just what ever libraries you want.\nSome might work through duck-typing. Those that that have a C backend (etc) won’t. Which is most serious computational packages in python.\n\nOn the upside: as long as you stay inside that walled garden most things are well tested and used by a tone of people, so  you won’t run into many sharp edges.\n\nOn the significant downside, many things are outside of that walled garden.\nIncluding efficient ways to solve small systems.\nLike things other than traditional neural networks.\n(E.g. compare torchdiffeq vs DiffEqFlux. On LokaVoltra, DIffEqFlux has solved the problem in less time that it took to even run a single forward pass in torchdiffeq)"}]}]}],"thread_ts":"1614178341.085700","parent_user_id":"U0138UTB7A4"}]