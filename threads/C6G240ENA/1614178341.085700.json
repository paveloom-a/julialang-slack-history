[{"client_msg_id":"1929b266-aa1e-41db-ac44-3fc4fbaec7f6","type":"message","text":"Is this (below) an accurate view of one of the the differences with Julia's autodiff ecosystem and Python's autodiff frameworks?\n\nIt seems to me likes long as your code is written in normal Julia code, the Julia autodiff packages will probably work(?) at doing the autodiff in ways you want! Which leads to things like being able to backprop through other Julia packages like powerful differential equation solvers. \n\nWhereas in Python, with Tensorflow and Pytorch, you have to build the computational graph using the objects made for that purpose with knowing in mind in advance that you want to autodiff.\n\nIf anyone has some some other significant differences, feel free to suggest! I would love to look into them","user":"U0138UTB7A4","ts":"1614178341.085700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"v3=Rt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is this (below) an accurate view of one of the the differences with Julia's autodiff ecosystem and Python's autodiff frameworks?\n\nIt seems to me likes long as your code is written in normal Julia code, the Julia autodiff packages will probably work(?) at doing the autodiff in ways you want! Which leads to things like being able to backprop through other Julia packages like powerful differential equation solvers. \n\nWhereas in Python, with Tensorflow and Pytorch, you have to build the computational graph using the objects made for that purpose with knowing in mind in advance that you want to autodiff.\n\nIf anyone has some some other significant differences, feel free to suggest! I would love to look into them"}]}]}],"thread_ts":"1614178341.085700","reply_count":5,"reply_users_count":3,"latest_reply":"1614200479.101800","reply_users":["U6A936746","U01CQTKB86N","U0138UTB7A4"],"subscribed":false},{"client_msg_id":"d466506d-83fc-4061-b8bb-d129046312c2","type":"message","text":"Don’t talk about TensorFlow 2.0 or Pytorch using the word Computational Graph.\nWhile technically applicable to dynmaic, it does have strong connections to the explict graph structure TensorFlow. 1.0 used","user":"U6A936746","ts":"1614178528.085900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vsmW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Don’t talk about TensorFlow 2.0 or Pytorch using the word Computational Graph.\nWhile technically applicable to dynmaic, it does have strong connections to the explict graph structure TensorFlow. 1.0 used"}]}]}],"thread_ts":"1614178341.085700","parent_user_id":"U0138UTB7A4"},{"client_msg_id":"6de03741-368d-4d5a-a981-88a62462846a","type":"message","text":"I would say yes, in Pytorch and TensorFlow the code does need to be written with AD specifically in mind, useful functions from those or made for those frameworks.\nRather than just what ever libraries you want.\nSome might work through duck-typing. Those that that have a C backend (etc) won’t. Which is most serious computational packages in python.\n\nOn the upside: as long as you stay inside that walled garden most things are well tested and used by a ton of people, so  you won’t run into many sharp edges.\n\nOn the significant downside, many things are outside of that walled garden.\nIncluding efficient ways to solve small systems.\nLike things other than traditional neural networks.\n(E.g. compare torchdiffeq vs DiffEqFlux. On LokaVoltra, DIffEqFlux has solved the problem in less time that it took to even run a single forward pass in torchdiffeq)","user":"U6A936746","ts":"1614178816.086100","team":"T68168MUP","edited":{"user":"U6A936746","ts":"1614185519.000000"},"blocks":[{"type":"rich_text","block_id":"Gjq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I would say yes, in Pytorch and TensorFlow the code does need to be written with AD specifically in mind, useful functions from those or made for those frameworks.\nRather than just what ever libraries you want.\nSome might work through duck-typing. Those that that have a C backend (etc) won’t. Which is most serious computational packages in python.\n\nOn the upside: as long as you stay inside that walled garden most things are well tested and used by a ton of people, so  you won’t run into many sharp edges.\n\nOn the significant downside, many things are outside of that walled garden.\nIncluding efficient ways to solve small systems.\nLike things other than traditional neural networks.\n(E.g. compare torchdiffeq vs DiffEqFlux. On LokaVoltra, DIffEqFlux has solved the problem in less time that it took to even run a single forward pass in torchdiffeq)"}]}]}],"thread_ts":"1614178341.085700","parent_user_id":"U0138UTB7A4","reactions":[{"name":"+1","users":["USBKT1275"],"count":1}]},{"client_msg_id":"174b28db-5fba-4687-be72-17cdcd77c604","type":"message","text":"An excellent question! (and answer)","user":"U01CQTKB86N","ts":"1614185394.086300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dIGV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"An excellent question! (and answer)"}]}]}],"thread_ts":"1614178341.085700","parent_user_id":"U0138UTB7A4","reactions":[{"name":"heart","users":["U0138UTB7A4"],"count":1}]},{"client_msg_id":"6e4b62ac-2e74-4065-a1ed-1950fadd6266","type":"message","text":"Thank you for the response!! That helps me get an insight on limitations in Python and how incredible Julia is!\n\nFirst I thought all of AD was magic. I thought that all of AD used Dual Numbers. Then I learned what Reverse Mode AD was and realized I've actually seen and used reverse mode AD in Python before. Then I realized that in Julia you can AD anything/most things!! Now it is even more magical haha\n\nThough I don't quite understand what you meant by computational graphs in TF2.0 and PyTorch. I had thought that all of AD had to somehow construct the graph to be able to AD through it. But I guess there are other techniques!","user":"U0138UTB7A4","ts":"1614197934.101300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RuR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thank you for the response!! That helps me get an insight on limitations in Python and how incredible Julia is!\n\nFirst I thought all of AD was magic. I thought that all of AD used Dual Numbers. Then I learned what Reverse Mode AD was and realized I've actually seen and used reverse mode AD in Python before. Then I realized that in Julia you can AD anything/most things!! Now it is even more magical haha\n\nThough I don't quite understand what you meant by computational graphs in TF2.0 and PyTorch. I had thought that all of AD had to somehow construct the graph to be able to AD through it. But I guess there are other techniques!"}]}]}],"thread_ts":"1614178341.085700","parent_user_id":"U0138UTB7A4"},{"client_msg_id":"4265e7f8-d29e-4484-9038-48976dfa05d1","type":"message","text":"Consider:\n```function my_pow(x, n)\n     net = x\n     for i in 1:n-1\n        net *= x\n     end\n     return net\nend```\nThat can’t construct a static computational graph. (and as a rule tensorflow people mean *static* computational graph when they say computational graph).\nBecause at compile time you don’t know how many iterations of the loop will occur.\nSince it depends on the variable `n`.\nSo you can’t write out the computational structure of all operations that occur\n(You actually can’t construct this in TensorFlow 1.0 without using some of the limitted dynamic extensions. I think this also causes problems for symbolic differentiation for similar reasons.).\n\nWhat you can do is create a tape (sometimes the word trace is used interchangably, occationally it is used slightly differently).\nA record of every operation that is done.\nAnd that is all we need.\nInfact that  will generally be simpler than a graph.\nConsider\n```function double_pos_square_neg(x)\n    y = if x &gt; 0\n        2*x\n    else  # x &lt;= 0\n        x^2\n    end\n    return y\nend```\nHere we don’t need to generate the full graph, if the input is `x=1` since we don’t care what happens in the `x&lt;0` branch.\nWe just need the instructions on the branch that was taken, since the other branch can’t affect our derivative since it didn’t occur.\nOut tape thus doesn’t have to record it.\n\nTo do a reverse mode AD the we need to walk that tape backwards “pullingback” the derivative through each operation that was recorded.\nHere is video of me explaining both forard then reverse in like 5 min\n<https://youtu.be/B4NfkkkJ7rs?t=119>\nhere is slides <https://raw.githack.com/oxinabox/ChainRulesJuliaCon2020/main/out/build/index.html#6>\n\nHere is a very small reverse mode AD using ChainRules\n<https://juliadiff.org/ChainRulesCore.jl/stable/autodiff/operator_overloading.html#ReverseDiffZero>\nSee it constructs the tape recording how to pullback each operation that was preformed on the tracked variable, then it triggers them in sequence the propagate that gradient back until it is for the right variables.\n\n(It does get blurry, Zygote will actually work out the code for the other branch, but won’t use it)","user":"U6A936746","ts":"1614200479.101800","team":"T68168MUP","attachments":[{"service_name":"YouTube","service_url":"https://www.youtube.com/","title":"JuliaCon 2020 | ChainRules.jl | Lyndon White","title_link":"https://youtu.be/B4NfkkkJ7rs?t=119","author_name":"The Julia Programming Language","author_link":"https://www.youtube.com/user/JuliaLanguage","thumb_url":"https://i.ytimg.com/vi/B4NfkkkJ7rs/hqdefault.jpg","thumb_width":480,"thumb_height":360,"fallback":"YouTube Video: JuliaCon 2020 | ChainRules.jl | Lyndon White","video_html":"<iframe width=\"400\" height=\"225\" src=\"https://www.youtube.com/embed/B4NfkkkJ7rs?start=119&feature=oembed&start=119&autoplay=1&iv_load_policy=3\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","video_html_width":400,"video_html_height":225,"from_url":"https://youtu.be/B4NfkkkJ7rs?t=119","service_icon":"https://a.slack-edge.com/80588/img/unfurl_icons/youtube.png","id":1,"original_url":"https://youtu.be/B4NfkkkJ7rs?t=119"}],"blocks":[{"type":"rich_text","block_id":"1XS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Consider:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function my_pow(x, n)\n     net = x\n     for i in 1:n-1\n        net *= x\n     end\n     return net\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nThat can’t construct a static computational graph. (and as a rule tensorflow people mean "},{"type":"text","text":"static","style":{"bold":true}},{"type":"text","text":" computational graph when they say computational graph).\nBecause at compile time you don’t know how many iterations of the loop will occur.\nSince it depends on the variable "},{"type":"text","text":"n","style":{"code":true}},{"type":"text","text":".\nSo you can’t write out the computational structure of all operations that occur\n(You actually can’t construct this in TensorFlow 1.0 without using some of the limitted dynamic extensions. I think this also causes problems for symbolic differentiation for similar reasons.).\n\nWhat you can do is create a tape (sometimes the word trace is used interchangably, occationally it is used slightly differently).\nA record of every operation that is done.\nAnd that is all we need.\nInfact that  will generally be simpler than a graph.\nConsider\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function double_pos_square_neg(x)\n    y = if x > 0\n        2*x\n    else  # x <= 0\n        x^2\n    end\n    return y\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nHere we don’t need to generate the full graph, if the input is "},{"type":"text","text":"x=1","style":{"code":true}},{"type":"text","text":" since we don’t care what happens in the "},{"type":"text","text":"x<0","style":{"code":true}},{"type":"text","text":" branch.\nWe just need the instructions on the branch that was taken, since the other branch can’t affect our derivative since it didn’t occur.\nOut tape thus doesn’t have to record it.\n\nTo do a reverse mode AD the we need to walk that tape backwards “pullingback” the derivative through each operation that was recorded.\nHere is video of me explaining both forard then reverse in like 5 min\n"},{"type":"link","url":"https://youtu.be/B4NfkkkJ7rs?t=119"},{"type":"text","text":"\nhere is slides "},{"type":"link","url":"https://raw.githack.com/oxinabox/ChainRulesJuliaCon2020/main/out/build/index.html#6"},{"type":"text","text":"\n\nHere is a very small reverse mode AD using ChainRules\n"},{"type":"link","url":"https://juliadiff.org/ChainRulesCore.jl/stable/autodiff/operator_overloading.html#ReverseDiffZero"},{"type":"text","text":"\nSee it constructs the tape recording how to pullback each operation that was preformed on the tracked variable, then it triggers them in sequence the propagate that gradient back until it is for the right variables.\n\n(It does get blurry, Zygote will actually work out the code for the other branch, but won’t use it)"}]}]}],"thread_ts":"1614178341.085700","parent_user_id":"U0138UTB7A4"}]