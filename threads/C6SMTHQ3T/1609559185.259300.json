[{"client_msg_id":"45b51b37-ec4c-48b0-a9c1-1c1574d2ebf3","type":"message","text":"Anyone have experience dealing with threading overhead? I'm wondering if there's some API I'm not aware of that could make it actually feasible to compete with MKL for small matrices.\nCreating tasks seems to be faster than using channels.\n<https://github.com/JuliaLinearAlgebra/Octavian.jl/issues/24#issuecomment-753420485>","user":"UAUPJLBQX","ts":"1609559185.259300","team":"T68168MUP","edited":{"user":"UAUPJLBQX","ts":"1609559225.000000"},"blocks":[{"type":"rich_text","block_id":"mKtdO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Anyone have experience dealing with threading overhead? I'm wondering if there's some API I'm not aware of that could make it actually feasible to compete with MKL for small matrices.\nCreating tasks seems to be faster than using channels.\n"},{"type":"link","url":"https://github.com/JuliaLinearAlgebra/Octavian.jl/issues/24#issuecomment-753420485"}]}]}],"thread_ts":"1609559185.259300","reply_count":11,"reply_users_count":3,"latest_reply":"1610336891.291300","reply_users":["UC6SUUPRC","UAUPJLBQX","UBFATSCKU"],"subscribed":false,"reactions":[{"name":"+1","users":["U7THT3TM3","U881D0W2C"],"count":2}]},{"type":"message","subtype":"thread_broadcast","text":"I’m quite interested in this approach but I’m wondering what’s the difference between `@spawn` and this approach, should one always call that C API directly to get minimal threading overhead?","user":"UC6SUUPRC","ts":"1610055133.278000","thread_ts":"1609559185.259300","root":{"client_msg_id":"45b51b37-ec4c-48b0-a9c1-1c1574d2ebf3","type":"message","text":"Anyone have experience dealing with threading overhead? I'm wondering if there's some API I'm not aware of that could make it actually feasible to compete with MKL for small matrices.\nCreating tasks seems to be faster than using channels.\n<https://github.com/JuliaLinearAlgebra/Octavian.jl/issues/24#issuecomment-753420485>","user":"UAUPJLBQX","ts":"1609559185.259300","team":"T68168MUP","edited":{"user":"UAUPJLBQX","ts":"1609559225.000000"},"blocks":[{"type":"rich_text","block_id":"mKtdO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Anyone have experience dealing with threading overhead? I'm wondering if there's some API I'm not aware of that could make it actually feasible to compete with MKL for small matrices.\nCreating tasks seems to be faster than using channels.\n"},{"type":"link","url":"https://github.com/JuliaLinearAlgebra/Octavian.jl/issues/24#issuecomment-753420485"}]}]}],"thread_ts":"1609559185.259300","reply_count":11,"reply_users_count":3,"latest_reply":"1610336891.291300","reply_users":["UC6SUUPRC","UAUPJLBQX","UBFATSCKU"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"Mk3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’m quite interested in this approach but I’m wondering what’s the difference between "},{"type":"text","text":"@spawn","style":{"code":true}},{"type":"text","text":" and this approach, should one always call that C API directly to get minimal threading overhead?"}]}]}],"client_msg_id":"130e3d74-e79f-4f2b-887f-680b920318a6"},{"client_msg_id":"4a9a60c4-6630-4e50-85c1-ef94e6f1c35e","type":"message","text":"MKL exceeds 300 GFLOPS for 100x100 matrices on my cascadelake desktop, that puts the matmul at around 6.5 microseconds.\nThat is many times faster than just `@benchmark wait(Threads.@spawn(() -&gt; nothing))`, on a different computer:\n```julia&gt; @benchmark wait(Threads.@spawn(() -&gt; nothing))\nBenchmarkTools.Trial:\n  memory estimate:  416 bytes\n  allocs estimate:  4\n  --------------\n  minimum time:     1.418 μs (0.00% GC)\n  median time:      42.268 μs (0.00% GC)\n  mean time:        40.971 μs (0.00% GC)\n  maximum time:     65.329 μs (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     1```\n40 microseond mean or 42 median for a trivial task that does no work.\nVersus 6.5 microseconds to perform the entire matmul.\n\nSo I don't really know what I can do, but `@spawn` is definitely not it.","user":"UAUPJLBQX","ts":"1610095343.287400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"NYQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"MKL exceeds 300 GFLOPS for 100x100 matrices on my cascadelake desktop, that puts the matmul at around 6.5 microseconds.\nThat is many times faster than just "},{"type":"text","text":"@benchmark wait(Threads.@spawn(() -> nothing))","style":{"code":true}},{"type":"text","text":", on a different computer:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> @benchmark wait(Threads.@spawn(() -> nothing))\nBenchmarkTools.Trial:\n  memory estimate:  416 bytes\n  allocs estimate:  4\n  --------------\n  minimum time:     1.418 μs (0.00% GC)\n  median time:      42.268 μs (0.00% GC)\n  mean time:        40.971 μs (0.00% GC)\n  maximum time:     65.329 μs (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     1"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"40 microseond mean or 42 median for a trivial task that does no work.\nVersus 6.5 microseconds to perform the entire matmul.\n\nSo I don't really know what I can do, but "},{"type":"text","text":"@spawn","style":{"code":true}},{"type":"text","text":" is definitely not it."}]}]}],"thread_ts":"1609559185.259300","parent_user_id":"UAUPJLBQX"},{"client_msg_id":"1453a736-5e4e-49bc-8f29-537b33094346","type":"message","text":"The low level approach I was taking was hoping to try and bypass the scheduler, by pinning the tasks to a specific thread, like `Threads.@threads` <https://github.com/JuliaLang/julia/blob/83bee67631bc3d532b6b0bc47b02753ad5865673/base/threadingconstructs.jl#L21>","user":"UAUPJLBQX","ts":"1610095667.287600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lo7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The low level approach I was taking was hoping to try and bypass the scheduler, by pinning the tasks to a specific thread, like "},{"type":"text","text":"Threads.@threads","style":{"code":true}},{"type":"text","text":" "},{"type":"link","url":"https://github.com/JuliaLang/julia/blob/83bee67631bc3d532b6b0bc47b02753ad5865673/base/threadingconstructs.jl#L21"}]}]}],"thread_ts":"1609559185.259300","parent_user_id":"UAUPJLBQX"},{"client_msg_id":"d6504e67-6f3e-405f-ad36-6677b0b739af","type":"message","text":"I see, so I guess if there is a static schedule known it’s best not to use Julia’s?","user":"UC6SUUPRC","ts":"1610125066.288200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5l7/z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I see, so I guess if there is a static schedule known it’s best not to use Julia’s?"}]}]}],"thread_ts":"1609559185.259300","parent_user_id":"UAUPJLBQX"},{"client_msg_id":"39e402a4-2605-4567-8a7f-17a533a8bdf4","type":"message","text":"For comparison, same computer as the `wait(Threads.@spawn(() -&gt; nothing))`:\n```julia&gt; @benchmark wait(PaddedMatrices.runfunc(() -&gt; nothing, 2))\nBenchmarkTools.Trial:\n  memory estimate:  476 bytes\n  allocs estimate:  5\n  --------------\n  minimum time:     2.067 μs (0.00% GC)\n  median time:      2.640 μs (0.00% GC)\n  mean time:        2.644 μs (0.00% GC)\n  maximum time:     8.880 μs (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     9```\n(Requires PaddedMatrices master. I'll probably make a release this weekend.)","user":"UAUPJLBQX","ts":"1610125815.288400","team":"T68168MUP","edited":{"user":"UAUPJLBQX","ts":"1610126646.000000"},"blocks":[{"type":"rich_text","block_id":"ZIC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"For comparison, same computer as the "},{"type":"text","text":"wait(Threads.@spawn(() -> nothing))","style":{"code":true}},{"type":"text","text":":\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> @benchmark wait(PaddedMatrices.runfunc(() -> nothing, 2))\nBenchmarkTools.Trial:\n  memory estimate:  476 bytes\n  allocs estimate:  5\n  --------------\n  minimum time:     2.067 μs (0.00% GC)\n  median time:      2.640 μs (0.00% GC)\n  mean time:        2.644 μs (0.00% GC)\n  maximum time:     8.880 μs (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     9"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"(Requires PaddedMatrices master. I'll probably make a release this weekend.)"}]}]}],"thread_ts":"1609559185.259300","parent_user_id":"UAUPJLBQX"},{"client_msg_id":"a00afd43-9d11-4c41-a626-4fc7c04e7e6d","type":"message","text":"The `2` also is the id of the thread it runs on. I figured it'd probably help when starting a lot of tasks to pin each to their own, separate, thread.","user":"UAUPJLBQX","ts":"1610125889.288600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Q0B","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The "},{"type":"text","text":"2","style":{"code":true}},{"type":"text","text":" also is the id of the thread it runs on. I figured it'd probably help when starting a lot of tasks to pin each to their own, separate, thread."}]}]}],"thread_ts":"1609559185.259300","parent_user_id":"UAUPJLBQX"},{"client_msg_id":"9bb62533-a61a-40cd-9025-6a41a5fe2a7b","type":"message","text":"Should I make a PR to add a `Threads.@spawnat` macro (doing the above)?\nWhy isn't there one already?","user":"UAUPJLBQX","ts":"1610126593.288800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oIt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Should I make a PR to add a "},{"type":"text","text":"Threads.@spawnat","style":{"code":true}},{"type":"text","text":" macro (doing the above)?\nWhy isn't there one already?"}]}]}],"thread_ts":"1609559185.259300","parent_user_id":"UAUPJLBQX"},{"client_msg_id":"f721c459-ca99-4e6c-8e3e-633c7cd87de5","type":"message","text":"so if I understand correctly, `PaddedMatrices.runfunc(() -&gt; nothing, 2)` runs a task on a specified thread (which is `2` in this case) instead of schedule that at runtime, so if I want to use it to run on multiple threads, I will need to manually specify the thread id?","user":"UC6SUUPRC","ts":"1610128846.289100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"O/Z6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"so if I understand correctly, "},{"type":"text","text":"PaddedMatrices.runfunc(() -> nothing, 2)","style":{"code":true}},{"type":"text","text":" runs a task on a specified thread (which is "},{"type":"text","text":"2","style":{"code":true}},{"type":"text","text":" in this case) instead of schedule that at runtime, so if I want to use it to run on multiple threads, I will need to manually specify the thread id?"}]}]}],"thread_ts":"1609559185.259300","parent_user_id":"UAUPJLBQX"},{"client_msg_id":"2e6b3131-8fd2-4e80-91c5-f2ee2f1a84d1","type":"message","text":"it seems `@threads` is using the scheduler as well, I’m wondering if this adds extra overhead? <https://github.com/JuliaLang/julia/blob/83bee67631bc3d532b6b0bc47b02753ad5865673/base/threadingconstructs.jl#L30>","user":"UC6SUUPRC","ts":"1610130417.289300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"L8=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it seems "},{"type":"text","text":"@threads","style":{"code":true}},{"type":"text","text":" is using the scheduler as well, I’m wondering if this adds extra overhead? "},{"type":"link","url":"https://github.com/JuliaLang/julia/blob/83bee67631bc3d532b6b0bc47b02753ad5865673/base/threadingconstructs.jl#L30"}]}]}],"thread_ts":"1609559185.259300","parent_user_id":"UAUPJLBQX"},{"client_msg_id":"65db06d4-cd5e-4f75-92ba-2394c6e35b11","type":"message","text":"PaddedMatrices effectively calls `schedule` too. Both it and and `@threads` just set the tasks to be sticky and pin it to a thread first.\nPaddedMatrices will not run tasks on the main thread `threadid() == 1` however, so you need to start it with at least 1 more thread than the number of physical cores to get full utilization. `runfunc` is also zero indexed, so `1` corresponds to `threadid() == 2`, and is the lowest acceptable number.\n\n2 microseconds is still too high to compete with MKL. Based on the rate of performance change with increasing number of threads that I actually observe, the overhead can seem higher than that in practice.\nIt'd be great if there were some lower overhead API, but at the moment my knowledge of what is going on stops where the Julia code does.","user":"UAUPJLBQX","ts":"1610153135.289700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IRKXE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"PaddedMatrices effectively calls "},{"type":"text","text":"schedule","style":{"code":true}},{"type":"text","text":" too. Both it and and "},{"type":"text","text":"@threads","style":{"code":true}},{"type":"text","text":" just set the tasks to be sticky and pin it to a thread first.\nPaddedMatrices will not run tasks on the main thread "},{"type":"text","text":"threadid() == 1","style":{"code":true}},{"type":"text","text":" however, so you need to start it with at least 1 more thread than the number of physical cores to get full utilization. "},{"type":"text","text":"runfunc","style":{"code":true}},{"type":"text","text":" is also zero indexed, so "},{"type":"text","text":"1","style":{"code":true}},{"type":"text","text":" corresponds to "},{"type":"text","text":"threadid() == 2","style":{"code":true}},{"type":"text","text":", and is the lowest acceptable number.\n\n2 microseconds is still too high to compete with MKL. Based on the rate of performance change with increasing number of threads that I actually observe, the overhead can seem higher than that in practice.\nIt'd be great if there were some lower overhead API, but at the moment my knowledge of what is going on stops where the Julia code does."}]}]}],"thread_ts":"1609559185.259300","parent_user_id":"UAUPJLBQX"},{"client_msg_id":"45ba891e-d819-4d06-97b6-14dd9787692f","type":"message","text":"<@UAUPJLBQX> `@spawn` must be fairly expensive because it must save enough context to allow a task to call back into the runtime system. If you're running lots of tasks in Julia the switching overhead seems to bottom out around 1 us.  For OpenMP I think it's about 0.5 us (their context is simpler).\n\nSome other systems (perhaps MKL) have a separate queue for small tasks which are guaranteed to run to completion without needing much context.","user":"UBFATSCKU","ts":"1610336891.291300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"I3p","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UAUPJLBQX"},{"type":"text","text":" "},{"type":"text","text":"@spawn","style":{"code":true}},{"type":"text","text":" must be fairly expensive because it must save enough context to allow a task to call back into the runtime system. If you're running lots of tasks in Julia the switching overhead seems to bottom out around 1 us.  For OpenMP I think it's about 0.5 us (their context is simpler).\n\nSome other systems (perhaps MKL) have a separate queue for small tasks which are guaranteed to run to completion without needing much context."}]}]}],"thread_ts":"1609559185.259300","parent_user_id":"UAUPJLBQX"}]