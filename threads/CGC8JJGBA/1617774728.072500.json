[{"client_msg_id":"5C16BDC1-68F1-4DF9-BC54-5EACB7B9DF82","type":"message","text":"It is accumulated over the batch. If you wanted to obtain the gradient for each batch separately. You need a for loop.","user":"UCD4Z3NJZ","ts":"1617774728.072500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xzY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It is accumulated over the batch. If you wanted to obtain the gradient for each batch separately. You need a for loop."}]}]}],"thread_ts":"1617774728.072500","reply_count":1,"reply_users_count":1,"latest_reply":"1617848265.072600","reply_users":["USHAP9C9K"],"is_locked":false,"subscribed":false},{"client_msg_id":"65c50d70-39d8-47e1-8f28-1cae0d496ec3","type":"message","text":"If I use a loop, will I lose the acceleration from the CUDA's parallel computing?","user":"USHAP9C9K","ts":"1617848265.072600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vPS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If I use a loop, will I lose the acceleration from the CUDA's parallel computing?"}]}]}],"thread_ts":"1617774728.072500","parent_user_id":"UCD4Z3NJZ","pinned_to":["CGC8JJGBA"],"pinned_info":{"channel":"CGC8JJGBA","pinned_by":"USHAP9C9K","pinned_ts":1617848496}}]