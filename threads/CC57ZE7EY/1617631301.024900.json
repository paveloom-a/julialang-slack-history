[{"client_msg_id":"a0c00ed2-3d43-42ab-ba96-fa3c2049a571","type":"message","text":"Hello,\nI would like to implement my own static transformer. Let's call it Denormalizer:\n\n```mutable struct Denormalizer &lt;: Static\n    max_value::Float64\nend\n\nMLJ.transform(n::Denormalizer, _, y) = n.max_value * y\nMLJ.inverse_transform(n::Denormalizer, _, y) = y / n.max_value```\nI want to put this in a pipeline to bring the prediction range from [0, 1] to [0, max_value]:\n\n```pipe = @pipeline(Standardizer(ordered_factor=true, count=true), OneHotEncoder(ordered_factor=false), self_tuning_nn, Denormalizer(max_duration))```\nMy concern is whether MLJ is capable of passing the target value back to the `self_tuning_nn` (using `inverse_transform`). Will it will work?\n\nAlso, I don't know why the `MLJ.transform` must take an additional dummy parameter `_`. I saw it in the `Averager` example here: <https://alan-turing-institute.github.io/MLJ.jl/dev/transformers/#Static-transformers-1>.\n\nCan somebody help me with these issues?","user":"U01TNL120P3","ts":"1617631301.024900","team":"T68168MUP","edited":{"user":"U01TNL120P3","ts":"1617633067.000000"},"blocks":[{"type":"rich_text","block_id":"08nsa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello,\nI would like to implement my own static transformer. Let's call it Denormalizer:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"mutable struct Denormalizer <: Static\n    max_value::Float64\nend\n\nMLJ.transform(n::Denormalizer, _, y) = n.max_value * y\nMLJ.inverse_transform(n::Denormalizer, _, y) = y / n.max_value"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nI want to put this in a pipeline to bring the prediction range from [0, 1] to [0, max_value]:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"pipe = @pipeline(Standardizer(ordered_factor=true, count=true), OneHotEncoder(ordered_factor=false), self_tuning_nn, Denormalizer(max_duration))"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nMy concern is whether MLJ is capable of passing the target value back to the "},{"type":"text","text":"self_tuning_nn","style":{"code":true}},{"type":"text","text":" (using "},{"type":"text","text":"inverse_transform","style":{"code":true}},{"type":"text","text":"). Will it will work?\n\nAlso, I don't know why the "},{"type":"text","text":"MLJ.transform","style":{"code":true}},{"type":"text","text":" must take an additional dummy parameter "},{"type":"text","text":"_","style":{"code":true}},{"type":"text","text":". I saw it in the "},{"type":"text","text":"Averager","style":{"code":true}},{"type":"text","text":" example here: "},{"type":"link","url":"https://alan-turing-institute.github.io/MLJ.jl/dev/transformers/#Static-transformers-1"},{"type":"text","text":".\n\nCan somebody help me with these issues?"}]}]}],"thread_ts":"1617631301.024900","reply_count":4,"reply_users_count":1,"latest_reply":"1617747755.026700","reply_users":["UD0SQV5LL"],"is_locked":false,"subscribed":false},{"client_msg_id":"4121cb04-6f1f-4a7d-94f4-25244160abe3","type":"message","text":"<@U01TNL120P3> Thanks for your query. I’m not sure I understand your first question. To what precisely do you wish to apply `inverse_transform`? You will not be able to apply `inverse_transform` to a machine bound to `pipe`. I’m guessing that what you want to do is standardize your target before supplying it to the `self_tuning_nn` and then applying the inverse of that (learned) transformation to the the predictions of the `self_tuning_nn` . For transformations of the target that you wish to invert in this way, you use the keyword `target=…`. So in your example, something like:\n```pipe = @pipeline(Standardizer(...), OneHotEncoder(...), self_tuning_nn, target=Standardizer(...))```","user":"UD0SQV5LL","ts":"1617747416.026100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jRg","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U01TNL120P3"},{"type":"text","text":" Thanks for your query. I’m not sure I understand your first question. To what precisely do you wish to apply "},{"type":"text","text":"inverse_transform","style":{"code":true}},{"type":"text","text":"? You will not be able to apply "},{"type":"text","text":"inverse_transform","style":{"code":true}},{"type":"text","text":" to a machine bound to "},{"type":"text","text":"pipe","style":{"code":true}},{"type":"text","text":". I’m guessing that what you want to do is standardize your target before supplying it to the "},{"type":"text","text":"self_tuning_nn","style":{"code":true}},{"type":"text","text":" and then applying the inverse of that (learned) transformation to the the predictions of the "},{"type":"text","text":"self_tuning_nn","style":{"code":true}},{"type":"text","text":" . For transformations of the target that you wish to invert in this way, you use the keyword "},{"type":"text","text":"target=…","style":{"code":true}},{"type":"text","text":". So in your example, something like:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"pipe = @pipeline(Standardizer(...), OneHotEncoder(...), self_tuning_nn, target=Standardizer(...))"}]}]}],"thread_ts":"1617631301.024900","parent_user_id":"U01TNL120P3"},{"client_msg_id":"3b7ec8e7-6e33-43db-a7f5-f951844edbd6","type":"message","text":"The first `Standardizer` is for the features, the second for transforming *and* inverse transforming the target. (If you want to use functions instead of a `Unsupervised` model, then  you need to specify `inverse=…` as well).","user":"UD0SQV5LL","ts":"1617747542.026300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sCt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The first "},{"type":"text","text":"Standardizer","style":{"code":true}},{"type":"text","text":" is for the features, the second for transforming "},{"type":"text","text":"and","style":{"bold":true}},{"type":"text","text":" inverse transforming the target. (If you want to use functions instead of a "},{"type":"text","text":"Unsupervised","style":{"code":true}},{"type":"text","text":" model, then  you need to specify "},{"type":"text","text":"inverse=…","style":{"code":true}},{"type":"text","text":" as well)."}]}]}],"thread_ts":"1617631301.024900","parent_user_id":"U01TNL120P3"},{"client_msg_id":"c3bc7e56-2f61-4994-be08-9a9b9c2c65f7","type":"message","text":"If instead, you are only interested in applying a transformation to predictions, then your workflow is correct, but there is no interface point for inverting this post-prediction transformation in the pipeline context.","user":"UD0SQV5LL","ts":"1617747675.026500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"53ub","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If instead, you are only interested in applying a transformation to predictions, then your workflow is correct, but there is no interface point for inverting this post-prediction transformation in the pipeline context."}]}]}],"thread_ts":"1617631301.024900","parent_user_id":"U01TNL120P3"},{"client_msg_id":"84094c6c-c28c-45c1-8fe2-2f02993da607","type":"message","text":"As for the blank parameter, this is for the `fitresult` (learned parameter). By definition a `Static` transformer has not learned parameters, and so this is always `nothing`.","user":"UD0SQV5LL","ts":"1617747755.026700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VuA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"As for the blank parameter, this is for the "},{"type":"text","text":"fitresult","style":{"code":true}},{"type":"text","text":" (learned parameter). By definition a "},{"type":"text","text":"Static","style":{"code":true}},{"type":"text","text":" transformer has not learned parameters, and so this is always "},{"type":"text","text":"nothing","style":{"code":true}},{"type":"text","text":"."}]}]}],"thread_ts":"1617631301.024900","parent_user_id":"U01TNL120P3"}]