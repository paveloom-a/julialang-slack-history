[{"client_msg_id":"8f7d3613-db92-4a31-9df2-8b23be41f410","type":"message","text":"Hi, I am new to Turing and also Bayesian in general (looking into it because compared to PyMC3 or Stan it seems easier ) and so I am trying the simple coinflip example in the tutorial. I was wondering, how does one extract the posterior (which may have no analytical form in general) and then use it as the prior in the next model with different data? Basically I mean running the coinflip example again with the prior as the posterior from the previous run. (I know in this particular example there is an analytical way, but I want the general method)","user":"U01EF0QVAB0","ts":"1610067192.308700","team":"T68168MUP","edited":{"user":"U01EF0QVAB0","ts":"1610067222.000000"},"blocks":[{"type":"rich_text","block_id":"2TE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I am new to Turing and also Bayesian in general (looking into it because compared to PyMC3 or Stan it seems easier ) and so I am trying the simple coinflip example in the tutorial. I was wondering, how does one extract the posterior (which may have no analytical form in general) and then use it as the prior in the next model with different data? Basically I mean running the coinflip example again with the prior as the posterior from the previous run. (I know in this particular example there is an analytical way, but I want the general method)"}]}]}],"thread_ts":"1610067192.308700","reply_count":10,"reply_users_count":3,"latest_reply":"1610071234.310700","reply_users":["UN97XTLCV","U01EF0QVAB0","UHDQQ4GN6"],"subscribed":false},{"client_msg_id":"906a9f44-a50f-4f6b-be04-302082b7a1a4","type":"message","text":"So semantically the prior is the _marignal distribution_ for the given parameter. When you do MCMC, you’re leveraging Bayes’ theorem to update your parameter distributions using data. The equivalent of the “prior” after you have sampled is the _marginal posterior distribution_. So for a generic setup, if you can retrieve the marginal posterior, you can use that parameterization for a future run.\n\nMore concretely, let’s say you have a parameter i.i.d. by a normal,\n```x ~ Normal()```\nnow, you perform MCMC and you have a bunch of samples of the posterior. This is not the exact posterior (ie understand that samples only let you approximate the posterior), but let’s say we look at the samples and they look like a normal distribution, still. I can “fit” a normal distribution to the samples, and this distribution would then be the prior for the next run.","user":"UN97XTLCV","ts":"1610067764.308900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3FZ1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So semantically the prior is the "},{"type":"text","text":"marignal distribution","style":{"italic":true}},{"type":"text","text":" for the given parameter. When you do MCMC, you’re leveraging Bayes’ theorem to update your parameter distributions using data. The equivalent of the “prior” after you have sampled is the "},{"type":"text","text":"marginal posterior distribution","style":{"italic":true}},{"type":"text","text":". So for a generic setup, if you can retrieve the marginal posterior, you can use that parameterization for a future run.\n\nMore concretely, let’s say you have a parameter i.i.d. by a normal,\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"x ~ Normal()"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"now, you perform MCMC and you have a bunch of samples of the posterior. This is not the exact posterior (ie understand that samples only let you approximate the posterior), but let’s say we look at the samples and they look like a normal distribution, still. I can “fit” a normal distribution to the samples, and this distribution would then be the prior for the next run."}]}]}],"thread_ts":"1610067192.308700","parent_user_id":"U01EF0QVAB0"},{"client_msg_id":"56c038d3-64af-4afc-934c-d05f76cb568d","type":"message","text":"Oh I see, so its not possible to do some sort of lets say eCDF interpolation on the posterior samples and then sample from those for the next round? You have to fit a parametric distribution?","user":"U01EF0QVAB0","ts":"1610067999.309100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wPVE4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oh I see, so its not possible to do some sort of lets say eCDF interpolation on the posterior samples and then sample from those for the next round? You have to fit a parametric distribution?"}]}]}],"thread_ts":"1610067192.308700","parent_user_id":"U01EF0QVAB0"},{"client_msg_id":"38c308e6-b961-4462-85f9-8bad2bd078ed","type":"message","text":"you should not necessarily expect the marginal posterior dist to be the same as the marginal prior dist,, and in some cases there is actually a closed-form solution for the posterior distribution (look up conjugate priors), so in practice you can’t just blindly take your chain outputs and fit them and update. There’s also something to be said about “bayesian formalism” which says you shouldn’t fit your priors with the same data you’re planning to fit, since that’s kind of “cheating” (I think this boils down to “don’t overfit your data”)","user":"UN97XTLCV","ts":"1610068006.309300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Msm0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you should not necessarily expect the marginal posterior dist to be the same as the marginal prior dist,, and in some cases there is actually a closed-form solution for the posterior distribution (look up conjugate priors), so in practice you can’t just blindly take your chain outputs and fit them and update. There’s also something to be said about “bayesian formalism” which says you shouldn’t fit your priors with the same data you’re planning to fit, since that’s kind of “cheating” (I think this boils down to “don’t overfit your data”)"}]}]}],"thread_ts":"1610067192.308700","parent_user_id":"U01EF0QVAB0"},{"client_msg_id":"c2b2d264-22aa-495b-8637-281836ddc9be","type":"message","text":"well generally you can do whatever you want, the model you fit the first round will be different than the second model though","user":"UN97XTLCV","ts":"1610068030.309500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zs7G","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"well generally you can do whatever you want, the model you fit the first round will be different than the second model though"}]}]}],"thread_ts":"1610067192.308700","parent_user_id":"U01EF0QVAB0"},{"client_msg_id":"87e06f53-6e5b-4076-94a1-6db6af4dc5c8","type":"message","text":"Yea I get that part, I'm just wondering if the posterior from lets say a \"training\" phase could be used as the prior when you get the new independent test data","user":"U01EF0QVAB0","ts":"1610068078.309700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ApX1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yea I get that part, I'm just wondering if the posterior from lets say a \"training\" phase could be used as the prior when you get the new independent test data"}]}]}],"thread_ts":"1610067192.308700","parent_user_id":"U01EF0QVAB0"},{"client_msg_id":"c94ffa91-67f7-4815-9613-50622c1d0d9f","type":"message","text":"I've heard this is one of the advantages of Bayesian but I don't know how one could do it aside from the analytical cases with conjugate priors/posteriors","user":"U01EF0QVAB0","ts":"1610068116.309900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"v9z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I've heard this is one of the advantages of Bayesian but I don't know how one could do it aside from the analytical cases with conjugate priors/posteriors"}]}]}],"thread_ts":"1610067192.308700","parent_user_id":"U01EF0QVAB0"},{"client_msg_id":"03b3d467-cf43-4be4-a549-c5fa86b60387","type":"message","text":"yes in the most generic sense of the word “prior”. The catch is that posterior samples only give you an approximation of a distribution, so if your prior in the first run is `x ~ Normal()` the prior for `x` in your second model is fundamentally different, so if you’re fitting a dataset you’re technically using two different models","user":"UN97XTLCV","ts":"1610068266.310100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"idU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yes in the most generic sense of the word “prior”. The catch is that posterior samples only give you an approximation of a distribution, so if your prior in the first run is "},{"type":"text","text":"x ~ Normal()","style":{"code":true}},{"type":"text","text":" the prior for "},{"type":"text","text":"x","style":{"code":true}},{"type":"text","text":" in your second model is fundamentally different, so if you’re fitting a dataset you’re technically using two different models"}]}]}],"thread_ts":"1610067192.308700","parent_user_id":"U01EF0QVAB0"},{"client_msg_id":"85ba7789-b474-46aa-8cd3-90815356211d","type":"message","text":"Hmm so basically in Turing there is no way to use or define some nonparametric empirical distributions (based on the posterior samples)? Aka use the histogram of the posterior samples as the prior somehow (without resorting to fitting that distribution parametrically)","user":"U01EF0QVAB0","ts":"1610068405.310300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dk1j","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hmm so basically in Turing there is no way to use or define some nonparametric empirical distributions (based on the posterior samples)? Aka use the histogram of the posterior samples as the prior somehow (without resorting to fitting that distribution parametrically)"}]}]}],"thread_ts":"1610067192.308700","parent_user_id":"U01EF0QVAB0"},{"client_msg_id":"18a51bb6-a410-406a-b1e7-a5e061525394","type":"message","text":"yeah you can probably set that up, you’d have to either sub-type the Distributions.jl interface or just hand code the sampling. AFAIK there is not a simple eCDF distribution that just works like you may want","user":"UN97XTLCV","ts":"1610069624.310500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yjfZ8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yeah you can probably set that up, you’d have to either sub-type the Distributions.jl interface or just hand code the sampling. AFAIK there is not a simple eCDF distribution that just works like you may want"}]}]}],"thread_ts":"1610067192.308700","parent_user_id":"U01EF0QVAB0"},{"client_msg_id":"cd6bf4db-10b5-48fc-9f0a-41c73f02e245","type":"message","text":"There used to be `EmpiricalUnivariateDistribution` one in Distributions: <https://github.com/JuliaStats/Distributions.jl/pull/800>","user":"UHDQQ4GN6","ts":"1610071234.310700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"l02s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"There used to be "},{"type":"text","text":"EmpiricalUnivariateDistribution","style":{"code":true}},{"type":"text","text":" one in Distributions: "},{"type":"link","url":"https://github.com/JuliaStats/Distributions.jl/pull/800"}]}]}],"thread_ts":"1610067192.308700","parent_user_id":"U01EF0QVAB0"}]