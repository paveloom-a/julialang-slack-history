[{"client_msg_id":"8f7d3613-db92-4a31-9df2-8b23be41f410","type":"message","text":"Hi, I am new to Turing and also Bayesian in general (looking into it because compared to PyMC3 or Stan it seems easier ) and so I am trying the simple coinflip example in the tutorial. I was wondering, how does one extract the posterior (which may have no analytical form in general) and then use it as the prior in the next model with different data? Basically I mean running the coinflip example again with the prior as the posterior from the previous run. (I know in this particular example there is an analytical way, but I want the general method)","user":"U01EF0QVAB0","ts":"1610067192.308700","team":"T68168MUP","edited":{"user":"U01EF0QVAB0","ts":"1610067222.000000"},"blocks":[{"type":"rich_text","block_id":"2TE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I am new to Turing and also Bayesian in general (looking into it because compared to PyMC3 or Stan it seems easier ) and so I am trying the simple coinflip example in the tutorial. I was wondering, how does one extract the posterior (which may have no analytical form in general) and then use it as the prior in the next model with different data? Basically I mean running the coinflip example again with the prior as the posterior from the previous run. (I know in this particular example there is an analytical way, but I want the general method)"}]}]}],"thread_ts":"1610067192.308700","reply_count":1,"reply_users_count":1,"latest_reply":"1610067764.308900","reply_users":["UN97XTLCV"],"subscribed":false},{"client_msg_id":"906a9f44-a50f-4f6b-be04-302082b7a1a4","type":"message","text":"So semantically the prior is the _marignal distribution_ for the given parameter. When you do MCMC, you’re leveraging Bayes’ theorem to update your parameter distributions using data. The equivalent of the “prior” after you have sampled is the _marginal posterior distribution_. So for a generic setup, if you can retrieve the marginal posterior, you can use that parameterization for a future run.\n\nMore concretely, let’s say you have a parameter i.i.d. by a normal,\n```x ~ Normal()```\nnow, you perform MCMC and you have a bunch of samples of the posterior. This is not the exact posterior (ie understand that samples only let you approximate the posterior), but let’s say we look at the samples and they look like a normal distribution, still. I can “fit” a normal distribution to the samples, and this distribution would then be the prior for the next run.","user":"UN97XTLCV","ts":"1610067764.308900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3FZ1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So semantically the prior is the "},{"type":"text","text":"marignal distribution","style":{"italic":true}},{"type":"text","text":" for the given parameter. When you do MCMC, you’re leveraging Bayes’ theorem to update your parameter distributions using data. The equivalent of the “prior” after you have sampled is the "},{"type":"text","text":"marginal posterior distribution","style":{"italic":true}},{"type":"text","text":". So for a generic setup, if you can retrieve the marginal posterior, you can use that parameterization for a future run.\n\nMore concretely, let’s say you have a parameter i.i.d. by a normal,\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"x ~ Normal()"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"now, you perform MCMC and you have a bunch of samples of the posterior. This is not the exact posterior (ie understand that samples only let you approximate the posterior), but let’s say we look at the samples and they look like a normal distribution, still. I can “fit” a normal distribution to the samples, and this distribution would then be the prior for the next run."}]}]}],"thread_ts":"1610067192.308700","parent_user_id":"U01EF0QVAB0"}]