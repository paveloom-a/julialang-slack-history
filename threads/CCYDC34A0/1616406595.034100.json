[{"client_msg_id":"5873a25a-8a1b-4da1-8968-8d7b40e746ef","type":"message","text":"Does anyone have suggestions for debugging poor sampler performance? I have a relatively simply multivariate regression (all covariates are computed independently) and it's really ridiculously slow with NUTS and makes an enormous number of allocations.","user":"U01H36BUDJB","ts":"1616406595.034100","team":"T68168MUP","edited":{"user":"U01H36BUDJB","ts":"1616406607.000000"},"blocks":[{"type":"rich_text","block_id":"j+V","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does anyone have suggestions for debugging poor sampler performance? I have a relatively simply multivariate regression (all covariates are computed independently) and it's really ridiculously slow with NUTS and makes an enormous number of allocations."}]}]}],"thread_ts":"1616406595.034100","reply_count":13,"reply_users_count":2,"latest_reply":"1616410876.037700","reply_users":["UC0SY9JFP","U01H36BUDJB"],"subscribed":false},{"client_msg_id":"F914A91B-41AB-4D65-8732-59AE50C70930","type":"message","text":"Have you had a look at the performance tips on <http://turing.ml|turing.ml>?","user":"UC0SY9JFP","ts":"1616407242.034800","team":"T68168MUP","edited":{"user":"UC0SY9JFP","ts":"1616407256.000000"},"blocks":[{"type":"rich_text","block_id":"Xbh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Have you had a look at the performance tips on turing.ml?"}]}]}],"thread_ts":"1616406595.034100","parent_user_id":"U01H36BUDJB"},{"client_msg_id":"6de416c9-6f39-4eef-b0a9-6934d0efecbb","type":"message","text":"Yes.","user":"U01H36BUDJB","ts":"1616407364.035100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PI=/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes."}]}]}],"thread_ts":"1616406595.034100","parent_user_id":"U01H36BUDJB"},{"client_msg_id":"C04BBBB0-112F-4C49-8AB7-1EB2C05BB47B","type":"message","text":"Is it possible to post the model? ","user":"UC0SY9JFP","ts":"1616407435.035600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uXIv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is it possible to post the model? "}]}]}],"thread_ts":"1616406595.034100","parent_user_id":"U01H36BUDJB"},{"client_msg_id":"2d0ef142-44eb-4ce0-b78b-5499cf97a93a","type":"message","text":"It seems to be much faster to fit separate linear regression models for each variable individually than it is to fit one model for all at once, despite all variables being independent.","user":"U01H36BUDJB","ts":"1616407460.035800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MsgO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It seems to be much faster to fit separate linear regression models for each variable individually than it is to fit one model for all at once, despite all variables being independent."}]}]}],"thread_ts":"1616406595.034100","parent_user_id":"U01H36BUDJB"},{"client_msg_id":"fe7286e2-f663-461b-98ca-bf0c45966dd5","type":"message","text":"Sure.\n```@model function multi_linreg(x, y, ::Type{T} = Float64; σᵦ=1.0,σ₀=10.0,a=2,b=3,β=missing,β₀=missing,ν=missing,\n        nobsv=missing, nvars=missing) where {T}\n    \"\"\"\n    Fits a separate linear regression model with homeoscedastic variance for each column X, Y.\n    \"\"\"\n    @assert !ismissing(x)\n    @assert !ismissing(y) || (!ismissing(nobsv) &amp;&amp; !ismissing(nvars))\n    if ismissing(y)\n        y = Matrix{T}(undef,nobsv,nvars)\n    else\n        nobsv = size(y,1)\n        nvars = size(y,2)\n    end\n    # set a tight prior on coeffs since we do not expect large rates of annual change\n    β ~ MvNormal(zeros(nvars),σᵦ)\n    β₀ ~ MvNormal(zeros(nvars),σ₀)\n    # assume diagonal variance of observation noise; likely not true!\n    ν ~ filldist(InverseGamma(a,b), nvars)\n    for i in 1:nobsv\n#         y[i,:] ~ MvNormal(β₀ .+ β.*x[i], ν)\n        for j in 1:nvars\n            y[i,j] ~ Normal(β₀[j] + β[j]*x[i,j], sqrt(ν[j]))\n        end\n    end\n    return y\nend```","user":"U01H36BUDJB","ts":"1616407488.036000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"J5wH=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Sure.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@model function multi_linreg(x, y, ::Type{T} = Float64; σᵦ=1.0,σ₀=10.0,a=2,b=3,β=missing,β₀=missing,ν=missing,\n        nobsv=missing, nvars=missing) where {T}\n    \"\"\"\n    Fits a separate linear regression model with homeoscedastic variance for each column X, Y.\n    \"\"\"\n    @assert !ismissing(x)\n    @assert !ismissing(y) || (!ismissing(nobsv) && !ismissing(nvars))\n    if ismissing(y)\n        y = Matrix{T}(undef,nobsv,nvars)\n    else\n        nobsv = size(y,1)\n        nvars = size(y,2)\n    end\n    # set a tight prior on coeffs since we do not expect large rates of annual change\n    β ~ MvNormal(zeros(nvars),σᵦ)\n    β₀ ~ MvNormal(zeros(nvars),σ₀)\n    # assume diagonal variance of observation noise; likely not true!\n    ν ~ filldist(InverseGamma(a,b), nvars)\n    for i in 1:nobsv\n#         y[i,:] ~ MvNormal(β₀ .+ β.*x[i], ν)\n        for j in 1:nvars\n            y[i,j] ~ Normal(β₀[j] + β[j]*x[i,j], sqrt(ν[j]))\n        end\n    end\n    return y\nend"}]}]}],"thread_ts":"1616406595.034100","parent_user_id":"U01H36BUDJB"},{"client_msg_id":"9a851b52-4234-46e1-9333-78f491a84744","type":"message","text":"Note that `y[i,:] ~ MvNormal(β₀ .+ β.*x[i], ν)` doesn't work due to a bug with Turing's missing value handling. There is no MvNormal likelihood for type `Union{Missing,T}` . I meant to post an issue about that a while ago, but I think I forgot...","user":"U01H36BUDJB","ts":"1616407573.036200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7cKi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Note that "},{"type":"text","text":"y[i,:] ~ MvNormal(β₀ .+ β.*x[i], ν)","style":{"code":true}},{"type":"text","text":" doesn't work due to a bug with Turing's missing value handling. There is no MvNormal likelihood for type "},{"type":"text","text":"Union{Missing,T}","style":{"code":true}},{"type":"text","text":" . I meant to post an issue about that a while ago, but I think I forgot..."}]}]}],"thread_ts":"1616406595.034100","parent_user_id":"U01H36BUDJB"},{"client_msg_id":"c09ad390-6930-4783-9018-9b4ba630c28b","type":"message","text":"I'm guessing this has to do with the fact that the output is Matrix-valued, which probably slows down autodiff when building the Jacobian. In this specific case, the Jacobian should actually be very sparse, but I'm not sure how to take advantage of that through Turing.","user":"U01H36BUDJB","ts":"1616408569.036400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TBfZI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm guessing this has to do with the fact that the output is Matrix-valued, which probably slows down autodiff when building the Jacobian. In this specific case, the Jacobian should actually be very sparse, but I'm not sure how to take advantage of that through Turing."}]}]}],"thread_ts":"1616406595.034100","parent_user_id":"U01H36BUDJB"},{"client_msg_id":"19df9ca8-bbd6-429a-b1e1-9a4197d7d05a","type":"message","text":"Two ideas: 1. HMC and NUTS can be pretty sensitive to the paramterization you use. As `σ₀=10` I would suggest to reparameterize the model such that the prior on beta is `β₀ ~ MvNormal(zeros(nvars),1)`  and then use `β₀ * σ₀` later on. 2. You might want to use a hand written likelihood function. You can increment the target value (the log joint) using `Turing.addlogprob! llh`  where `llh` is the computed likelihood, e.g. `llh = sum(logpdf(Normal(), xi) for xi in x)`.","user":"UC0SY9JFP","ts":"1616409172.036600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pNZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Two ideas: 1. HMC and NUTS can be pretty sensitive to the paramterization you use. As "},{"type":"text","text":"σ₀=10","style":{"code":true}},{"type":"text","text":" I would suggest to reparameterize the model such that the prior on beta is "},{"type":"text","text":"β₀ ~ MvNormal(zeros(nvars),1)","style":{"code":true}},{"type":"text","text":"  and then use "},{"type":"text","text":"β₀ * σ₀","style":{"code":true}},{"type":"text","text":" later on. 2. You might want to use a hand written likelihood function. You can increment the target value (the log joint) using "},{"type":"text","text":"Turing.addlogprob! llh","style":{"code":true}},{"type":"text","text":"  where "},{"type":"text","text":"llh","style":{"code":true}},{"type":"text","text":" is the computed likelihood, e.g. "},{"type":"text","text":"llh = sum(logpdf(Normal(), xi) for xi in x)","style":{"code":true}},{"type":"text","text":"."}]}]}],"thread_ts":"1616406595.034100","parent_user_id":"U01H36BUDJB","reactions":[{"name":"+1","users":["U01H36BUDJB"],"count":1}]},{"client_msg_id":"482a6878-600c-4c31-abd0-32526583b49d","type":"message","text":"Lastly, I would try to get rid of this code:\n```    @assert !ismissing(x)\n    @assert !ismissing(y) || (!ismissing(nobsv) &amp;&amp; !ismissing(nvars))\n    if ismissing(y)\n        y = Matrix{T}(undef,nobsv,nvars)\n    else\n        nobsv = size(y,1)\n        nvars = size(y,2)\n    end```\nas it will be evaluated at each iteration / each step in NUTS. This will result in a lot of unnecessary allocations.","user":"UC0SY9JFP","ts":"1616409293.036800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DLf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Lastly, I would try to get rid of this code:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"    @assert !ismissing(x)\n    @assert !ismissing(y) || (!ismissing(nobsv) && !ismissing(nvars))\n    if ismissing(y)\n        y = Matrix{T}(undef,nobsv,nvars)\n    else\n        nobsv = size(y,1)\n        nvars = size(y,2)\n    end"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"as it will be evaluated at each iteration / each step in NUTS. This will result in a lot of unnecessary allocations."}]}]}],"thread_ts":"1616406595.034100","parent_user_id":"U01H36BUDJB"},{"client_msg_id":"5f099cc5-13b4-4c01-a0f3-f0a12ef43213","type":"message","text":"Thanks! I don't quite get the benefit of point 2 above. Shouldn't this be done automatically by Turing? Why would handwriting it help?","user":"U01H36BUDJB","ts":"1616409414.037100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JLAG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks! I don't quite get the benefit of point 2 above. Shouldn't this be done automatically by Turing? Why would handwriting it help?"}]}]}],"thread_ts":"1616406595.034100","parent_user_id":"U01H36BUDJB"},{"client_msg_id":"307eeac6-6073-4114-904d-7c4b95cb606e","type":"message","text":"Turing does something similar as in point 2 but it is often possible to write more optimised code for the evaluation of the log likelihood function.","user":"UC0SY9JFP","ts":"1616409600.037300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tpE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Turing does something similar as in point 2 but it is often possible to write more optimised code for the evaluation of the log likelihood function."}]}]}],"thread_ts":"1616406595.034100","parent_user_id":"U01H36BUDJB"},{"client_msg_id":"f2fd276b-3f5a-4ef7-9926-e2e722d79cb3","type":"message","text":"One issue here is that I have a lot of missing values. Is there a way to control how many missing values Turing treats as sampled parameters? Or a way to ignore them completely?","user":"U01H36BUDJB","ts":"1616410051.037500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WJHC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"One issue here is that I have a lot of missing values. Is there a way to control how many missing values Turing treats as sampled parameters? Or a way to ignore them completely?"}]}]}],"thread_ts":"1616406595.034100","parent_user_id":"U01H36BUDJB"},{"client_msg_id":"27fd4e65-3639-4d9e-8fe2-8df16d1e96f8","type":"message","text":"You can ignore them completely by passing only the non-missing ones to the function, of course, or you write your own likelihood function for `y` which ignores all missing values. Turing will only hijack tilde operators, e.g. `y[i,j] ~ Normal(β₀[j] + β[j]*x[i,j], sqrt(ν[j]))` , and if you do special treatment for `y` like I suggested above then there will be no handling of missing values (sampling) by Turing.","user":"UC0SY9JFP","ts":"1616410876.037700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"44=tA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You can ignore them completely by passing only the non-missing ones to the function, of course, or you write your own likelihood function for "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" which ignores all missing values. Turing will only hijack tilde operators, e.g. "},{"type":"text","text":"y[i,j] ~ Normal(β₀[j] + β[j]*x[i,j], sqrt(ν[j]))","style":{"code":true}},{"type":"text","text":" , and if you do special treatment for "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" like I suggested above then there will be no handling of missing values (sampling) by Turing."}]}]}],"thread_ts":"1616406595.034100","parent_user_id":"U01H36BUDJB"}]