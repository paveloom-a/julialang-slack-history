[{"client_msg_id":"865232ee-c16a-498b-8906-3c53af6a9d74","type":"message","text":"The bugged code (if I try to force covariance matrix to be symmetry and positive definite externally) and its error message attaches:\n```using Random\nusing DifferentialEquations\nusing GenericLinearAlgebra\nusing Statistics\nusing Distributions\nusing Turing\n# data\nu_account = [1.0; 2.0];\n@model bayes_lna(y) = begin\n    log_delta ~ Normal(0.0,1.0) \n    delta = [exp(log_delta)]\n    phi = [delta  0; 10^-7 delta] # covariance matrix \n    D,V = GenericLinearAlgebra.eigen(GenericLinearAlgebra.Symmetric(phi))  # get eigen-values and eigen-vectos\n    D .= max.(D,1e-12) # force eigen values to be positive\n    phi_new = V*GenericLinearAlgebra.Diagonal(D)*V' # create new symmetry and positive definite matrix\n    y[:] ~ MvNormal([0.0,0.0],GenericLinearAlgebra.Symmetric(phi_new))\nend\nRandom.seed!(87654)\niterations = 20\nchain = sample(bayes_lna(u_account), NUTS(0.65),iterations)```","user":"U01Q398M3QB","ts":"1616578091.056700","team":"T68168MUP","edited":{"user":"U01Q398M3QB","ts":"1616578570.000000"},"blocks":[{"type":"rich_text","block_id":"Ynsh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The bugged code (if I try to force covariance matrix to be symmetry and positive definite externally) and its error message attaches:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Random\nusing DifferentialEquations\nusing GenericLinearAlgebra\nusing Statistics\nusing Distributions\nusing Turing\n# data\nu_account = [1.0; 2.0];\n@model bayes_lna(y) = begin\n    log_delta ~ Normal(0.0,1.0) \n    delta = [exp(log_delta)]\n    phi = [delta  0; 10^-7 delta] # covariance matrix \n    D,V = GenericLinearAlgebra.eigen(GenericLinearAlgebra.Symmetric(phi))  # get eigen-values and eigen-vectos\n    D .= max.(D,1e-12) # force eigen values to be positive\n    phi_new = V*GenericLinearAlgebra.Diagonal(D)*V' # create new symmetry and positive definite matrix\n    y[:] ~ MvNormal([0.0,0.0],GenericLinearAlgebra.Symmetric(phi_new))\nend\nRandom.seed!(87654)\niterations = 20\nchain = sample(bayes_lna(u_account), NUTS(0.65),iterations)"}]}]}],"thread_ts":"1616578091.056700","reply_count":18,"reply_users_count":4,"latest_reply":"1616665510.078500","reply_users":["U8T9JUA5R","U01Q398M3QB","U680T6770","U6C937ENB"],"is_locked":false,"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"I don't have any experience with ProximalOperators.jl, so I can't comment on this part. In general, if you have full control over the model and work with custom distributions, probably it would be much more efficient if you do not work with the covariance matrix `C` directly but instead with the lower-triangular matrix `L` in its decomposition `C = LL'`. In the example you show `phi` is actually already a lower-triangular matrix. Generally, you then just have to enforce that the diagonal entries of `L` are non-negative (e.g., by using `exp` or `log1pexp` ), the off-diagonal elements of `L` can be arbitrary. BTW I am a bit surprised you have to use `GenericLinearAlgebra` here - at least `Symmetric`  and `Diagonal` from `LinearAlgebra` should work fine with dual numbers (so maybe it is just due to `eigen`?). In fact, AFAIK PDMats (which is used by the constructors of `MvNormal`) only defines special dispatches for `LinearAlgebra.Symmetric` but not GenericLinearAlgebra.","user":"U8T9JUA5R","ts":"1616580167.061100","thread_ts":"1616578091.056700","root":{"client_msg_id":"865232ee-c16a-498b-8906-3c53af6a9d74","type":"message","text":"The bugged code (if I try to force covariance matrix to be symmetry and positive definite externally) and its error message attaches:\n```using Random\nusing DifferentialEquations\nusing GenericLinearAlgebra\nusing Statistics\nusing Distributions\nusing Turing\n# data\nu_account = [1.0; 2.0];\n@model bayes_lna(y) = begin\n    log_delta ~ Normal(0.0,1.0) \n    delta = [exp(log_delta)]\n    phi = [delta  0; 10^-7 delta] # covariance matrix \n    D,V = GenericLinearAlgebra.eigen(GenericLinearAlgebra.Symmetric(phi))  # get eigen-values and eigen-vectos\n    D .= max.(D,1e-12) # force eigen values to be positive\n    phi_new = V*GenericLinearAlgebra.Diagonal(D)*V' # create new symmetry and positive definite matrix\n    y[:] ~ MvNormal([0.0,0.0],GenericLinearAlgebra.Symmetric(phi_new))\nend\nRandom.seed!(87654)\niterations = 20\nchain = sample(bayes_lna(u_account), NUTS(0.65),iterations)```","user":"U01Q398M3QB","ts":"1616578091.056700","team":"T68168MUP","edited":{"user":"U01Q398M3QB","ts":"1616578570.000000"},"blocks":[{"type":"rich_text","block_id":"Ynsh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The bugged code (if I try to force covariance matrix to be symmetry and positive definite externally) and its error message attaches:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Random\nusing DifferentialEquations\nusing GenericLinearAlgebra\nusing Statistics\nusing Distributions\nusing Turing\n# data\nu_account = [1.0; 2.0];\n@model bayes_lna(y) = begin\n    log_delta ~ Normal(0.0,1.0) \n    delta = [exp(log_delta)]\n    phi = [delta  0; 10^-7 delta] # covariance matrix \n    D,V = GenericLinearAlgebra.eigen(GenericLinearAlgebra.Symmetric(phi))  # get eigen-values and eigen-vectos\n    D .= max.(D,1e-12) # force eigen values to be positive\n    phi_new = V*GenericLinearAlgebra.Diagonal(D)*V' # create new symmetry and positive definite matrix\n    y[:] ~ MvNormal([0.0,0.0],GenericLinearAlgebra.Symmetric(phi_new))\nend\nRandom.seed!(87654)\niterations = 20\nchain = sample(bayes_lna(u_account), NUTS(0.65),iterations)"}]}]}],"thread_ts":"1616578091.056700","reply_count":18,"reply_users_count":4,"latest_reply":"1616665510.078500","reply_users":["U8T9JUA5R","U01Q398M3QB","U680T6770","U6C937ENB"],"is_locked":false,"subscribed":false},"blocks":[{"type":"rich_text","block_id":"05o","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I don't have any experience with ProximalOperators.jl, so I can't comment on this part. In general, if you have full control over the model and work with custom distributions, probably it would be much more efficient if you do not work with the covariance matrix "},{"type":"text","text":"C","style":{"code":true}},{"type":"text","text":" directly but instead with the lower-triangular matrix "},{"type":"text","text":"L","style":{"code":true}},{"type":"text","text":" in its decomposition "},{"type":"text","text":"C = LL'","style":{"code":true}},{"type":"text","text":". In the example you show "},{"type":"text","text":"phi","style":{"code":true}},{"type":"text","text":" is actually already a lower-triangular matrix. Generally, you then just have to enforce that the diagonal entries of "},{"type":"text","text":"L","style":{"code":true}},{"type":"text","text":" are non-negative (e.g., by using "},{"type":"text","text":"exp","style":{"code":true}},{"type":"text","text":" or "},{"type":"text","text":"log1pexp","style":{"code":true}},{"type":"text","text":" ), the off-diagonal elements of "},{"type":"text","text":"L","style":{"code":true}},{"type":"text","text":" can be arbitrary. BTW I am a bit surprised you have to use "},{"type":"text","text":"GenericLinearAlgebra","style":{"code":true}},{"type":"text","text":" here - at least "},{"type":"text","text":"Symmetric","style":{"code":true}},{"type":"text","text":"  and "},{"type":"text","text":"Diagonal","style":{"code":true}},{"type":"text","text":" from "},{"type":"text","text":"LinearAlgebra","style":{"code":true}},{"type":"text","text":" should work fine with dual numbers (so maybe it is just due to "},{"type":"text","text":"eigen","style":{"code":true}},{"type":"text","text":"?). In fact, AFAIK PDMats (which is used by the constructors of "},{"type":"text","text":"MvNormal","style":{"code":true}},{"type":"text","text":") only defines special dispatches for "},{"type":"text","text":"LinearAlgebra.Symmetric","style":{"code":true}},{"type":"text","text":" but not GenericLinearAlgebra."}]}]}],"client_msg_id":"9b05f06e-d571-4bcc-8abc-95ddaceacc68"},{"type":"message","text":"<@U8T9JUA5R> Hi! David, thanks for replying so quickly. In my original code. The covariance matrix *C* is calculated by an ODE function which might become not symmetry nor positive definite due to the numerical errors. I do not have a good way to get its lower triangular decomposition *L*. I will change GenericLinearAlgebra back into LinearAlgebra.\n\nI just checked the matrix factorizations in LinearAlgebra.jl   I feel that if I could use the SVD decomposition to replace the cholesky decomposition in MvNomral(). Then, I do not need to make the matrix to be symmetry and positive definite. Is this achievable?","files":[{"id":"F01S9C3RSQ3","created":1616581526,"timestamp":1616581526,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01Q398M3QB","editable":false,"size":83242,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01S9C3RSQ3/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01S9C3RSQ3/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01S9C3RSQ3-c3ec7b227d/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01S9C3RSQ3-c3ec7b227d/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01S9C3RSQ3-c3ec7b227d/image_360.png","thumb_360_w":360,"thumb_360_h":340,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01S9C3RSQ3-c3ec7b227d/image_480.png","thumb_480_w":480,"thumb_480_h":453,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01S9C3RSQ3-c3ec7b227d/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01S9C3RSQ3-c3ec7b227d/image_720.png","thumb_720_w":720,"thumb_720_h":680,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01S9C3RSQ3-c3ec7b227d/image_800.png","thumb_800_w":800,"thumb_800_h":755,"original_w":826,"original_h":780,"thumb_tiny":"AwAtADC5KszMRGwUY6n1p0KuqYkIJzxing806gBv8XTNKc560YyTzQaADPtS0gHvS0ANHNLTQPQ06gA5z/8AWoPWkzyecUpoAOaUU3jvinZFADBmnUxTk4p9ACYOTj9aGXJ6n8DTDIMn5f1pVcEgbf1oAUIOuW/M0/AznHNGBRQB/9k=","permalink":"https://julialang.slack.com/files/U01Q398M3QB/F01S9C3RSQ3/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01S9C3RSQ3-d5f96acf1e","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"PnmQ","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U8T9JUA5R"},{"type":"text","text":" Hi! David, thanks for replying so quickly. In my original code. The covariance matrix "},{"type":"text","text":"C","style":{"bold":true}},{"type":"text","text":" is calculated by an ODE function which might become not symmetry nor positive definite due to the numerical errors. I do not have a good way to get its lower triangular decomposition "},{"type":"text","text":"L","style":{"bold":true}},{"type":"text","text":". I will change GenericLinearAlgebra back into LinearAlgebra.\n\nI just checked the matrix factorizations in LinearAlgebra.jl   I feel that if I could use the SVD decomposition to replace the cholesky decomposition in MvNomral(). Then, I do not need to make the matrix to be symmetry and positive definite. Is this achievable?"}]}]}],"user":"U01Q398M3QB","display_as_bot":false,"ts":"1616581529.062700","edited":{"user":"U01Q398M3QB","ts":"1616581563.000000"},"thread_ts":"1616578091.056700","parent_user_id":"U01Q398M3QB"},{"client_msg_id":"b71cdd99-c6f9-43e8-9b9c-1e97ab40fea0","type":"message","text":"OK, so if you can't work with the decomposition directly, you have to avoid the numerical issues in some way. If you want to use `MvNormal` with SVD, you would have to write your own subtype of `AbstractPDMat` (from PDMats) that uses SVD. I guess you could implement a custom `AbstractMvNormal` distribution instead equally well. Maybe PositiveFactorizations could be an alternative to ProximalOperators, one could consider the closest psd matrix in the Frobenius norm (<http://www.sciencedirect.com/science/article/pii/0024379588902236>; polar decomposition is available e.g. in MatrixFactorizations.jl), some other regularized covariance estimation methods (as e.g. in CovarianceEstimation.jl), or alternatively just some manual regularization with a diagonal matrix (but the other approaches might be more sophisticated). In any case, the main problem/difficulty will be that the functions (the factorizations and logpdf evaluations) should be differentiable, either with AD directly or by using some custom gradient implementation, to be able to use NUTS. I know that e.g. ChainRules contains a adjoint definition for `svd` , so you should be able to use Zygote (at least for this part).","user":"U8T9JUA5R","ts":"1616590269.064600","team":"T68168MUP","attachments":[{"title":"Computing a nearest symmetric positive semidefinite matrix","title_link":"http://www.sciencedirect.com/science/article/pii/0024379588902236","text":"The nearest symmetric positive semidefinite matrix in the Frobenius norm to an arbitrary real matrix A is shown to be (B + H)/2, where H is the symmet…","fallback":"Computing a nearest symmetric positive semidefinite matrix","from_url":"http://www.sciencedirect.com/science/article/pii/0024379588902236","thumb_url":"https://ars.els-cdn.com/content/image/1-s2.0-S0024379521X00067-cov150h.gif","thumb_width":103,"thumb_height":150,"service_icon":"https://www.sciencedirect.com/favicon.ico","service_name":"sciencedirect.com","id":1,"original_url":"http://www.sciencedirect.com/science/article/pii/0024379588902236"}],"blocks":[{"type":"rich_text","block_id":"o7X","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"OK, so if you can't work with the decomposition directly, you have to avoid the numerical issues in some way. If you want to use "},{"type":"text","text":"MvNormal","style":{"code":true}},{"type":"text","text":" with SVD, you would have to write your own subtype of "},{"type":"text","text":"AbstractPDMat","style":{"code":true}},{"type":"text","text":" (from PDMats) that uses SVD. I guess you could implement a custom "},{"type":"text","text":"AbstractMvNormal","style":{"code":true}},{"type":"text","text":" distribution instead equally well. Maybe PositiveFactorizations could be an alternative to ProximalOperators, one could consider the closest psd matrix in the Frobenius norm ("},{"type":"link","url":"http://www.sciencedirect.com/science/article/pii/0024379588902236"},{"type":"text","text":"; polar decomposition is available e.g. in MatrixFactorizations.jl), some other regularized covariance estimation methods (as e.g. in CovarianceEstimation.jl), or alternatively just some manual regularization with a diagonal matrix (but the other approaches might be more sophisticated). In any case, the main problem/difficulty will be that the functions (the factorizations and logpdf evaluations) should be differentiable, either with AD directly or by using some custom gradient implementation, to be able to use NUTS. I know that e.g. ChainRules contains a adjoint definition for "},{"type":"text","text":"svd","style":{"code":true}},{"type":"text","text":" , so you should be able to use Zygote (at least for this part)."}]}]}],"thread_ts":"1616578091.056700","parent_user_id":"U01Q398M3QB"},{"type":"message","text":"<@U8T9JUA5R> Thanks very much David. However, by using polar decomposition in MatrixFactorizations.jl to formulate nearest symmetry positive semidefinite matrix and then add small positive definite matrix to make it positive definite still doesn't work well all the time due to the numerical errors.\n\nI test 2000 iterations in Julia 1.6.0 since polar decomposition in  MatrixFactorizations.jl does not work in Julia 1.5\n\nTest 1 (set small positive definite matrix be 1e-12*Matrix(1.0*I,2,2) )\n```using Random\nusing LinearAlgebra\nusing Statistics\nusing Distributions\nusing Turing\nusing MatrixFactorizations\n\n# data\nu_account = [1.0; 2.0];\n@model bayes_lna(y) = begin\n    log_delta ~ Normal(0.0,1.0) \n    delta = [exp(log_delta)]\n    phi = [delta  0; 10^-7 delta]\n    B = (phi+phi')/2\n    H = polar(B,alg=:newton).H\n    phi_new = (B+H)/2 + 1e-12*Matrix(1.0*I,2,2)\n    y[:] ~ MvNormal([0.0,0.0],phi_new) \nend\nRandom.seed!(87654)\niterations = 2000\nchain = sample(bayes_lna(u_account), NUTS(0.65),iterations)```\nThen the code could work well.\n```┌ Info: Found initial step size\n│   ϵ = 0.8\n└ @ Turing.Inference C:\\Users\\Yushang\\.julia\\packages\\Turing\\uAz5c\\src\\inference\\hmc.jl:195\nSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\nChains MCMC chain (2000×13×1 Array{Float64, 3}):\n\nIterations        = 1:2000\nThinning interval = 1\nChains            = 1\nSamples per chain = 2000\nparameters        = log_delta\ninternals         = acceptance_rate, hamiltonian_energy, hamiltonian_energy_error, is_accept, log_density, lp, max_hamiltonian_energy_error, n_steps, nom_step_size, numerical_error, step_size, tree_depth\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse        ess      rhat \n      Symbol   Float64   Float64    Float64   Float64    Float64   Float64 \n\n   log_delta    0.6325    0.6441     0.0144    0.0215   809.5618    1.0000\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n   log_delta   -0.5494    0.1965    0.5782    1.0488    2.0147```\nTest 2 ((set small positive definite matrix be 1e-10*Matrix(1.0*I,2,2) ))\n```using Random\nusing LinearAlgebra\nusing Statistics\nusing Distributions\nusing Turing\nusing MatrixFactorizations\n\n# data\nu_account = [1.0; 2.0];\n@model bayes_lna(y) = begin\n    log_delta ~ Normal(0.0,1.0) \n    delta = [exp(log_delta)]\n    phi = [delta  0; 10^-7 delta]\n    B = (phi+phi')/2\n    H = polar(B,alg=:newton).H\n    phi_new = (B+H)/2 + 1e-10*Matrix(1.0*I,2,2)\n    y[:] ~ MvNormal([0.0,0.0],phi_new) \nend\nRandom.seed!(87654)\niterations = 2000\nchain = sample(bayes_lna(u_account), NUTS(0.65),iterations)```\nThen, it will lead to positive definite errors: (error messages attaches). Could you help me with this?","files":[{"id":"F01SBHH5GQ3","created":1616612483,"timestamp":1616612483,"name":"error message.pdf","title":"error message.pdf","mimetype":"application/pdf","filetype":"pdf","pretty_type":"PDF","user":"U01Q398M3QB","editable":false,"size":38269,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01SBHH5GQ3/error_message.pdf","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01SBHH5GQ3/download/error_message.pdf","thumb_pdf":"https://files.slack.com/files-tmb/T68168MUP-F01SBHH5GQ3-2a71ff1730/error_message_thumb_pdf.png","thumb_pdf_w":935,"thumb_pdf_h":1210,"permalink":"https://julialang.slack.com/files/U01Q398M3QB/F01SBHH5GQ3/error_message.pdf","permalink_public":"https://slack-files.com/T68168MUP-F01SBHH5GQ3-36f1e5d5b2","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"R59","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U8T9JUA5R"},{"type":"text","text":" Thanks very much David. However, by using polar decomposition in MatrixFactorizations.jl to formulate nearest symmetry positive semidefinite matrix and then add small positive definite matrix to make it positive definite still doesn't work well all the time due to the numerical errors.\n\nI test 2000 iterations in Julia 1.6.0 since polar decomposition in  MatrixFactorizations.jl does not work in Julia 1.5\n\nTest 1 (set small positive definite matrix be 1e-12*Matrix(1.0*I,2,2) )\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Random\nusing LinearAlgebra\nusing Statistics\nusing Distributions\nusing Turing\nusing MatrixFactorizations\n\n# data\nu_account = [1.0; 2.0];\n@model bayes_lna(y) = begin\n    log_delta ~ Normal(0.0,1.0) \n    delta = [exp(log_delta)]\n    phi = [delta  0; 10^-7 delta]\n    B = (phi+phi')/2\n    H = polar(B,alg=:newton).H\n    phi_new = (B+H)/2 + 1e-12*Matrix(1.0*I,2,2)\n    y[:] ~ MvNormal([0.0,0.0],phi_new) \nend\nRandom.seed!(87654)\niterations = 2000\nchain = sample(bayes_lna(u_account), NUTS(0.65),iterations)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Then the code could work well.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"┌ Info: Found initial step size\n│   ϵ = 0.8\n└ @ Turing.Inference C:\\Users\\Yushang\\.julia\\packages\\Turing\\uAz5c\\src\\inference\\hmc.jl:195\nSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\nChains MCMC chain (2000×13×1 Array{Float64, 3}):\n\nIterations        = 1:2000\nThinning interval = 1\nChains            = 1\nSamples per chain = 2000\nparameters        = log_delta\ninternals         = acceptance_rate, hamiltonian_energy, hamiltonian_energy_error, is_accept, log_density, lp, max_hamiltonian_energy_error, n_steps, nom_step_size, numerical_error, step_size, tree_depth\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse        ess      rhat \n      Symbol   Float64   Float64    Float64   Float64    Float64   Float64 \n\n   log_delta    0.6325    0.6441     0.0144    0.0215   809.5618    1.0000\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n   log_delta   -0.5494    0.1965    0.5782    1.0488    2.0147"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nTest 2 ((set small positive definite matrix be 1e-10*Matrix(1.0*I,2,2) ))\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Random\nusing LinearAlgebra\nusing Statistics\nusing Distributions\nusing Turing\nusing MatrixFactorizations\n\n# data\nu_account = [1.0; 2.0];\n@model bayes_lna(y) = begin\n    log_delta ~ Normal(0.0,1.0) \n    delta = [exp(log_delta)]\n    phi = [delta  0; 10^-7 delta]\n    B = (phi+phi')/2\n    H = polar(B,alg=:newton).H\n    phi_new = (B+H)/2 + 1e-10*Matrix(1.0*I,2,2)\n    y[:] ~ MvNormal([0.0,0.0],phi_new) \nend\nRandom.seed!(87654)\niterations = 2000\nchain = sample(bayes_lna(u_account), NUTS(0.65),iterations)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Then, it will lead to positive definite errors: (error messages attaches). Could you help me with this?"}]}]}],"user":"U01Q398M3QB","display_as_bot":false,"ts":"1616612512.070100","edited":{"user":"U01Q398M3QB","ts":"1616612614.000000"},"thread_ts":"1616578091.056700","parent_user_id":"U01Q398M3QB"},{"client_msg_id":"db71265f-42eb-453b-a2b3-8e66b7b12aea","type":"message","text":"<@U8T9JUA5R> <@U6C937ENB> <@UHDNY2YMA> <@U6H9SJKCH> @Hi! Daivd, Moritz, Tor and Kai,  I'm not very clear about how to create subtype of Distributions.AbstractMvNormal  by replacing the original build-in Cholesky() decomposition in MvNormal()  with a new svd() decomposition in ChainRule.jl rrule. Could you you help me with this?\n\nAcutally, I'm very curious about how the Julia Gaussian process promises its covariance matrixes are symmetry and positive definite. Could we apply the similar tactic here? Thanks a lot in advance!\n\nSome possible useful disclosures and package attaches.\n\nChainRule.jl     svd\n<https://github.com/JuliaDiff/ChainRules.jl/blob/master/src/rulesets/LinearAlgebra/factorization.jl>\n\n*<https://discourse.julialang.org/t/using-flux-to-optimize-a-function-of-the-singular-values/39687/3|Using Flux to optimize a function of the Singular Values>*\n<https://discourse.julialang.org/t/using-flux-to-optimize-a-function-of-the-singular-values/39687/3>\n\nAuto differentiation over linear algebras (a Zygote extension)\n<https://github.com/GiggleLiu/BackwardsLinalg.jl>","user":"U01Q398M3QB","ts":"1616615322.070900","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"Using Flux to optimize a function of the Singular Values","title_link":"https://discourse.julialang.org/t/using-flux-to-optimize-a-function-of-the-singular-values/39687/3","text":"It appears that you are using a very old version of Flux, later versions do not use TrackedArrays anymore. Maybe you could try to update Flux to the latest version?","fallback":"JuliaLang: Using Flux to optimize a function of the Singular Values","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","ts":1589799658,"from_url":"https://discourse.julialang.org/t/using-flux-to-optimize-a-function-of-the-singular-values/39687/3","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/using-flux-to-optimize-a-function-of-the-singular-values/39687/3"}],"blocks":[{"type":"rich_text","block_id":"eXbzR","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U8T9JUA5R"},{"type":"text","text":" "},{"type":"user","user_id":"U6C937ENB"},{"type":"text","text":" "},{"type":"user","user_id":"UHDNY2YMA"},{"type":"text","text":" "},{"type":"user","user_id":"U6H9SJKCH"},{"type":"text","text":" @Hi! Daivd, Moritz, Tor and Kai,  I'm not very clear about how to create subtype of Distributions.AbstractMvNormal  by replacing the original build-in Cholesky() decomposition in MvNormal()  with a new svd() decomposition in ChainRule.jl rrule. Could you you help me with this?\n\nAcutally, I'm very curious about how the Julia Gaussian process promises its covariance matrixes are symmetry and positive definite. Could we apply the similar tactic here? Thanks a lot in advance!\n\nSome possible useful disclosures and package attaches.\n\nChainRule.jl     svd\n"},{"type":"link","url":"https://github.com/JuliaDiff/ChainRules.jl/blob/master/src/rulesets/LinearAlgebra/factorization.jl"},{"type":"text","text":"\n\n"},{"type":"link","url":"https://discourse.julialang.org/t/using-flux-to-optimize-a-function-of-the-singular-values/39687/3","text":"Using Flux to optimize a function of the Singular Values","style":{"bold":true}},{"type":"text","text":"\n"},{"type":"link","url":"https://discourse.julialang.org/t/using-flux-to-optimize-a-function-of-the-singular-values/39687/3"},{"type":"text","text":"\n\nAuto differentiation over linear algebras (a Zygote extension)\n"},{"type":"link","url":"https://github.com/GiggleLiu/BackwardsLinalg.jl"}]}]}],"thread_ts":"1616578091.056700","parent_user_id":"U01Q398M3QB"},{"client_msg_id":"c869fe9e-0cdc-48f0-90c6-b8da155fff76","type":"message","text":"<@U8T9JUA5R> Hi! David, I need your help. Is there a way to zero out the numerical errors raised during the calculation of nearest semipositive definite matrix in the code below? It should be a PD matrix after all the construction steps.\n```using Random\nusing LinearAlgebra\nusing Statistics\nusing Distributions\nusing Turing\nusing MatrixFactorizations\n# data\nu_account = [1.0; 2.0];\n@model bayes_lna(y) = begin\n    log_delta ~ Normal(0.0,1.0) \n    delta = [exp(log_delta)]\n    phi = [delta  0; 10^-7 delta]\n    B = (phi+phi')/2\n    H = polar(B,alg=:newton).H\n    phi_new = (B+H)/2 + 1e-10*Matrix(1.0*I,2,2)\n    y[:] ~ MvNormal([0.0,0.0],phi_new) \nend\nRandom.seed!(87654)\niterations = 2000\nchain = sample(bayes_lna(u_account), NUTS(0.65),iterations)```\n","user":"U01Q398M3QB","ts":"1616644544.071400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KZylN","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U8T9JUA5R"},{"type":"text","text":" Hi! David, I need your help. Is there a way to zero out the numerical errors raised during the calculation of nearest semipositive definite matrix in the code below? It should be a PD matrix after all the construction steps.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Random\nusing LinearAlgebra\nusing Statistics\nusing Distributions\nusing Turing\nusing MatrixFactorizations\n# data\nu_account = [1.0; 2.0];\n@model bayes_lna(y) = begin\n    log_delta ~ Normal(0.0,1.0) \n    delta = [exp(log_delta)]\n    phi = [delta  0; 10^-7 delta]\n    B = (phi+phi')/2\n    H = polar(B,alg=:newton).H\n    phi_new = (B+H)/2 + 1e-10*Matrix(1.0*I,2,2)\n    y[:] ~ MvNormal([0.0,0.0],phi_new) \nend\nRandom.seed!(87654)\niterations = 2000\nchain = sample(bayes_lna(u_account), NUTS(0.65),iterations)"}]},{"type":"rich_text_section","elements":[]}]}],"thread_ts":"1616578091.056700","parent_user_id":"U01Q398M3QB"},{"client_msg_id":"8877a69a-7212-4078-8f3f-50873c662970","type":"message","text":"<@U687RKK0E> <@U67SCG4HG> <@U678RKJ5A> <@U680T6770> <@U9MU673LL> <@UBTLBECD7> <@UHFBL1W4X> <@UDXST8ARK> <@U6771NKS6> <@U011TRN4QBU> Hi! Developers of LinearAlgebra.jl eigen.jl\n\nI'm currently using Turing.jl for Bayesian inference. During the process, the variables are automatically change into ForwardDiff.Dual type and <@U8T9JUA5R> David helped me find that eigen() does not work with dual numbers. Could you help me with this? The code and error messages attache below. Thanks a lot in advance!\n\n```Turing.setadbackend(:forwarddiff)\nusing Random\nusing DifferentialEquations\nusing LinearAlgebra\nusing Statistics\nusing Distributions\nusing Turing\n# data\nu_account = [1.0; 2.0];\n@model bayes_lna(y) = begin\n    log_delta ~ Normal(0.0,1.0) \n    delta = [exp(log_delta)]\n    phi = [delta  0; 10^-7 delta] # covariance matrix \n    D,V = LinearAlgebra.eigen(LinearAlgebra.Symmetric(phi))  # get eigen-values and eigen-vectos\n    D .= max.(D,1e-12) # force eigen values to be positive\n    phi_new = V*LinearAlgebra.Diagonal(D)*V' # create new symmetry and positive definite matrix\n    y[:] ~ MvNormal([0.0,0.0],LinearAlgebra.Symmetric(phi_new))\nend\nRandom.seed!(87654)\niterations = 20\nchain = sample(bayes_lna(u_account), NUTS(0.65),iterations)```\n\n```MethodError: no method matching eigen!(::Symmetric{ForwardDiff.Dual{ForwardDiff.Tag{Turing.Core.var\"#f#7\"{DynamicPPL.TypedVarInfo{NamedTuple{(:log_delta,), Tuple{DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:log_delta, Tuple{}}, Int64}, Vector{Normal{Float64}}, Vector{DynamicPPL.VarName{:log_delta, Tuple{}}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}}}, Float64}, DynamicPPL.Model{var\"#49#50\", (:y,), (), (), Tuple{Vector{Float64}}, Tuple{}}, DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40}, (), AdvancedHMC.DiagEuclideanMetric}}, DynamicPPL.DefaultContext}, Float64}, Float64, 1}, Matrix{ForwardDiff.Dual{ForwardDiff.Tag{Turing.Core.var\"#f#7\"{DynamicPPL.TypedVarInfo{NamedTuple{(:log_delta,), Tuple{DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:log_delta, Tuple{}}, Int64}, Vector{Normal{Float64}}, Vector{DynamicPPL.VarName{:log_delta, Tuple{}}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}}}, Float64}, DynamicPPL.Model{var\"#49#50\", (:y,), (), (), Tuple{Vector{Float64}}, Tuple{}}, DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40}, (), AdvancedHMC.DiagEuclideanMetric}}, DynamicPPL.DefaultContext}, Float64}, Float64, 1}}}; sortby=nothing)\nClosest candidates are:\n  eigen!(::StridedMatrix{T}; permute, scale, sortby) where T&lt;:Union{Float32, Float64} at C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.6\\LinearAlgebra\\src\\eigen.jl:148\n  eigen!(::StridedMatrix{T}; permute, scale, sortby) where T&lt;:Union{ComplexF32, ComplexF64} at C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.6\\LinearAlgebra\\src\\eigen.jl:171\n  eigen!(::StridedMatrix{T}, ::StridedMatrix{T}; sortby) where T&lt;:Union{Float32, Float64} at C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.6\\LinearAlgebra\\src\\eigen.jl:427\n  ...```\n","user":"U01Q398M3QB","ts":"1616646126.071600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DJZ=","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U687RKK0E"},{"type":"text","text":" "},{"type":"user","user_id":"U67SCG4HG"},{"type":"text","text":" "},{"type":"user","user_id":"U678RKJ5A"},{"type":"text","text":" "},{"type":"user","user_id":"U680T6770"},{"type":"text","text":" "},{"type":"user","user_id":"U9MU673LL"},{"type":"text","text":" "},{"type":"user","user_id":"UBTLBECD7"},{"type":"text","text":" "},{"type":"user","user_id":"UHFBL1W4X"},{"type":"text","text":" "},{"type":"user","user_id":"UDXST8ARK"},{"type":"text","text":" "},{"type":"user","user_id":"U6771NKS6"},{"type":"text","text":" "},{"type":"user","user_id":"U011TRN4QBU"},{"type":"text","text":" Hi! Developers of LinearAlgebra.jl eigen.jl\n\nI'm currently using Turing.jl for Bayesian inference. During the process, the variables are automatically change into ForwardDiff.Dual type and "},{"type":"user","user_id":"U8T9JUA5R"},{"type":"text","text":" David helped me find that eigen() does not work with dual numbers. Could you help me with this? The code and error messages attache below. Thanks a lot in advance!\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Turing.setadbackend(:forwarddiff)\nusing Random\nusing DifferentialEquations\nusing LinearAlgebra\nusing Statistics\nusing Distributions\nusing Turing\n# data\nu_account = [1.0; 2.0];\n@model bayes_lna(y) = begin\n    log_delta ~ Normal(0.0,1.0) \n    delta = [exp(log_delta)]\n    phi = [delta  0; 10^-7 delta] # covariance matrix \n    D,V = LinearAlgebra.eigen(LinearAlgebra.Symmetric(phi))  # get eigen-values and eigen-vectos\n    D .= max.(D,1e-12) # force eigen values to be positive\n    phi_new = V*LinearAlgebra.Diagonal(D)*V' # create new symmetry and positive definite matrix\n    y[:] ~ MvNormal([0.0,0.0],LinearAlgebra.Symmetric(phi_new))\nend\nRandom.seed!(87654)\niterations = 20\nchain = sample(bayes_lna(u_account), NUTS(0.65),iterations)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"MethodError: no method matching eigen!(::Symmetric{ForwardDiff.Dual{ForwardDiff.Tag{Turing.Core.var\"#f#7\"{DynamicPPL.TypedVarInfo{NamedTuple{(:log_delta,), Tuple{DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:log_delta, Tuple{}}, Int64}, Vector{Normal{Float64}}, Vector{DynamicPPL.VarName{:log_delta, Tuple{}}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}}}, Float64}, DynamicPPL.Model{var\"#49#50\", (:y,), (), (), Tuple{Vector{Float64}}, Tuple{}}, DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40}, (), AdvancedHMC.DiagEuclideanMetric}}, DynamicPPL.DefaultContext}, Float64}, Float64, 1}, Matrix{ForwardDiff.Dual{ForwardDiff.Tag{Turing.Core.var\"#f#7\"{DynamicPPL.TypedVarInfo{NamedTuple{(:log_delta,), Tuple{DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:log_delta, Tuple{}}, Int64}, Vector{Normal{Float64}}, Vector{DynamicPPL.VarName{:log_delta, Tuple{}}}, Vector{Float64}, Vector{Set{DynamicPPL.Selector}}}}}, Float64}, DynamicPPL.Model{var\"#49#50\", (:y,), (), (), Tuple{Vector{Float64}}, Tuple{}}, DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40}, (), AdvancedHMC.DiagEuclideanMetric}}, DynamicPPL.DefaultContext}, Float64}, Float64, 1}}}; sortby=nothing)\nClosest candidates are:\n  eigen!(::StridedMatrix{T}; permute, scale, sortby) where T<:Union{Float32, Float64} at C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.6\\LinearAlgebra\\src\\eigen.jl:148\n  eigen!(::StridedMatrix{T}; permute, scale, sortby) where T<:Union{ComplexF32, ComplexF64} at C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.6\\LinearAlgebra\\src\\eigen.jl:171\n  eigen!(::StridedMatrix{T}, ::StridedMatrix{T}; sortby) where T<:Union{Float32, Float64} at C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.6\\LinearAlgebra\\src\\eigen.jl:427\n  ..."}]},{"type":"rich_text_section","elements":[]}]}],"thread_ts":"1616578091.056700","parent_user_id":"U01Q398M3QB"},{"client_msg_id":"44353B94-EA3D-41A1-A09F-F0AFD50D7BAA","type":"message","text":"Do you have FenericLinearAlgebra loaded? If so, could you try to wrap phi in Hermitian instead of Symmetric?","user":"U680T6770","ts":"1616651619.073000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WeHe","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do you have FenericLinearAlgebra loaded? If so, could you try to wrap phi in Hermitian instead of Symmetric?"}]}]}],"thread_ts":"1616578091.056700","parent_user_id":"U01Q398M3QB"},{"client_msg_id":"302d47c9-b1c4-4ed9-83cf-4eb6b19e2a39","type":"message","text":"<@U680T6770> Thanks Andreas, I tried GenricLinearAlgebra.Hermitian, thisworks for the 2 by 2 covariance matrix! However, when I tried 3 by 3 covariance matrix, it says matrix not Hermitian. Could you help me with this? Thanks a lot in advance!\n```using Random\nusing DifferentialEquations\nusing LinearAlgebra\nusing Statistics\nusing Distributions\nusing Turing\nusing GenericLinearAlgebra\nTuring.setadbackend(:forwarddiff)\n# data\nu_account = [1.0; 2.0];\n@model bayes_lna(y) = begin\n    log_delta ~ Normal(0.0,1.0) \n    delta = [exp(log_delta)]\n    phi = [delta  0; 10^-7 delta] # covariance matrix \n    D,V = eigen(GenericLinearAlgebra.Hermitian(phi))  # get eigen-values and eigen-vectos\n    D .= max.(D,1e-12) # force eigen values to be positive\n    phi_new = V*Diagonal(D)*V' # create new symmetry and positive definite matrix\n    y[:] ~ MvNormal([0.0,0.0],phi_new)\nend\nRandom.seed!(87654)\niterations = 200\nchain = sample(bayes_lna(u_account), NUTS(0.65),iterations)```\n```┌ Info: Found initial step size\n│   ϵ = 0.8\n└ @ Turing.Inference C:\\Users\\Yushang\\.julia\\packages\\Turing\\uAz5c\\src\\inference\\hmc.jl:195\nSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\nChains MCMC chain (200×13×1 Array{Float64, 3}):\n\nIterations        = 1:200\nThinning interval = 1\nChains            = 1\nSamples per chain = 200\nparameters        = log_delta\ninternals         = acceptance_rate, hamiltonian_energy, hamiltonian_energy_error, is_accept, log_density, lp, max_hamiltonian_energy_error, n_steps, nom_step_size, numerical_error, step_size, tree_depth\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse        ess      rhat \n      Symbol   Float64   Float64    Float64   Float64    Float64   Float64 \n\n   log_delta    0.6260    0.6524     0.0461    0.0462   104.8564    1.0000\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n   log_delta   -0.4480    0.1357    0.5691    0.9888    2.0273```\nHowever, if I increase the covariance matrix to the 3 by 3 this will still leads to the error PosDefException: matrix is not Hermitian; Cholesky factorization failed.\n\n```using Random\nusing DifferentialEquations\nusing LinearAlgebra\nusing Statistics\nusing Distributions\nusing Turing\nusing GenericLinearAlgebra\nTuring.setadbackend(:forwarddiff)\n# data\nu_account = [1.0; 2.0; 3.0];\n@model bayes_lna(y) = begin\n    log_delta ~ Normal(0.0,1.0) \n    delta = [exp(log_delta)]\n    phi = [delta 10^-8 10^-9; 10^-3 delta 10^-6; 10^-4 10^-7 delta] # covariance matrix \n    D,V = eigen(GenericLinearAlgebra.Hermitian(phi))  # get eigen-values and eigen-vectos\n    D .= max.(D,1e-12) # force eigen values to be positive\n    phi_new = V*Diagonal(D)*V' # create new symmetry and positive definite matrix\n    y[:] ~ MvNormal([0.0,0.0,0.0],phi_new)\nend\nRandom.seed!(87564)\niterations = 200\nchain = sample(bayes_lna(u_account), NUTS(0.65),iterations)```\n```PosDefException: matrix is not Hermitian; Cholesky factorization failed.\n\nStacktrace:\n  [1] checkpositivedefinite(info::Int64)\n    @ LinearAlgebra C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.6\\LinearAlgebra\\src\\factorization.jl:18\n  [2] cholesky!(A::Matrix{Float64}, ::Val{false}; check::Bool)\n    @ LinearAlgebra C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.6\\LinearAlgebra\\src\\cholesky.jl:282\n  [3] #cholesky#130\n    @ C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.6\\LinearAlgebra\\src\\cholesky.jl:378 [inlined]\n  [4] cholesky (repeats 2 times)\n    @ C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.6\\LinearAlgebra\\src\\cholesky.jl:378 [inlined]\n  [5] PDMat\n    @ ~\\.julia\\packages\\PDMats\\Rw2Hf\\src\\pdmat.jl:19 [inlined]\n  [6] MvNormal(μ::Vector{Float64}, Σ::Matrix{Float64})\n    @ Distributions ~\\.julia\\packages\\Distributions\\cNe2C\\src\\multivariate\\mvnormal.jl:211\n  [7] #5\n    @ .\\In[5]:18 [inlined]\n  [8] (::var\"#5#6\")(_rng::Random._GLOBAL_RNG, _model::DynamicPPL.Model{var\"#5#6\", (:y,), (), (), Tuple{Vector{Float64}}, Tuple{}}, _varinfo::DynamicPPL.UntypedVarInfo{DynamicPPL.Metadata{Dict{DynamicPPL.VarName, Int64}, Vector{Distribution}, Vector{DynamicPPL.VarName}, Vector{Real}, Vector{Set{DynamicPPL.Selector}}}, Float64}, _sampler::DynamicPPL.SampleFromUniform, _context::DynamicPPL.DefaultContext, y::Vector{Float64})\n    @ Main .\\none:0\n  [9] macro expansion\n    @ ~\\.julia\\packages\\DynamicPPL\\wf0dU\\src\\model.jl:0 [inlined]\n [10] _evaluate\n    @ ~\\.julia\\packages\\DynamicPPL\\wf0dU\\src\\model.jl:154 [inlined]\n [11] evaluate_threadunsafe\n    @ ~\\.julia\\packages\\DynamicPPL\\wf0dU\\src\\model.jl:127 [inlined]\n [12] Model\n    @ ~\\.julia\\packages\\DynamicPPL\\wf0dU\\src\\model.jl:92 [inlined]\n [13] VarInfo\n    @ ~\\.julia\\packages\\DynamicPPL\\wf0dU\\src\\varinfo.jl:126 [inlined]\n [14] VarInfo\n    @ ~\\.julia\\packages\\DynamicPPL\\wf0dU\\src\\varinfo.jl:125 [inlined]\n [15] step(rng::Random._GLOBAL_RNG, model::DynamicPPL.Model{var\"#5#6\", (:y,), (), (), Tuple{Vector{Float64}}, Tuple{}}, spl::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40}, (), AdvancedHMC.DiagEuclideanMetric}}; resume_from::Nothing, kwargs::Base.Iterators.Pairs{Symbol, Int64, Tuple{Symbol}, NamedTuple{(:nadapts,), Tuple{Int64}}})\n    @ DynamicPPL ~\\.julia\\packages\\DynamicPPL\\wf0dU\\src\\sampler.jl:73\n [16] macro expansion\n    @ ~\\.julia\\packages\\AbstractMCMC\\oou1a\\src\\sample.jl:97 [inlined]\n [17] macro expansion\n    @ ~\\.julia\\packages\\ProgressLogging\\6KXlp\\src\\ProgressLogging.jl:328 [inlined]\n [18] (::AbstractMCMC.var\"#20#21\"{Bool, String, Nothing, Int64, Int64, Base.Iterators.Pairs{Symbol, Int64, Tuple{Symbol}, NamedTuple{(:nadapts,), Tuple{Int64}}}, Random._GLOBAL_RNG, DynamicPPL.Model{var\"#5#6\", (:y,), (), (), Tuple{Vector{Float64}}, Tuple{}}, DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40}, (), AdvancedHMC.DiagEuclideanMetric}}, Int64, Int64})()\n    @ AbstractMCMC ~\\.julia\\packages\\AbstractMCMC\\oou1a\\src\\logging.jl:11\n [19] with_logstate(f::Function, logstate::Any)\n    @ Base.CoreLogging .\\logging.jl:491\n [20] with_logger(f::Function, logger::LoggingExtras.TeeLogger{Tuple{LoggingExtras.EarlyFilteredLogger{ConsoleProgressMonitor.ProgressLogger, AbstractMCMC.var\"#1#3\"{Module}}, LoggingExtras.EarlyFilteredLogger{Base.CoreLogging.SimpleLogger, AbstractMCMC.var\"#2#4\"{Module}}}})\n    @ Base.CoreLogging .\\logging.jl:603\n [21] with_progresslogger(f::Function, _module::Module, logger::Base.CoreLogging.SimpleLogger)\n    @ AbstractMCMC ~\\.julia\\packages\\AbstractMCMC\\oou1a\\src\\logging.jl:34\n [22] macro expansion\n    @ ~\\.julia\\packages\\AbstractMCMC\\oou1a\\src\\logging.jl:10 [inlined]\n [23] mcmcsample(rng::Random._GLOBAL_RNG, model::DynamicPPL.Model{var\"#5#6\", (:y,), (), (), Tuple{Vector{Float64}}, Tuple{}}, sampler::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40}, (), AdvancedHMC.DiagEuclideanMetric}}, N::Int64; progress::Bool, progressname::String, callback::Nothing, discard_initial::Int64, thinning::Int64, chain_type::Type, kwargs::Base.Iterators.Pairs{Symbol, Int64, Tuple{Symbol}, NamedTuple{(:nadapts,), Tuple{Int64}}})\n    @ AbstractMCMC ~\\.julia\\packages\\AbstractMCMC\\oou1a\\src\\sample.jl:88\n [24] sample(rng::Random._GLOBAL_RNG, model::DynamicPPL.Model{var\"#5#6\", (:y,), (), (), Tuple{Vector{Float64}}, Tuple{}}, sampler::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40}, (), AdvancedHMC.DiagEuclideanMetric}}, N::Int64; chain_type::Type, resume_from::Nothing, progress::Bool, nadapts::Int64, discard_adapt::Bool, discard_initial::Int64, kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})\n    @ Turing.Inference ~\\.julia\\packages\\Turing\\uAz5c\\src\\inference\\hmc.jl:140\n [25] sample\n    @ ~\\.julia\\packages\\Turing\\uAz5c\\src\\inference\\hmc.jl:123 [inlined]\n [26] #sample#2\n    @ ~\\.julia\\packages\\Turing\\uAz5c\\src\\inference\\Inference.jl:142 [inlined]\n [27] sample\n    @ ~\\.julia\\packages\\Turing\\uAz5c\\src\\inference\\Inference.jl:142 [inlined]\n [28] #sample#1\n    @ ~\\.julia\\packages\\Turing\\uAz5c\\src\\inference\\Inference.jl:132 [inlined]\n [29] sample(model::DynamicPPL.Model{var\"#5#6\", (:y,), (), (), Tuple{Vector{Float64}}, Tuple{}}, alg::NUTS{Turing.Core.ForwardDiffAD{40}, (), AdvancedHMC.DiagEuclideanMetric}, N::Int64)\n    @ Turing.Inference ~\\.julia\\packages\\Turing\\uAz5c\\src\\inference\\Inference.jl:132\n [30] top-level scope\n    @ In[5]:22\n [31] eval\n    @ .\\boot.jl:360 [inlined]\n [32] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n    @ Base .\\loading.jl:1094```","user":"U01Q398M3QB","ts":"1616656456.074300","team":"T68168MUP","edited":{"user":"U01Q398M3QB","ts":"1616656571.000000"},"blocks":[{"type":"rich_text","block_id":"1oWK","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U680T6770"},{"type":"text","text":" Thanks Andreas, I tried GenricLinearAlgebra.Hermitian, thisworks for the 2 by 2 covariance matrix! However, when I tried 3 by 3 covariance matrix, it says matrix not Hermitian. Could you help me with this? Thanks a lot in advance!\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Random\nusing DifferentialEquations\nusing LinearAlgebra\nusing Statistics\nusing Distributions\nusing Turing\nusing GenericLinearAlgebra\nTuring.setadbackend(:forwarddiff)\n# data\nu_account = [1.0; 2.0];\n@model bayes_lna(y) = begin\n    log_delta ~ Normal(0.0,1.0) \n    delta = [exp(log_delta)]\n    phi = [delta  0; 10^-7 delta] # covariance matrix \n    D,V = eigen(GenericLinearAlgebra.Hermitian(phi))  # get eigen-values and eigen-vectos\n    D .= max.(D,1e-12) # force eigen values to be positive\n    phi_new = V*Diagonal(D)*V' # create new symmetry and positive definite matrix\n    y[:] ~ MvNormal([0.0,0.0],phi_new)\nend\nRandom.seed!(87654)\niterations = 200\nchain = sample(bayes_lna(u_account), NUTS(0.65),iterations)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"┌ Info: Found initial step size\n│   ϵ = 0.8\n└ @ Turing.Inference C:\\Users\\Yushang\\.julia\\packages\\Turing\\uAz5c\\src\\inference\\hmc.jl:195\nSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\nChains MCMC chain (200×13×1 Array{Float64, 3}):\n\nIterations        = 1:200\nThinning interval = 1\nChains            = 1\nSamples per chain = 200\nparameters        = log_delta\ninternals         = acceptance_rate, hamiltonian_energy, hamiltonian_energy_error, is_accept, log_density, lp, max_hamiltonian_energy_error, n_steps, nom_step_size, numerical_error, step_size, tree_depth\n\nSummary Statistics\n  parameters      mean       std   naive_se      mcse        ess      rhat \n      Symbol   Float64   Float64    Float64   Float64    Float64   Float64 \n\n   log_delta    0.6260    0.6524     0.0461    0.0462   104.8564    1.0000\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n   log_delta   -0.4480    0.1357    0.5691    0.9888    2.0273"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"However, if I increase the covariance matrix to the 3 by 3 this will still leads to the error PosDefException: matrix is not Hermitian; Cholesky factorization failed.\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Random\nusing DifferentialEquations\nusing LinearAlgebra\nusing Statistics\nusing Distributions\nusing Turing\nusing GenericLinearAlgebra\nTuring.setadbackend(:forwarddiff)\n# data\nu_account = [1.0; 2.0; 3.0];\n@model bayes_lna(y) = begin\n    log_delta ~ Normal(0.0,1.0) \n    delta = [exp(log_delta)]\n    phi = [delta 10^-8 10^-9; 10^-3 delta 10^-6; 10^-4 10^-7 delta] # covariance matrix \n    D,V = eigen(GenericLinearAlgebra.Hermitian(phi))  # get eigen-values and eigen-vectos\n    D .= max.(D,1e-12) # force eigen values to be positive\n    phi_new = V*Diagonal(D)*V' # create new symmetry and positive definite matrix\n    y[:] ~ MvNormal([0.0,0.0,0.0],phi_new)\nend\nRandom.seed!(87564)\niterations = 200\nchain = sample(bayes_lna(u_account), NUTS(0.65),iterations)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"PosDefException: matrix is not Hermitian; Cholesky factorization failed.\n\nStacktrace:\n  [1] checkpositivedefinite(info::Int64)\n    @ LinearAlgebra C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.6\\LinearAlgebra\\src\\factorization.jl:18\n  [2] cholesky!(A::Matrix{Float64}, ::Val{false}; check::Bool)\n    @ LinearAlgebra C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.6\\LinearAlgebra\\src\\cholesky.jl:282\n  [3] #cholesky#130\n    @ C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.6\\LinearAlgebra\\src\\cholesky.jl:378 [inlined]\n  [4] cholesky (repeats 2 times)\n    @ C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.6\\LinearAlgebra\\src\\cholesky.jl:378 [inlined]\n  [5] PDMat\n    @ ~\\.julia\\packages\\PDMats\\Rw2Hf\\src\\pdmat.jl:19 [inlined]\n  [6] MvNormal(μ::Vector{Float64}, Σ::Matrix{Float64})\n    @ Distributions ~\\.julia\\packages\\Distributions\\cNe2C\\src\\multivariate\\mvnormal.jl:211\n  [7] #5\n    @ .\\In[5]:18 [inlined]\n  [8] (::var\"#5#6\")(_rng::Random._GLOBAL_RNG, _model::DynamicPPL.Model{var\"#5#6\", (:y,), (), (), Tuple{Vector{Float64}}, Tuple{}}, _varinfo::DynamicPPL.UntypedVarInfo{DynamicPPL.Metadata{Dict{DynamicPPL.VarName, Int64}, Vector{Distribution}, Vector{DynamicPPL.VarName}, Vector{Real}, Vector{Set{DynamicPPL.Selector}}}, Float64}, _sampler::DynamicPPL.SampleFromUniform, _context::DynamicPPL.DefaultContext, y::Vector{Float64})\n    @ Main .\\none:0\n  [9] macro expansion\n    @ ~\\.julia\\packages\\DynamicPPL\\wf0dU\\src\\model.jl:0 [inlined]\n [10] _evaluate\n    @ ~\\.julia\\packages\\DynamicPPL\\wf0dU\\src\\model.jl:154 [inlined]\n [11] evaluate_threadunsafe\n    @ ~\\.julia\\packages\\DynamicPPL\\wf0dU\\src\\model.jl:127 [inlined]\n [12] Model\n    @ ~\\.julia\\packages\\DynamicPPL\\wf0dU\\src\\model.jl:92 [inlined]\n [13] VarInfo\n    @ ~\\.julia\\packages\\DynamicPPL\\wf0dU\\src\\varinfo.jl:126 [inlined]\n [14] VarInfo\n    @ ~\\.julia\\packages\\DynamicPPL\\wf0dU\\src\\varinfo.jl:125 [inlined]\n [15] step(rng::Random._GLOBAL_RNG, model::DynamicPPL.Model{var\"#5#6\", (:y,), (), (), Tuple{Vector{Float64}}, Tuple{}}, spl::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40}, (), AdvancedHMC.DiagEuclideanMetric}}; resume_from::Nothing, kwargs::Base.Iterators.Pairs{Symbol, Int64, Tuple{Symbol}, NamedTuple{(:nadapts,), Tuple{Int64}}})\n    @ DynamicPPL ~\\.julia\\packages\\DynamicPPL\\wf0dU\\src\\sampler.jl:73\n [16] macro expansion\n    @ ~\\.julia\\packages\\AbstractMCMC\\oou1a\\src\\sample.jl:97 [inlined]\n [17] macro expansion\n    @ ~\\.julia\\packages\\ProgressLogging\\6KXlp\\src\\ProgressLogging.jl:328 [inlined]\n [18] (::AbstractMCMC.var\"#20#21\"{Bool, String, Nothing, Int64, Int64, Base.Iterators.Pairs{Symbol, Int64, Tuple{Symbol}, NamedTuple{(:nadapts,), Tuple{Int64}}}, Random._GLOBAL_RNG, DynamicPPL.Model{var\"#5#6\", (:y,), (), (), Tuple{Vector{Float64}}, Tuple{}}, DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40}, (), AdvancedHMC.DiagEuclideanMetric}}, Int64, Int64})()\n    @ AbstractMCMC ~\\.julia\\packages\\AbstractMCMC\\oou1a\\src\\logging.jl:11\n [19] with_logstate(f::Function, logstate::Any)\n    @ Base.CoreLogging .\\logging.jl:491\n [20] with_logger(f::Function, logger::LoggingExtras.TeeLogger{Tuple{LoggingExtras.EarlyFilteredLogger{ConsoleProgressMonitor.ProgressLogger, AbstractMCMC.var\"#1#3\"{Module}}, LoggingExtras.EarlyFilteredLogger{Base.CoreLogging.SimpleLogger, AbstractMCMC.var\"#2#4\"{Module}}}})\n    @ Base.CoreLogging .\\logging.jl:603\n [21] with_progresslogger(f::Function, _module::Module, logger::Base.CoreLogging.SimpleLogger)\n    @ AbstractMCMC ~\\.julia\\packages\\AbstractMCMC\\oou1a\\src\\logging.jl:34\n [22] macro expansion\n    @ ~\\.julia\\packages\\AbstractMCMC\\oou1a\\src\\logging.jl:10 [inlined]\n [23] mcmcsample(rng::Random._GLOBAL_RNG, model::DynamicPPL.Model{var\"#5#6\", (:y,), (), (), Tuple{Vector{Float64}}, Tuple{}}, sampler::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40}, (), AdvancedHMC.DiagEuclideanMetric}}, N::Int64; progress::Bool, progressname::String, callback::Nothing, discard_initial::Int64, thinning::Int64, chain_type::Type, kwargs::Base.Iterators.Pairs{Symbol, Int64, Tuple{Symbol}, NamedTuple{(:nadapts,), Tuple{Int64}}})\n    @ AbstractMCMC ~\\.julia\\packages\\AbstractMCMC\\oou1a\\src\\sample.jl:88\n [24] sample(rng::Random._GLOBAL_RNG, model::DynamicPPL.Model{var\"#5#6\", (:y,), (), (), Tuple{Vector{Float64}}, Tuple{}}, sampler::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40}, (), AdvancedHMC.DiagEuclideanMetric}}, N::Int64; chain_type::Type, resume_from::Nothing, progress::Bool, nadapts::Int64, discard_adapt::Bool, discard_initial::Int64, kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})\n    @ Turing.Inference ~\\.julia\\packages\\Turing\\uAz5c\\src\\inference\\hmc.jl:140\n [25] sample\n    @ ~\\.julia\\packages\\Turing\\uAz5c\\src\\inference\\hmc.jl:123 [inlined]\n [26] #sample#2\n    @ ~\\.julia\\packages\\Turing\\uAz5c\\src\\inference\\Inference.jl:142 [inlined]\n [27] sample\n    @ ~\\.julia\\packages\\Turing\\uAz5c\\src\\inference\\Inference.jl:142 [inlined]\n [28] #sample#1\n    @ ~\\.julia\\packages\\Turing\\uAz5c\\src\\inference\\Inference.jl:132 [inlined]\n [29] sample(model::DynamicPPL.Model{var\"#5#6\", (:y,), (), (), Tuple{Vector{Float64}}, Tuple{}}, alg::NUTS{Turing.Core.ForwardDiffAD{40}, (), AdvancedHMC.DiagEuclideanMetric}, N::Int64)\n    @ Turing.Inference ~\\.julia\\packages\\Turing\\uAz5c\\src\\inference\\Inference.jl:132\n [30] top-level scope\n    @ In[5]:22\n [31] eval\n    @ .\\boot.jl:360 [inlined]\n [32] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n    @ Base .\\loading.jl:1094"}]}]}],"thread_ts":"1616578091.056700","parent_user_id":"U01Q398M3QB"},{"client_msg_id":"E3B51D68-3E65-4888-860B-81E48E964CC2","type":"message","text":"Do you want to move this discussion to discourse <https://discourse.julialang.org|https://discourse.julialang.org> ? It gets a bit unwieldy here ","user":"U6C937ENB","ts":"1616656582.076000","team":"T68168MUP","attachments":[{"title":"JuliaLang","title_link":"https://discourse.julialang.org/","text":"The Julia programming language forum: discuss usage, development, packages, and community.","fallback":"JuliaLang","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","from_url":"https://discourse.julialang.org/","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","service_name":"discourse.julialang.org","id":1,"original_url":"https://discourse.julialang.org"}],"blocks":[{"type":"rich_text","block_id":"oJ/r","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do you want to move this discussion to discourse "},{"type":"link","url":"https://discourse.julialang.org","text":"https://discourse.julialang.org"},{"type":"text","text":" ? It gets a bit unwieldy here "}]}]}],"thread_ts":"1616578091.056700","parent_user_id":"U01Q398M3QB"},{"client_msg_id":"7b6cca15-9b16-4e49-a57a-9352edfcc141","type":"message","text":"Thanks Moritz, I gonna do this now and post the link here soon.","user":"U01Q398M3QB","ts":"1616656795.076300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2dZw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks Moritz, I gonna do this now and post the link here soon."}]}]}],"thread_ts":"1616578091.056700","parent_user_id":"U01Q398M3QB"},{"client_msg_id":"003aea24-4982-476c-9596-e79cd44feca4","type":"message","text":"`V*Diagonal(D)*V'` is generally not symmetric in floating point arithmetic so try making it `y[:] ~ MvNormal(Symmetric(phi_new))`","user":"U680T6770","ts":"1616657042.076500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"C/IFT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"V*Diagonal(D)*V'","style":{"code":true}},{"type":"text","text":" is generally not symmetric in floating point arithmetic so try making it "},{"type":"text","text":"y[:] ~ MvNormal(Symmetric(phi_new))","style":{"code":true}}]}]}],"thread_ts":"1616578091.056700","parent_user_id":"U01Q398M3QB"},{"type":"message","text":"<@U680T6770> Thanks Andreas, Symmetric() solves this 3 by 3 covariance matrix cases. However, when I test the same tactic in my real code where the covariance is solved by an ODE function. In that case, calling Symmetric() on V*Diagonal(D)*V' will still lead to the error \"PosDefException: matrix is not positive definite; Cholesky factorization failed.\" Is there a way to zero out the floating point arithmetic instead of calling Symmetric()? Error picture attaches","files":[{"id":"F01SRMHHX97","created":1616658611,"timestamp":1616658611,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01Q398M3QB","editable":false,"size":71526,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01SRMHHX97/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01SRMHHX97/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01SRMHHX97-dae3e879e4/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01SRMHHX97-dae3e879e4/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01SRMHHX97-dae3e879e4/image_360.png","thumb_360_w":360,"thumb_360_h":312,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01SRMHHX97-dae3e879e4/image_480.png","thumb_480_w":480,"thumb_480_h":416,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01SRMHHX97-dae3e879e4/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01SRMHHX97-dae3e879e4/image_720.png","thumb_720_w":720,"thumb_720_h":625,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01SRMHHX97-dae3e879e4/image_800.png","thumb_800_w":800,"thumb_800_h":694,"original_w":929,"original_h":806,"thumb_tiny":"AwApADCnS5pKXimAc0ZNH4Un4UAFIelLSHpQAtFKOlGKAEooxRigApD0pcUHpQA8Yx0o49KTtRQIXj0o49KSigA4pG6GlpG+6aAP/9k=","permalink":"https://julialang.slack.com/files/U01Q398M3QB/F01SRMHHX97/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01SRMHHX97-0ac2b0a251","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"RHR","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U680T6770"},{"type":"text","text":" Thanks Andreas, Symmetric() solves this 3 by 3 covariance matrix cases. However, when I test the same tactic in my real code where the covariance is solved by an ODE function. In that case, calling Symmetric() on V*Diagonal(D)*V' will still lead to the error \"PosDefException: matrix is not positive definite; Cholesky factorization failed.\" Is there a way to zero out the floating point arithmetic instead of calling Symmetric()? Error picture attaches"}]}]}],"user":"U01Q398M3QB","display_as_bot":false,"ts":"1616658614.076700","edited":{"user":"U01Q398M3QB","ts":"1616658655.000000"},"thread_ts":"1616578091.056700","parent_user_id":"U01Q398M3QB"},{"client_msg_id":"eacb0137-91fe-4a4c-97f2-dae9aed80723","type":"message","text":"The problem isn’t related to `Symmetric`. I suppose `1e-12` isn’t large enough once the problem size grows. Just to narrow the the error sources, try making it `1e-6` and see what happens.","user":"U680T6770","ts":"1616658923.077300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Zkp3m","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The problem isn’t related to "},{"type":"text","text":"Symmetric","style":{"code":true}},{"type":"text","text":". I suppose "},{"type":"text","text":"1e-12","style":{"code":true}},{"type":"text","text":" isn’t large enough once the problem size grows. Just to narrow the the error sources, try making it "},{"type":"text","text":"1e-6","style":{"code":true}},{"type":"text","text":" and see what happens."}]}]}],"thread_ts":"1616578091.056700","parent_user_id":"U01Q398M3QB"},{"client_msg_id":"2950ed12-ea62-40d6-9c82-a526f89d5ddb","type":"message","text":"<@U680T6770> Thanks very much Andreas. The PosDefException error doesn't raise but the code is current stuck for about 20 mins with only 4 iterations. I will let you know whether the error will raise again when it finised.","user":"U01Q398M3QB","ts":"1616660110.077500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"shQ","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U680T6770"},{"type":"text","text":" Thanks very much Andreas. The PosDefException error doesn't raise but the code is current stuck for about 20 mins with only 4 iterations. I will let you know whether the error will raise again when it finised."}]}]}],"thread_ts":"1616578091.056700","parent_user_id":"U01Q398M3QB"},{"client_msg_id":"98ad12e4-6282-4c6c-8ec8-bef0e237ef42","type":"message","text":"<@U680T6770> Thanks very much Andreas and everyone. My problem of forcing the dual type covariance matrix to be positive definite in inference from Turing.jl has been solved under your help and instruction. However, I just tested that my code takes forever to start sampling due to the complicated ODE for covariance matrix. The error message says as below and I gonna double check every functions.  Do you have some idea about the possible issues? Thanks a lot in advance!\n```┌ Warning: dt &lt;= dtmin. Aborting. There is either an error in your model specification or the true solution is unstable.\n└ @ SciMLBase C:\\Users\\Yushang\\.julia\\packages\\SciMLBase\\fypD8\\src\\integrator_interface.jl:345\n┌ Warning: dt &lt;= dtmin. Aborting. There is either an error in your model specification or the true solution is unstable.\n└ @ SciMLBase C:\\Users\\Yushang\\.julia\\packages\\SciMLBase\\fypD8\\src\\integrator_interface.jl:345\n┌ Warning: dt &lt;= dtmin. Aborting. There is either an error in your model specification or the true solution is unstable.\n└ @ SciMLBase C:\\Users\\Yushang\\.julia\\packages\\SciMLBase\\fypD8\\src\\integrator_interface.jl:345\n┌ Warning: dt &lt;= dtmin. Aborting. There is either an error in your model specification or the true solution is unstable.\n└ @ SciMLBase C:\\Users\\Yushang\\.julia\\packages\\SciMLBase\\fypD8\\src\\integrator_interface.jl:345```","user":"U01Q398M3QB","ts":"1616665159.077900","team":"T68168MUP","edited":{"user":"U01Q398M3QB","ts":"1616665192.000000"},"blocks":[{"type":"rich_text","block_id":"OkTt","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U680T6770"},{"type":"text","text":" Thanks very much Andreas and everyone. My problem of forcing the dual type covariance matrix to be positive definite in inference from Turing.jl has been solved under your help and instruction. However, I just tested that my code takes forever to start sampling due to the complicated ODE for covariance matrix. The error message says as below and I gonna double check every functions.  Do you have some idea about the possible issues? Thanks a lot in advance!\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"┌ Warning: dt <= dtmin. Aborting. There is either an error in your model specification or the true solution is unstable.\n└ @ SciMLBase C:\\Users\\Yushang\\.julia\\packages\\SciMLBase\\fypD8\\src\\integrator_interface.jl:345\n┌ Warning: dt <= dtmin. Aborting. There is either an error in your model specification or the true solution is unstable.\n└ @ SciMLBase C:\\Users\\Yushang\\.julia\\packages\\SciMLBase\\fypD8\\src\\integrator_interface.jl:345\n┌ Warning: dt <= dtmin. Aborting. There is either an error in your model specification or the true solution is unstable.\n└ @ SciMLBase C:\\Users\\Yushang\\.julia\\packages\\SciMLBase\\fypD8\\src\\integrator_interface.jl:345\n┌ Warning: dt <= dtmin. Aborting. There is either an error in your model specification or the true solution is unstable.\n└ @ SciMLBase C:\\Users\\Yushang\\.julia\\packages\\SciMLBase\\fypD8\\src\\integrator_interface.jl:345"}]}]}],"thread_ts":"1616578091.056700","parent_user_id":"U01Q398M3QB"},{"client_msg_id":"e06a7c4d-6904-4a34-83a9-59aad520d1d5","type":"message","text":"It’s coming from the numerical ODE integration and can happen for several reasons. Often it happens when the ODE parameters become very extreme but it’s hard to say if that is what is going on here.","user":"U680T6770","ts":"1616665483.078300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9bV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It’s coming from the numerical ODE integration and can happen for several reasons. Often it happens when the ODE parameters become very extreme but it’s hard to say if that is what is going on here."}]}]}],"thread_ts":"1616578091.056700","parent_user_id":"U01Q398M3QB"},{"client_msg_id":"0cf44d72-708e-4558-88df-7538367c8482","type":"message","text":"There might be some suggestions in the FAQ in the DifferentialEquations docs","user":"U680T6770","ts":"1616665510.078500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ula","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"There might be some suggestions in the FAQ in the DifferentialEquations docs"}]}]}],"thread_ts":"1616578091.056700","parent_user_id":"U01Q398M3QB"}]