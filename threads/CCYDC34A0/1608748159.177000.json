[{"client_msg_id":"d9cc74f8-f1d7-4cdb-9ce2-55fbaabede23","type":"message","text":"Hello everyone\nI was trying to write a custom gradient in `reversediff` for lambertw function from <https://github.com/jlapeyre/LambertW.jl|this package>. Here is my code\n```using ReverseDiff, LambertW\n\nlambert_ext(x::Float64) = lambertw(x)\nlambert_ext(x::Array{Float64, 1}) = lambertw.(x)\n\nlambert_ext(x::ReverseDiff.TrackedVector) = ReverseDiff.track(lambert_ext, x)\n\nReverseDiff.@grad function lambert_ext(x)\n    xv = ReverseDiff.value(x)\n    return lambert_ext(xv), function (Δ)\n        w = lambert_ext(xv)\n        return (Δ.*((xv.*w)./(w.+1)), )\n    end\nend\n\ninputs = [1.0]\nw = lambert_ext(inputs)\nprintln(\"actual derivative: $((inputs.*w)./(w.+1))\")\nprintln(\"derivative reversediff: $(ReverseDiff.gradient(lambert_ext, inputs))\")\n\ninputs = [1.0, 2.0]\nw = lambert_ext(inputs)\nprintln(\"actual derivative: $((inputs.*w)./(w.+1))\")\nprintln(\"derivative reversediff: $(ReverseDiff.gradient(lambert_ext, inputs))\")```\nThe method works fine with input array of shape (1,). But I'm getting this error with size (2,)\n\n```ERROR: LoadError: DimensionMismatch(\"new dimensions (2, 2) must be consistent with array size 2\")\nStacktrace:\n [1] (::Base.var\"#throw_dmrsa#213\")(::Tuple{Int64,Int64}, ::Int64) at ./reshapedarray.jl:41\n [2] reshape at ./reshapedarray.jl:45 [inlined]\n [3] reshape at ./reshapedarray.jl:116 [inlined]\n [4] seeded_reverse_pass!(::Array{Float64,1}, ::ReverseDiff.TrackedArray{Float64,Float64,1,Array{Float64,1},Array{Float64,1}}, ::ReverseDiff.TrackedArray{Float64,Float64,1,Array{Float64,1},Array{Float64,1}}, ::ReverseDiff.GradientTape{typeof(lambert_ext),ReverseDiff.TrackedArray{Float64,Float64,1,Array{Float64,1},Array{Float64,1}},ReverseDiff.TrackedArray{Float64,Float64,1,Array{Float64,1},Array{Float64,1}}}) at /home/ritesh/.julia/packages/ReverseDiff/jFRo1/src/api/utils.jl:45\n [5] seeded_reverse_pass!(::Array{Float64,1}, ::ReverseDiff.GradientTape{typeof(lambert_ext),ReverseDiff.TrackedArray{Float64,Float64,1,Array{Float64,1},Array{Float64,1}},ReverseDiff.TrackedArray{Float64,Float64,1,Array{Float64,1},Array{Float64,1}}}) at /home/ritesh/.julia/packages/ReverseDiff/jFRo1/src/api/tape.jl:47\n [6] gradient(::Function, ::Array{Float64,1}, ::ReverseDiff.GradientConfig{ReverseDiff.TrackedArray{Float64,Float64,1,Array{Float64,1},Array{Float64,1}}}) at /home/ritesh/.julia/packages/ReverseDiff/jFRo1/src/api/gradients.jl:24\n [7] gradient(::Function, ::Array{Float64,1}) at /home/ritesh/.julia/packages/ReverseDiff/jFRo1/src/api/gradients.jl:22\n [8] top-level scope at /home/ritesh/Desktop/lambert_der.jl:24\n [9] include(::String) at ./client.jl:457\n [10] top-level scope at REPL[1]:1\nin expression starting at /home/ritesh/Desktop/lambert_der.jl:24```\nCan someone help me with this?","user":"UR43MA5DY","ts":"1608748159.177000","team":"T68168MUP","edited":{"user":"UR43MA5DY","ts":"1608748320.000000"},"blocks":[{"type":"rich_text","block_id":"OLMj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello everyone\nI was trying to write a custom gradient in `reversediff` for lambertw function from "},{"type":"link","url":"https://github.com/jlapeyre/LambertW.jl","text":"this package"},{"type":"text","text":". Here is my code\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using ReverseDiff, LambertW\n\nlambert_ext(x::Float64) = lambertw(x)\nlambert_ext(x::Array{Float64, 1}) = lambertw.(x)\n\nlambert_ext(x::ReverseDiff.TrackedVector) = ReverseDiff.track(lambert_ext, x)\n\nReverseDiff.@grad function lambert_ext(x)\n    xv = ReverseDiff.value(x)\n    return lambert_ext(xv), function (Δ)\n        w = lambert_ext(xv)\n        return (Δ.*((xv.*w)./(w.+1)), )\n    end\nend\n\ninputs = [1.0]\nw = lambert_ext(inputs)\nprintln(\"actual derivative: $((inputs.*w)./(w.+1))\")\nprintln(\"derivative reversediff: $(ReverseDiff.gradient(lambert_ext, inputs))\")\n\ninputs = [1.0, 2.0]\nw = lambert_ext(inputs)\nprintln(\"actual derivative: $((inputs.*w)./(w.+1))\")\nprintln(\"derivative reversediff: $(ReverseDiff.gradient(lambert_ext, inputs))\")"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"The method works fine with input array of shape (1,). But I'm getting this error with size (2,)\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: LoadError: DimensionMismatch(\"new dimensions (2, 2) must be consistent with array size 2\")\nStacktrace:\n [1] (::Base.var\"#throw_dmrsa#213\")(::Tuple{Int64,Int64}, ::Int64) at ./reshapedarray.jl:41\n [2] reshape at ./reshapedarray.jl:45 [inlined]\n [3] reshape at ./reshapedarray.jl:116 [inlined]\n [4] seeded_reverse_pass!(::Array{Float64,1}, ::ReverseDiff.TrackedArray{Float64,Float64,1,Array{Float64,1},Array{Float64,1}}, ::ReverseDiff.TrackedArray{Float64,Float64,1,Array{Float64,1},Array{Float64,1}}, ::ReverseDiff.GradientTape{typeof(lambert_ext),ReverseDiff.TrackedArray{Float64,Float64,1,Array{Float64,1},Array{Float64,1}},ReverseDiff.TrackedArray{Float64,Float64,1,Array{Float64,1},Array{Float64,1}}}) at /home/ritesh/.julia/packages/ReverseDiff/jFRo1/src/api/utils.jl:45\n [5] seeded_reverse_pass!(::Array{Float64,1}, ::ReverseDiff.GradientTape{typeof(lambert_ext),ReverseDiff.TrackedArray{Float64,Float64,1,Array{Float64,1},Array{Float64,1}},ReverseDiff.TrackedArray{Float64,Float64,1,Array{Float64,1},Array{Float64,1}}}) at /home/ritesh/.julia/packages/ReverseDiff/jFRo1/src/api/tape.jl:47\n [6] gradient(::Function, ::Array{Float64,1}, ::ReverseDiff.GradientConfig{ReverseDiff.TrackedArray{Float64,Float64,1,Array{Float64,1},Array{Float64,1}}}) at /home/ritesh/.julia/packages/ReverseDiff/jFRo1/src/api/gradients.jl:24\n [7] gradient(::Function, ::Array{Float64,1}) at /home/ritesh/.julia/packages/ReverseDiff/jFRo1/src/api/gradients.jl:22\n [8] top-level scope at /home/ritesh/Desktop/lambert_der.jl:24\n [9] include(::String) at ./client.jl:457\n [10] top-level scope at REPL[1]:1\nin expression starting at /home/ritesh/Desktop/lambert_der.jl:24"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nCan someone help me with this?"}]}]}],"thread_ts":"1608748159.177000","reply_count":5,"reply_users_count":2,"latest_reply":"1608799704.179600","reply_users":["U8T9JUA5R","UR43MA5DY"],"subscribed":false},{"client_msg_id":"3b1bf0a6-9f8e-4939-a591-1db9f56e17c2","type":"message","text":"`ReverseDiff.gradient` expects a real-valued function. You should use `ReverseDiff.jacobian(lambert_ext, inputs)` or `ReverseDiff.gradient(sum ∘ lambert_ext, inputs)` instead.","user":"U8T9JUA5R","ts":"1608767687.177500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"M=Q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ReverseDiff.gradient","style":{"code":true}},{"type":"text","text":" expects a real-valued function. You should use "},{"type":"text","text":"ReverseDiff.jacobian(lambert_ext, inputs)","style":{"code":true}},{"type":"text","text":" or "},{"type":"text","text":"ReverseDiff.gradient(sum ∘ lambert_ext, inputs)","style":{"code":true}},{"type":"text","text":" instead."}]}]}],"thread_ts":"1608748159.177000","parent_user_id":"UR43MA5DY"},{"client_msg_id":"27f23631-1d6a-456e-b1ac-8dad3410ad98","type":"message","text":"<@U8T9JUA5R> Thank you :smiley:","user":"UR43MA5DY","ts":"1608777861.177700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"k6nX","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U8T9JUA5R"},{"type":"text","text":" Thank you "},{"type":"emoji","name":"smiley"}]}]}],"thread_ts":"1608748159.177000","parent_user_id":"UR43MA5DY"},{"client_msg_id":"a152279f-43db-41c2-8508-75dff94ac785","type":"message","text":"<@U8T9JUA5R> could you point me to some example in which defining custom gradient for forwarddiff backend is shown?","user":"UR43MA5DY","ts":"1608797129.178900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"r1vzW","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U8T9JUA5R"},{"type":"text","text":" could you point me to some example in which defining custom gradient for forwarddiff backend is shown?"}]}]}],"thread_ts":"1608748159.177000","parent_user_id":"UR43MA5DY"},{"client_msg_id":"3ea5238c-e442-47f2-94dd-13f1bdb187fc","type":"message","text":"You could use DiffRules to define a custom derivative of the scalar function `lambertw`, see e.g. <https://github.com/JuliaDiff/DiffRules.jl/blob/master/src/rules.jl|https://github.com/JuliaDiff/DiffRules.jl/blob/master/src/rules.jl>","user":"U8T9JUA5R","ts":"1608799140.179400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xLML","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You could use DiffRules to define a custom derivative of the scalar function "},{"type":"text","text":"lambertw","style":{"code":true}},{"type":"text","text":", see e.g. "},{"type":"link","url":"https://github.com/JuliaDiff/DiffRules.jl/blob/master/src/rules.jl","text":"https://github.com/JuliaDiff/DiffRules.jl/blob/master/src/rules.jl"}]}]}],"thread_ts":"1608748159.177000","parent_user_id":"UR43MA5DY"},{"client_msg_id":"7350bf2f-24bd-4f5e-9cf6-3f478288c8b6","type":"message","text":"Thank you","user":"UR43MA5DY","ts":"1608799704.179600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5Lxu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thank you"}]}]}],"thread_ts":"1608748159.177000","parent_user_id":"UR43MA5DY"}]