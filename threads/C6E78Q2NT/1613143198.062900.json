[{"client_msg_id":"eb267fe1-4400-4425-adb2-868bef09f02b","type":"message","text":"<@U6A0PD8CR> thanks for your dagger ml presentation. I had to leave early, but is the idea that even in models with a  serial dependency structure you can just start a new ~forward pass~ fraction of a batch as long as the first gpu is cleared ?","user":"UDGT4PM41","ts":"1613143198.062900","team":"T68168MUP","edited":{"user":"UDGT4PM41","ts":"1613152856.000000"},"blocks":[{"type":"rich_text","block_id":"TCzaq","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U6A0PD8CR"},{"type":"text","text":" thanks for your dagger ml presentation. I had to leave early, but is the idea that even in models with a  serial dependency structure you can just start a new "},{"type":"text","text":"forward pass","style":{"strike":true}},{"type":"text","text":" fraction of a batch as long as the first gpu is cleared ?"}]}]}],"thread_ts":"1613143198.062900","reply_count":26,"reply_users_count":3,"latest_reply":"1613152577.070000","reply_users":["U6A0PD8CR","UDGT4PM41","UH9KWTTD3"],"subscribed":false},{"client_msg_id":"1eee4fe3-a709-41ef-83dd-f48dcf585e6c","type":"message","text":"That wasn't the point I made, but that is still potentially possible if you express batching within the DAG.","user":"U6A0PD8CR","ts":"1613150513.063200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UrKL6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That wasn't the point I made, but that is still potentially possible if you express batching within the DAG."}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41"},{"client_msg_id":"889abaa8-1579-4f98-9e3a-085632bd6dbc","type":"message","text":"The point is, if you have operations which are independent of each other, then make them independent \"thunks\" in the DAG, and Dagger will figure out the rest.","user":"U6A0PD8CR","ts":"1613150570.063400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"po2Tq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The point is, if you have operations which are independent of each other, then make them independent \"thunks\" in the DAG, and Dagger will figure out the rest."}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41"},{"client_msg_id":"42d59acb-a243-4ae6-b03a-5c1c4162d4e0","type":"message","text":"Also, if you do the model instantiation with Dagger, you get heterogeneous execution \"for free\", since Dagger knows about multithreading, CUDA GPUs, and AMD GPUs, and will use them freely if you let it.","user":"U6A0PD8CR","ts":"1613150766.063600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0Zjc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Also, if you do the model instantiation with Dagger, you get heterogeneous execution \"for free\", since Dagger knows about multithreading, CUDA GPUs, and AMD GPUs, and will use them freely if you let it."}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41"},{"client_msg_id":"6c3b1646-853f-4e6b-aa58-239c1b32a44c","type":"message","text":"Yes but many (most?) DL models have  a serial  data dependency structure","user":"UDGT4PM41","ts":"1613151009.063800","team":"T68168MUP","edited":{"user":"UDGT4PM41","ts":"1613151189.000000"},"blocks":[{"type":"rich_text","block_id":"ltWkf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes but many (most?) DL models have  a serial  data dependency structure"}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41"},{"client_msg_id":"ab5c24d2-0698-455c-8808-4d7c214f7041","type":"message","text":"At the layer level at least","user":"UDGT4PM41","ts":"1613151077.064000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LiOa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"At the layer level at least"}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41"},{"client_msg_id":"c733a82b-38e1-47cc-ba94-2a98171b5405","type":"message","text":"<@UH9KWTTD3> am I correct here ?","user":"UDGT4PM41","ts":"1613151209.064400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vvi","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UH9KWTTD3"},{"type":"text","text":" am I correct here ?"}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41"},{"client_msg_id":"e7a2ea89-1ec2-47f5-b597-b9ae51b51139","type":"message","text":"Some CV models have branches","user":"UDGT4PM41","ts":"1613151226.064900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gsN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Some CV models have branches"}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41"},{"client_msg_id":"6226d802-a128-44e8-aaea-b59c70598cc2","type":"message","text":"So I don't see how parallelism based on independent execution would be generally useful","user":"UDGT4PM41","ts":"1613151312.065100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FkYp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So I don't see how parallelism based on independent execution would be generally useful"}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41"},{"client_msg_id":"2fa5868a-f073-4dc1-a76a-c92cab9c0868","type":"message","text":"Yes it is serial per batch. But you can parallelize different batches followed by a final serial operation to average across the batches. This would require each “thunk” being a batch, and I believe you would need to encode the fact that different batches are processed with different copies of the model into the DAG.","user":"UH9KWTTD3","ts":"1613151432.065300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tP1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes it is serial per batch. But you can parallelize different batches followed by a final serial operation to average across the batches. This would require each “thunk” being a batch, and I believe you would need to encode the fact that different batches are processed with different copies of the model into the DAG."}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41"},{"client_msg_id":"13cb9474-47b7-4a1e-9529-b2474fb3dfe7","type":"message","text":"Well, each operation within a batch is a thunk, if you want to be able to exploit within-model parallelism as well. But generally yes, you'd express each batch as either a thunk or set of thunks, and have a final \"reducer\" thunk to combine their results.","user":"U6A0PD8CR","ts":"1613151522.065500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VoG4m","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Well, each operation within a batch is a thunk, if you want to be able to exploit within-model parallelism as well. But generally yes, you'd express each batch as either a thunk or set of thunks, and have a final \"reducer\" thunk to combine their results."}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41"},{"client_msg_id":"020eb1dd-33cc-4e72-9e02-f5647fadf0e2","type":"message","text":"Like you say there is parallelism available within a layer as well so if it is possible to nest DAGs maybe there’s something you could do there, but I don’t know how.","user":"UH9KWTTD3","ts":"1613151523.065700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"d9xWf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Like you say there is parallelism available within a layer as well so if it is possible to nest DAGs maybe there’s something you could do there, but I don’t know how."}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41"},{"client_msg_id":"ff5932b8-0f5b-415d-bcfe-d333d4b912f6","type":"message","text":"Ha beat me to typing it out","user":"UH9KWTTD3","ts":"1613151539.065900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2e0s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ha beat me to typing it out"}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41"},{"client_msg_id":"b6a9f676-bf35-44b1-a5d2-d233c237bd81","type":"message","text":"But of course, Dagger can't turn a serial algorithm into a parallel algorithm unless you show it the full dependency graph.","user":"U6A0PD8CR","ts":"1613151584.066100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uMZJt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But of course, Dagger can't turn a serial algorithm into a parallel algorithm unless you show it the full dependency graph."}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41","reactions":[{"name":"point_up::skin-tone-5","users":["UH9KWTTD3"],"count":1}]},{"client_msg_id":"72b64795-f2cb-47db-9ade-a2ca984fc35a","type":"message","text":"<@UH9KWTTD3> isn't that data parallelism","user":"UDGT4PM41","ts":"1613151605.066300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lPCm","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UH9KWTTD3"},{"type":"text","text":" isn't that data parallelism"}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41"},{"client_msg_id":"e3a00e1c-ab30-4017-a2e3-18d4410fb108","type":"message","text":"The two concepts we’re describing here in the context of Dagger are model parallelism and data parallelism in the ML literature. But like Julian said, the level of parallelism in either regime is a consequence of the DAG.","user":"UH9KWTTD3","ts":"1613151643.066500","team":"T68168MUP","edited":{"user":"UH9KWTTD3","ts":"1613152151.000000"},"blocks":[{"type":"rich_text","block_id":"Lqzp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The two concepts we’re describing here in the context of Dagger are model parallelism and data parallelism in the ML literature. But like Julian said, the level of parallelism in either regime is a consequence of the DAG."}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41","reactions":[{"name":"100","users":["UDGT4PM41","U6A0PD8CR"],"count":2}]},{"client_msg_id":"e770613f-fb07-44e8-babe-f95de5f3d450","type":"message","text":"Yup that’s data parallelism and Julian described model parallelism. The latter is at best “funky” in other frameworks. So if Dagger can automate that scheduling, then we’re a few steps ahead of the competition.","user":"UH9KWTTD3","ts":"1613151715.066800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0GI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yup that’s data parallelism and Julian described model parallelism. The latter is at best “funky” in other frameworks. So if Dagger can automate that scheduling, then we’re a few steps ahead of the competition."}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41","reactions":[{"name":"+1","users":["UDGT4PM41"],"count":1}]},{"client_msg_id":"fee38de0-a7fd-4727-8b70-10d2280f0fb2","type":"message","text":"Right but the original idea, as I understood in the meeting  was to shard out the layers so I thought that would only work if you split up the observations","user":"UDGT4PM41","ts":"1613151723.067000","team":"T68168MUP","edited":{"user":"UDGT4PM41","ts":"1613152021.000000"},"blocks":[{"type":"rich_text","block_id":"sh7r","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Right but the original idea, as I understood in the meeting  was to shard out the layers so I thought that would only work if you split up the observations"}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41"},{"client_msg_id":"93cadadf-e928-42c6-9808-ef7fd1c76fde","type":"message","text":"Cool","user":"UDGT4PM41","ts":"1613151754.067300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dvIB4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Cool"}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41"},{"client_msg_id":"29255922-ce43-45c4-aa23-ace3d9fd9149","type":"message","text":"Sorry I didn’t see the talk so I am not sure exactly what the original idea is. But at least based on the descriptions in this thread, it seems reasonable to me that Dagger would exploit model parallelism if it is there.\n\nLooking at <https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html>\nThe idea would be to thunk the operations so that the batch itself is split up. Then as the first linear layer on GPU 0 completes the first thunk of the batch, the second linear layer on GPU 1 can start processing that thunk. There is a pipeline window that needs to be filled here, so that’s the trick with model parallelism. You need to match the available hardware parallelism to the amount parallel work available to get maximum performance (and not restart the pipeline too often).","user":"UH9KWTTD3","ts":"1613152070.067700","team":"T68168MUP","edited":{"user":"UH9KWTTD3","ts":"1613152252.000000"},"blocks":[{"type":"rich_text","block_id":"lpT5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Sorry I didn’t see the talk so I am not sure exactly what the original idea is. But at least based on the descriptions in this thread, it seems reasonable to me that Dagger would exploit model parallelism if it is there.\n\nLooking at "},{"type":"link","url":"https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html"},{"type":"text","text":"\nThe idea would be to thunk the operations so that the batch itself is split up. Then as the first linear layer on GPU 0 completes the first thunk of the batch, the second linear layer on GPU 1 can start processing that thunk. There is a pipeline window that needs to be filled here, so that’s the trick with model parallelism. You need to match the available hardware parallelism to the amount parallel work available to get maximum performance (and not restart the pipeline too often)."}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41"},{"client_msg_id":"85d4fd98-a32c-4acc-ac12-d067d2f51152","type":"message","text":"Ok cool that's exactly what I had in mind","user":"UDGT4PM41","ts":"1613152124.067900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3r0F1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ok cool that's exactly what I had in mind"}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41"},{"client_msg_id":"37da4673-f04b-490d-9d5d-e69f21f4924a","type":"message","text":"I would think that the way to do this would be something that wraps maybe `fmap` to recurse the model then produce a DAG that is thunking the batch at the right granularity.","user":"UH9KWTTD3","ts":"1613152126.068100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gmuZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I would think that the way to do this would be something that wraps maybe "},{"type":"text","text":"fmap","style":{"code":true}},{"type":"text","text":" to recurse the model then produce a DAG that is thunking the batch at the right granularity."}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41","reactions":[{"name":"+1","users":["UDGT4PM41"],"count":1}]},{"client_msg_id":"caeacb98-8ce3-4354-9e3a-ef539172c5f3","type":"message","text":"Thanks for the ref","user":"UDGT4PM41","ts":"1613152130.068300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"BtoY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for the ref"}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41"},{"client_msg_id":"082c701b-4cbe-44d5-844b-24d62d2838af","type":"message","text":"No problem","user":"UH9KWTTD3","ts":"1613152260.069200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3Z=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"No problem"}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41"},{"client_msg_id":"2bf33e40-121e-4329-8c84-5116a17f26d9","type":"message","text":"That makes sense to me. The benefit we have over \"simpler\" distributed execution techniques is that Dagger has a full scheduler, with information about the specific processors available, and how utilized they are. The scheduler can use that information, combined with information about the thunks (such as how much they utilize a given processor, and how big their inputs are) to optimize data placement and compute placement.","user":"U6A0PD8CR","ts":"1613152277.069400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9m/kb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That makes sense to me. The benefit we have over \"simpler\" distributed execution techniques is that Dagger has a full scheduler, with information about the specific processors available, and how utilized they are. The scheduler can use that information, combined with information about the thunks (such as how much they utilize a given processor, and how big their inputs are) to optimize data placement and compute placement."}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41","reactions":[{"name":"+1","users":["UDGT4PM41"],"count":1}]},{"client_msg_id":"a37077e1-409b-416e-81b9-544f90edb027","type":"message","text":"By split up the observations I meant thunk them up from within a batch. I got my terminology mixed up. Forward pass was meant to be a fraction of a batch sent through the network","user":"UDGT4PM41","ts":"1613152484.069600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"79yYG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"By split up the observations I meant thunk them up from within a batch. I got my terminology mixed up. Forward pass was meant to be a fraction of a batch sent through the network"}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41","reactions":[{"name":"+1::skin-tone-5","users":["UH9KWTTD3"],"count":1}]},{"client_msg_id":"d6456100-388b-48f4-902a-d83baae83b5d","type":"message","text":"That sounds really useful","user":"UDGT4PM41","ts":"1613152577.070000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Zw14i","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That sounds really useful"}]}]}],"thread_ts":"1613143198.062900","parent_user_id":"UDGT4PM41"}]