[{"client_msg_id":"67191d21-e8de-4c41-ad7e-bbb0414f9678","type":"message","text":"Can anyone help me how I would convert a folder of CSVs files to a folder of arrow memory-mapped files using CSV.jl and/or Arrow.jl?\n\nI went through the docs and could not find anything about this.\n\nThat would be mostly for out-of-memory manipulation. Because each CSV would be 500MB (there is at least 60 of those) and I have 8GB RAM. I can do this in R using the `{arrow}` package, but I want to do this in Julia.","user":"U01QBF4PHKP","ts":"1615068696.247200","team":"T68168MUP","edited":{"user":"U01QBF4PHKP","ts":"1615068818.000000"},"blocks":[{"type":"rich_text","block_id":"STyl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can anyone help me how I would convert a folder of CSVs files to a folder of arrow memory-mapped files using CSV.jl and/or Arrow.jl?\n\nI went through the docs and could not find anything about this.\n\nThat would be mostly for out-of-memory manipulation. Because each CSV would be 500MB (there is at least 60 of those) and I have 8GB RAM. I can do this in R using the "},{"type":"text","text":"{arrow}","style":{"code":true}},{"type":"text","text":" package, but I want to do this in Julia."}]}]}],"thread_ts":"1615068696.247200","reply_count":11,"reply_users_count":2,"latest_reply":"1615072475.253300","reply_users":["U681ELA87","U01QBF4PHKP"],"subscribed":false},{"client_msg_id":"fb44d7ac-888a-4f56-ac66-88dc77b8ecad","type":"message","text":"We walk through kind of an example of this using `Tables.partitioner` in the <https://arrow.juliadata.org/dev/manual/#Arrow.write|docs>, there's also an issue where someone was doing exactly this: <https://github.com/JuliaData/Arrow.jl/issues/108>. Here's the docs for `Tables.partitioner` which should help as well (<https://tables.juliadata.org/stable/#Tables.partitioner>), at least if you're trying to write all the csv files out as a single arrow file with each file as a separate record batch in the arrow file. If you want to write each csv file as a separate arrow file entirely, then that's quite a bit more straightforward.","user":"U681ELA87","ts":"1615069736.247500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"E=qH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We walk through kind of an example of this using "},{"type":"text","text":"Tables.partitioner","style":{"code":true}},{"type":"text","text":" in the "},{"type":"link","url":"https://arrow.juliadata.org/dev/manual/#Arrow.write","text":"docs"},{"type":"text","text":", there's also an issue where someone was doing exactly this: "},{"type":"link","url":"https://github.com/JuliaData/Arrow.jl/issues/108"},{"type":"text","text":". Here's the docs for "},{"type":"text","text":"Tables.partitioner","style":{"code":true}},{"type":"text","text":" which should help as well ("},{"type":"link","url":"https://tables.juliadata.org/stable/#Tables.partitioner"},{"type":"text","text":"), at least if you're trying to write all the csv files out as a single arrow file with each file as a separate record batch in the arrow file. If you want to write each csv file as a separate arrow file entirely, then that's quite a bit more straightforward."}]}]}],"thread_ts":"1615068696.247200","parent_user_id":"U01QBF4PHKP","reactions":[{"name":"heart","users":["U01QBF4PHKP"],"count":1}]},{"client_msg_id":"09847c51-7c22-418c-b2e7-38ba0c1bb6d1","type":"message","text":"Wow thanks for the fast reply.\nLet me see if I got this straight:\n\n1. Multiple CSVs -&gt; Multiple Arrow file\n```for file in files\n   Arrow.write(file * \".arrow\", CSV.File(file * \".csv\"))\nend```\n2. Multiple CSVs -&gt; One Arrow file\n```Arrow.write(\"BIGFILE.arrow\", Tables.partitioner(files) do file\n           CSV.File(file * \".csv\", datarow=5, header=[\"var1\", \"var2\", \"var3\", ...])\n       end)```\nEdit: I got confused with `enter` and `shift` + `enter` for new lines in Slack. New to this....","user":"U01QBF4PHKP","ts":"1615070438.247700","team":"T68168MUP","edited":{"user":"U01QBF4PHKP","ts":"1615070681.000000"},"blocks":[{"type":"rich_text","block_id":"UQS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Wow thanks for the fast reply.\nLet me see if I got this straight:\n\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Multiple CSVs -> Multiple Arrow file"}]}],"style":"ordered","indent":0},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"for file in files\n   Arrow.write(file * \".arrow\", CSV.File(file * \".csv\"))\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"2. Multiple CSVs -> One Arrow file\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Arrow.write(\"BIGFILE.arrow\", Tables.partitioner(files) do file\n           CSV.File(file * \".csv\", datarow=5, header=[\"var1\", \"var2\", \"var3\", ...])\n       end)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nEdit: I got confused with "},{"type":"text","text":"enter","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"shift","style":{"code":true}},{"type":"text","text":" + "},{"type":"text","text":"enter","style":{"code":true}},{"type":"text","text":" for new lines in Slack. New to this...."}]}]}],"thread_ts":"1615068696.247200","parent_user_id":"U01QBF4PHKP"},{"client_msg_id":"e702d855-a86a-431b-ad73-9b0732538a8d","type":"message","text":"And then I can specify `Arrow.jl` to read all the `*.arrow` files in the example #1 as  `t = Arrow.Table(arrow_files_dir)`?","user":"U01QBF4PHKP","ts":"1615070834.248100","team":"T68168MUP","edited":{"user":"U01QBF4PHKP","ts":"1615070852.000000"},"blocks":[{"type":"rich_text","block_id":"4k=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And then I can specify "},{"type":"text","text":"Arrow.jl","style":{"code":true}},{"type":"text","text":" to read all the "},{"type":"text","text":"*.arrow","style":{"code":true}},{"type":"text","text":" files in the example #1 as  "},{"type":"text","text":"t = Arrow.Table(arrow_files_dir)","style":{"code":true}},{"type":"text","text":"?"}]}]}],"thread_ts":"1615068696.247200","parent_user_id":"U01QBF4PHKP"},{"client_msg_id":"1AE401DA-4493-48C6-9937-63E9E722936D","type":"message","text":"Yes That all sounds right to me","user":"U681ELA87","ts":"1615071138.248600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"AL9/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes That all sounds right to me"}]}]}],"thread_ts":"1615068696.247200","parent_user_id":"U01QBF4PHKP","reactions":[{"name":"heart","users":["U01QBF4PHKP"],"count":1}]},{"client_msg_id":"f342081d-195f-4234-82bd-2fb592a4c252","type":"message","text":"Thank you so MUCH!","user":"U01QBF4PHKP","ts":"1615071184.249900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3t5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thank you so MUCH!"}]}]}],"thread_ts":"1615068696.247200","parent_user_id":"U01QBF4PHKP"},{"client_msg_id":"6D056B2A-C49F-471E-99A0-5869E53A6F62","type":"message","text":"Oh wait, no, in the first example you’d have to read each arrow file separately. Arrow.jl doesn’t have a way to read a whole directory of arrow files","user":"U681ELA87","ts":"1615071190.250300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pknu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oh wait, no, in the first example you’d have to read each arrow file separately. Arrow.jl doesn’t have a way to read a whole directory of arrow files"}]}]}],"thread_ts":"1615068696.247200","parent_user_id":"U01QBF4PHKP"},{"client_msg_id":"C4BE7847-0020-4AB8-9499-155D16E97071","type":"message","text":"If you want a single table, you’d need to do example two","user":"U681ELA87","ts":"1615071216.250900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+jeD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If you want a single table, you’d need to do example two"}]}]}],"thread_ts":"1615068696.247200","parent_user_id":"U01QBF4PHKP"},{"client_msg_id":"BD94BC85-F4AB-4A52-9A52-F08881F4801B","type":"message","text":"But we could probably support reason g a whole directory as a single table. It’s just not as common a workflow with arrow I think because it natively supports multiple record batches in a single arrow file ","user":"U681ELA87","ts":"1615071261.252600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9wn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But we could probably support reason g a whole directory as a single table. It’s just not as common a workflow with arrow I think because it natively supports multiple record batches in a single arrow file "}]}]}],"thread_ts":"1615068696.247200","parent_user_id":"U01QBF4PHKP"},{"client_msg_id":"5d66ecbb-52db-4c8d-ae79-41644ae9b5d3","type":"message","text":"That would be indeed something quite useful because. I only have 8GB of RAM so if I have 50GB of data in several CSV files, I can convert every single CSV file to an arrow file and then I can store all of them in the same directory and use arrow to manipulate the data.","user":"U01QBF4PHKP","ts":"1615071774.252800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"v4BA5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That would be indeed something quite useful because. I only have 8GB of RAM so if I have 50GB of data in several CSV files, I can convert every single CSV file to an arrow file and then I can store all of them in the same directory and use arrow to manipulate the data."}]}]}],"thread_ts":"1615068696.247200","parent_user_id":"U01QBF4PHKP"},{"client_msg_id":"2999626a-00c1-4662-9e53-71b1363748ea","type":"message","text":"Even if you have 8gb ram, it should still work to concat all the csv files into a single arrow file. Now, you might have to adjust the workflow of how you use the single arrow file/table, but the arrow file is ultimately mmapped, so the OS will swap the actual underlying data into memory as needed when you’re accessing different parts","user":"U681ELA87","ts":"1615072012.253000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RLZF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Even if you have 8gb ram, it should still work to concat all the csv files into a single arrow file. Now, you might have to adjust the workflow of how you use the single arrow file/table, but the arrow file is ultimately mmapped, so the OS will swap the actual underlying data into memory as needed when you’re accessing different parts"}]}]}],"thread_ts":"1615068696.247200","parent_user_id":"U01QBF4PHKP","reactions":[{"name":"heart","users":["U01QBF4PHKP"],"count":1}]},{"client_msg_id":"66513cb1-fb5c-4ecb-91c5-6fcec46674eb","type":"message","text":"So this:\n\n```Arrow.write(\"BIGFILE.arrow\", Tables.partitioner(files) do file\n           CSV.File(file)\n       end)```\nworks with limited RAM? (The only constraint would be how much RAM the largest CSV file would require?, because of the iterator?)","user":"U01QBF4PHKP","ts":"1615072475.253300","team":"T68168MUP","edited":{"user":"U01QBF4PHKP","ts":"1615072519.000000"},"blocks":[{"type":"rich_text","block_id":"QS5CT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So this:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Arrow.write(\"BIGFILE.arrow\", Tables.partitioner(files) do file\n           CSV.File(file)\n       end)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nworks with limited RAM? (The only constraint would be how much RAM the largest CSV file would require?, because of the iterator?)"}]}]}],"thread_ts":"1615068696.247200","parent_user_id":"U01QBF4PHKP"}]