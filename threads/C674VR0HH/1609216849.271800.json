[{"client_msg_id":"085a18ff-b49e-40dd-87a7-1794056397bb","type":"message","text":"So I noticed with the MLJ.jl package, it seems that when you use PCA through it, it can't take in Matrix and the object must be a DataFrame? But then when you use MLJ.transform(PC, X) then X can be either a DataFrame or Matrix but using a DataFrame appears to be extremely inefficient. As in taking 450 seconds as opposed to 11 s. If you put in a DataFrame, it seems to return a DataFrame but if you put in a Matrix it returns something else but that can be coerced to a DF then Matrix and give the right result and this was 11s . Seems strange - any ideas why its inconsistent like this where you have to fit the model on a DF but then its better to use transform on a matrix then coerce it? I was wondering how matrix multiplication (which is what PCA is after its been fit already) could be so slow. My Input to the PCA is 640 x 327.5K and output is 640 x 100","user":"U01EF0QVAB0","ts":"1609216849.271800","team":"T68168MUP","edited":{"user":"U01EF0QVAB0","ts":"1609217057.000000"},"blocks":[{"type":"rich_text","block_id":"gBr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So I noticed with the MLJ.jl package, it seems that when you use PCA through it, it can't take in Matrix and the object must be a DataFrame? But then when you use MLJ.transform(PC, X) then X can be either a DataFrame or Matrix but using a DataFrame appears to be extremely inefficient. As in taking 450 seconds as opposed to 11 s. If you put in a DataFrame, it seems to return a DataFrame but if you put in a Matrix it returns something else but that can be coerced to a DF then Matrix and give the right result and this was 11s . Seems strange - any ideas why its inconsistent like this where you have to fit the model on a DF but then its better to use transform on a matrix then coerce it? I was wondering how matrix multiplication (which is what PCA is after its been fit already) could be so slow. My Input to the PCA is 640 x 327.5K and output is 640 x 100"}]}]}],"thread_ts":"1609216849.271800","reply_count":4,"reply_users_count":2,"latest_reply":"1609381026.291100","reply_users":["UAZP7LJLU","U01EF0QVAB0"],"subscribed":false},{"client_msg_id":"8fcdbd66-0a66-4ef5-9590-05136b530e20","type":"message","text":"I recommend you also ask this question at <#CC57ZE7EY|mlj> channel.\n\nFor now `MLJ.transform(PCAmach, X)` works with `X` either being a `Matrix` or any other compatible `Tables.jl` inputs. `Matrix` inputs may no longer be supported in the future.\nI recommend using a  `MatrixTable` instead of `Matrix` as it's a `Tables.jl` compatible source.\n Calling `MLJ.transform(PCAmach, X)` with `X` as `Matrix` or `MatrixTable` is only slightly faster that with `X` as `DataFrame` because of the  reduced allocations involved when calling the method. But it shouldn't be as bad as `11s` to `450s` .(Benchmarking on my PC with a `(1000x1000)` `DatraFrame` and `Matrix` shows no significant difference between the two asides allocation time.)\nPlease open an issue at <https://github.com/alan-turing-institute/MLJ.jl|MLJ> showing the code and benchmarks. (Are you bench-marking the coercion ?)\nGenerally if you already have a matrix lying around just construct a `MatrixTable` which just wraps the matrix as a `Tables.jl` compatible source but if you have a `DataFrame` maybe from reading a csv file, just use the `DataFrame` directly as converting to a `MatrixTable` would require first creating a matrix which involves a slight allocation cost.\n\n*Note:*\nConversion of `DataFrame`, `MatrixTables` and other `Tables.jl` sources to a matrix is done internally when  `MLJ.transform(PCA, X)` is called and involves some allocations except in the case that `X` is a `MatrixTable` .","user":"UAZP7LJLU","ts":"1609234603.273700","team":"T68168MUP","edited":{"user":"UAZP7LJLU","ts":"1609234895.000000"},"blocks":[{"type":"rich_text","block_id":"szLt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I recommend you also ask this question at "},{"type":"channel","channel_id":"CC57ZE7EY"},{"type":"text","text":" channel.\n\nFor now "},{"type":"text","text":"MLJ.transform(PCAmach, X)","style":{"code":true}},{"type":"text","text":" works with "},{"type":"text","text":"X","style":{"code":true}},{"type":"text","text":" either being a `Matrix` or any other compatible `Tables.jl` inputs. "},{"type":"text","text":"Matrix","style":{"code":true}},{"type":"text","text":" inputs may no longer be supported in the future.\nI recommend using a  "},{"type":"text","text":"MatrixTable","style":{"code":true}},{"type":"text","text":" instead of "},{"type":"text","text":"Matrix","style":{"code":true}},{"type":"text","text":" as it's a "},{"type":"text","text":"Tables.jl","style":{"code":true}},{"type":"text","text":" compatible source.\n Calling "},{"type":"text","text":"MLJ.transform(PCAmach, X)","style":{"code":true}},{"type":"text","text":" with "},{"type":"text","text":"X","style":{"code":true}},{"type":"text","text":" as "},{"type":"text","text":"Matrix","style":{"code":true}},{"type":"text","text":" or "},{"type":"text","text":"MatrixTable","style":{"code":true}},{"type":"text","text":" is only slightly faster that with "},{"type":"text","text":"X","style":{"code":true}},{"type":"text","text":" as "},{"type":"text","text":"DataFrame","style":{"code":true}},{"type":"text","text":" because of the  reduced allocations involved when calling the method. But it shouldn't be as bad as "},{"type":"text","text":"11s","style":{"code":true}},{"type":"text","text":" to "},{"type":"text","text":"450s","style":{"code":true}},{"type":"text","text":" .(Benchmarking on my PC with a "},{"type":"text","text":"(1000x1000)","style":{"code":true}},{"type":"text","text":" "},{"type":"text","text":"DatraFrame","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"Matrix","style":{"code":true}},{"type":"text","text":" shows no significant difference between the two asides allocation time.)\nPlease open an issue at "},{"type":"link","url":"https://github.com/alan-turing-institute/MLJ.jl","text":"MLJ"},{"type":"text","text":" showing the code and benchmarks. (Are you bench-marking the coercion ?)\nGenerally if you already have a matrix lying around just construct a "},{"type":"text","text":"MatrixTable","style":{"code":true}},{"type":"text","text":" which just wraps the matrix as a "},{"type":"text","text":"Tables.jl","style":{"code":true}},{"type":"text","text":" compatible source but if you have a "},{"type":"text","text":"DataFrame","style":{"code":true}},{"type":"text","text":" maybe from reading a csv file, just use the "},{"type":"text","text":"DataFrame","style":{"code":true}},{"type":"text","text":" directly as converting to a "},{"type":"text","text":"MatrixTable","style":{"code":true}},{"type":"text","text":" would require first creating a matrix which involves a slight allocation cost.\n\n"},{"type":"text","text":"Note:","style":{"bold":true}},{"type":"text","text":"\nConversion of "},{"type":"text","text":"DataFrame","style":{"code":true}},{"type":"text","text":", "},{"type":"text","text":"MatrixTables","style":{"code":true}},{"type":"text","text":" and other "},{"type":"text","text":"Tables.jl","style":{"code":true}},{"type":"text","text":" sources to a matrix is done internally when  "},{"type":"text","text":"MLJ.transform(PCA, X)","style":{"code":true}},{"type":"text","text":" is called and involves some allocations except in the case that "},{"type":"text","text":"X","style":{"code":true}},{"type":"text","text":" is a "},{"type":"text","text":"MatrixTable","style":{"code":true}},{"type":"text","text":" ."}]}]}],"thread_ts":"1609216849.271800","parent_user_id":"U01EF0QVAB0"},{"client_msg_id":"e5abeb7e-77b3-40a4-9ede-286fca51fb08","type":"message","text":"In this case I had compared using the Matrix directly vs a DataFrame directly, but the coercion doesn't take long anyways. For a 670 x 350K dataset there was a huge difference, maybe for very large high dim datasets in particular large p the differential is bigger? Strange why MLJ.jl won't support matrices directly in the future as that is a fundamental data structure in Julia and in ML/stats in general.","user":"U01EF0QVAB0","ts":"1609268463.285800","team":"T68168MUP","edited":{"user":"U01EF0QVAB0","ts":"1609268515.000000"},"blocks":[{"type":"rich_text","block_id":"Z43Ug","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In this case I had compared using the Matrix directly vs a DataFrame directly, but the coercion doesn't take long anyways. For a 670 x 350K dataset there was a huge difference, maybe for very large high dim datasets in particular large p the differential is bigger? Strange why MLJ.jl won't support matrices directly in the future as that is a fundamental data structure in Julia and in ML/stats in general."}]}]}],"thread_ts":"1609216849.271800","parent_user_id":"U01EF0QVAB0"},{"client_msg_id":"2ce748fb-2274-4243-a376-e90989318005","type":"message","text":"<@U01EF0QVAB0> Oh you meant 350,000 features, i didn't quite see the `K` in `350K` and thought you meant 350 features.\nFor datasets containing that number of features creation of a `Matrix`  from a `DataFrame` using the `Tables.matrix`  method (which MLJ uses)  is quite slow and allocation heavy probably due to type instability. For this case i recommend wrapping the matrix in a `MatrixTable` with the method `MLJ.table(given_matrix_data, names=vector_of_feature_names)` before use. To MLJ the `MatrixTable` just provides a way of passing a `Matrix` and is efficient for your use case.\nJust curious are you doing NLP?","user":"UAZP7LJLU","ts":"1609324249.289900","team":"T68168MUP","edited":{"user":"UAZP7LJLU","ts":"1609324419.000000"},"blocks":[{"type":"rich_text","block_id":"av8","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U01EF0QVAB0"},{"type":"text","text":" Oh you meant 350,000 features, i didn't quite see the `K` in `350K` and thought you meant 350 features.\nFor datasets containing that number of features creation of a "},{"type":"text","text":"Matrix","style":{"code":true}},{"type":"text","text":"  from a "},{"type":"text","text":"DataFrame","style":{"code":true}},{"type":"text","text":" using the "},{"type":"text","text":"Tables.matrix","style":{"code":true}},{"type":"text","text":"  method (which MLJ uses)  is quite slow and allocation heavy probably due to type instability. For this case i recommend wrapping the matrix in a "},{"type":"text","text":"MatrixTable","style":{"code":true}},{"type":"text","text":" with the method "},{"type":"text","text":"MLJ.table(given_matrix_data, names=vector_of_feature_names)","style":{"code":true}},{"type":"text","text":" before use. To MLJ the "},{"type":"text","text":"MatrixTable","style":{"code":true}},{"type":"text","text":" just provides a way of passing a "},{"type":"text","text":"Matrix","style":{"code":true}},{"type":"text","text":" and is efficient for your use case.\nJust curious are you doing NLP?"}]}]}],"thread_ts":"1609216849.271800","parent_user_id":"U01EF0QVAB0"},{"client_msg_id":"ee6980b2-7d73-420b-b16f-e9350b0c4b2c","type":"message","text":"I am working on the GTZAN audio music data to try to learn more about ML, and I am using more conventional ML methods first prior to heading to deep learning. I essentially FFTed (and took amplitude) the data first, which reduced dimension by half and then using PCA to feed it into the rest of the supervised learning algorithms. Thanks, that seems to help the comp time. It was strange to me how the data structure choice could affect it so much.","user":"U01EF0QVAB0","ts":"1609381026.291100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QL6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am working on the GTZAN audio music data to try to learn more about ML, and I am using more conventional ML methods first prior to heading to deep learning. I essentially FFTed (and took amplitude) the data first, which reduced dimension by half and then using PCA to feed it into the rest of the supervised learning algorithms. Thanks, that seems to help the comp time. It was strange to me how the data structure choice could affect it so much."}]}]}],"thread_ts":"1609216849.271800","parent_user_id":"U01EF0QVAB0","reactions":[{"name":"+1::skin-tone-4","users":["UAZP7LJLU"],"count":1}]}]