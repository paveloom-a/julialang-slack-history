[{"client_msg_id":"c97e6c28-309d-4a03-8d47-a3698ed8d569","type":"message","text":"<@U681ELA87> Sorry to keep bugging you - I'm having some issues writing arrow files. Details in thread.","user":"U8JP5B9T2","ts":"1611083623.072300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0OR","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U681ELA87"},{"type":"text","text":" Sorry to keep bugging you - I'm having some issues writing arrow files. Details in thread."}]}]}],"thread_ts":"1611083623.072300","reply_count":4,"reply_users_count":2,"latest_reply":"1611086939.073000","reply_users":["U8JP5B9T2","U681ELA87"],"subscribed":false},{"client_msg_id":"5d93aaf4-1b91-42ac-910e-35fbaaa6f52a","type":"message","text":"I've got a list of ~ 1200 CSV files that I'm trying to read into an arrow file. Reading them all into a DataFrame takes less than a second, and then writing that DataFrame to an arrow file also takes less than a second\n\n```julia&gt; @time let df = DataFrame()\n          for profile in paths\n              append!(df, CSV.File(profile, datarow=5, header=[\"clade\", \"NCBI_taxid\", \"abundance\", \"additional_species\"]))\n          end\n          Arrow.write(\"test.arrow\", df)\n       end\n  0.608143 seconds (670.79 k allocations: 172.351 MiB, 14.65% gc time)```\nBut trying to do this with the Tables.partitioner, I let it run for ~5 min and it still wasn't done\n\n```julia&gt; Arrow.write(\"test2.arrow\", Tables.partitioner(paths) do profile\n           CSV.File(profile, datarow=5, header=[\"clade\", \"NCBI_taxid\", \"abundance\", \"additional_species\"])\n       end) # hangs...```\nDon't know if it's relevant, but the file after writing from the DataFrame is ~39 Mb, but the partioner one quickly gets to 2.9 Mb and then doesn't get bigger","user":"U8JP5B9T2","ts":"1611083628.072400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lRqo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I've got a list of ~ 1200 CSV files that I'm trying to read into an arrow file. Reading them all into a DataFrame takes less than a second, and then writing that DataFrame to an arrow file also takes less than a second\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> @time let df = DataFrame()\n          for profile in paths\n              append!(df, CSV.File(profile, datarow=5, header=[\"clade\", \"NCBI_taxid\", \"abundance\", \"additional_species\"]))\n          end\n          Arrow.write(\"test.arrow\", df)\n       end\n  0.608143 seconds (670.79 k allocations: 172.351 MiB, 14.65% gc time)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"But trying to do this with the Tables.partitioner, I let it run for ~5 min and it still wasn't done\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> Arrow.write(\"test2.arrow\", Tables.partitioner(paths) do profile\n           CSV.File(profile, datarow=5, header=[\"clade\", \"NCBI_taxid\", \"abundance\", \"additional_species\"])\n       end) # hangs..."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Don't know if it's relevant, but the file after writing from the DataFrame is ~39 Mb, but the partioner one quickly gets to 2.9 Mb and then doesn't get bigger"}]}]}],"thread_ts":"1611083623.072300","parent_user_id":"U8JP5B9T2"},{"client_msg_id":"7c992b34-e9dc-4556-8900-97d13b4f5de9","type":"message","text":"Hmmmm, sounds suspicious. Can you post all this in an Arrow.jl issue and I'll try to dig in tonight to see what's going on? Also, do you know if it's hanging if you just do, say 10 files?","user":"U681ELA87","ts":"1611083936.072600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"I5X2G","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hmmmm, sounds suspicious. Can you post all this in an Arrow.jl issue and I'll try to dig in tonight to see what's going on? Also, do you know if it's hanging if you just do, say 10 files?"}]}]}],"thread_ts":"1611083623.072300","parent_user_id":"U8JP5B9T2"},{"client_msg_id":"e2645d22-893e-4381-abae-b61612c7915c","type":"message","text":"I haven't tested that, but I can and will add to issue :thumbsup:","user":"U8JP5B9T2","ts":"1611085608.072800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qDWA=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I haven't tested that, but I can and will add to issue "},{"type":"emoji","name":"thumbsup"}]}]}],"thread_ts":"1611083623.072300","parent_user_id":"U8JP5B9T2"},{"client_msg_id":"60aa05c1-4491-4dfb-903d-1d1941f00383","type":"message","text":"<https://github.com/JuliaData/Arrow.jl/issues/108> and I sent you an email with the files","user":"U8JP5B9T2","ts":"1611086939.073000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=SmCn","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/JuliaData/Arrow.jl/issues/108"},{"type":"text","text":" and I sent you an email with the files"}]}]}],"thread_ts":"1611083623.072300","parent_user_id":"U8JP5B9T2"}]