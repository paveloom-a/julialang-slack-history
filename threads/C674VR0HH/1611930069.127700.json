[{"client_msg_id":"c577d242-39cc-47fc-a94d-b4e0c571b6b5","type":"message","text":"Hi folks, I am having a small issue with importing datasets. I have 2 files, both in CSV, and they are relatively large (2GB each), I want to import them and then merge them on the basis of unique ID they have. Right now, I am using `CSV.jl` and my computer is protesting weirdly when I try to import them in Julia.\n\nBefore, I was trying to do same thing in R but again I was not able to get them through merging process. In Julia, I am not able to even import the second file (only one is importing). Wondering if there are some nice libraries which could make it possible? I sure, Julia must be having something, that I am missing!!\n\nThanks in advance!","user":"U01A0S07875","ts":"1611930069.127700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5JD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi folks, I am having a small issue with importing datasets. I have 2 files, both in CSV, and they are relatively large (2GB each), I want to import them and then merge them on the basis of unique ID they have. Right now, I am using "},{"type":"text","text":"CSV.jl","style":{"code":true}},{"type":"text","text":" and my computer is protesting weirdly when I try to import them in Julia.\n\nBefore, I was trying to do same thing in R but again I was not able to get them through merging process. In Julia, I am not able to even import the second file (only one is importing). Wondering if there are some nice libraries which could make it possible? I sure, Julia must be having something, that I am missing!!\n\nThanks in advance!"}]}]}],"thread_ts":"1611930069.127700","reply_count":59,"reply_users_count":6,"latest_reply":"1612011897.172900","reply_users":["UBF9YRB6H","U01A0S07875","U01CQTKB86N","U67431ELR","U681ELA87","U8JAMQGQY"],"subscribed":false},{"client_msg_id":"8847990a-1818-40ff-82c8-b9e835da5ced","type":"message","text":"Show us the errors!","user":"UBF9YRB6H","ts":"1611930543.127800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WqbB2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Show us the errors!"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"ed7b6684-e00a-4d8f-967e-1b25fd0c389a","type":"message","text":"Unfortunately, there is no error yet. It is just my system is protesting and hangs :disappointed:","user":"U01A0S07875","ts":"1611930621.128000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"T5Ctw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Unfortunately, there is no error yet. It is just my system is protesting and hangs "},{"type":"emoji","name":"disappointed"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875","reactions":[{"name":"scream","users":["UBF9YRB6H"],"count":1}]},{"client_msg_id":"3cdb85ad-18d5-4362-9127-aad1789c3ff8","type":"message","text":"are you on the latest versions?","user":"UBF9YRB6H","ts":"1611930695.128500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GuE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"are you on the latest versions?"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"1A72B8FC-FF83-44F3-8388-00707AB85144","type":"message","text":"That’s not that big a file.. Maybe import just some rows, and check the inferred types, and change if rewuired.","user":"U01CQTKB86N","ts":"1611930826.130100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FcX2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That’s not that big a file.. Maybe import just some rows, and check the inferred types, and change if rewuired."}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"type":"message","text":"I think, yes!! Everything else works perfectly with me, I am just struggling large size of data maybe?","files":[{"id":"F01LBR79CF5","created":1611930775,"timestamp":1611930775,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01A0S07875","editable":false,"size":4392,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01LBR79CF5/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01LBR79CF5/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01LBR79CF5-3b14fac992/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01LBR79CF5-3b14fac992/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01LBR79CF5-3b14fac992/image_360.png","thumb_360_w":212,"thumb_360_h":104,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01LBR79CF5-3b14fac992/image_160.png","original_w":212,"original_h":104,"thumb_tiny":"AwAXADCu4+Y8dzTcZxgdaGzvb6mkyR3NIB2VHVf1o3Jn7lI6MmNwxkZH0ptMBaSiigBzk7mHuaQknvSv99vqabQA+SRpCu7HyjAxTKKKACiiigD/2Q==","permalink":"https://julialang.slack.com/files/U01A0S07875/F01LBR79CF5/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01LBR79CF5-b28bb24514","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"4ZNf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think, yes!! Everything else works perfectly with me, I am just struggling large size of data maybe?"}]}]}],"user":"U01A0S07875","display_as_bot":false,"ts":"1611930832.130300","thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"25335e0b-1286-4c6f-bcbc-0a9a8622cab9","type":"message","text":"no, for DataFrames and CSV","user":"UBF9YRB6H","ts":"1611930870.130700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tmp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"no, for DataFrames and CSV"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"type":"message","text":"Oops, my bad.","files":[{"id":"F01LBRH7K7D","created":1611930901,"timestamp":1611930901,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01A0S07875","editable":false,"size":38999,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01LBRH7K7D/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01LBRH7K7D/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01LBRH7K7D-3968b7e69a/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01LBRH7K7D-3968b7e69a/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01LBRH7K7D-3968b7e69a/image_360.png","thumb_360_w":360,"thumb_360_h":249,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01LBRH7K7D-3968b7e69a/image_480.png","thumb_480_w":480,"thumb_480_h":332,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01LBRH7K7D-3968b7e69a/image_160.png","original_w":517,"original_h":358,"thumb_tiny":"AwAhADCrSikHSlHegAPsaQk+tL+dNJPqaADJoJJ60ZPrSUAOFKKQdKUGgAptOppoASiiigAooooAKKKKACiiigD/2Q==","permalink":"https://julialang.slack.com/files/U01A0S07875/F01LBRH7K7D/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01LBRH7K7D-b59fd1af04","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"f0c","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oops, my bad."}]}]}],"user":"U01A0S07875","display_as_bot":false,"ts":"1611930905.130900","thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"ec57c2a2-7230-4d2a-8859-83d1a2705e48","type":"message","text":"<@U01CQTKB86N>subset of data imports perfectly! I don't think there is any issue with the CSV file (I exported it after processing from R).","user":"U01A0S07875","ts":"1611930967.131300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MlJ7","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U01CQTKB86N"},{"type":"text","text":"subset of data imports perfectly! I don't think there is any issue with the CSV file (I exported it after processing from R)."}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"27a0dcc4-dab1-4aac-828a-dc3466fc362f","type":"message","text":"looks like you are on the most up to date versions","user":"UBF9YRB6H","ts":"1611930986.131500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"B285","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"looks like you are on the most up to date versions"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"467b19dc-53ca-49a6-b03c-4e8683892b5c","type":"message","text":"I keep on updating after every few days. I hope its not bad to be on the most up to date versions?","user":"U01A0S07875","ts":"1611931024.131700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QQok","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I keep on updating after every few days. I hope its not bad to be on the most up to date versions?"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"19392299-D4FB-4BCA-9421-D934350CF3C2","type":"message","text":"Maybe you’re out of memory, or perhaps other processes are taking CPU.","user":"U01CQTKB86N","ts":"1611931075.132800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ax=f","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Maybe you’re out of memory, or perhaps other processes are taking CPU."}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"F1B9693B-B213-4F55-963E-B34BEE51CDF5","type":"message","text":"Just yesterday my mac was working really badly after upgrading xcode.","user":"U01CQTKB86N","ts":"1611931162.133900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9o1eO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Just yesterday my mac was working really badly after upgrading xcode."}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"ec5d4767-9997-4c83-a308-9ea1d47753d5","type":"message","text":"That's true! I was keeping eyes on system monitor when I did that for the 3rd (or maybe 4th) time. Memory consumption reached to like 99.7%\n\nBut everything else was closed that time. Only Julia was just importing data :sweat_smile:","user":"U01A0S07875","ts":"1611931211.134100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kDIir","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That's true! I was keeping eyes on system monitor when I did that for the 3rd (or maybe 4th) time. Memory consumption reached to like 99.7%\n\nBut everything else was closed that time. Only Julia was just importing data "},{"type":"emoji","name":"sweat_smile"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"b532beb6-7801-4111-93cb-8421be3a59b5","type":"message","text":"no, you should definitely be on the most up to date versions","user":"UBF9YRB6H","ts":"1611931224.134300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MlPQc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"no, you should definitely be on the most up to date versions"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875","reactions":[{"name":"+1","users":["U01A0S07875"],"count":1}]},{"client_msg_id":"b20ae9d0-79c5-4a0f-8b42-96269abd3477","type":"message","text":"Any suggestions on what I can do? I just want to merge those two files and then I think I won't need to do that heavy task (if it is heavy) ever :sweat_smile:","user":"U01A0S07875","ts":"1611931304.134600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/6h8y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Any suggestions on what I can do? I just want to merge those two files and then I think I won't need to do that heavy task (if it is heavy) ever "},{"type":"emoji","name":"sweat_smile"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"3A422FFB-7419-466E-B32B-C31974BEFDAC","type":"message","text":"Which OS? MacOS handles overuse really well. Julia used 50 GB for one test task, I have 16 GB, and it was stillresponsive.","user":"U01CQTKB86N","ts":"1611931395.136900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Vw8c","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Which OS? MacOS handles overuse really well. Julia used 50 GB for one test task, I have 16 GB, and it was stillresponsive."}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"8e800e9d-0f30-451e-b161-e5b488f92928","type":"message","text":"I am on <https://pop.system76.com/|Pop!_OS>","user":"U01A0S07875","ts":"1611931443.137600","team":"T68168MUP","attachments":[{"title":"Pop!_OS by System76","title_link":"https://pop.system76.com/","text":"Imagine an OS for the software developer, maker and computer science professional who uses their computer as a tool to discover and create. Welcome to Pop!_OS.","fallback":"Pop!_OS by System76","from_url":"https://pop.system76.com/","service_icon":"https://pop.system76.com/icon-16.png","service_name":"Pop!_OS by System76","id":1,"original_url":"https://pop.system76.com/"}],"blocks":[{"type":"rich_text","block_id":"a5d","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am on "},{"type":"link","url":"https://pop.system76.com/","text":"Pop!_OS"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"9A4A0FC1-6131-435F-A21E-47E80FE25CE6","type":"message","text":"It probably uses swap a lot, and swap  in your case s slow?","user":"U01CQTKB86N","ts":"1611931466.138400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"O=FT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It probably uses swap a lot, and swap  in your case s slow?"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"B7647F19-43F8-44E8-A013-66B63AD8B0A4","type":"message","text":"You should try to optimise the column data types, from64 bit int to 16 bits, that sort of thing.","user":"U01CQTKB86N","ts":"1611931551.139800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MthW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You should try to optimise the column data types, from64 bit int to 16 bits, that sort of thing."}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"cd5b52d5-5f94-4a79-b511-2063e7a8b4ef","type":"message","text":"I was seeing system monitor constantly and Swap was also reaching high. When both memory and swap was high, my system hanged. :disappointed:","user":"U01A0S07875","ts":"1611931568.140000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9WMv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I was seeing system monitor constantly and Swap was also reaching high. When both memory and swap was high, my system hanged. "},{"type":"emoji","name":"disappointed"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"82EF0FFB-1E90-43A5-8BB4-5D03ECA211BD","type":"message","text":"Buy more RAM. :nerd_face:","user":"U01CQTKB86N","ts":"1611931603.140500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VG2pG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Buy more RAM. "},{"type":"emoji","name":"nerd_face"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"0f297058-b7d4-4116-b426-33879404846d","type":"message","text":"I just did today. I extended to 8 GB (maximum in my case). I think I should buy a new laptop. lol","user":"U01A0S07875","ts":"1611931634.140700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FSCCH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I just did today. I extended to 8 GB (maximum in my case). I think I should buy a new laptop. lol"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"024A2C54-5C89-4CF7-8564-7E7627E37E94","type":"message","text":"Another option, deploy on AWS.","user":"U01CQTKB86N","ts":"1611931669.141500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UiBSZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Another option, deploy on AWS."}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"fea05e00-5f87-4c69-8aa4-566b6fc5685d","type":"message","text":"I think this calls for cc'ing <@U681ELA87>, unfortuately.","user":"UBF9YRB6H","ts":"1611931670.141700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lUlYy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think this calls for cc'ing "},{"type":"user","user_id":"U681ELA87"},{"type":"text","text":", unfortuately."}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875","reactions":[{"name":"+1","users":["U01A0S07875"],"count":1},{"name":"100","users":["U01A0S07875"],"count":1}]},{"client_msg_id":"F56D1839-1FB4-4451-AC8B-9C72F075DEE3","type":"message","text":"In one case I was able to reduce RAM usage by more than half by using smaller datatypes.","user":"U01CQTKB86N","ts":"1611931725.143300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VvS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In one case I was able to reduce RAM usage by more than half by using smaller datatypes."}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875","reactions":[{"name":"+1","users":["U01A0S07875"],"count":1}]},{"client_msg_id":"17f7d408-b573-4536-ba5c-8e6717eeff1c","type":"message","text":"You could try reading one CSV file, write it to an Arrow file, restart Julia, do the same for the other one, and then load both.","user":"U67431ELR","ts":"1611932335.150000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IbG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You could try reading one CSV file, write it to an Arrow file, restart Julia, do the same for the other one, and then load both."}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"39C5D867-30B8-4529-80CE-AE186441D46F","type":"message","text":"You could pass `lazystrings = true`, that tends to help a lot on large files with string columns","user":"U681ELA87","ts":"1611932458.152000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3B8bj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You could pass "},{"type":"text","text":"lazystrings = true","style":{"code":true}},{"type":"text","text":", that tends to help a lot on large files with string columns"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"EFCE5B8F-0238-4298-87C0-FAD870C2D0CB","type":"message","text":"<@U67431ELR> ‘s idea is also good","user":"U681ELA87","ts":"1611932493.152800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7wzL","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U67431ELR"},{"type":"text","text":" ‘s idea is also good"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"12477735-FAC5-4F43-B838-8EA42CED9601","type":"message","text":"I’m experimenting for an easy way to run Julia in Jupyter in docker in ECS. (Yes, I know it’s not rocket science.) I’m planning some blog posts about this. So far I’ve set up AWS account, some users, ECR, tried Julia in a Lambda, set up MFA with Authy. I’m going to use the datascience-notebook image (it has 1.5.3).\n\n<https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html|https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html>","user":"U01CQTKB86N","ts":"1611932644.155200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KvA86","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’m experimenting for an easy way to run Julia in Jupyter in docker in ECS. (Yes, I know it’s not rocket science.) I’m planning some blog posts about this. So far I’ve set up AWS account, some users, ECR, tried Julia in a Lambda, set up MFA with Authy. I’m going to use the datascience-notebook image (it has 1.5.3).\n\n"},{"type":"link","url":"https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html","text":"https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"8142df87-994f-4aa7-901c-f670d2f69909","type":"message","text":"Thanks for that <@U681ELA87>, just one thing. Should I use `CSV.File(\"path\", lazystring=true)` like this?\n\nWould it trouble me if I declare data as DataFrame later?","user":"U01A0S07875","ts":"1611932701.155400","team":"T68168MUP","edited":{"user":"U01A0S07875","ts":"1611932838.000000"},"blocks":[{"type":"rich_text","block_id":"85bRC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for that "},{"type":"user","user_id":"U681ELA87"},{"type":"text","text":", just one thing. Should I use "},{"type":"text","text":"CSV.File(\"path\", lazystring=true)","style":{"code":true}},{"type":"text","text":" like this?\n\nWould it trouble me if I declare data as DataFrame later?"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"390e3b70-3408-468a-a065-9359d00a0e19","type":"message","text":"Or you can do `CSV.read(file, DataFrame; lazystrings=true)`. what that does is avoid materializing full string objects until you access them later. The column type is `LazyStringVector`, but you can't mutate the elements, so if you need to mutate elements, you'll have to `copy(col)`.","user":"U681ELA87","ts":"1611933790.155700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zPtkt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Or you can do "},{"type":"text","text":"CSV.read(file, DataFrame; lazystrings=true)","style":{"code":true}},{"type":"text","text":". what that does is avoid materializing full string objects until you access them later. The column type is "},{"type":"text","text":"LazyStringVector","style":{"code":true}},{"type":"text","text":", but you can't mutate the elements, so if you need to mutate elements, you'll have to "},{"type":"text","text":"copy(col)","style":{"code":true}},{"type":"text","text":"."}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"9b548f20-a3e4-4fc6-bbcc-b06d13eacb64","type":"message","text":"i can confirm that I am able to import data using `lazystrings=true` However, I am still not able to merge it because it requires me to add data in DataFrame, but let me try `CSV.read(file, DataFrame; lazystrings=true)` now","user":"U01A0S07875","ts":"1611934989.156000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Jk+SJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"i can confirm that I am able to import data using "},{"type":"text","text":"lazystrings=true","style":{"code":true}},{"type":"text","text":" However, I am still not able to merge it because it requires me to add data in DataFrame, but let me try "},{"type":"text","text":"CSV.read(file, DataFrame; lazystrings=true)","style":{"code":true}},{"type":"text","text":" now"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"27e170d2-0b3d-4e8a-a826-31583510799e","type":"message","text":"Yes! `CSV.read(file, DataFrame; lazystrings=true)` works perfectly. And, my RAM consumption is also pretty low to 60% after importing datasets.\n\nNow, I am facing same issue (computer is protesting and hangs) when I am trying to use `innerjoin()` to merge those datasets on the basis of common column. <@U681ELA87> any suggestion to do that efficiently?","user":"U01A0S07875","ts":"1611936370.158400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MvC1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes! "},{"type":"text","text":"CSV.read(file, DataFrame; lazystrings=true)","style":{"code":true}},{"type":"text","text":" works perfectly. And, my RAM consumption is also pretty low to 60% after importing datasets.\n\nNow, I am facing same issue (computer is protesting and hangs) when I am trying to use "},{"type":"text","text":"innerjoin()","style":{"code":true}},{"type":"text","text":" to merge those datasets on the basis of common column. "},{"type":"user","user_id":"U681ELA87"},{"type":"text","text":" any suggestion to do that efficiently?"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"3c60b19d-b3d9-4073-b4d3-47ee9a754f7f","type":"message","text":"That's probably a better question for <@U8JAMQGQY> or <@U67431ELR>; not sure if <https://github.com/JuliaData/DataFrames.jl/pull/2612> will help, but it might be worth trying.","user":"U681ELA87","ts":"1611936542.158600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Yh=P","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That's probably a better question for "},{"type":"user","user_id":"U8JAMQGQY"},{"type":"text","text":" or "},{"type":"user","user_id":"U67431ELR"},{"type":"text","text":"; not sure if "},{"type":"link","url":"https://github.com/JuliaData/DataFrames.jl/pull/2612"},{"type":"text","text":" will help, but it might be worth trying."}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"f9511427-d796-4f44-9d0d-fa0a21e861f0","type":"message","text":"I think if you are RAM constrained, `innerjoin` is going to be a bottleneck for now","user":"UBF9YRB6H","ts":"1611936579.158800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TCW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think if you are RAM constrained, "},{"type":"text","text":"innerjoin","style":{"code":true}},{"type":"text","text":" is going to be a bottleneck for now"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"b27f78f4-22df-41fc-9939-cc8cff89ecf3","type":"message","text":"The issue is I have imported around 5 GB data now in Julia (two files of 3+2). Do you know any better option <@UBF9YRB6H>? I just want to merge and export in CSV (or any format for now!)","user":"U01A0S07875","ts":"1611936790.159000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yHvoQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The issue is I have imported around 5 GB data now in Julia (two files of 3+2). Do you know any better option "},{"type":"user","user_id":"UBF9YRB6H"},{"type":"text","text":"? I just want to merge and export in CSV (or any format for now!)"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"2a72ff80-3607-463c-bd5e-172c381f5622","type":"message","text":"What is `nrow` for both data frames?","user":"U8JAMQGQY","ts":"1611936820.159200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"NM9X","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What is "},{"type":"text","text":"nrow","style":{"code":true}},{"type":"text","text":" for both data frames?"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"f8403ce3-a421-4470-a438-cf3447fcdabd","type":"message","text":"For the first one it is 18276411 and for the second file it is 11875841","user":"U01A0S07875","ts":"1611936882.159400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TgI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"For the first one it is 18276411 and for the second file it is 11875841"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"034425f5-7331-4466-bca7-0323662a913b","type":"message","text":"It should drop all which don't match, and my understanding is that we'll have data of about 25% left after that","user":"U01A0S07875","ts":"1611936936.159600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"O/qw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It should drop all which don't match, and my understanding is that we'll have data of about 25% left after that"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"ab3b15e5-623d-431c-b712-b21c156c189b","type":"message","text":"You could lazily drop things that don't match first and then join after. That should heldp","user":"UBF9YRB6H","ts":"1611937098.159800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nuJQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You could lazily drop things that don't match first and then join after. That should heldp"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"f5922ada-f435-4b79-be50-960a29a90b31","type":"message","text":"Do you mean just keep intersecting IDs? I think it is a better idea. How can I do that? I think this way:\n\n```df1 = filter(row-intersect(df1.id, df2.id), df1)```","user":"U01A0S07875","ts":"1611937515.160000","team":"T68168MUP","edited":{"user":"U01A0S07875","ts":"1611937615.000000"},"blocks":[{"type":"rich_text","block_id":"YoczF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do you mean just keep intersecting IDs? I think it is a better idea. How can I do that? I think this way:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"df1 = filter(row-intersect(df1.id, df2.id), df1)"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"7fae235f-2e69-41c0-8091-e27c78513891","type":"message","text":"since you are so RAM constrained I would be careful to use the `pair` syntax inside that `filter` call so you are only acting on columns.\n\n```intersecting_ids = intersect(df1.id, df2.id)\nfilter(:id =&gt; (id -&gt; in(id, intersectind_ids), df1)```\n","user":"UBF9YRB6H","ts":"1611937711.160300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"swQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"since you are so RAM constrained I would be careful to use the "},{"type":"text","text":"pair","style":{"code":true}},{"type":"text","text":" syntax inside that "},{"type":"text","text":"filter","style":{"code":true}},{"type":"text","text":" call so you are only acting on columns.\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"intersecting_ids = intersect(df1.id, df2.id)\nfilter(:id => (id -> in(id, intersectind_ids), df1)"}]},{"type":"rich_text_section","elements":[]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"0d487880-1a1c-45d1-bd70-e6cb9686c9ad","type":"message","text":"Just change the intersection to `Set` first. Since the row count for both data frames is similar currently not much can be helped apart from what Peter suggests.","user":"U8JAMQGQY","ts":"1611938110.160500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Oys","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Just change the intersection to "},{"type":"text","text":"Set","style":{"code":true}},{"type":"text","text":" first. Since the row count for both data frames is similar currently not much can be helped apart from what Peter suggests."}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"7ebbba09-ece4-4194-87c4-4e2d42dab961","type":"message","text":"Would JuliaDB work any better? I’ve never tried it, but I got the impression that it might work for datasets larger than memory. I might be wrong.","user":"U01CQTKB86N","ts":"1611938480.160700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MCOO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Would JuliaDB work any better? I’ve never tried it, but I got the impression that it might work for datasets larger than memory. I might be wrong."}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"9ef22d2c-20f5-483b-8193-a060e68b8a4c","type":"message","text":"I already run the code Peter shared. It is taking long, but let's see if that works. I am hoping that it will!","user":"U01A0S07875","ts":"1611938560.160900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jZYvG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I already run the code Peter shared. It is taking long, but let's see if that works. I am hoping that it will!"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"5c47e626-51b2-46da-9321-f06e1c417195","type":"message","text":"Indeed, their web page says “Data larger than memory” :white_check_mark:","user":"U01CQTKB86N","ts":"1611938576.161100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"c4a","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Indeed, their web page says “Data larger than memory” "},{"type":"emoji","name":"white_check_mark"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"99ac68ea-bbdf-4767-9164-ec149b9785bc","type":"message","text":"Will try that if filtering didn't work.","user":"U01A0S07875","ts":"1611938634.161300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KaZ/T","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Will try that if filtering didn't work."}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"f94e1bc9-4e12-4e8f-b31e-874836157206","type":"message","text":"Basically, filter() is taking too long. But, I just checked, there are 5130597 intersecting IDs. I was right, they are pretty less. So I need to drop everything else. I think, filtering should be best but it is running from last 40 minutes :confused:","user":"U01A0S07875","ts":"1611940444.161500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rJgj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Basically, filter() is taking too long. But, I just checked, there are 5130597 intersecting IDs. I was right, they are pretty less. So I need to drop everything else. I think, filtering should be best but it is running from last 40 minutes "},{"type":"emoji","name":"confused"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"5b241ff3-c906-4dcf-9523-6f92df343149","type":"message","text":"Try with `filter(:id =&gt; in(intersectind_ids), df1)` . Otherwise `in` will be called for each row and will create a new `Set` everytime.","user":"U67431ELR","ts":"1611942251.161700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"L81oV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Try with "},{"type":"text","text":"filter(:id => in(intersectind_ids), df1)","style":{"code":true}},{"type":"text","text":" . Otherwise "},{"type":"text","text":"in","style":{"code":true}},{"type":"text","text":" will be called for each row and will create a new "},{"type":"text","text":"Set","style":{"code":true}},{"type":"text","text":" everytime."}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875","reactions":[{"name":"+1","users":["U01A0S07875"],"count":1}]},{"client_msg_id":"573f8153-801e-47a9-b19c-d28771533e23","type":"message","text":"Trying that now","user":"U01A0S07875","ts":"1611942764.161900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Wv9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Trying that now"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"5a6c9edb-716f-46b7-bf56-2b6b2f1e64de","type":"message","text":"I think filter() is taking forever for me.","user":"U01A0S07875","ts":"1611944225.162200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zpm3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think filter() is taking forever for me."}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"715c0edb-fcf1-4a5a-a310-8f1109f815f2","type":"message","text":"did you try it with `Set`?","user":"UBF9YRB6H","ts":"1611947112.162400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ojyQ3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"did you try it with "},{"type":"text","text":"Set","style":{"code":true}},{"type":"text","text":"?"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"581f3ecd-39f8-4bb4-a5bf-c49e7025d791","type":"message","text":"maybe do\n\n```to_keep = in.(df1.id, Ref(intersection_set))\ndf1[to_keep, :]```","user":"UBF9YRB6H","ts":"1611947153.162600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"klp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"maybe do\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"to_keep = in.(df1.id, Ref(intersection_set))\ndf1[to_keep, :]"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"65a68c2c-606d-4aff-906a-bbda8d3e522a","type":"message","text":"Yes, I solved the problem by committing treason. I did the same procedure you told in R and reduced the data file's size to 1.4 GB and 620 MB :slightly_smiling_face:\n\nNow, I am trying to merge it. I think it won't be an issue since I already reduced their size.\n\nStill wondering, how can it be done in R much faster than in Julia. Only reason  I started using Julia was performance!! :confused:","user":"U01A0S07875","ts":"1611948618.162800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qMnO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes, I solved the problem by committing treason. I did the same procedure you told in R and reduced the data file's size to 1.4 GB and 620 MB "},{"type":"emoji","name":"slightly_smiling_face"},{"type":"text","text":"\n\nNow, I am trying to merge it. I think it won't be an issue since I already reduced their size.\n\nStill wondering, how can it be done in R much faster than in Julia. Only reason  I started using Julia was performance!! "},{"type":"emoji","name":"confused"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"426523b0-b671-4ce3-8e93-2d58c81f8487","type":"message","text":"The filtering was what was slow?\n\nThe answer is probably that Julia is faster, but our implementation is using more memory than it needs to in some way and R's algorithm (written in C++) is doing a better job containing memory. So your computer is going to swap in Julia but not in R","user":"UBF9YRB6H","ts":"1611948762.163000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jEX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The filtering was what was slow?\n\nThe answer is probably that Julia is faster, but our implementation is using more memory than it needs to in some way and R's algorithm (written in C++) is doing a better job containing memory. So your computer is going to swap in Julia but not in R"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"dd9fc2fd-daca-43c6-9bc5-571f0065f53f","type":"message","text":"but an MWE would be cool to help us figure out how to improve","user":"UBF9YRB6H","ts":"1611948782.163200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"f3r3O","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"but an MWE would be cool to help us figure out how to improve"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"8b7d2b17-0a18-4306-adcf-d0469441a5d2","type":"message","text":"With `Set` it should be very fast","user":"U8JAMQGQY","ts":"1611949169.164500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"i+H","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"With "},{"type":"text","text":"Set","style":{"code":true}},{"type":"text","text":" it should be very fast"}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"60ecf82f-03ff-4cc6-9f73-72b34d9b4f29","type":"message","text":"I was using `Tidyverse` there. I did use `Set` but I didn't spend more than 2 minutes in that (I was loosing patience). I should play more in morning perhaps to see the magic of `Set`.","user":"U01A0S07875","ts":"1611949534.165200","team":"T68168MUP","edited":{"user":"U01A0S07875","ts":"1611949980.000000"},"blocks":[{"type":"rich_text","block_id":"BczlJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I was using "},{"type":"text","text":"Tidyverse","style":{"code":true}},{"type":"text","text":" there. I did use "},{"type":"text","text":"Set","style":{"code":true}},{"type":"text","text":" but I didn't spend more than 2 minutes in that (I was loosing patience). I should play more in morning perhaps to see the magic of "},{"type":"text","text":"Set","style":{"code":true}},{"type":"text","text":"."}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"},{"client_msg_id":"0236a2d5-348c-41ec-b0de-6d5c4228cae2","type":"message","text":"It would be interesting to know whether your operation eventually finishes in a reasonable amount of time. I don't see any reason why our `filter` should be slower than the tidyverse here so that's kind of disappointing.","user":"U67431ELR","ts":"1612011897.172900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eD6rV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It would be interesting to know whether your operation eventually finishes in a reasonable amount of time. I don't see any reason why our "},{"type":"text","text":"filter","style":{"code":true}},{"type":"text","text":" should be slower than the tidyverse here so that's kind of disappointing."}]}]}],"thread_ts":"1611930069.127700","parent_user_id":"U01A0S07875"}]