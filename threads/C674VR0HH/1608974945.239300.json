[{"client_msg_id":"40bb0ef9-f2b7-4982-b4c6-287c0181bb20","type":"message","text":"Is it possible to use Apache Arrow to create a DataFrame that is larger than the available system memory?\nI don’t really need this ATM, but for peace of mind, I would like the capability to create a DataFrame of, say, 200 GB on my MacBook Pro that has 16 GB memory.","user":"U01CQTKB86N","ts":"1608974945.239300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MqCmj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is it possible to use Apache Arrow to create a DataFrame that is larger than the available system memory?\nI don’t really need this ATM, but for peace of mind, I would like the capability to create a DataFrame of, say, 200 GB on my MacBook Pro that has 16 GB memory."}]}]}],"thread_ts":"1608974945.239300","reply_count":47,"reply_users_count":5,"latest_reply":"1609258450.283600","reply_users":["U01CQTKB86N","U67431ELR","U8JP5B9T2","U012UUNBFM0","U01FAHWCMFF"],"subscribed":false},{"client_msg_id":"3ac7ddb9-c8a2-4be2-8bec-e7cf51233edf","type":"message","text":"Pandas page related to this: <https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html>","user":"U01CQTKB86N","ts":"1608976377.239400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"R/H","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Pandas page related to this: "},{"type":"link","url":"https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"3c42663b-aa86-46b7-b922-3e0a32ea9a7d","type":"message","text":"What would the DataFrames.jl API look like if there was a DaskDataFrames.jl that used the same API?","user":"U01CQTKB86N","ts":"1608976653.239600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lhS6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What would the DataFrames.jl API look like if there was a DaskDataFrames.jl that used the same API?"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"4752922d-e2f3-4768-965f-44562fe8b025","type":"message","text":"Maybe with `DataFrame(..., copycols=false)`?","user":"U67431ELR","ts":"1608990807.240000","team":"T68168MUP","edited":{"user":"U67431ELR","ts":"1608990822.000000"},"blocks":[{"type":"rich_text","block_id":"yal","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Maybe with "},{"type":"text","text":"DataFrame(..., copycols=false)","style":{"code":true}},{"type":"text","text":"?"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"5bcbcbf6-cfc1-4f51-8aa1-0e6e0de83789","type":"message","text":"Well, Arrow.jl documentation is really sparse, so based on that I wouldn’t know how to create it in-memory, but I could read something off disk. I also expect that it would fail for some reason, but I’m not sure.","user":"U01CQTKB86N","ts":"1608992163.240300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wA92","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Well, Arrow.jl documentation is really sparse, so based on that I wouldn’t know how to create it in-memory, but I could read something off disk. I also expect that it would fail for some reason, but I’m not sure."}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"ee0af6d6-c6b9-47d1-82c2-f07887b4eeaa","type":"message","text":"Ah if the file doesn't exist already I'm not sure how you could create it.","user":"U67431ELR","ts":"1608992253.240500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"K3J6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah if the file doesn't exist already I'm not sure how you could create it."}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"2d0ec291-b990-4217-bc71-e72f8ca9c612","type":"message","text":"I could try to create a file with Python.","user":"U01CQTKB86N","ts":"1608994698.240700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"h1SlJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I could try to create a file with Python."}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"c14e8a8c-236d-4beb-8432-0bd09c0155f9","type":"message","text":"Arrow.jl allows you to write in chunks. Docs could definitely use more fleshing out, but look at `Tables.partioner` <https://tables.juliadata.org/stable/#Tables.partitioner|https://tables.juliadata.org/stable/#Tables.partitioner>","user":"U8JP5B9T2","ts":"1609001233.240900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"drY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Arrow.jl allows you to write in chunks. Docs could definitely use more fleshing out, but look at "},{"type":"text","text":"Tables.partioner","style":{"code":true}},{"type":"text","text":" "},{"type":"link","url":"https://tables.juliadata.org/stable/#Tables.partitioner","text":"https://tables.juliadata.org/stable/#Tables.partitioner"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"ca8051c8-f0f5-415c-b7c8-7dd3c5a431a4","type":"message","text":"I'm using it to stream a bunch of individual csv files into a giant Arrow file. Pretty sure only one csv needs to be held in memory at a time","user":"U8JP5B9T2","ts":"1609001307.241100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"BNW1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm using it to stream a bunch of individual csv files into a giant Arrow file. Pretty sure only one csv needs to be held in memory at a time"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"67907bc3-897f-4fe1-91cc-c0ae7936f46b","type":"message","text":"I think I’ll load some NYC cab data to get data to test this out. <https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page>","user":"U01CQTKB86N","ts":"1609003855.241300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+c4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think I’ll load some NYC cab data to get data to test this out. "},{"type":"link","url":"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"f1d40683-dbcc-4152-9188-6bb5da83a37b","type":"message","text":"Is there a good web scraping package in Julia?","user":"U01CQTKB86N","ts":"1609003971.241500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GS62d","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a good web scraping package in Julia?"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"76340d00-fba6-49e5-82e0-97f7d1b5f676","type":"message","text":"I did some scraping for another hobby project in R earlier this year. This downloads Titanic passenger data from the wikipedia page. (Not sure if it still works.)\n```read_as_tibble &lt;- function(n) {\n  xpath &lt;-str_c(c('//*[@id=\"mw-content-text\"]/div[1]/table[', n, ']'), \n                collapse = '')\n  \n  t &lt;- read_html('<https://en.wikipedia.org/wiki/Passengers_of_the_RMS_Titanic>') %&gt;% \n    rvest::html_nodes('body') %&gt;% \n    xml2::xml_find_all(xpath) %&gt;%\n    html_table(fill = T)\n  t &lt;- t[[1]]\n  \n  tibble(Name = t$Name,\n         Age = t$Age,\n         Hometown = t$Hometown,\n         Destination = t$Destination) %&gt;%\n    add_column(DataSource = 'wiki')\n}```","user":"U01CQTKB86N","ts":"1609004216.241700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3rp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I did some scraping for another hobby project in R earlier this year. This downloads Titanic passenger data from the wikipedia page. (Not sure if it still works.)\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"read_as_tibble <- function(n) {\n  xpath <-str_c(c('//*[@id=\"mw-content-text\"]/div[1]/table[', n, ']'), \n                collapse = '')\n  \n  t <- read_html('"},{"type":"link","url":"https://en.wikipedia.org/wiki/Passengers_of_the_RMS_Titanic"},{"type":"text","text":"') %>% \n    rvest::html_nodes('body') %>% \n    xml2::xml_find_all(xpath) %>%\n    html_table(fill = T)\n  t <- t[[1]]\n  \n  tibble(Name = t$Name,\n         Age = t$Age,\n         Hometown = t$Hometown,\n         Destination = t$Destination) %>%\n    add_column(DataSource = 'wiki')\n}"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"3cd79072-b34d-4b09-b044-56a46446b538","type":"message","text":"Ah! The arrow package in R has in its documents code that downloads NYC taxi data in parquet format, so I’ll just use that.\n<https://cran.r-project.org/web/packages/arrow/vignettes/dataset.html>","user":"U01CQTKB86N","ts":"1609005019.241900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MId","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah! The arrow package in R has in its documents code that downloads NYC taxi data in parquet format, so I’ll just use that.\n"},{"type":"link","url":"https://cran.r-project.org/web/packages/arrow/vignettes/dataset.html"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"cc592f12-1219-4dac-a21c-e38b7413655a","type":"message","text":"R has surprising packages. The Finnish health authorities have data download in an obscure format I never heard about, tried Python for it I think, and then just used an R package that worked straight away, let me see…","user":"U01CQTKB86N","ts":"1609005131.242100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LFBP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"R has surprising packages. The Finnish health authorities have data download in an obscure format I never heard about, tried Python for it I think, and then just used an R package that worked straight away, let me see…"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"a624d501-4491-4bd3-9593-919c82e6fcf1","type":"message","text":"That was in JSON-stat, and downloading with rjstat worked straight away. My thoughts with Python were just WTF…","user":"U01CQTKB86N","ts":"1609005287.242300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3TCYR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That was in JSON-stat, and downloading with rjstat worked straight away. My thoughts with Python were just WTF…"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"5e36aeaf-8f93-4da1-a0e4-94ba010ff9e7","type":"message","text":":open_mouth:","user":"U01CQTKB86N","ts":"1609019080.248300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"v42","elements":[{"type":"rich_text_section","elements":[{"type":"emoji","name":"open_mouth"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"a7b7549a-8d6e-4eac-9fc9-fe6a19d1e05c","type":"message","text":"I downloaded 28 GB of Parquet files with R, read them with R’s Arrow, wrote one single feather files that’s 105 GB. Then opened that feather file with Julia.\n```using DataFrames, Arrow\ndf = DataFrame(Arrow.Table(\"nyc-taxi/feather/part-0.feather\"))\n1,231,773,935 rows × 20 columns (omitted printing of 15 columns)```","user":"U01CQTKB86N","ts":"1609019237.248500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7cYvp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I downloaded 28 GB of Parquet files with R, read them with R’s Arrow, wrote one single feather files that’s 105 GB. Then opened that feather file with Julia.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using DataFrames, Arrow\ndf = DataFrame(Arrow.Table(\"nyc-taxi/feather/part-0.feather\"))\n1,231,773,935 rows × 20 columns (omitted printing of 15 columns)"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"658c121d-9f37-444c-9639-d2972cb14604","type":"message","text":"I think it is actually working. It’s a bit hard to tell as I don’t know the correct answers.\n```describe(df, :min, :max, :mean, cols=:passenger_count)\n1 rows × 4 columns\n\nvariable\tmin\tmax\tmean\nSymbol\tInt8\tInt8\tFloat64\n1\tpassenger_count\t-127\t97\t1.66498```","user":"U01CQTKB86N","ts":"1609021180.248900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8g1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think it is actually working. It’s a bit hard to tell as I don’t know the correct answers.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"describe(df, :min, :max, :mean, cols=:passenger_count)\n1 rows × 4 columns\n\nvariable\tmin\tmax\tmean\nSymbol\tInt8\tInt8\tFloat64\n1\tpassenger_count\t-127\t97\t1.66498"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N","reactions":[{"name":"tada","users":["U67431ELR"],"count":1}]},{"client_msg_id":"1a3f1453-302a-4a69-ab4d-7064fda255f8","type":"message","text":"I am impressed.","user":"U01CQTKB86N","ts":"1609021189.249100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"AIh/b","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am impressed."}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"a4f43a95-3646-4d3b-83c6-72d581e340d1","type":"message","text":"R complained this when I tried the same:\n```Error: summarize() is not currently implemented for Arrow Datasets. Call collect() first to pull data into R.```","user":"U01CQTKB86N","ts":"1609021228.249300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Vumi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"R complained this when I tried the same:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Error: summarize() is not currently implemented for Arrow Datasets. Call collect() first to pull data into R."}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"aae7f0b1-378b-466e-932b-587d1151a67b","type":"message","text":"Getting summary statistics in 3 minutes is not bad.","user":"U01CQTKB86N","ts":"1609021657.249500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wmO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Getting summary statistics in 3 minutes is not bad."}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"81353634-21fd-4d44-a53b-eac699b2f0cf","type":"message","text":"I’ll need to make a comparison with Python’s arrow (dask?) to see if the answers are correct. That’s going to take some time.","user":"U01CQTKB86N","ts":"1609022044.249900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"v3h0l","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’ll need to make a comparison with Python’s arrow (dask?) to see if the answers are correct. That’s going to take some time."}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"c02314a8-c309-497f-80e1-5142ea364809","type":"message","text":"Interested in this:grinning:","user":"U012UUNBFM0","ts":"1609085200.251100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"EHO=y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Interested in this"},{"type":"emoji","name":"grinning"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"84a41ce5-4122-4329-9ad4-feaac4f726cd","type":"message","text":"<@U012UUNBFM0> My code is here: <https://github.com/StatisticalMice/ProjectsPublic/tree/main/NycTaxiData>","user":"U01CQTKB86N","ts":"1609146171.258900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jXjmF","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U012UUNBFM0"},{"type":"text","text":" My code is here: "},{"type":"link","url":"https://github.com/StatisticalMice/ProjectsPublic/tree/main/NycTaxiData"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"b2bdb2da-3659-4ee6-961c-5e93e2c6af00","type":"message","text":"‘Reading’ a 107 GB parquet file as DataFrame took 6 seconds, and calculating extrema for a column 54 seconds.","user":"U01CQTKB86N","ts":"1609146288.259100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Dkx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"‘Reading’ a 107 GB parquet file as DataFrame took 6 seconds, and calculating extrema for a column 54 seconds."}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"acbc16df-99a5-4238-9a30-4d74d8ea05d6","type":"message","text":"Trying to plot a histogram for that column started using 30 GB of RAM, and I killed it.","user":"U01CQTKB86N","ts":"1609146322.259300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PGaZ0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Trying to plot a histogram for that column started using 30 GB of RAM, and I killed it."}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"95a1a7f1-a1be-4571-95d2-888d900c0847","type":"message","text":"The R code can be used to download the parquet files and write out as feather.","user":"U01CQTKB86N","ts":"1609146514.259500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Giq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The R code can be used to download the parquet files and write out as feather."}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"58460b65-1e68-489f-940f-e01752be3bbb","type":"message","text":"Oh my.. :scream:","user":"U01CQTKB86N","ts":"1609151376.259700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Rw/UA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oh my.. "},{"type":"emoji","name":"scream"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"f077de6f-cba0-46cf-a3c9-ecf46e921685","type":"message","text":"Dask read in the Parquet files and calculated min/max for one column in ten seconds.","user":"U01CQTKB86N","ts":"1609151419.259900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"czItt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Dask read in the Parquet files and calculated min/max for one column in ten seconds."}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"dfe58ba6-0a8a-4152-9fed-825014a3f82d","type":"message","text":"The answer was the same.","user":"U01CQTKB86N","ts":"1609151719.260100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"64W9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The answer was the same."}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"3277d128-84f1-47d7-8f46-7fa1ff0a906a","type":"message","text":"Great, thanks for sharing.","user":"U012UUNBFM0","ts":"1609183716.264700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"r5CV6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Great, thanks for sharing."}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"708a2a05-408b-49e5-bc6c-177d467a6719","type":"message","text":"You’re welcome.","user":"U01CQTKB86N","ts":"1609185518.265000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PfnSX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You’re welcome."}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"05130c6a-ec7d-4281-adf2-0dac0b2313d5","type":"message","text":"Ah, “‘Reading’ a 107 GB parquet file as DataFrame took 6 seconds” means an arrow file, not parquet file.","user":"U01CQTKB86N","ts":"1609187280.265300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hQYz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah, “‘Reading’ a 107 GB parquet file as DataFrame took 6 seconds” means an arrow file, not parquet file."}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"c99b45e5-b45f-4410-b5c7-301edfd65825","type":"message","text":"It's surprising that computing the extrema is so slow. Is it the same with other operations, e.g. `sum`?","user":"U67431ELR","ts":"1609191587.265900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"k1865","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It's surprising that computing the extrema is so slow. Is it the same with other operations, e.g. "},{"type":"text","text":"sum","style":{"code":true}},{"type":"text","text":"?"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"E15C34B9-FF69-42A0-B52D-D15F6A5EAD2D","type":"message","text":"It’s probably something related to the machine only having 16 GB RAM?","user":"U01CQTKB86N","ts":"1609192926.266900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SK2L","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It’s probably something related to the machine only having 16 GB RAM?"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"cd6ee1f1-cc66-4d89-af11-4f323e440658","type":"message","text":"But then how is Dask so fast?","user":"U67431ELR","ts":"1609193087.267100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vUo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But then how is Dask so fast?"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"efc4a4be-826a-4981-af15-65e95499c484","type":"message","text":"<@U67431ELR> It should be possible to measure and graph the file I/O and memory allocation behaviour of the Python program and compare it with the Julia program. It’s just that I don’t have time to figure out how exactly to do this, and also I don’t know what the Julia program would be; I only have a working Python/Dask version of it. Here’s a good starting point that I found: <http://www.brendangregg.com/overview.html>","user":"U01CQTKB86N","ts":"1609238329.274200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"cLpsy","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U67431ELR"},{"type":"text","text":" It should be possible to measure and graph the file I/O and memory allocation behaviour of the Python program and compare it with the Julia program. It’s just that I don’t have time to figure out how exactly to do this, and also I don’t know what the Julia program would be; I only have a working Python/Dask version of it. Here’s a good starting point that I found: "},{"type":"link","url":"http://www.brendangregg.com/overview.html"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"517ee8de-fe60-4434-9fa9-0b2880378b25","type":"message","text":"<@U8JP5B9T2> Can you post your implementation of streaming individual CSVfiles into a giant arrow file? I've been trying to do this but it didn't work :disappointed:","user":"U01FAHWCMFF","ts":"1609257519.280800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zZ2","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U8JP5B9T2"},{"type":"text","text":" Can you post your implementation of streaming individual CSVfiles into a giant arrow file? I've been trying to do this but it didn't work "},{"type":"emoji","name":"disappointed"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"d16246bc-9935-4fd0-95c3-ca4ce25523bc","type":"message","text":"```Arrow.write(\"data/test.arrow\", Tables.partitioner(filepaths) do file\n    @info file\n    sample = stoolsample(basename(file))\n    tax = CSV.File(file, header=[:taxon, :taxid, :abundance, :additional_species],\n                    skipto=5) |&gt; DataFrame\n    tax.taxon = map(last ∘ parsetaxa, tax.taxon)\n    tax[!, :sample] .= sampleid(sample)\n    return select(tax, [:sample, :taxon, :taxid, :abundance])\nend)```","user":"U8JP5B9T2","ts":"1609257616.281000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PMa","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Arrow.write(\"data/test.arrow\", Tables.partitioner(filepaths) do file\n    @info file\n    sample = stoolsample(basename(file))\n    tax = CSV.File(file, header=[:taxon, :taxid, :abundance, :additional_species],\n                    skipto=5) |> DataFrame\n    tax.taxon = map(last ∘ parsetaxa, tax.taxon)\n    tax[!, :sample] .= sampleid(sample)\n    return select(tax, [:sample, :taxon, :taxid, :abundance])\nend)"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N","reactions":[{"name":"raised_hands","users":["U01FAHWCMFF"],"count":1}]},{"client_msg_id":"0ecd6827-b72f-441a-bbca-33c1987fe267","type":"message","text":"Where `filepaths` is an array of paths to individual CSV files","user":"U8JP5B9T2","ts":"1609257634.281200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+s5Wk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Where "},{"type":"text","text":"filepaths","style":{"code":true}},{"type":"text","text":" is an array of paths to individual CSV files"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"92064840-438d-4808-bbf4-5e4c70d5b4f1","type":"message","text":"Wow I thought the `Tables.partitioner` func was only for actual tables not filepaths, great to know, Thanks for this","user":"U01FAHWCMFF","ts":"1609257692.281400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/Y2q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Wow I thought the "},{"type":"text","text":"Tables.partitioner","style":{"code":true}},{"type":"text","text":" func was only for actual tables not filepaths, great to know, Thanks for this"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"1a3b1178-69ec-4d5b-b738-72f11ecb7928","type":"message","text":"Well, the return value of the `func` has to be a table","user":"U8JP5B9T2","ts":"1609257734.281600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"d+D","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Well, the return value of the "},{"type":"text","text":"func","style":{"code":true}},{"type":"text","text":" has to be a table"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"217fa3c1-dbbc-4658-a366-d12fb86aed43","type":"message","text":"Do you know if this implementation is possible? Ive been working on this for a while but again failed.\n\n1. Read a really big file (of any type) line by line.\n2. Do some data processing on the data to get the format of choice (table presumably)\n3. Append the newly formatted data to an arrow file line by line or chunk by chunk","user":"U01FAHWCMFF","ts":"1609257848.282000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"b8Mi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do you know if this implementation is possible? Ive been working on this for a while but again failed.\n\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Read a really big file (of any type) line by line."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Do some data processing on the data to get the format of choice (table presumably)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Append the newly formatted data to an arrow file line by line or chunk by chunk"}]}],"style":"ordered","indent":0}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"beb8aca4-ccfb-4c6b-97e6-62f55ff0611d","type":"message","text":"So here for each path, I'm reading the CSV into a DataFrame, then do some operations on the DataFrame, and return a DataFrame. The simplest version would be something like `Arrow.write(\"data/test.arrow\", Tables.partitioner(f-&gt; CSV.read(f, DataFrame), filepaths))`","user":"U8JP5B9T2","ts":"1609257872.282200","team":"T68168MUP","edited":{"user":"U8JP5B9T2","ts":"1609257879.000000"},"blocks":[{"type":"rich_text","block_id":"R5P","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So here for each path, I'm reading the CSV into a DataFrame, then do some operations on the DataFrame, and return a DataFrame. The simplest version would be something like "},{"type":"text","text":"Arrow.write(\"data/test.arrow\", Tables.partitioner(f-> CSV.read(f, DataFrame), filepaths))","style":{"code":true}}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N","reactions":[{"name":"raised_hands","users":["U01FAHWCMFF"],"count":1}]},{"client_msg_id":"c2d905cd-e485-4d19-95cc-c7533e8c18c4","type":"message","text":"Oh I see - yeah, that should be possible. There's probably a more efficient/elegant way to do this, but what' I'd probably do is something like use CSV to read in chunks of the file at a time (you may need to MMap or something to know where lines are)","user":"U8JP5B9T2","ts":"1609258202.282900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HtshF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oh I see - yeah, that should be possible. There's probably a more efficient/elegant way to do this, but what' I'd probably do is something like use CSV to read in chunks of the file at a time (you may need to MMap or something to know where lines are)"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"30400e76-d951-4568-b476-7418d0343a5d","type":"message","text":"Just checked your profile, you're in biojulia so let me explain the problem more clearly :slightly_smiling_face:\n\nSo Im trying to convert a large compressed VCF file (50gb+) into ARROW. I don't really want to rely on virtual memory because thats not gauranteed in production.\n\nTherefore converting + writing in chunks seems like the ideal solution\n\nMMaping compressed files isn't useful last time I checked *","user":"U01FAHWCMFF","ts":"1609258326.283100","team":"T68168MUP","edited":{"user":"U01FAHWCMFF","ts":"1609258392.000000"},"blocks":[{"type":"rich_text","block_id":"9cwG2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Just checked your profile, you're in biojulia so let me explain the problem more clearly "},{"type":"emoji","name":"slightly_smiling_face"},{"type":"text","text":"\n\nSo Im trying to convert a large compressed VCF file (50gb+) into ARROW. I don't really want to rely on virtual memory because thats not gauranteed in production.\n\nTherefore converting + writing in chunks seems like the ideal solution\n\nMMaping compressed files isn't useful last time I checked *"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"be196252-5313-4f9e-bbb5-a3904304928f","type":"message","text":"Ahh, ok. Is your VCF compressed?","user":"U8JP5B9T2","ts":"1609258422.283400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+Vrq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ahh, ok. Is your VCF compressed?"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"},{"client_msg_id":"5d92fd64-d418-4a96-b566-fdf6b5bd955e","type":"message","text":"yes vcf.gz","user":"U01FAHWCMFF","ts":"1609258450.283600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0K=Io","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yes vcf.gz"}]}]}],"thread_ts":"1608974945.239300","parent_user_id":"U01CQTKB86N"}]