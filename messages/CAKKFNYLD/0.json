{"cursor": 0, "messages": [{"client_msg_id":"276c16fe-02d9-4215-b422-1688e1eee8ce","type":"message","text":"Hi there! I will be looking for info about Automa.jl in a couple of weeks. Is that the right place ? :slightly_smiling_face:","user":"U01FR2HFJ7M","ts":"1607995590.151400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YBBG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi there! I will be looking for info about Automa.jl in a couple of weeks. Is that the right place ? "},{"type":"emoji","name":"slightly_smiling_face"}]}]}]},{"client_msg_id":"c47babcb-f114-4c66-91d3-b7d0d064a092","type":"message","text":"probably - if you haven't already, I recomend this as a place to start: <https://biojulia.net/post/automa1/>","user":"U8JP5B9T2","ts":"1607997392.152200","team":"T68168MUP","attachments":[{"service_name":"BioJulia","title":"Tutorial to Automa: Part 1 | BioJulia","title_link":"https://biojulia.net/post/automa1/","text":"Find this notebook at <https://github.com/jakobnissen/automa_tutorial> In bioinformatics, we have a saying: The first step of any bioinformatics project is to define a new file format, incompatible with all previous ones. The situation might not be quite as bad as the saying implies, but it is true that we have a lot of different file formats, representing the various kinds of data we work with. For that reason, creating file parsers is a central task in bioinformatics, and has almost become a craft in itself.","fallback":"BioJulia: Tutorial to Automa: Part 1 | BioJulia","thumb_url":"https://BioJulia.github.io/images/logo_huf2e28fc1e802707079b8e0ffee62f4dc_19447_300x300_fit_lanczos_2.png","ts":1598797689,"from_url":"https://biojulia.net/post/automa1/","thumb_width":300,"thumb_height":300,"service_icon":"https://biojulia.net/images/icon_huf2e28fc1e802707079b8e0ffee62f4dc_19447_192x192_fill_lanczos_center_2.png","id":1,"original_url":"https://biojulia.net/post/automa1/"}],"blocks":[{"type":"rich_text","block_id":"Q/6ZT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"probably - if you haven't already, I recomend this as a place to start: "},{"type":"link","url":"https://biojulia.net/post/automa1/"}]}]}],"reactions":[{"name":"+1","users":["U01FR2HFJ7M","U6QGE7S86","USBKT1275"],"count":3}]},{"client_msg_id":"0f994bf7-e425-4a67-9609-6640638dc8c7","type":"message","text":"Super stoked to announce that my fork of PopGen.jl will be merging into BioJulia/PopGen.jl and continue living there :smile:","user":"UM4TSHKF1","ts":"1608055262.153900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kzPUy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Super stoked to announce that my fork of PopGen.jl will be merging into BioJulia/PopGen.jl and continue living there "},{"type":"emoji","name":"smile"}]}]}],"reactions":[{"name":"clapping","users":["U69BL50BF","U7HAYKY9X","U8JP5B9T2","UCAFZ51L3","U01BR0AKMRQ","UB197FRCL","U6QGE7S86","U9TCDH0E7","ULWFF2Z8U"],"count":9},{"name":"fast_parrot","users":["U69BL50BF","U7HAYKY9X","U6QGE7S86"],"count":3}]},{"type":"message","text":"","user":"USU9FRPEU","ts":"1608072977.155100","team":"T68168MUP","attachments":[{"fallback":"[December 15th, 2020 5:53 PM] ayman: In biology we deal with alot of large compressed files.\n\nCommon file extensions include .fasta.gz and .vcf.gz\n\nCan’t seem to find a good out of the box readers that use multithreading for regular files and compressed files.","ts":"1608072815.225800","author_id":"U01FAHWCMFF","author_subname":"Ayman Al Baz","channel_id":"C6A044SQH","channel_name":"helpdesk","is_msg_unfurl":true,"is_reply_unfurl":true,"text":"In biology we deal with alot of large compressed files.\n\nCommon file extensions include .fasta.gz and .vcf.gz\n\nCan’t seem to find a good out of the box readers that use multithreading for regular files and compressed files.","author_name":"Ayman Al Baz","author_link":"https://julialang.slack.com/team/U01FAHWCMFF","author_icon":"https://avatars.slack-edge.com/2020-11-23/1516965530134_1cf6ed3767c34aaf6d40_48.png","mrkdwn_in":["text"],"color":"D0D0D0","from_url":"https://julialang.slack.com/archives/C6A044SQH/p1608072815225800?thread_ts=1608072566225200&cid=C6A044SQH","is_share":true,"footer":"From a thread in #helpdesk"}]},{"client_msg_id":"c2631a6d-006b-4bcb-8ffd-503f6b6490d1","type":"message","text":"Thanks for reposting here, ya I’m just wondering if there were any libraries that could read large .Fastq.gz and .vcf.gz (primarily this) using multithreading. I know of single threaded readers but I’m just looking for faster solutions","user":"U01FAHWCMFF","ts":"1608073307.156900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"noTy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for reposting here, ya I’m just wondering if there were any libraries that could read large .Fastq.gz and .vcf.gz (primarily this) using multithreading. I know of single threaded readers but I’m just looking for faster solutions"}]}]}]},{"client_msg_id":"a93f8535-a961-4de8-89c0-a7e1fdf7f282","type":"message","text":"<@UC2AEGPC2> is probably the one to contact about this","user":"USU9FRPEU","ts":"1608073689.158500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KtW","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UC2AEGPC2"},{"type":"text","text":" is probably the one to contact about this"}]}]}]},{"client_msg_id":"e58841cd-7840-4c6e-b982-866b4bd9d4b5","type":"message","text":"I think <@U7HAYKY9X> has also thought about how to make automa parsers multithreaded, though I don't know if any work has been done there.","user":"U8JP5B9T2","ts":"1608074164.159500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9LHkn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think "},{"type":"user","user_id":"U7HAYKY9X"},{"type":"text","text":" has also thought about how to make automa parsers multithreaded, though I don't know if any work has been done there."}]}]}]},{"client_msg_id":"f906d99d-d3fc-46f8-bf6d-a9edc549981c","type":"message","text":"There are not, unfortunately. The bottleneck in reading these files is the decompression, so perhaps you could call a binary for multithreaded decompression like pigz? If you're willing to have a slightly lower compression ratio, you can compress it in the BGZF format and read it with CodecBGZF.jl, which is quite fast (and support random access).","user":"U7HAYKY9X","ts":"1608103489.165800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Jcpj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"There are not, unfortunately. The bottleneck in reading these files is the decompression, so perhaps you could call a binary for multithreaded decompression like pigz? If you're willing to have a slightly lower compression ratio, you can compress it in the BGZF format and read it with CodecBGZF.jl, which is quite fast (and support random access)."}]}]}],"thread_ts":"1608103489.165800","reply_count":2,"reply_users_count":2,"latest_reply":"1608196142.181600","reply_users":["UM4TSHKF1","U7HAYKY9X"],"subscribed":false},{"client_msg_id":"c9c60331-d5b2-47ae-bde0-055af044b0c2","type":"message","text":"Speaking more broadly, Automa (that is, parsing itself) will probably never be multithreaded since the process is inherently serial (but it can usually parse at several GB/second, do it'll basically never be the problem). So it comes down to implementing a multithreaded decompressor package in Julia. Which is probably really hard","user":"U7HAYKY9X","ts":"1608103721.169000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OSmOE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Speaking more broadly, Automa (that is, parsing itself) will probably never be multithreaded since the process is inherently serial (but it can usually parse at several GB/second, do it'll basically never be the problem). So it comes down to implementing a multithreaded decompressor package in Julia. Which is probably really hard"}]}]}]},{"client_msg_id":"fa5c99f7-868c-442b-95eb-15bf039c0f24","type":"message","text":"Hm, perhaps it would be possible to do the *parsing* in one thread, but then offload all the *work* to another thread. It would have to be done on a parser-by-parser level. Probably not needed for FASTX at least (not sure about VCF, I think that parser is a little unmaintained and rusty)","user":"U7HAYKY9X","ts":"1608106009.170600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vTtIE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hm, perhaps it would be possible to do the "},{"type":"text","text":"parsing","style":{"bold":true}},{"type":"text","text":" in one thread, but then offload all the "},{"type":"text","text":"work","style":{"bold":true}},{"type":"text","text":" to another thread. It would have to be done on a parser-by-parser level. Probably not needed for FASTX at least (not sure about VCF, I think that parser is a little unmaintained and rusty)"}]}]}]},{"client_msg_id":"b58ec672-c674-4adb-8b28-c904d705c2e0","type":"message","text":"Question (I'm quite naive on parallelization) -\n\nIs it possible to have state machines that can start somewhere in the middle, given sufficient signal? For example, in a fastq file, I could jump to the middle of the file, and start scanning until I hit a newline followed by `@` - There are a bunch of other unambiguous entry points to certain states. So I could just say \"I don't know my state until I hit some signal.\" I would imagine you'd waste a bit of time reading the same bytes twice, but it could be worth it (or maybe not it it's really multiple GB / sec)\n\nI definitely think being able to kick state information out to different threads so that the actual bytes can be processed could make a big difference","user":"U8JP5B9T2","ts":"1608130816.175800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j3kcR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Question (I'm quite naive on parallelization) -\n\nIs it possible to have state machines that can start somewhere in the middle, given sufficient signal? For example, in a fastq file, I could jump to the middle of the file, and start scanning until I hit a newline followed by "},{"type":"text","text":"@","style":{"code":true}},{"type":"text","text":" - There are a bunch of other unambiguous entry points to certain states. So I could just say \"I don't know my state until I hit some signal.\" I would imagine you'd waste a bit of time reading the same bytes twice, but it could be worth it (or maybe not it it's really multiple GB / sec)\n\nI definitely think being able to kick state information out to different threads so that the actual bytes can be processed could make a big difference"}]}]}]},{"client_msg_id":"1b7eeb64-c6b3-40f1-a279-506bb67e2eb3","type":"message","text":"You can do that, yeah. FASTX.jl already starts and stops the state machine at each record. I suppose you could have multiple state machines running in parallel if you wanted to. This would be a question of implementing it in the packages that uses Automa, not in Automa itself","user":"U7HAYKY9X","ts":"1608137352.177300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"u+Ue","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You can do that, yeah. FASTX.jl already starts and stops the state machine at each record. I suppose you could have multiple state machines running in parallel if you wanted to. This would be a question of implementing it in the packages that uses Automa, not in Automa itself"}]}]}]},{"client_msg_id":"32893a03-feb5-4bcd-b2c4-c916c4aae0c2","type":"message","text":"But, I mean, my laptop reads FASTQ files using FASTX with about 1.5 GB/s (uncompressed, with <https://github.com/BioJulia/Automa.jl/pull/60>) , so it'll probably not be worth the effort. Especially if you would have to scan the file first to find the entry points (though you could do that in a pure SIMD loop). I'd much rather see\n• Multithreaded decompression\n• Use of ReadDataStores.jl","user":"U7HAYKY9X","ts":"1608137569.179100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"014pI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But, I mean, my laptop reads FASTQ files using FASTX with about 1.5 GB/s (uncompressed, with "},{"type":"link","url":"https://github.com/BioJulia/Automa.jl/pull/60"},{"type":"text","text":") , so it'll probably not be worth the effort. Especially if you would have to scan the file first to find the entry points (though you could do that in a pure SIMD loop). I'd much rather see\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Multithreaded decompression"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Use of ReadDataStores.jl"}]}],"style":"bullet","indent":0}]}],"reactions":[{"name":"100","users":["U8JP5B9T2"],"count":1}]},{"client_msg_id":"e3326d93-cf34-4f87-b266-dabc41f93600","type":"message","text":"Thanks for the replies guys. While yes FASTA/Q parsing is fast, VCF parsing remains quite slow on my machine with roughly a read rate of 20mb/s (uncompressed) using the pre-existing VCF tools in julia","user":"U01FAHWCMFF","ts":"1608164297.180800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Sqp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for the replies guys. While yes FASTA/Q parsing is fast, VCF parsing remains quite slow on my machine with roughly a read rate of 20mb/s (uncompressed) using the pre-existing VCF tools in julia"}]}]}]},{"client_msg_id":"4aeb7771-9191-4575-b280-74e1d70168c6","type":"message","text":"Motivated by nothing but contranianism and pettiness, I have now managed to improve the FASTQ parsing times on the <https://github.com/lh3/biofast/tree/master/fqcnt> benchmark quite substantially. Here is current (observed) time versus the one originally reported:\n```Observed:\nRust                   0.43 s\nC                      0.53 s\nJulia                  0.57 s\nJulia (w. compiletime) 3.39 s\nJulia 1.6 (w. c. time) 2.22 s\n\nReported:\nRust:                    0.8 s\nC                        1.4 s\nJulia:                   2.6 s\nJulia (w. compiletime): 13.6 s```\nUnfortunately, Julia 1.6 sees a time of 0.65 seconds, probably due to <https://github.com/JuliaLang/julia/issues/38947>, but if that was fixed, one could extrapolate a speed of 2.1 seconds including compile time.\nThere still seem to be some way to go. The breakdown in time spent is approximately:\n1/3 time on the underlying IO (about 6.4 GB/s)\n1/3 time spent on the actual parsing (also about 6.4 GB/s)\n1/3 time seeps between the cracks, due to inefficiencies in TranscodingStreams.jl, the Reader interface in Automa, and the constant `eof` checking. That's where to improve.","user":"U7HAYKY9X","ts":"1608393170.185800","team":"T68168MUP","edited":{"user":"U7HAYKY9X","ts":"1608456242.000000"},"blocks":[{"type":"rich_text","block_id":"m=wdS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Motivated by nothing but contranianism and pettiness, I have now managed to improve the FASTQ parsing times on the "},{"type":"link","url":"https://github.com/lh3/biofast/tree/master/fqcnt"},{"type":"text","text":" benchmark quite substantially. Here is current (observed) time versus the one originally reported:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Observed:\nRust                   0.43 s\nC                      0.53 s\nJulia                  0.57 s\nJulia (w. compiletime) 3.39 s\nJulia 1.6 (w. c. time) 2.22 s\n\nReported:\nRust:                    0.8 s\nC                        1.4 s\nJulia:                   2.6 s\nJulia (w. compiletime): 13.6 s"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Unfortunately, Julia 1.6 sees a time of 0.65 seconds, probably due to "},{"type":"link","url":"https://github.com/JuliaLang/julia/issues/38947"},{"type":"text","text":", but if that was fixed, one could extrapolate a speed of 2.1 seconds including compile time.\nThere still seem to be some way to go. The breakdown in time spent is approximately:\n1/3 time on the underlying IO (about 6.4 GB/s)\n1/3 time spent on the actual parsing (also about 6.4 GB/s)\n1/3 time seeps between the cracks, due to inefficiencies in TranscodingStreams.jl, the Reader interface in Automa, and the constant "},{"type":"text","text":"eof","style":{"code":true}},{"type":"text","text":" checking. That's where to improve."}]}]}],"reactions":[{"name":"raised_hands","users":["U01FAHWCMFF","UKG4WF8PJ","UGU761DU2","UM4TSHKF1","U01HD5VFXJM"],"count":5},{"name":"100","users":["U6C937ENB"],"count":1}]},{"client_msg_id":"361dbe45-11a4-4224-a2de-6b71c709d060","type":"message","text":"Hi there! Small quick question. I am looking for a package for Genetic Algorithms with possibly autotune of parameters. I have found Evolutionary.jl so far, but I might have missed other options. Any recommandation ?","user":"U01FR2HFJ7M","ts":"1608775090.188300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Mkv6Q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi there! Small quick question. I am looking for a package for Genetic Algorithms with possibly autotune of parameters. I have found Evolutionary.jl so far, but I might have missed other options. Any recommandation ?"}]}]}]},{"client_msg_id":"69b0273b-598a-4979-b271-9fecc75e6769","type":"message","text":"GalacticOptim has the following: <https://galacticoptim.sciml.ai/dev/global_optimizers/global/>","user":"U69BL50BF","ts":"1608775298.188500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nJyeM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"GalacticOptim has the following: "},{"type":"link","url":"https://galacticoptim.sciml.ai/dev/global_optimizers/global/"}]}]}]},{"client_msg_id":"c1d58afd-47a8-49fb-a394-2cba45c35944","type":"message","text":"This looks like a great option, as it also includes Evolutionary.jl. Thanks <@U69BL50BF>","user":"U01FR2HFJ7M","ts":"1608781976.189500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uuF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This looks like a great option, as it also includes Evolutionary.jl. Thanks "},{"type":"user","user_id":"U69BL50BF"}]}]}]},{"client_msg_id":"6e614c8e-b86f-49c8-82a3-9a79ee99fcc4","type":"message","text":"Any idea where I can chat/contact people working/using Evolutionary.jl (other than GH issues)?","user":"U01FR2HFJ7M","ts":"1609824439.191600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ttnW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Any idea where I can chat/contact people working/using Evolutionary.jl (other than GH issues)?"}]}]}]},{"client_msg_id":"a26626b3-7813-4fc0-8253-e7f56f17df1d","type":"message","text":"Hi all, I had mentioned way back that I was going to make a PyCall wrapper for Hail (<http://hail.is>) and I’m finally finding time to do it. I have one quick style question for you all. In hail you can read in a vcf file into a MatrixTable, let’s call it mt, and then print out information about it with `mt.describe`, `mt.show`, and so on. Those functions all work fine on the PyObject in PyCall, but I was thinking it might be better style to wrap them in function to make them more Julia-like. For example, PyHail.describe(mt). Any thoughts?","user":"USBRJS6BU","ts":"1610257403.195800","team":"T68168MUP","attachments":[{"title":"Hail |  Index ","title_link":"http://hail.is/","text":"Hail Index Page","fallback":"Hail |  Index ","from_url":"http://hail.is/","service_icon":"https://hail.is/hail_logo_sq-sm-opt.ico","service_name":"hail.is","id":1,"original_url":"http://hail.is"}],"blocks":[{"type":"rich_text","block_id":"/TW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi all, I had mentioned way back that I was going to make a PyCall wrapper for Hail ("},{"type":"link","url":"http://hail.is"},{"type":"text","text":") and I’m finally finding time to do it. I have one quick style question for you all. In hail you can read in a vcf file into a MatrixTable, let’s call it mt, and then print out information about it with "},{"type":"text","text":"mt.describe","style":{"code":true}},{"type":"text","text":", "},{"type":"text","text":"mt.show","style":{"code":true}},{"type":"text","text":", and so on. Those functions all work fine on the PyObject in PyCall, but I was thinking it might be better style to wrap them in function to make them more Julia-like. For example, PyHail.describe(mt). Any thoughts?"}]}]}]}]}