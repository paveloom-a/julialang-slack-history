{"cursor": 0, "messages": [{"type":"message","subtype":"channel_join","ts":"1608169106.205800","user":"U01GWRMB43Y","text":"<@U01GWRMB43Y> has joined the channel","inviter":"UD0SQV5LL"},{"client_msg_id":"cd0f1e65-242b-41d1-9b4b-82cf8f5492d9","type":"message","text":"I think I am hitting a Zygote 0.5 bug. Can we increase the compat bound so that Flux can use Zygote 0.6?","user":"U7YD3DKL2","ts":"1608572338.208600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nq1T","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think I am hitting a Zygote 0.5 bug. Can we increase the compat bound so that Flux can use Zygote 0.6?"}]}]}],"thread_ts":"1608572338.208600","reply_count":9,"reply_users_count":2,"latest_reply":"1608577514.210400","reply_users":["U7YD3DKL2","UM30MT6RF"],"subscribed":false},{"client_msg_id":"418d3bd5-eec4-4802-8b0e-0e099411379c","type":"message","text":"<https://sychen52.github.io/2020/11/11/IsJuliaReadyForDeepLearning.html>","user":"UDGT4PM41","ts":"1608837807.210900","team":"T68168MUP","attachments":[{"service_name":"Yangzai’s Blog","title":"Is Julia Ready for Deep Learning","title_link":"https://sychen52.github.io/2020/11/11/IsJuliaReadyForDeepLearning.html","text":"I heard about Swift and Julia for deep learning during <http://fast.ai|fast.ai>’s course. Then I spent quite some time to try figuring out whether any of these languages is a better choice for deep learning than Python. However, soon I realized that Chris Lattner left Google and Swift for Tensorflow project is slowing down and even Jeremy Howard is less passionate about it. Then the only choice left for now is Julia AFAIK. The Question is whether Julia is ready for deep learning.","fallback":"Yangzai’s Blog: Is Julia Ready for Deep Learning","ts":1605058098,"from_url":"https://sychen52.github.io/2020/11/11/IsJuliaReadyForDeepLearning.html","id":1,"original_url":"https://sychen52.github.io/2020/11/11/IsJuliaReadyForDeepLearning.html"}],"blocks":[{"type":"rich_text","block_id":"Vc0","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://sychen52.github.io/2020/11/11/IsJuliaReadyForDeepLearning.html"}]}]}]},{"client_msg_id":"B9B8E182-A041-4C85-B43B-831E9605154A","type":"message","text":"Any owners of the FluxML organization around?","user":"U7THT3TM3","ts":"1609191265.212400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6E5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Any owners of the FluxML organization around?"}]}]}]},{"client_msg_id":"0F87BA95-44AA-4409-AA69-F092949A1866","type":"message","text":"Could you (temporarily) make Tim (<@U68A3ASP9>) an owner of the FluxML organization so that he can finish setting up Buildkite for GPU CI?","user":"U7THT3TM3","ts":"1609191301.213400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"njrS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Could you (temporarily) make Tim ("},{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":") an owner of the FluxML organization so that he can finish setting up Buildkite for GPU CI?"}]}]}]},{"client_msg_id":"907500A7-BA7B-4ACB-9EB5-9397A5EFFACC","type":"message","text":"You can convert him back to a regular member once he's done.","user":"U7THT3TM3","ts":"1609191345.213800","team":"T68168MUP","edited":{"user":"U7THT3TM3","ts":"1609191361.000000"},"blocks":[{"type":"rich_text","block_id":"=Yn1O","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You can convert him back to a regular member once he's done."}]}]}]},{"client_msg_id":"282846ef-5896-4a1f-a1d8-744b9ea17e10","type":"message","text":"Anyone around who maintains the webpage <http://fluxml.ai|fluxml.ai> ? It throws a 404 when hitting the \"Try it out\" button pointing to this address: <https://fluxml.ai/getting_started.html> .","user":"ULL3KSGBS","ts":"1609223126.215500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vqrwl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Anyone around who maintains the webpage "},{"type":"link","url":"http://fluxml.ai","text":"fluxml.ai"},{"type":"text","text":" ? It throws a 404 when hitting the \"Try it out\" button pointing to this address: "},{"type":"link","url":"https://fluxml.ai/getting_started.html"},{"type":"text","text":" ."}]}]}]},{"client_msg_id":"71574f36-01d6-450a-a95b-c3401fa4a9fc","type":"message","text":"Fixed, thanks <@ULL3KSGBS>! Sorry about that, we are updating that part of the website","user":"UC4QQPG4A","ts":"1609226322.216400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7vD2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Fixed, thanks "},{"type":"user","user_id":"ULL3KSGBS"},{"type":"text","text":"! Sorry about that, we are updating that part of the website"}]}]}]},{"type":"message","subtype":"channel_join","ts":"1609333499.218500","user":"U01HYEN2ES0","text":"<@U01HYEN2ES0> has joined the channel","inviter":"UEP056STX"},{"client_msg_id":"efe5ad02-f4ec-43e1-b783-2018f78380a6","type":"message","text":"Not seeing as much Flux discussion here lately, are people discussing Flux elsewhere?","user":"U9RDM8ZGT","ts":"1609886938.219800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uVz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not seeing as much Flux discussion here lately, are people discussing Flux elsewhere?"}]}]}],"thread_ts":"1609886938.219800","reply_count":3,"reply_users_count":2,"latest_reply":"1609889372.220900","reply_users":["UGU761DU2","U9RDM8ZGT"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"This channel was boosted by a lot of help-focused threads. Not exactly sure why those tapered off, but I haven't seen one in a while despite no noticeable change in the rate of related help threads on Discourse.","user":"UMY1LV01G","ts":"1609892625.222300","thread_ts":"1609886938.219800","root":{"client_msg_id":"efe5ad02-f4ec-43e1-b783-2018f78380a6","type":"message","text":"Not seeing as much Flux discussion here lately, are people discussing Flux elsewhere?","user":"U9RDM8ZGT","ts":"1609886938.219800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uVz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not seeing as much Flux discussion here lately, are people discussing Flux elsewhere?"}]}]}],"thread_ts":"1609886938.219800","reply_count":10,"reply_users_count":3,"latest_reply":"1609892625.222300","reply_users":["UGU761DU2","U9RDM8ZGT","UMY1LV01G"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"n6NP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This channel was boosted by a lot of help-focused threads. Not exactly sure why those tapered off, but I haven't seen one in a while despite no noticeable change in the rate of related help threads on Discourse."}]}]}],"client_msg_id":"569de717-8292-4474-91bb-4e4c4fc8ed09","reactions":[{"name":"+1","users":["UGU761DU2"],"count":1}]},{"client_msg_id":"f25eef3a-9f87-435b-be58-f6a0954b4550","type":"message","text":"This doesn't seem to be the only slack channel effected.","user":"U9RDM8ZGT","ts":"1609892673.222800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KX/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This doesn't seem to be the only slack channel effected."}]}]}]},{"client_msg_id":"290e1c62-f354-4331-aac5-eff43d82fc49","type":"message","text":"<#C690QRAA3|machine-learning> traffic seems way down as well. I'm guessing it's the holiday lull, but it's possible that there's some movement away from Slack - but there isn't (wasn't) even a <#C7120PCUQ|flux> channel over on Zulip until I just made one.","user":"U9RDM8ZGT","ts":"1609892760.224300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4+M","elements":[{"type":"rich_text_section","elements":[{"type":"channel","channel_id":"C690QRAA3"},{"type":"text","text":" traffic seems way down as well. I'm guessing it's the holiday lull, but it's possible that there's some movement away from Slack - but there isn't (wasn't) even a "},{"type":"channel","channel_id":"C7120PCUQ"},{"type":"text","text":" channel over on Zulip until I just made one."}]}]}]},{"client_msg_id":"df604fa4-805d-49fb-9bc8-b1006555b2b2","type":"message","text":"There aren't that many flux related posts on Zulip that aren't also design posts, so we just stuck to using the ML stream","user":"UMY1LV01G","ts":"1609893094.226000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nh=n9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"There aren't that many flux related posts on Zulip that aren't also design posts, so we just stuck to using the ML stream"}]}]}]},{"client_msg_id":"e0367249-dfea-45e9-ba4d-4693c3cd191d","type":"message","text":"I can tell you that GH issue discussion and comments are quite active right now :) Zulip ML streams are likely quieter because of the holiday too.","user":"UMY1LV01G","ts":"1609893186.227900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"U0vj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I can tell you that GH issue discussion and comments are quite active right now :) Zulip ML streams are likely quieter because of the holiday too."}]}]}]},{"type":"message","text":"gradient falls on StructArray. loss is calculated alright. <@UC4QQPG4A>, any suggestions how to fix that? Thanks!","files":[{"id":"F01HZ4UFMSS","created":1610000287,"timestamp":1610000287,"name":"Untitled","title":"Untitled","mimetype":"text/plain","filetype":"julia","pretty_type":"Julia","user":"UJZNB9CSV","editable":true,"size":1095,"mode":"snippet","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01HZ4UFMSS/untitled","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01HZ4UFMSS/download/untitled","permalink":"https://julialang.slack.com/files/UJZNB9CSV/F01HZ4UFMSS/untitled","permalink_public":"https://slack-files.com/T68168MUP-F01HZ4UFMSS-2700fc7140","edit_link":"https://julialang.slack.com/files/UJZNB9CSV/F01HZ4UFMSS/untitled/edit","preview":"julia> d[1]\n2×128 StructArray(::Array{Bool,2}, ::Array{Bool,2}, ::Array{Bool,2}) with eltype NamedTuple{(:x1, :x2, :x3),Tuple{Bool,Bool,Bool}}:\n (x1 = 1, x2 = 1, x3 = 0)  (x1 = 1, x2 = 0, x3 = 0)  …  (x1 = 0, x2 = 1, x3 = 0)  (x1 = 0, x2 = 1, x3 = 0)\n (x1 = 0, x2 = 0, x3 = 1)  (x1 = 0, x2 = 1, x3 = 1)     (x1 = 1, x2 = 0, x3 = 1)  (x1 = 1, x2 = 0, x3 = 1)\n","preview_highlight":"<div class=\"CodeMirror cm-s-default CodeMirrorServer\" oncopy=\"if(event.clipboardData){event.clipboardData.setData('text/plain',window.getSelection().toString().replace(/\\u200b/g,''));event.preventDefault();event.stopPropagation();}\">\n<div class=\"CodeMirror-code\">\n<div><pre><span class=\"cm-variable\">julia</span><span class=\"cm-operator\">&gt;</span> <span class=\"cm-variable\">d</span>[<span class=\"cm-number\">1</span>]</pre></div>\n<div><pre><span class=\"cm-number\">2</span><span class=\"cm-operator\">×</span><span class=\"cm-number\">128</span> <span class=\"cm-builtin\">StructArray</span>(<span class=\"cm-builtin\">::Array{Bool,2</span><span class=\"cm-builtin\">}</span>, <span class=\"cm-builtin\">::Array{Bool,2</span><span class=\"cm-builtin\">}</span>, <span class=\"cm-builtin\">::Array{Bool,2</span><span class=\"cm-builtin\">}</span>) <span class=\"cm-variable\">with</span> <span class=\"cm-variable\">eltype</span> <span class=\"cm-variable\">NamedTuple</span>{(<span class=\"cm-builtin\">:x1</span>, <span class=\"cm-builtin\">:x2</span>, <span class=\"cm-builtin\">:x3</span>),<span class=\"cm-variable\">Tuple</span>{<span class=\"cm-variable\">Bool</span>,<span class=\"cm-variable\">Bool</span>,<span class=\"cm-variable\">Bool</span>}}<span class=\"cm-operator\">:</span></pre></div>\n<div><pre> (<span class=\"cm-variable\">x1</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">1</span>, <span class=\"cm-variable\">x2</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">1</span>, <span class=\"cm-variable\">x3</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">0</span>)  (<span class=\"cm-variable\">x1</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">1</span>, <span class=\"cm-variable\">x2</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">0</span>, <span class=\"cm-variable\">x3</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">0</span>)  <span class=\"cm-variable\">…</span>  (<span class=\"cm-variable\">x1</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">0</span>, <span class=\"cm-variable\">x2</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">1</span>, <span class=\"cm-variable\">x3</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">0</span>)  (<span class=\"cm-variable\">x1</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">0</span>, <span class=\"cm-variable\">x2</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">1</span>, <span class=\"cm-variable\">x3</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">0</span>)</pre></div>\n<div><pre> (<span class=\"cm-variable\">x1</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">0</span>, <span class=\"cm-variable\">x2</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">0</span>, <span class=\"cm-variable\">x3</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">1</span>)  (<span class=\"cm-variable\">x1</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">0</span>, <span class=\"cm-variable\">x2</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">1</span>, <span class=\"cm-variable\">x3</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">1</span>)     (<span class=\"cm-variable\">x1</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">1</span>, <span class=\"cm-variable\">x2</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">0</span>, <span class=\"cm-variable\">x3</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">1</span>)  (<span class=\"cm-variable\">x1</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">1</span>, <span class=\"cm-variable\">x2</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">0</span>, <span class=\"cm-variable\">x3</span> <span class=\"cm-operator\">=</span> <span class=\"cm-number\">1</span>)</pre></div>\n</div>\n</div>\n","lines":20,"lines_more":15,"preview_is_truncated":true,"is_starred":false,"has_rich_preview":false}],"upload":true,"blocks":[{"type":"rich_text","block_id":"ldvQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"gradient falls on StructArray. loss is calculated alright. "},{"type":"user","user_id":"UC4QQPG4A"},{"type":"text","text":", any suggestions how to fix that? Thanks!"}]}]}],"user":"UJZNB9CSV","display_as_bot":false,"ts":"1610000289.228000","edited":{"user":"UJZNB9CSV","ts":"1610000544.000000"},"client_msg_id":"50a496f4-2627-4a38-a6a2-61df78cc359c"},{"client_msg_id":"BDC7DBEB-8688-41D1-A89F-73060BECB256","type":"message","text":"Has there been much work on latency reduction in Flux? SnoopCompile etc. I’m keen to improve latency in some downstream packages so thought I’d check","user":"U8MPCDJAY","ts":"1610210034.230300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jUHR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Has there been much work on latency reduction in Flux? SnoopCompile etc. I’m keen to improve latency in some downstream packages so thought I’d check"}]}]}]},{"client_msg_id":"1ff27999-0a3f-4f6b-ba64-e5ff871457ef","type":"message","text":"I haven't seen or heard of anything, but it would be very much welcome","user":"UMY1LV01G","ts":"1610215332.230700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Rr7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I haven't seen or heard of anything, but it would be very much welcome"}]}]}]},{"client_msg_id":"fc9e3e64-bb75-45f0-9061-e69009d6d6d2","type":"message","text":"IIRC Zygote and CUDA are the two biggest offenders, in that order","user":"UMY1LV01G","ts":"1610215353.231200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"R2S","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"IIRC Zygote and CUDA are the two biggest offenders, in that order"}]}]}]},{"client_msg_id":"230f77c4-4bd1-45e9-aa52-dc5a17b5e052","type":"message","text":"Yes, that would be awesome! I think it might even be interesting to see if for Zygote, we could use the new experimental infrastructure to just not specialize at all in the compiler parts.","user":"UM30MT6RF","ts":"1610221400.233900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ywJh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes, that would be awesome! I think it might even be interesting to see if for Zygote, we could use the new experimental infrastructure to just not specialize at all in the compiler parts."}]}]}]},{"client_msg_id":"2358C6D3-0314-4760-958A-0CB67AB51264","type":"message","text":"Ok cool. I can’t promise much, but thought it would be good to learn the tools on something core like Zygote. I’ll open a PR if I make any progress","user":"U8MPCDJAY","ts":"1610224415.235400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9Cg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ok cool. I can’t promise much, but thought it would be good to learn the tools on something core like Zygote. I’ll open a PR if I make any progress"}]}]}],"reactions":[{"name":"+1","users":["UMY1LV01G","UH9KWTTD3","UM30MT6RF"],"count":3}]},{"client_msg_id":"d37a3d0f-b85e-4b53-bd45-517efcad3685","type":"message","text":"That'd be brilliant! Although zygote by itself seems to have gotten better anecdotally. It would be a huge help if you'd gather your findings in an issue/ PR anyway to see where we can go with it. Flux still takes longer than I'd expect to precompile.","user":"UC4QQPG4A","ts":"1610227373.240800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ab9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That'd be brilliant! Although zygote by itself seems to have gotten better anecdotally. It would be a huge help if you'd gather your findings in an issue/ PR anyway to see where we can go with it. Flux still takes longer than I'd expect to precompile."}]}]}],"thread_ts":"1610227373.240800","reply_count":1,"reply_users_count":1,"latest_reply":"1610231008.241000","reply_users":["U8MPCDJAY"],"subscribed":false},{"client_msg_id":"f67d1a43-41d4-4711-8d43-96b7e0df139a","type":"message","text":"Hi, I have changed activation functions in my models from `relu` to `sigmoid` and I save them into `.bson` which used to work with `relu` . Now, I am getting `typeof` error with sigmoid. Do not understand why. Thanks for any help. Error:\n\n```julia&gt; res = BSON.load(file_with_dictionary_results)\nERROR: MethodError: no method matching typeof(σ)()\nStacktrace:\n [1] (::BSON.var\"#37#38\")(::Dict{Symbol,Any}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/extensions.jl:76\n [2] _raise_recursive(::Dict{Symbol,Any}, ::IdDict{Any,Any}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/read.jl:79\n [3] raise_recursive(::Dict{Symbol,Any}, ::IdDict{Any,Any}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/read.jl:89\n [4] (::BSON.var\"#39#40\"{IdDict{Any,Any}})(::Dict{Symbol,Any}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/extensions.jl:124\n [5] iterate at ./generator.jl:47 [inlined]\n [6] collect_to!(::Array{Any,1}, ::Base.Generator{Array{Any,1},BSON.var\"#39#40\"{IdDict{Any,Any}}}, ::Int64, ::Int64) at ./array.jl:732\n [7] collect_to!(::Array{Real,1}, ::Base.Generator{Array{Any,1},BSON.var\"#39#40\"{IdDict{Any,Any}}}, ::Int64, ::Int64) at ./array.jl:740 (repeats 3 times)\n [8] collect_to_with_first!(::Array{Int64,1}, ::Int64, ::Base.Generator{Array{Any,1},BSON.var\"#39#40\"{IdDict{Any,Any}}}, ::Int64) at ./array.jl:710\n [9] _collect(::Array{Any,1}, ::Base.Generator{Array{Any,1},BSON.var\"#39#40\"{IdDict{Any,Any}}}, ::Base.EltypeUnknown, ::Base.HasShape{1}) at ./array.jl:704\n [10] collect_similar(::Array{Any,1}, ::Base.Generator{Array{Any,1},BSON.var\"#39#40\"{IdDict{Any,Any}}}) at ./array.jl:628\n [11] map(::Function, ::Array{Any,1}) at ./abstractarray.jl:2162\n [12] newstruct_raw(::IdDict{Any,Any}, ::Type{T} where T, ::Dict{Symbol,Any}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/extensions.jl:124\n [13] (::BSON.var\"#43#44\")(::Dict{Symbol,Any}, ::IdDict{Any,Any}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/extensions.jl:140\n [14] raise_recursive(::Dict{Symbol,Any}, ::IdDict{Any,Any}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/read.jl:88\n [15] (::BSON.var\"#21#22\"{IdDict{Any,Any}})(::Dict{Symbol,Any}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/read.jl:94\n [16] applychildren!(::BSON.var\"#21#22\"{IdDict{Any,Any}}, ::Array{Any,1}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/BSON.jl:28\n [17] raise_recursive at /Users/luboshanus/.julia/packages/BSON/XAts7/src/read.jl:94 [inlined]\n [18] (::BSON.var\"#39#40\"{IdDict{Any,Any}})(::Array{Any,1}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/extensions.jl:124\n [19] iterate at ./generator.jl:47 [inlined]\n [20] collect_to!(::Array{Array{Any,1},1}, ::Base.Generator{Array{Any,1},BSON.var\"#39#40\"{IdDict{Any,Any}}}, ::Int64, ::Int64) at ./array.jl:732\n [21] collect_to_with_first!(::Array{Array{Any,1},1}, ::Array{Any,1}, ::Base.Generator{Array{Any,1},BSON.var\"#39#40\"{IdDict{Any,Any}}}, ::Int64) at ./array.jl:710\n [22] _collect(::Array{Any,1}, ::Base.Generator{Array{Any,1},BSON.var\"#39#40\"{IdDict{Any,Any}}}, ::Base.EltypeUnknown, ::Base.HasShape{1}) at ./array.jl:704\n [23] collect_similar(::Array{Any,1}, ::Base.Generator{Array{Any,1},BSON.var\"#39#40\"{IdDict{Any,Any}}}) at ./array.jl:628\n [24] map(::Function, ::Array{Any,1}) at ./abstractarray.jl:2162\n [25] newstruct_raw(::IdDict{Any,Any}, ::Type{T} where T, ::Dict{Symbol,Any}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/extensions.jl:124\n [26] (::BSON.var\"#43#44\")(::Dict{Symbol,Any}, ::IdDict{Any,Any}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/extensions.jl:140\n [27] raise_recursive(::Dict{Symbol,Any}, ::IdDict{Any,Any}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/read.jl:88\n [28] raise_recursive at /Users/luboshanus/.julia/packages/BSON/XAts7/src/read.jl:99 [inlined]\n [29] load(::String) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/read.jl:104\n [30] top-level scope at REPL[20]:1```","user":"UP345RMJR","ts":"1610360933.246600","team":"T68168MUP","edited":{"user":"UP345RMJR","ts":"1610360997.000000"},"blocks":[{"type":"rich_text","block_id":"PifXU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I have changed activation functions in my models from "},{"type":"text","text":"relu","style":{"code":true}},{"type":"text","text":" to "},{"type":"text","text":"sigmoid","style":{"code":true}},{"type":"text","text":" and I save them into "},{"type":"text","text":".bson","style":{"code":true}},{"type":"text","text":" which used to work with "},{"type":"text","text":"relu","style":{"code":true}},{"type":"text","text":" . Now, I am getting "},{"type":"text","text":"typeof","style":{"code":true}},{"type":"text","text":" error with sigmoid. Do not understand why. Thanks for any help. Error:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> res = BSON.load(file_with_dictionary_results)\nERROR: MethodError: no method matching typeof(σ)()\nStacktrace:\n [1] (::BSON.var\"#37#38\")(::Dict{Symbol,Any}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/extensions.jl:76\n [2] _raise_recursive(::Dict{Symbol,Any}, ::IdDict{Any,Any}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/read.jl:79\n [3] raise_recursive(::Dict{Symbol,Any}, ::IdDict{Any,Any}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/read.jl:89\n [4] (::BSON.var\"#39#40\"{IdDict{Any,Any}})(::Dict{Symbol,Any}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/extensions.jl:124\n [5] iterate at ./generator.jl:47 [inlined]\n [6] collect_to!(::Array{Any,1}, ::Base.Generator{Array{Any,1},BSON.var\"#39#40\"{IdDict{Any,Any}}}, ::Int64, ::Int64) at ./array.jl:732\n [7] collect_to!(::Array{Real,1}, ::Base.Generator{Array{Any,1},BSON.var\"#39#40\"{IdDict{Any,Any}}}, ::Int64, ::Int64) at ./array.jl:740 (repeats 3 times)\n [8] collect_to_with_first!(::Array{Int64,1}, ::Int64, ::Base.Generator{Array{Any,1},BSON.var\"#39#40\"{IdDict{Any,Any}}}, ::Int64) at ./array.jl:710\n [9] _collect(::Array{Any,1}, ::Base.Generator{Array{Any,1},BSON.var\"#39#40\"{IdDict{Any,Any}}}, ::Base.EltypeUnknown, ::Base.HasShape{1}) at ./array.jl:704\n [10] collect_similar(::Array{Any,1}, ::Base.Generator{Array{Any,1},BSON.var\"#39#40\"{IdDict{Any,Any}}}) at ./array.jl:628\n [11] map(::Function, ::Array{Any,1}) at ./abstractarray.jl:2162\n [12] newstruct_raw(::IdDict{Any,Any}, ::Type{T} where T, ::Dict{Symbol,Any}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/extensions.jl:124\n [13] (::BSON.var\"#43#44\")(::Dict{Symbol,Any}, ::IdDict{Any,Any}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/extensions.jl:140\n [14] raise_recursive(::Dict{Symbol,Any}, ::IdDict{Any,Any}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/read.jl:88\n [15] (::BSON.var\"#21#22\"{IdDict{Any,Any}})(::Dict{Symbol,Any}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/read.jl:94\n [16] applychildren!(::BSON.var\"#21#22\"{IdDict{Any,Any}}, ::Array{Any,1}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/BSON.jl:28\n [17] raise_recursive at /Users/luboshanus/.julia/packages/BSON/XAts7/src/read.jl:94 [inlined]\n [18] (::BSON.var\"#39#40\"{IdDict{Any,Any}})(::Array{Any,1}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/extensions.jl:124\n [19] iterate at ./generator.jl:47 [inlined]\n [20] collect_to!(::Array{Array{Any,1},1}, ::Base.Generator{Array{Any,1},BSON.var\"#39#40\"{IdDict{Any,Any}}}, ::Int64, ::Int64) at ./array.jl:732\n [21] collect_to_with_first!(::Array{Array{Any,1},1}, ::Array{Any,1}, ::Base.Generator{Array{Any,1},BSON.var\"#39#40\"{IdDict{Any,Any}}}, ::Int64) at ./array.jl:710\n [22] _collect(::Array{Any,1}, ::Base.Generator{Array{Any,1},BSON.var\"#39#40\"{IdDict{Any,Any}}}, ::Base.EltypeUnknown, ::Base.HasShape{1}) at ./array.jl:704\n [23] collect_similar(::Array{Any,1}, ::Base.Generator{Array{Any,1},BSON.var\"#39#40\"{IdDict{Any,Any}}}) at ./array.jl:628\n [24] map(::Function, ::Array{Any,1}) at ./abstractarray.jl:2162\n [25] newstruct_raw(::IdDict{Any,Any}, ::Type{T} where T, ::Dict{Symbol,Any}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/extensions.jl:124\n [26] (::BSON.var\"#43#44\")(::Dict{Symbol,Any}, ::IdDict{Any,Any}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/extensions.jl:140\n [27] raise_recursive(::Dict{Symbol,Any}, ::IdDict{Any,Any}) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/read.jl:88\n [28] raise_recursive at /Users/luboshanus/.julia/packages/BSON/XAts7/src/read.jl:99 [inlined]\n [29] load(::String) at /Users/luboshanus/.julia/packages/BSON/XAts7/src/read.jl:104\n [30] top-level scope at REPL[20]:1"}]}]}]},{"client_msg_id":"61cc67ba-92cb-4482-bebc-1821952feec4","type":"message","text":"<https://github.com/sdobber/FluxArchitectures/issues/15>","user":"UDGT4PM41","ts":"1611078237.006300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0MSe","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/sdobber/FluxArchitectures/issues/15"}]}]}]},{"client_msg_id":"fce1a509-74dd-4a4a-966f-77e9ada5a3f7","type":"message","text":"Can I print the current learning rate when using learning rate decay?","user":"U014K4SE396","ts":"1611161820.007200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OOk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can I print the current learning rate when using learning rate decay?"}]}]}],"thread_ts":"1611161820.007200","reply_count":1,"reply_users_count":1,"latest_reply":"1611163357.010100","reply_users":["UH9KWTTD3"],"subscribed":false},{"client_msg_id":"77fae6f8-0842-414f-8350-fa7e4f1ad056","type":"message","text":"Thoughts on <https://github.com/FluxML/Flux.jl/pull/1471|https://github.com/FluxML/Flux.jl/pull/1471>?","user":"UC4QQPG4A","ts":"1611162745.007500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0KPe8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thoughts on "},{"type":"link","url":"https://github.com/FluxML/Flux.jl/pull/1471","text":"https://github.com/FluxML/Flux.jl/pull/1471"},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"28a4ca71-529f-47d9-9a88-d35823a67295","type":"message","text":"With that PR you could define a prehook to print it pretty trivially","user":"UC4QQPG4A","ts":"1611162771.008400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sqwG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"With that PR you could define a prehook to print it pretty trivially"}]}]}]},{"client_msg_id":"0eb9bdf5-3a3d-48fa-be7e-6422ff2cc72e","type":"message","text":"Oh you could also just define a callback to accept the optimiser and print the learning rate like that","user":"UC4QQPG4A","ts":"1611162815.009500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8jSf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oh you could also just define a callback to accept the optimiser and print the learning rate like that"}]}]}],"reactions":[{"name":"+1","users":["U014K4SE396"],"count":1}]},{"client_msg_id":"078a6475-fe20-4bc0-b5ee-69ad01a4d215","type":"message","text":"Thank you and looking forward to that","user":"U014K4SE396","ts":"1611163073.010000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Z1=R","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thank you and looking forward to that"}]}]}]},{"client_msg_id":"f18e0e93-36e8-4265-8b2a-871db69af432","type":"message","text":"Hi! New to Flux/Zygote from Pytorch, what are the best ways to make your functions play nice with Float32?","user":"UM9Q1BM9Q","ts":"1611451037.002200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Zd3a","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi! New to Flux/Zygote from Pytorch, what are the best ways to make your functions play nice with Float32?"}]}]}]},{"client_msg_id":"d0b154b4-dd04-44e3-893a-86b49d2fa174","type":"message","text":"To elaborate: let's say i have a function `f(x, arg1) = x - arg1`. If x is Float32, Julia's behavior is to treat arg1 as a Float64, and the function will return a Float64","user":"UM9Q1BM9Q","ts":"1611451136.003800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"diFR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"To elaborate: let's say i have a function "},{"type":"text","text":"f(x, arg1) = x - arg1","style":{"code":true}},{"type":"text","text":". If x is Float32, Julia's behavior is to treat arg1 as a Float64, and the function will return a Float64"}]}]}],"thread_ts":"1611451136.003800","reply_count":2,"reply_users_count":1,"latest_reply":"1611451815.008400","reply_users":["UH9KWTTD3"],"subscribed":false},{"client_msg_id":"a07c6a0f-7d19-4b88-8636-b03af5d2b741","type":"message","text":"and this, logically, makes Julia complain about type mismatches and/or spends a lot of resources on type inference","user":"UM9Q1BM9Q","ts":"1611451198.004600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=qs8n","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"and this, logically, makes Julia complain about type mismatches and/or spends a lot of resources on type inference"}]}]}]},{"client_msg_id":"43e36bcb-6b8d-42b1-ad7d-63bc43217d39","type":"message","text":"i'm sure there's a really simple way to do this, i just can't find it online","user":"UM9Q1BM9Q","ts":"1611451262.005600","team":"T68168MUP","edited":{"user":"UM9Q1BM9Q","ts":"1611451309.000000"},"blocks":[{"type":"rich_text","block_id":"KzO9u","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"i'm sure there's a really simple way to do this, i just can't find it online"}]}]}]},{"client_msg_id":"f17c4667-c8c9-4f70-bba9-81cc41cb5eb1","type":"message","text":"`f` should only promote to Float64 if arg is already. Viz.\n```julia&gt; f(1.0, 2f0) |&gt; typeof\nFloat64\n\njulia&gt; f(1f0, 2f0) |&gt; typeof\nFloat32```\nWithout more context, I assume you are seeing that floating point constants are 64-bit by default in Julia. That's easily remedied by appending an `f0` though.","user":"UMY1LV01G","ts":"1611451673.007700","team":"T68168MUP","edited":{"user":"UMY1LV01G","ts":"1611451684.000000"},"blocks":[{"type":"rich_text","block_id":"k9n+r","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"f","style":{"code":true}},{"type":"text","text":" should only promote to Float64 if arg is already. Viz.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> f(1.0, 2f0) |> typeof\nFloat64\n\njulia> f(1f0, 2f0) |> typeof\nFloat32"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Without more context, I assume you are seeing that floating point constants are 64-bit by default in Julia. That's easily remedied by appending an "},{"type":"text","text":"f0","style":{"code":true}},{"type":"text","text":" though."}]}]}]},{"client_msg_id":"0c50cadb-5fe5-42cf-a6cb-0769d4632563","type":"message","text":"thanks for the replies! so this requires adding `f0` or using something like `f(x::T, arg1) = x - T(arg1)` like <@UH9KWTTD3> said","user":"UM9Q1BM9Q","ts":"1611451935.010300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WJUR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"thanks for the replies! so this requires adding "},{"type":"text","text":"f0","style":{"code":true}},{"type":"text","text":" or using something like "},{"type":"text","text":"f(x::T, arg1) = x - T(arg1)","style":{"code":true}},{"type":"text","text":" like "},{"type":"user","user_id":"UH9KWTTD3"},{"type":"text","text":" said"}]}]}]},{"client_msg_id":"00b52b3c-684d-4b3b-9a99-fd9afd52882a","type":"message","text":"so if i want to distribute my model, training etc over several functions with numerical arguments, is there a more \"scalable\" way to do this? like a nice design pattern?","user":"UM9Q1BM9Q","ts":"1611451978.011700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9ok","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"so if i want to distribute my model, training etc over several functions with numerical arguments, is there a more \"scalable\" way to do this? like a nice design pattern?"}]}]}]},{"client_msg_id":"4edc9636-6588-4c5f-8c51-db65d3684a71","type":"message","text":"I think you'll have to provide an example we can actually dig into, because f should never return Float64 if you're only passing it Float32s.","user":"UMY1LV01G","ts":"1611452004.012200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lbE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think you'll have to provide an example we can actually dig into, because f should never return Float64 if you're only passing it Float32s."}]}]}]},{"client_msg_id":"7f5b1f91-b218-47b7-aa42-5d7c48b7d59c","type":"message","text":"sure, here's the function that was causing me grief: `function soft_threshold(x, λ)`\n    `relu(x - λ) - relu(-x - λ)`\n`end`","user":"UM9Q1BM9Q","ts":"1611452055.013200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"f6Z+G","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"sure, here's the function that was causing me grief: "},{"type":"text","text":"function soft_threshold(x, λ)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    relu(x - λ) - relu(-x - λ)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"end","style":{"code":true}}]}]}]},{"client_msg_id":"2b917b55-b1cf-42b5-8c96-61d4592c0a4a","type":"message","text":"with `x::Array{Float32,N}, λ=some_float`","user":"UM9Q1BM9Q","ts":"1611452111.014000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"aoYu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"with "},{"type":"text","text":"x::Array{Float32,N}, λ=some_float","style":{"code":true}}]}]}]},{"client_msg_id":"2974DDCF-9855-45B8-B037-41D3935C90A3","type":"message","text":"So lambda needs to specifically be Float32","user":"UH9KWTTD3","ts":"1611452133.014500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GI1M","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So lambda needs to specifically be Float32"}]}]}]},{"client_msg_id":"1f364db1-ad45-4a8b-9db9-a95b628fa08a","type":"message","text":"And how/where is relu defined?","user":"UMY1LV01G","ts":"1611452147.015100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vg7EU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And how/where is relu defined?"}]}]}]},{"client_msg_id":"EF027CB9-E7E8-4A6D-95C7-2E300DF80DBB","type":"message","text":"The right approach would be to use `f0` like Brian suggested","user":"UH9KWTTD3","ts":"1611452169.015900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ZtG57","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The right approach would be to use "},{"type":"text","text":"f0","style":{"code":true}},{"type":"text","text":" like Brian suggested"}]}]}],"reactions":[{"name":"+1","users":["UM9Q1BM9Q"],"count":1}]},{"client_msg_id":"9732b350-e16f-45ef-9001-699039017ce8","type":"message","text":"good point. this is the relu from Flux. `relu(x) = max(zero(x), x)`","user":"UM9Q1BM9Q","ts":"1611452190.016300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VPg4d","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"good point. this is the relu from Flux. "},{"type":"text","text":"relu(x) = max(zero(x), x)","style":{"code":true}}]}]}]},{"client_msg_id":"8d383806-dfa7-40be-beb8-c08d5efb5257","type":"message","text":"but let's say i have a bunch of functions that have float arguments","user":"UM9Q1BM9Q","ts":"1611452221.017200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WP6j","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"but let's say i have a bunch of functions that have float arguments"}]}]}]},{"client_msg_id":"f121427d-eb34-4ba4-a69e-8e156399ff66","type":"message","text":"is the best way to do this to explicitly put every float argument in with an `f0`?","user":"UM9Q1BM9Q","ts":"1611452284.018200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0wfM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is the best way to do this to explicitly put every float argument in with an "},{"type":"text","text":"f0","style":{"code":true}},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"dac259d3-e694-49c4-88bf-6ccaed402180","type":"message","text":"This only applies to constant floats, which I imagine you'll have very few of","user":"UMY1LV01G","ts":"1611452324.018600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0uFbh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This only applies to constant floats, which I imagine you'll have very few of"}]}]}]},{"client_msg_id":"da0d8819-a9a6-4a14-8932-8b29cf1c330a","type":"message","text":"fair","user":"UM9Q1BM9Q","ts":"1611452357.019500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Yat/l","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"fair"}]}]}]},{"client_msg_id":"d8778982-4dac-4281-9423-d6b207328e64","type":"message","text":"Flux already inits model params to float32 by default","user":"UMY1LV01G","ts":"1611452359.019800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LaCJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Flux already inits model params to float32 by default"}]}]}]},{"client_msg_id":"7b016954-8982-46f5-bbd1-c941424de176","type":"message","text":"ok cool, this should make my life easier","user":"UM9Q1BM9Q","ts":"1611452418.021200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"52qm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ok cool, this should make my life easier"}]}]}]},{"client_msg_id":"bd4e3f18-32db-4f63-b5b7-8b0d8a023676","type":"message","text":"thanks!","user":"UM9Q1BM9Q","ts":"1611452421.021400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JGH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"thanks!"}]}]}]},{"client_msg_id":"226DE114-C9FB-4164-B6FD-989545B96587","type":"message","text":"Or when you create an array like `x = rand(Float32, 3, 4)`. Basically, only when you create and allocate variables should you need to explicitly say “I want a Float32.” The rest should all be handled via Julia’s <https://docs.julialang.org/en/v1/manual/conversion-and-promotion/#Promotion|promotion system>.","user":"UH9KWTTD3","ts":"1611452547.023300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vv8K1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Or when you create an array like "},{"type":"text","text":"x = rand(Float32, 3, 4)","style":{"code":true}},{"type":"text","text":". Basically, only when you create and allocate variables should you need to explicitly say “I want a Float32.” The rest should all be handled via Julia’s "},{"type":"link","url":"https://docs.julialang.org/en/v1/manual/conversion-and-promotion/#Promotion","text":"promotion system"},{"type":"text","text":"."}]}]}],"reactions":[{"name":"+1","users":["UM9Q1BM9Q"],"count":1}]},{"client_msg_id":"c8a62483-1a69-4b1b-9cf8-ec57280bf0c4","type":"message","text":"hi folks, a quick question out of curiosity: what kinds of compiler optimizations happen in flux in regards to the networks that get built? is there any tensor graph optimization happening? we are building a particularly unique general purpose customizable optimization package for Julia and the techniques we implemented fit really well for optimizing signal flow graphs. we are curious about what's already happening in Flux and we'd like to try applying those techniques to flux nets soon\\ :rocket:","user":"U01K2JB9GPJ","ts":"1611747369.027200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"997","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hi folks, a quick question out of curiosity: what kinds of compiler optimizations happen in flux in regards to the networks that get built? is there any tensor graph optimization happening? we are building a particularly unique general purpose customizable optimization package for Julia and the techniques we implemented fit really well for optimizing signal flow graphs. we are curious about what's already happening in Flux and we'd like to try applying those techniques to flux nets soon\\ "},{"type":"emoji","name":"rocket"}]}]}],"thread_ts":"1611747369.027200","reply_count":2,"reply_users_count":2,"latest_reply":"1611765648.027600","reply_users":["UC4QQPG4A","UDGT4PM41"],"subscribed":false},{"client_msg_id":"5b63a507-7dd8-413d-9551-f3bcb9046518","type":"message","text":"Is it possible to change the AD backend in Flux?","user":"URLJH245D","ts":"1611790245.028000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hL2i","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is it possible to change the AD backend in Flux?"}]}]}]},{"type":"message","subtype":"bot_message","text":"`function g(x)     if x &lt; 0          print(\"Enter function name: \")         getfield(Base, Symbol(readline()))(x)     else         2*x^3 + 4*x^2 +5*x     end end`  `julia&gt; g'(-pi/6) Enter function name: sin ERROR: Can't differentiate foreigncall expression ` g'(2) works fine but the above case fails, why?","ts":"1611804254.028400","username":"[gitter] <adhikarirsr>","bot_id":"B795XHD0X","blocks":[{"type":"section","block_id":"matterbridge_c05cuukg27ibs0bi3tjg","text":{"type":"mrkdwn","text":"`function g(x)\n    if x &lt; 0 \n        print(\"Enter function name: \")\n        getfield(Base, Symbol(readline()))(x)\n    else\n        2*x^3 + 4*x^2 +5*x\n    end\nend`\n\n`julia&gt; g'(-pi/6)\nEnter function name: sin\nERROR: Can't differentiate foreigncall expression\n`\ng'(2) works fine but the above case fails, why?","verbatim":false}}]},{"type":"message","subtype":"bot_message","text":"&gt; &gt;```function g(x) &gt;     if x &lt; 0  &gt;         print(\"Enter function name: \") &gt;         getfield(Base, Symbol(readline()))(x) &gt;     else &gt;         2*x^3 + 4*x^2 +5*x &gt;     end &gt; end``` &gt;  &gt; `julia&gt; g'(-pi/6) &gt; Enter function name: sin &gt; ERROR: Can't differentiate foreigncall expression &gt; ` &gt; g'(2) works fine but the above case fails, why?  ","ts":"1611804398.028500","username":"[gitter] <adhikarirsr>","bot_id":"B795XHD0X","blocks":[{"type":"section","block_id":"matterbridge_c05cuukg27ibs0bi3tjg","text":{"type":"mrkdwn","text":"&gt; &gt;```function g(x)\n&gt;     if x &lt; 0 \n&gt;         print(\"Enter function name: \")\n&gt;         getfield(Base, Symbol(readline()))(x)\n&gt;     else\n&gt;         2*x^3 + 4*x^2 +5*x\n&gt;     end\n&gt; end```\n&gt; \n&gt; `julia&gt; g'(-pi/6)\n&gt; Enter function name: sin\n&gt; ERROR: Can't differentiate foreigncall expression\n&gt; `\n&gt; g'(2) works fine but the above case fails, why?\n\n","verbatim":false}}]},{"type":"message","subtype":"bot_message","text":"`using Flux using Zygote: forwarddiff using Trebuchet  function shoot(wind, angle, weight)   Trebuchet.shoot((wind, Trebuchet.deg2rad(angle), weight))[2] end `  `julia&gt; shoot'(5,50,220) ERROR: MethodError: no method matching (::Zygote.var\"#43#44\"{typeof(shoot)})(::Int64, ::Int64, ::Int64)`  This  also fails","ts":"1611806377.028600","username":"[gitter] <adhikarirsr>","bot_id":"B795XHD0X","blocks":[{"type":"section","block_id":"matterbridge_c05cuukg27ibs0bi3tjg","text":{"type":"mrkdwn","text":"`using Flux\nusing Zygote: forwarddiff\nusing Trebuchet\n\nfunction shoot(wind, angle, weight)\n  Trebuchet.shoot((wind, Trebuchet.deg2rad(angle), weight))[2]\nend\n`\n\n`julia&gt; shoot'(5,50,220)\nERROR: MethodError: no method matching (::Zygote.var\"#43#44\"{typeof(shoot)})(::Int64, ::Int64, ::Int64)`\n\nThis  also fails","verbatim":false}}]},{"type":"message","subtype":"bot_message","text":"here's the stacktrace for g(x):  `julia&gt; g'(-pi/6)  Enter function name: sin ERROR: Can't differentiate foreigncall expression Stacktrace:  [1] error(::String) at ./error.jl:33  [2] Symbol at ./boot.jl:438 [inlined]  [3] (::typeof(∂(Symbol)))(::Nothing) at /home/user/.julia/packages/Zygote/ggM8Z/src/compiler/interface2.jl:0  [4] g at /home/user/julia_control/cm_control.jl:17 [inlined]  [5] (::typeof(∂(g)))(::Float64) at /home/user/.julia/packages/Zygote/ggM8Z/src/compiler/interface2.jl:0  [6] (::Zygote.var\"#41#42\"{typeof(∂(g))})(::Float64) at /home/user/.julia/packages/Zygote/ggM8Z/src/compiler/interface.jl:40  [7] gradient(::Function, ::Float64) at /home/user/.julia/packages/Zygote/ggM8Z/src/compiler/interface.jl:49  [8] (::Zygote.var\"#43#44\"{typeof(g)})(::Float64) at /home/user/.julia/packages/Zygote/ggM8Z/src/compiler/interface.jl:52  [9] top-level scope at none:1`  ","ts":"1611821151.029000","username":"[gitter] <adhikarirsr>","bot_id":"B795XHD0X","blocks":[{"type":"section","block_id":"matterbridge_c05cuukg27ibs0bi3tjg","text":{"type":"mrkdwn","text":"here's the stacktrace for g(x):\n\n`julia&gt; g'(-pi/6) \nEnter function name: sin\nERROR: Can't differentiate foreigncall expression\nStacktrace:\n [1] error(::String) at ./error.jl:33\n [2] Symbol at ./boot.jl:438 [inlined]\n [3] (::typeof(∂(Symbol)))(::Nothing) at /home/user/.julia/packages/Zygote/ggM8Z/src/compiler/interface2.jl:0\n [4] g at /home/user/julia_control/cm_control.jl:17 [inlined]\n [5] (::typeof(∂(g)))(::Float64) at /home/user/.julia/packages/Zygote/ggM8Z/src/compiler/interface2.jl:0\n [6] (::Zygote.var\"#41#42\"{typeof(∂(g))})(::Float64) at /home/user/.julia/packages/Zygote/ggM8Z/src/compiler/interface.jl:40\n [7] gradient(::Function, ::Float64) at /home/user/.julia/packages/Zygote/ggM8Z/src/compiler/interface.jl:49\n [8] (::Zygote.var\"#43#44\"{typeof(g)})(::Float64) at /home/user/.julia/packages/Zygote/ggM8Z/src/compiler/interface.jl:52\n [9] top-level scope at none:1`\n\n","verbatim":false}}]},{"type":"message","subtype":"bot_message","text":"Here's the one for Trebuchet: `julia&gt; shoot'(5,55,200) ERROR: MethodError: no method matching (::Zygote.var\"#43#44\"{typeof(shoot)})(::Int64, ::Int64, ::Int64) Closest candidates are:   #43(::Any) at /home/user/.julia/packages/Zygote/ggM8Z/src/compiler/interface.jl:52 Stacktrace:  [1] top-level scope at none:1`","ts":"1611821546.029100","username":"[gitter] <adhikarirsr>","bot_id":"B795XHD0X","blocks":[{"type":"section","block_id":"matterbridge_c05cuukg27ibs0bi3tjg","text":{"type":"mrkdwn","text":"Here's the one for Trebuchet:\n`julia&gt; shoot'(5,55,200)\nERROR: MethodError: no method matching (::Zygote.var\"#43#44\"{typeof(shoot)})(::Int64, ::Int64, ::Int64)\nClosest candidates are:\n  #43(::Any) at /home/user/.julia/packages/Zygote/ggM8Z/src/compiler/interface.jl:52\nStacktrace:\n [1] top-level scope at none:1`","verbatim":false}}]},{"type":"message","subtype":"bot_message","text":"thanks @Dhairya what is `@nograd Symbol` and what does it do? I am new to `zygote`. How should I fix trebuchet?","ts":"1611861292.030600","username":"[gitter] <adhikarirsr>","bot_id":"B795XHD0X","blocks":[{"type":"section","block_id":"matterbridge_c05cuukg27ibs0bi3tjg","text":{"type":"mrkdwn","text":"thanks @Dhairya what is `@nograd Symbol` and what does it do? I am new to `zygote`. How should I fix trebuchet?","verbatim":false}}]},{"client_msg_id":"6124ec5c-9cab-49b9-8c84-f427828fbeb1","type":"message","text":"I’m trying to extend Flux’s `Recur` to return both the cell and hidden state. When I do that, training on the GPU breaks, whereas it works fine on the CPU.  Does anyone have an idea why?\nMWE is the following, and stacktrace is in the thread:\n```using Flux\n\nmutable struct FullRecur{T}\n    cell::T\n    init\n    state\nend\nFullRecur(m, h = Flux.hidden(m)) = FullRecur(m, h, h)\n\nfunction (m::FullRecur)(xs...)\n    h, y = m.cell(m.state, xs...)\n    m.state = h\n    return h   # &lt;--- return value changed from `Recur`\nend\n\nFlux.@functor FullRecur cell, init\n\nlstm = FullRecur(Flux.LSTMCell(10,5)) |&gt; gpu\n\nx = rand(Float32, 10,30) |&gt; gpu\ny = rand(Float32, 10,30) |&gt; gpu\n\nm = (x) -&gt; cat(lstm(x)..., dims=1) |&gt; gpu\nloss(x,y) = Flux.mse(m(x),y)\n\nFlux.train!(loss, Flux.params(lstm), [(x,y)], ADAM())\n# ERROR: MethodError: no method matching size(::Nothing, ::Int64)```\n","user":"UNZKG0909","ts":"1611929867.031200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"np9Qm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’m trying to extend Flux’s "},{"type":"text","text":"Recur","style":{"code":true}},{"type":"text","text":" to return both the cell and hidden state. When I do that, training on the GPU breaks, whereas it works fine on the CPU.  Does anyone have an idea why?\nMWE is the following, and stacktrace is in the thread:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Flux\n\nmutable struct FullRecur{T}\n    cell::T\n    init\n    state\nend\nFullRecur(m, h = Flux.hidden(m)) = FullRecur(m, h, h)\n\nfunction (m::FullRecur)(xs...)\n    h, y = m.cell(m.state, xs...)\n    m.state = h\n    return h   # <--- return value changed from `Recur`\nend\n\nFlux.@functor FullRecur cell, init\n\nlstm = FullRecur(Flux.LSTMCell(10,5)) |> gpu\n\nx = rand(Float32, 10,30) |> gpu\ny = rand(Float32, 10,30) |> gpu\n\nm = (x) -> cat(lstm(x)..., dims=1) |> gpu\nloss(x,y) = Flux.mse(m(x),y)\n\nFlux.train!(loss, Flux.params(lstm), [(x,y)], ADAM())\n# ERROR: MethodError: no method matching size(::Nothing, ::Int64)"}]},{"type":"rich_text_section","elements":[]}]}],"thread_ts":"1611929867.031200","reply_count":5,"reply_users_count":2,"latest_reply":"1611947066.032400","reply_users":["UNZKG0909","UC4QQPG4A"],"subscribed":false},{"client_msg_id":"64a18a96-b76c-4785-a176-d9c5d76bbdb2","type":"message","text":"<https://discourse.julialang.org/t/1d-gan-with-flux/54179|https://discourse.julialang.org/t/1d-gan-with-flux/54179>","user":"UDGT4PM41","ts":"1611937465.031600","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"1D GAN with Flux","title_link":"https://discourse.julialang.org/t/1d-gan-with-flux/54179","text":"Hey everyone. So I’ve been trying to use Flux for Generative Adversarial Networks (GAN). And unfortunately, it’s not working properly. At the moment, I’m trying to replicate a 1D GAN example (this one here 1D GAN with Keras). But the results are just not good. First, the discriminator takes a lot to properly learn to discern the data, and the generator is even worse. I was wondering if anyone here has implemented GANs on Flux, and perhaps can show how to implement this 1D example. I’ve based my...","fallback":"JuliaLang: 1D GAN with Flux","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","ts":1611924742,"from_url":"https://discourse.julialang.org/t/1d-gan-with-flux/54179","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/1d-gan-with-flux/54179"}],"blocks":[{"type":"rich_text","block_id":"ktd","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://discourse.julialang.org/t/1d-gan-with-flux/54179","text":"https://discourse.julialang.org/t/1d-gan-with-flux/54179"}]}]}]},{"client_msg_id":"5a1faf5f-f687-4c34-8aaa-34678af50402","type":"message","text":"I'm super new to Flux so probably a very basic question but how do I get Flux to recognize the parameters of my custom layer? Based on docs I tried this but doesn't seem to work:\n```struct LinearCombo{T}\n    c :: T\nend\n(lc::LinearCombo)(x, y) = @. x + lc.c * y\nFlux.@functor LinearCombo\nparams(LinearCombo(1)) # gives Params([])```","user":"UUMJUCYRK","ts":"1612051204.034500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YXXfN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm super new to Flux so probably a very basic question but how do I get Flux to recognize the parameters of my custom layer? Based on docs I tried this but doesn't seem to work:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"struct LinearCombo{T}\n    c :: T\nend\n(lc::LinearCombo)(x, y) = @. x + lc.c * y\nFlux.@functor LinearCombo\nparams(LinearCombo(1)) # gives Params([])"}]}]}],"thread_ts":"1612051204.034500","reply_count":4,"reply_users_count":3,"latest_reply":"1612080823.035300","reply_users":["UMY1LV01G","UUMJUCYRK","UNG2XJJP3"],"subscribed":false},{"client_msg_id":"39ca300e-7dcb-4812-97ab-93c5b75ec6f1","type":"message","text":"<https://julialang.slack.com/archives/C7120PCUQ/p1612217592004700>","user":"UM70NJEER","ts":"1612246737.036100","team":"T68168MUP","attachments":[{"from_url":"https://julialang.slack.com/archives/C7120PCUQ/p1612217592004700","fallback":"[February 1st, 2021 2:13 PM] lazarus.alon: Hello, it seems that the new LSTM, doesn't allow to put the activation function within the call, as the source code says :`LSTM(in::Integer, out::Integer, σ = tanh),`  any idea how could I achieve the same result. `Chain(LSTM(3,10), Flux.relu, Dense(10,1))` gives problems also because of broadcasting.","ts":"1612217592.004700","author_id":"UM70NJEER","author_subname":"Lazaro","channel_id":"C7120PCUQ","channel_name":"flux","is_msg_unfurl":true,"text":"Hello, it seems that the new LSTM, doesn't allow to put the activation function within the call, as the source code says :`LSTM(in::Integer, out::Integer, σ = tanh),`  any idea how could I achieve the same result. `Chain(LSTM(3,10), Flux.relu, Dense(10,1))` gives problems also because of broadcasting.","author_name":"Lazaro","author_link":"https://julialang.slack.com/team/UM70NJEER","author_icon":"https://avatars.slack-edge.com/2019-10-29/801496800483_d1d644dfbd61c7e4a7f8_48.png","mrkdwn_in":["text"],"id":1,"original_url":"https://julialang.slack.com/archives/C7120PCUQ/p1612217592004700","footer":"Posted in #flux"}],"blocks":[{"type":"rich_text","block_id":"wES","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://julialang.slack.com/archives/C7120PCUQ/p1612217592004700"}]}]}],"thread_ts":"1612246737.036100","reply_count":14,"reply_users_count":2,"latest_reply":"1612370238.050100","reply_users":["UMY1LV01G","UM70NJEER"],"subscribed":false},{"client_msg_id":"f76ebaa6-e01c-47ff-977a-2c29611fb1f3","type":"message","text":"I am training via Flux right now and it seems to be consuming up to JULIA_NUM_THREADS CPUs. Rudimentary GitHub searching for spawn and Threads is not getting me anywhere.\n\nWhich part of the stack is causing this? Or, alternatively, is none of it causing it?","user":"USDM93QF8","ts":"1612317735.042200","team":"T68168MUP","edited":{"user":"USDM93QF8","ts":"1612319100.000000"},"blocks":[{"type":"rich_text","block_id":"sB4nJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am training via Flux right now and it seems to be consuming up to JULIA_NUM_THREADS CPUs. Rudimentary GitHub searching for spawn and Threads is not getting me anywhere.\n\nWhich part of the stack is causing this? Or, alternatively, is none of it causing it?"}]}]}]},{"client_msg_id":"9b4d8561-70eb-4f60-a5d4-7624774ac572","type":"message","text":"Blas is","user":"U6N6VQE30","ts":"1612338737.044900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"otZlM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Blas is"}]}]}],"reactions":[{"name":"point_up","users":["UMY1LV01G","USDM93QF8"],"count":2}]},{"client_msg_id":"ca946b7b-04cc-4379-bb87-54c96cb432a5","type":"message","text":"We respect the usual settings of a Julia instance. Blas would already multithread some stuff, but tweaking the environment variable would allow more threads to be used for other tasks etc.","user":"UC4QQPG4A","ts":"1612338772.045900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8xjXI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We respect the usual settings of a Julia instance. Blas would already multithread some stuff, but tweaking the environment variable would allow more threads to be used for other tasks etc."}]}]}]},{"client_msg_id":"bf2fa43b-a348-40b4-b30a-0332f0b6d9c0","type":"message","text":"Got it, thanks. I had been training multiple models in parallel, which was using all my allocated threads (expected) but then I stopped training in parallel and was still using all threads (what?). This explains it and lets me reduce peak RAM demand while not taking too much of a latency penalty","user":"USDM93QF8","ts":"1612368068.048400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YMucv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Got it, thanks. I had been training multiple models in parallel, which was using all my allocated threads (expected) but then I stopped training in parallel and was still using all threads (what?). This explains it and lets me reduce peak RAM demand while not taking too much of a latency penalty"}]}]}]},{"client_msg_id":"0e45b01e-c636-427e-a4c9-7cdb53a25d11","type":"message","text":"Yeah, at work we also stumbled upon that.\nIf you want to make lots of things in parallel by yourself and not let blas take many threads, you can do.\n```BLAS.set_num_threads(1)```\nin fact, we have\n```function __init__()\n  if Threads.nthreads() &gt; 1\n    @warn \"Detected JULIA_NUM_THREADS &gt; 1. Setting BLAS threads to 1.\"\n    BLAS.set_num_threads(1)\n  end\nend```\nin one of our internal libraries :smile:","user":"USBKT1275","ts":"1612370033.050000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"C7GF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah, at work we also stumbled upon that.\nIf you want to make lots of things in parallel by yourself and not let blas take many threads, you can do.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"BLAS.set_num_threads(1)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"in fact, we have\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function __init__()\n  if Threads.nthreads() > 1\n    @warn \"Detected JULIA_NUM_THREADS > 1. Setting BLAS threads to 1.\"\n    BLAS.set_num_threads(1)\n  end\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"in one of our internal libraries "},{"type":"emoji","name":"smile"}]}]}]},{"client_msg_id":"1fd94bb6-9cfa-431b-9cce-74185dae85a2","type":"message","text":"Is it kind of like a slap on the wrist? :joy: :joy:","user":"UC4QQPG4A","ts":"1612371087.050800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Jgt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is it kind of like a slap on the wrist? "},{"type":"emoji","name":"joy"},{"type":"text","text":" "},{"type":"emoji","name":"joy"}]}]}]},{"client_msg_id":"377581ef-465c-42d2-9ac9-26fd711eb1b3","type":"message","text":"I've successfully used Flux for multi-label classification several times, using this example from the model zoo as my starting point:\n\n<https://github.com/FluxML/model-zoo/blob/master/vision/mnist/mlp.jl>\n\nToday I needed binary classification. I naively assumed I could achieve this by not one-hot-encoding the labels, having just a single output from the final layer, and replacing the `logitcrossentropy` loss function with `Flux.Losses.logitbinarycrossentropy`, with no other changes. However, this does not work at all. The model is not learning.\n\nWhen I revert to multi-label classification (one-hot-encoded labels, two outputs from the final layer, logitcrossentropy loss) everything works as expected.\n\nI'm feeling really stupid. What am I missing?","user":"UGQRDMRCG","ts":"1612454297.055900","team":"T68168MUP","edited":{"user":"UGQRDMRCG","ts":"1612455395.000000"},"blocks":[{"type":"rich_text","block_id":"XHAHN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I've successfully used Flux for multi-label classification several times, using this example from the model zoo as my starting point:\n\n"},{"type":"link","url":"https://github.com/FluxML/model-zoo/blob/master/vision/mnist/mlp.jl"},{"type":"text","text":"\n\nToday I needed binary classification. I naively assumed I could achieve this by not one-hot-encoding the labels, having just a single output from the final layer, and replacing the "},{"type":"text","text":"logitcrossentropy","style":{"code":true}},{"type":"text","text":" loss function with "},{"type":"text","text":"Flux.Losses.logitbinarycrossentropy","style":{"code":true}},{"type":"text","text":", with no other changes. However, this does not work at all. The model is not learning.\n\nWhen I revert to multi-label classification (one-hot-encoded labels, two outputs from the final layer, logitcrossentropy loss) everything works as expected.\n\nI'm feeling really stupid. What am I missing?"}]}]}],"thread_ts":"1612454297.055900","reply_count":3,"reply_users_count":2,"latest_reply":"1612455430.056500","reply_users":["UMY1LV01G","UGQRDMRCG"],"subscribed":false},{"client_msg_id":"7d4f0c16-788d-4e51-b3f4-41e852d6fd9a","type":"message","text":"Hi, I stumbled across some posts on Discourse comparing the speed of Flux with Pytorch. I know there’s been intense development done to Flux, both extending it’s API and accelerating specific operations, but I couldn’t find any recent speed comparisons.","user":"UPM0H43C7","ts":"1612468619.058800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2sEs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I stumbled across some posts on Discourse comparing the speed of Flux with Pytorch. I know there’s been intense development done to Flux, both extending it’s API and accelerating specific operations, but I couldn’t find any recent speed comparisons."}]}]}]},{"type":"message","text":"What would be the recommended approach to add a new optimizer to Flux?","user":"U9MD78Z9N","ts":"1612555966.067500","team":"T68168MUP","thread_ts":"1612555966.067500","reply_count":2,"reply_users_count":2,"latest_reply":"1612556097.068300","reply_users":["UH9KWTTD3","U9MD78Z9N"],"subscribed":false},{"client_msg_id":"de12c684-f8b0-498b-9016-0c1709486273","type":"message","text":"I saved a model with @save like so:\n`julia&gt; @save \"count_ones_model.bson\" model`\nNow when I try to load it back in I get:\n`julia&gt; @load \"count_ones_model.bson\" mymod`\nERROR: KeyError: key :mymod not found\nStacktrace:\n [1] getindex(::Dict{Symbol,Any}, ::Symbol) at ./dict.jl:467\n [2] top-level scope at /home/phil/.julia/packages/BSON/XAts7/src/BSON.jl:53","user":"U9RDM8ZGT","ts":"1612563825.073100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nRh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I saved a model with @save like so:\n"},{"type":"text","text":"julia> @save \"count_ones_model.bson\" model","style":{"code":true}},{"type":"text","text":"\nNow when I try to load it back in I get:\n"},{"type":"text","text":"julia> @load \"count_ones_model.bson\" mymod","style":{"code":true}},{"type":"text","text":"\nERROR: KeyError: key :mymod not found\nStacktrace:\n [1] getindex(::Dict{Symbol,Any}, ::Symbol) at ./dict.jl:467\n [2] top-level scope at /home/phil/.julia/packages/BSON/XAts7/src/BSON.jl:53"}]}]}],"thread_ts":"1612563825.073100","reply_count":2,"reply_users_count":2,"latest_reply":"1612565580.074500","reply_users":["UH9KWTTD3","U9RDM8ZGT"],"subscribed":false},{"client_msg_id":"9cdea95b-88dd-448c-a204-c1d73d20b1b0","type":"message","text":"Let's say I train an NN model in Flux using batchsizse 100 and after some number of epochs I get 95% accuracy. How could I examine some of the failing 5% of testcases given that the batchsize is 100? It seems pretty easily doable with batchsize 1, but that runs very slow.","user":"U9RDM8ZGT","ts":"1612649267.076900","team":"T68168MUP","edited":{"user":"U9RDM8ZGT","ts":"1612649297.000000"},"blocks":[{"type":"rich_text","block_id":"Q1Pee","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Let's say I train an NN model in Flux using batchsizse 100 and after some number of epochs I get 95% accuracy. How could I examine some of the failing 5% of testcases given that the batchsize is 100? It seems pretty easily doable with batchsize 1, but that runs very slow."}]}]}]},{"client_msg_id":"cc8cb69e-59d8-44df-ab52-306fce319c60","type":"message","text":"What I ended up doing was re-running DataLoader using batchsize=1 in order to more easily query the testcases... I'm hoping there's an easier way already built-in for doing this.","user":"U9RDM8ZGT","ts":"1612649390.078300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7w4I0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What I ended up doing was re-running DataLoader using batchsize=1 in order to more easily query the testcases... I'm hoping there's an easier way already built-in for doing this."}]}]}]}]}