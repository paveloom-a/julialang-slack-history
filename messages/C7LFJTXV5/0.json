{"cursor": 1, "messages": [{"client_msg_id":"82cbac36-6aa2-4d9d-8e67-e7dc1151dfb0","type":"message","text":"I think this used to be possible but I am getting \"Can't differentiate foreigncall expression\" now?","user":"UDVQM5U1Z","ts":"1613677424.155600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zhT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think this used to be possible but I am getting \"Can't differentiate foreigncall expression\" now?"}]}]}],"thread_ts":"1613677424.155600","reply_count":9,"reply_users_count":2,"latest_reply":"1613678608.158300","reply_users":["UH9KWTTD3","UDVQM5U1Z"],"subscribed":false},{"client_msg_id":"e8db1518-5a59-4900-a896-733a6d88cdbc","type":"message","text":"Hi I'm try the `ExpDecay` of Flux and seems it doesn't work.\n`opt = Flux.Optimiser(ExpDecay(0.001, 0.1, 1, 1e-4), Descent())`\n`for i=1:10`\n    `println(opt.os[2].eta)`\n    `Flux.train!(loss,θ,trainData,opt)`     \n`end`\nHere's my toy example. I thought the learning rate should decrease by 0.1 after each training following the document, but the toy return me same learning rate for 10 training. Am I missing anything here?","user":"U01977X150R","ts":"1613872391.004000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dxd3z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi I'm try the "},{"type":"text","text":"ExpDecay","style":{"code":true}},{"type":"text","text":" of Flux and seems it doesn't work.\n"},{"type":"text","text":"opt = Flux.Optimiser(ExpDecay(0.001, 0.1, 1, 1e-4), Descent())","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"for i=1:10","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    println(opt.os[2].eta)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    Flux.train!(loss,θ,trainData,opt)     ","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"end","style":{"code":true}},{"type":"text","text":"\nHere's my toy example. I thought the learning rate should decrease by 0.1 after each training following the document, but the toy return me same learning rate for 10 training. Am I missing anything here?"}]}]}],"thread_ts":"1613872391.004000","reply_count":3,"reply_users_count":2,"latest_reply":"1613874113.005400","reply_users":["UH9KWTTD3","UMY1LV01G"],"subscribed":false},{"client_msg_id":"610b85f4-b57c-46e8-8221-b2cb304af9df","type":"message","text":"I’m looking at implementing a “data front-end” for the MLJ implementation of some Flux models (<https://alan-turing-institute.github.io/MLJ.jl/dev/adding_models_for_general_use/#Implementing-a-data-front-end-1>). Doing so will avoid some copying that currently happens when one retrains a model after changing hyper-parameters (among other things). Currently we use `Flux.train!` which assumes features/inputs X and  labels/target y observations are “zipped”. So, in a simple example, `train!` is called on `[(X1, y1), (X2, y2), …]`  This “conflation” of inputs and target occurs nowhere else I know of in the MLJ ecosystem, and is currently a deal-breaker as far as a data front-end for MLJFlux. My question: *is there an essential performance reason for zipping the data?* Or can I just keep the two steams separate and replace `Flux.train!` with a custom version. <@UMY1LV01G> <@U8RHPM4KF>","user":"UD0SQV5LL","ts":"1614056466.020600","team":"T68168MUP","edited":{"user":"UD0SQV5LL","ts":"1614056813.000000"},"blocks":[{"type":"rich_text","block_id":"5V7v","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’m looking at implementing a “data front-end” for the MLJ implementation of some Flux models ("},{"type":"link","url":"https://alan-turing-institute.github.io/MLJ.jl/dev/adding_models_for_general_use/#Implementing-a-data-front-end-1"},{"type":"text","text":"). Doing so will avoid some copying that currently happens when one retrains a model after changing hyper-parameters (among other things). Currently we use "},{"type":"text","text":"Flux.train!","style":{"code":true}},{"type":"text","text":" which assumes features/inputs X and  labels/target y observations are “zipped”. So, in a simple example, "},{"type":"text","text":"train!","style":{"code":true}},{"type":"text","text":" is called on "},{"type":"text","text":"[(X1, y1), (X2, y2), …]","style":{"code":true}},{"type":"text","text":"  This “conflation” of inputs and target occurs nowhere else I know of in the MLJ ecosystem, and is currently a deal-breaker as far as a data front-end for MLJFlux. My question: "},{"type":"text","text":"is there an essential performance reason for zipping the data?","style":{"bold":true}},{"type":"text","text":" Or can I just keep the two steams separate and replace "},{"type":"text","text":"Flux.train!","style":{"code":true}},{"type":"text","text":" with a custom version. "},{"type":"user","user_id":"UMY1LV01G"},{"type":"text","text":" "},{"type":"user","user_id":"U8RHPM4KF"}]}]}]},{"client_msg_id":"418c13df-5271-4931-8525-604d94376b42","type":"message","text":"You can definitely replace that with a custom version. The thinking behind this design is that rather than thinking of the “minibatches” as zipped data and labels, it is a tuple of arguments to a function that is to be differentiated. There is no requirement for that, but outside of straightforward dl examples, we prolly want arguments to the objective function other than the data and labels","user":"UC4QQPG4A","ts":"1614059231.023400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"c/=7w","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You can definitely replace that with a custom version. The thinking behind this design is that rather than thinking of the “minibatches” as zipped data and labels, it is a tuple of arguments to a function that is to be differentiated. There is no requirement for that, but outside of straightforward dl examples, we prolly want arguments to the objective function other than the data and labels"}]}]}]},{"client_msg_id":"25db3af1-f515-4902-be8a-3261dc422629","type":"message","text":"`Flux.train!` is more of an end-user function than anything else and hardly the pinnacle of performance optimization, so I think it's safe to circumvent :slightly_smiling_face:","user":"UMY1LV01G","ts":"1614065379.025000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fc0Tn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Flux.train!","style":{"code":true}},{"type":"text","text":" is more of an end-user function than anything else and hardly the pinnacle of performance optimization, so I think it's safe to circumvent "},{"type":"emoji","name":"slightly_smiling_face"}]}]}]},{"client_msg_id":"1faf0dab-cf0b-473d-b2ef-0090166007ab","type":"message","text":"<@UC4QQPG4A>'s point about carrying through data that isn't strictly model inputs or targets is important though. One reason something like <http://Fast.ai|Fast.ai> is so frustrating to extend is because it doesn't allow for this and forces you to sneak metadata through in unique ways.","user":"UMY1LV01G","ts":"1614065471.026600","team":"T68168MUP","edited":{"user":"UMY1LV01G","ts":"1614065485.000000"},"blocks":[{"type":"rich_text","block_id":"lPuo","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UC4QQPG4A"},{"type":"text","text":"'s point about carrying through data that isn't strictly model inputs or targets is important though. One reason something like "},{"type":"link","url":"http://Fast.ai","text":"Fast.ai"},{"type":"text","text":" is so frustrating to extend is because it doesn't allow for this and forces you to sneak metadata through in unique ways."}]}]}]},{"client_msg_id":"9e0303c7-5be7-438c-b987-2c38ad705bba","type":"message","text":"Not relying on `zip` also opens up the possibility to make full use of smarter data containers like DataLoaders.jl (which supports random access loading on demand)","user":"UMY1LV01G","ts":"1614065663.028100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HTc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not relying on "},{"type":"text","text":"zip","style":{"code":true}},{"type":"text","text":" also opens up the possibility to make full use of smarter data containers like DataLoaders.jl (which supports random access loading on demand)"}]}]}]},{"client_msg_id":"84ab4094-d0c3-43f8-828a-213499e07644","type":"message","text":"I think it makes sense to rely on `Flux.train!` for the simple reason that any clever tricks you might want to do would end up as implementation details of the dataloader itself, not of the loop specifically. I think <https://github.com/FluxML/Flux.jl/pull/1471> is a net positive. It actually makes the callback system more robust, to mess with the loop outside of the cases we serve usually","user":"UC4QQPG4A","ts":"1614066298.032600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WLz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think it makes sense to rely on "},{"type":"text","text":"Flux.train!","style":{"code":true}},{"type":"text","text":" for the simple reason that any clever tricks you might want to do would end up as implementation details of the dataloader itself, not of the loop specifically. I think "},{"type":"link","url":"https://github.com/FluxML/Flux.jl/pull/1471"},{"type":"text","text":" is a net positive. It actually makes the callback system more robust, to mess with the loop outside of the cases we serve usually"}]}]}]},{"client_msg_id":"4a85d3eb-9e60-47cf-a9a4-a67dbca692d3","type":"message","text":"I think gradient accumulation is a good counterexample that is not an implementation detail of the dataloader. I do wish we collectively had something better than callbacks for customizing the train loop though. There are essentially `O(LOC in training loop)` possible extension points, and it's a slippery slope to something like <https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html> (which has what, 30+ hooks and methods?)","user":"UMY1LV01G","ts":"1614067536.038000","team":"T68168MUP","edited":{"user":"UMY1LV01G","ts":"1614067607.000000"},"blocks":[{"type":"rich_text","block_id":"NqQs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think gradient accumulation is a good counterexample that is not an implementation detail of the dataloader. I do wish we collectively had something better than callbacks for customizing the train loop though. There are essentially "},{"type":"text","text":"O(LOC in training loop)","style":{"code":true}},{"type":"text","text":" possible extension points, and it's a slippery slope to something like "},{"type":"link","url":"https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html"},{"type":"text","text":" (which has what, 30+ hooks and methods?)"}]}]}]},{"client_msg_id":"1a68f22d-5bbf-4b90-aee1-f91374f31f92","type":"message","text":"Gradient accumulation can happen at the Zygote level.","user":"UC4QQPG4A","ts":"1614081995.039300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ZLW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Gradient accumulation can happen at the Zygote level."}]}]}]},{"client_msg_id":"ca0298a2-b201-4e20-b622-a63cd7a73ec2","type":"message","text":"I don’t think there is a replacement for hooks that DL people want to use, currently. Callbacks are convenient for sequential models.","user":"UC4QQPG4A","ts":"1614082079.040400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/qhr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I don’t think there is a replacement for hooks that DL people want to use, currently. Callbacks are convenient for sequential models."}]}]}]},{"client_msg_id":"287b032e-1dc4-42d4-90a0-35039d1db4d3","type":"message","text":"I continue to dislike `Flux.train!` . Most people should implement custom training loops.","user":"U6A936746","ts":"1614085038.042300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"905I","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I continue to dislike "},{"type":"text","text":"Flux.train!","style":{"code":true}},{"type":"text","text":" . Most people should implement custom training loops."}]}]}],"reactions":[{"name":"+1","users":["UDXST8ARK"],"count":1}]},{"client_msg_id":"84fdb87d-bb23-49be-b55c-e186a95b5322","type":"message","text":"Callbacks seems like they are just a more limited and confusing way of doing it, esp since as Brian says, there are basically desire for them at every possibly step, and futher: also with every possible piece of intermediate state","user":"U6A936746","ts":"1614085132.043800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nBsAx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Callbacks seems like they are just a more limited and confusing way of doing it, esp since as Brian says, there are basically desire for them at every possibly step, and futher: also with every possible piece of intermediate state"}]}]}]},{"client_msg_id":"f7ddfad1-be37-4f2e-a8d9-84291b5406a6","type":"message","text":"Also the way train does batching and epochs with iterator constructs is just kinds of confusing to me. A custom loop is much clearer.","user":"U6A936746","ts":"1614085173.044700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RSW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Also the way train does batching and epochs with iterator constructs is just kinds of confusing to me. A custom loop is much clearer."}]}]}]},{"client_msg_id":"0f3f3b59-8cc6-45d7-853d-5723cca3f2b2","type":"message","text":"Maybe it's a silly idea, but wouldn't something basic like\n\n`function optimize!(loss, params, data, opt)\n    loss_value, back = pullback(() -&gt; loss(data), params)\n    grads = back(one(loss_value))\n    update!(opt, params, grads)\n    return loss_value, grads # for diagnostics\nend`\n\nbe a decent compromise? This way a custom loop is essentially as easy to write as a call to `train!` (It is just a one-line for loop)","user":"U6BJ9E351","ts":"1614087481.055600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"++J","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Maybe it's a silly idea, but wouldn't something basic like\n\n"},{"type":"text","text":"function optimize!(loss, params, data, opt)\n","style":{"code":true}},{"type":"text","text":"    loss_value, back = pullback(() -> loss(data), params)\n","style":{"code":true}},{"type":"text","text":"    grads = back(one(loss_value))\n","style":{"code":true}},{"type":"text","text":"    update!(opt, params, grads)\n","style":{"code":true}},{"type":"text","text":"    return loss_value, grads # for diagnostics\n","style":{"code":true}},{"type":"text","text":"end\n","style":{"code":true}},{"type":"text","text":"\nbe a decent compromise? This way a custom loop is essentially as easy to write as a call to "},{"type":"text","text":"train!","style":{"code":true}},{"type":"text","text":" (It is just a one-line for loop)"}]}]}]},{"client_msg_id":"92f74205-2553-404c-bb77-13810e206c04","type":"message","text":"See <https://github.com/FluxML/Flux.jl/pull/1471|https://github.com/FluxML/Flux.jl/pull/1471> which is doing basically that","user":"UC4QQPG4A","ts":"1614088155.056100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OT5n","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"See "},{"type":"link","url":"https://github.com/FluxML/Flux.jl/pull/1471","text":"https://github.com/FluxML/Flux.jl/pull/1471"},{"type":"text","text":" which is doing basically that"}]}]}],"reactions":[{"name":"+1","users":["U6BJ9E351"],"count":1}]},{"client_msg_id":"d1a4c9ec-b6e1-48b6-9f38-cd5105707bc8","type":"message","text":"That and <https://github.com/FluxML/Flux.jl/pull/1017|https://github.com/FluxML/Flux.jl/pull/1017>","user":"UC4QQPG4A","ts":"1614088263.056700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KW/FJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That and "},{"type":"link","url":"https://github.com/FluxML/Flux.jl/pull/1017","text":"https://github.com/FluxML/Flux.jl/pull/1017"}]}]}]},{"client_msg_id":"21d27f95-ba09-41c3-b2f3-f457575008d9","type":"message","text":"It makes sense for us to fix our dataloaders to act for tuples of arguments approach. Further, we should think about the needs beyond strict sequential computer vision tasks. Something that would consolidate a lot of data management repos in the wild","user":"UC4QQPG4A","ts":"1614088415.059200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"c4q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It makes sense for us to fix our dataloaders to act for tuples of arguments approach. Further, we should think about the needs beyond strict sequential computer vision tasks. Something that would consolidate a lot of data management repos in the wild"}]}]}]},{"client_msg_id":"4FA26759-CF92-4BE8-BF98-79B078338344","type":"message","text":"I don’t think <https://github.com/FluxML/Flux.jl/pull/1471|https://github.com/FluxML/Flux.jl/pull/1471> does what <@U6BJ9E351> suggested. The proposed `optimize!` is the same as the `step!` suggested in the comments of the PR. I think that’s something we all want.\n\n1471 could use `step!` but right now it looks like it is using the callback approach. I’m with Lyndon and Brian on this.","user":"UH9KWTTD3","ts":"1614093592.062700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kboqM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I don’t think "},{"type":"link","url":"https://github.com/FluxML/Flux.jl/pull/1471","text":"https://github.com/FluxML/Flux.jl/pull/1471"},{"type":"text","text":" does what "},{"type":"user","user_id":"U6BJ9E351"},{"type":"text","text":" suggested. The proposed "},{"type":"text","text":"optimize!","style":{"code":true}},{"type":"text","text":" is the same as the "},{"type":"text","text":"step!","style":{"code":true}},{"type":"text","text":" suggested in the comments of the PR. I think that’s something we all want.\n"},{"type":"text","text":"\n1471 could use "},{"type":"text","text":"step!","style":{"code":true}},{"type":"text","text":" but right now it looks like it is using the callback approach. I’m with Lyndon and Brian on this."}]}]}]},{"client_msg_id":"471C1B7A-07FF-4434-A66E-1FF2F7C7DD00","type":"message","text":"1017 looks pretty good though","user":"UH9KWTTD3","ts":"1614093648.063000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Phgz+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"1017 looks pretty good though"}]}]}]},{"client_msg_id":"b943d800-eefd-4b0b-a269-15452b44ddc6","type":"message","text":"I mean, the second one is the one he wants but the first one manages the callbacks. It's weird to say one is okay and the other isn't, because even without the hooks, passing the stuff into callbacks is strictly better than the current status","user":"UC4QQPG4A","ts":"1614095769.065200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YY/Tm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I mean, the second one is the one he wants but the first one manages the callbacks. It's weird to say one is okay and the other isn't, because even without the hooks, passing the stuff into callbacks is strictly better than the current status"}]}]}]},{"client_msg_id":"F4233BBD-D010-4B96-9E0D-7DFA8E2C8362","type":"message","text":"One expands the functionality of callbacks and the other collapses the forward pass, backwards pass, and parameter update into a single function. I like the latter but not the former.\n\nExpanding callbacks is only strictly better from the perspective of “can I do X with a callback.” It doesn’t address whether callbacks are clearest or most intuitive way to do X.","user":"UH9KWTTD3","ts":"1614096755.069000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/Vw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"One expands the functionality of callbacks and the other collapses the forward pass, backwards pass, and parameter update into a single function. I like the latter but not the former.\n"},{"type":"text","text":"\nExpanding callbacks is only strictly better from the perspective of “can I do X with a callback.” It doesn’t address whether callbacks are clearest or most intuitive way to do X."}]}]}]},{"client_msg_id":"3942994d-5d31-4917-8318-bd9852cc6f46","type":"message","text":"Sure, but isn't the effect restricting the usability of the callbacks for the tasks they are meant for? Today you can't do basic things like printing the current running loss, stop on nan loss/ gradients, debug gradients easily, or hook into a logger neatly.\n\nWhen they try to do non trivial tasks, we should absolutely guide them to the loop, but before that, :shrug:.","user":"UC4QQPG4A","ts":"1614101276.079300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eXAG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Sure, but isn't the effect restricting the usability of the callbacks for the tasks they are meant for? Today you can't do basic things like printing the current running loss, stop on nan loss/ gradients, debug gradients easily, or hook into a logger neatly.\n\nWhen they try to do non trivial tasks, we should absolutely guide them to the loop, but before that, "},{"type":"emoji","name":"shrug"},{"type":"text","text":"."}]}]}]},{"client_msg_id":"20e9ba4b-3454-4797-a7a8-47a6a52b87bd","type":"message","text":"The million-dollar question is where we draw the proverbial line in the sand. 1471 is a pretty decent 80/20 implementation, but do we freeze it there and clamp down on any further expansion?","user":"UMY1LV01G","ts":"1614101648.080600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"g9g","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The million-dollar question is where we draw the proverbial line in the sand. 1471 is a pretty decent 80/20 implementation, but do we freeze it there and clamp down on any further expansion?"}]}]}]},{"client_msg_id":"568928fd-4417-4c69-96d9-a17fca60d29a","type":"message","text":"Perhaps, but that can be left for the future. 1471 (minus the hooks) is not the 80/20 of anything. We have had callbacks for a while, and this is strictly improving that. Intentionally kneecapping them doesn't make sense. I'm assuming the consensus would be to deprecate them entirely?","user":"UC4QQPG4A","ts":"1614102997.092300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"D04","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Perhaps, but that can be left for the future. 1471 (minus the hooks) is not the 80/20 of anything. We have had callbacks for a while, and this is strictly improving that. Intentionally kneecapping them doesn't make sense. I'm assuming the consensus would be to deprecate them entirely?"}]}]}]},{"type":"message","text":"What's the meeting code for the ML and AD Development/Usage call?","user":"U9MD78Z9N","ts":"1614270806.095200","team":"T68168MUP"},{"client_msg_id":"c7814180-a072-47d3-a44b-1b47c5a3ff68","type":"message","text":"All the details should be on <https://www.google.com/calendar/event?eid=NDY0YW51c3JhdWtpcHZxam91YzFpazh2bjdfMjAyMTAyMjVUMTYzMDAwWiBqdWxpYWxhbmcub3JnX2tvbWF1YXFldDE0ZW9nOW9pdjNwNm83cG1nQGc&amp;ctz=America/Vancouver|https://www.google.com/calendar/event?eid=NDY0YW51c3JhdWtpcHZxam91YzFpazh2bjdfMjAyMT[…]X2tvbWF1YXFldDE0ZW9nOW9pdjNwNm83cG1nQGc>","user":"UMY1LV01G","ts":"1614271756.095500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TcfU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"All the details should be on "},{"type":"link","url":"https://www.google.com/calendar/event?eid=NDY0YW51c3JhdWtpcHZxam91YzFpazh2bjdfMjAyMTAyMjVUMTYzMDAwWiBqdWxpYWxhbmcub3JnX2tvbWF1YXFldDE0ZW9nOW9pdjNwNm83cG1nQGc&ctz=America/Vancouver","text":"https://www.google.com/calendar/event?eid=NDY0YW51c3JhdWtpcHZxam91YzFpazh2bjdfMjAyMT[…]X2tvbWF1YXFldDE0ZW9nOW9pdjNwNm83cG1nQGc"}]}]}]},{"client_msg_id":"73b3c2c7-38b6-43cd-859e-8aad78a6d4bb","type":"message","text":"Can you not join through <https://https>:&lt;//www.google.com/url?q=https://mit.zoom.us/j/96251790289&gt; directly?","user":"UMY1LV01G","ts":"1614271777.095900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hd+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can you not join through "},{"type":"link","url":"https://https"},{"type":"text","text":":"},{"type":"link","url":"//www.google.com/url?q=https://mit.zoom.us/j/96251790289"},{"type":"text","text":" directly?"}]}]}]},{"client_msg_id":"c24d835c-ae36-4489-8b57-0ea5d94e2438","type":"message","text":"he's on.","user":"U69BL50BF","ts":"1614271789.096200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ehn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"he's on."}]}]}]},{"client_msg_id":"c694143d-46b2-441e-9dc7-28aa32d2b4ad","type":"message","text":"Ah I see you already got on","user":"UMY1LV01G","ts":"1614271792.096400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wBUP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah I see you already got on"}]}]}]},{"client_msg_id":"f12f76c0-a7ed-450e-b7ef-84b7a86763f3","type":"message","text":"That's what I get for checking  <#C6G240ENA|autodiff> after posting","user":"UMY1LV01G","ts":"1614271832.096900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KqnOL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That's what I get for checking  "},{"type":"channel","channel_id":"C6G240ENA"},{"type":"text","text":" after posting"}]}]}]},{"client_msg_id":"eabb82d4-23cb-4bad-92ea-e68f95c8905e","type":"message","text":"Short progress update on the FastAI.jl development: it's not released yet, but large parts are already working and I've set up documentation, so check it out if you're interested: <https://lorenzoh.github.io/FastAI.jl/dev/README.html>\n\nI usually post over on the Zulip channel, but figured some people who might be interested are only active here. Let me know what you think!","user":"U010XUS4MT7","ts":"1614282534.099400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"03xd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Short progress update on the FastAI.jl development: it's not released yet, but large parts are already working and I've set up documentation, so check it out if you're interested: "},{"type":"link","url":"https://lorenzoh.github.io/FastAI.jl/dev/README.html"},{"type":"text","text":"\n\nI usually post over on the Zulip channel, but figured some people who might be interested are only active here. Let me know what you think!"}]}]}],"reactions":[{"name":"+1","users":["UGD4K0Z25"],"count":1}]},{"client_msg_id":"04f77148-53d9-474f-8721-171b9779cee8","type":"message","text":"Follow <https://julialang.zulipchat.com/#narrow/stream/237432-ml-ecosystem-coordination/topic/FastAI.2Ejl.20development/near/227827979> for more frequent updates","user":"U010XUS4MT7","ts":"1614282979.100000","team":"T68168MUP","attachments":[{"service_name":"Zulip","title":"JuliaLang","title_link":"https://julialang.zulipchat.com/#narrow/stream/237432-ml-ecosystem-coordination/topic/FastAI.2Ejl.20development/near/227827979","text":"This is the Zulip server for the Julia programming language community. We ask anyone joining to adhere to the Julia Code of Conduct. | To learn more about Julia, check out <https://julialang.org/>, or just come ask us here! | You can reach out to the admins of this Zulip by sending a direct message to @zulip-admins.","fallback":"Zulip: JuliaLang","thumb_url":"https://zulip-avatars.s3.amazonaws.com/7178/realm/icon.png?version=6","from_url":"https://julialang.zulipchat.com/#narrow/stream/237432-ml-ecosystem-coordination/topic/FastAI.2Ejl.20development/near/227827979","thumb_width":100,"thumb_height":100,"id":1,"original_url":"https://julialang.zulipchat.com/#narrow/stream/237432-ml-ecosystem-coordination/topic/FastAI.2Ejl.20development/near/227827979"}],"blocks":[{"type":"rich_text","block_id":"vaPg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Follow "},{"type":"link","url":"https://julialang.zulipchat.com/#narrow/stream/237432-ml-ecosystem-coordination/topic/FastAI.2Ejl.20development/near/227827979"},{"type":"text","text":" for more frequent updates"}]}]}]},{"client_msg_id":"27cefc8b-f685-459b-93be-f89d57efed89","type":"message","text":"```julia&gt; Flux.OneHotArray\nERROR: UndefVarError: OneHotArray not defined```","user":"U7YD3DKL2","ts":"1614283926.100800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gcME","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> Flux.OneHotArray\nERROR: UndefVarError: OneHotArray not defined"}]}]}]},{"client_msg_id":"4b9ce7b4-9e03-4e0b-b218-03fc53267f00","type":"message","text":"Where is OneHotArray defined?","user":"U7YD3DKL2","ts":"1614283931.101100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ydvpW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Where is OneHotArray defined?"}]}]}]},{"client_msg_id":"8b64941d-19b6-4149-a955-34fadc2f48c1","type":"message","text":"Are you using master?","user":"UH9KWTTD3","ts":"1614284377.101300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"cDm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Are you using master?"}]}]}]},{"client_msg_id":"a09ca423-f64b-48ce-960d-296a4f228b70","type":"message","text":"No. The last released version.","user":"U7YD3DKL2","ts":"1614284389.101800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lUUuf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"No. The last released version."}]}]}]},{"client_msg_id":"5123929f-f489-41ba-afff-08f3c05c2587","type":"message","text":"`OneHotArray` is still unreleased I think","user":"UH9KWTTD3","ts":"1614284390.101900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wg7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"OneHotArray","style":{"code":true}},{"type":"text","text":" is still unreleased I think"}]}]}]},{"client_msg_id":"884fe263-4084-44c0-88a3-9d300b22930e","type":"message","text":"But I just checked that PR was included since 0.11.4. <https://github.com/FluxML/Flux.jl/releases/tag/v0.11.4>","user":"U7YD3DKL2","ts":"1614284416.102600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"M2BFN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But I just checked that PR was included since 0.11.4. "},{"type":"link","url":"https://github.com/FluxML/Flux.jl/releases/tag/v0.11.4"}]}]}]},{"client_msg_id":"ff70017c-a595-439f-8096-9d4011ea789e","type":"message","text":"The release notes on Github are all screwed up. I’ve already told <@UC4QQPG4A> about it, and he’s working on fixing it for future releases.","user":"UH9KWTTD3","ts":"1614284431.102900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+axp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The release notes on Github are all screwed up. I’ve already told "},{"type":"user","user_id":"UC4QQPG4A"},{"type":"text","text":" about it, and he’s working on fixing it for future releases."}]}]}]},{"client_msg_id":"b59ae55f-153c-4276-b4b1-2a7a49263d03","type":"message","text":"Oh","user":"U7YD3DKL2","ts":"1614284442.103100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mLVQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oh"}]}]}]},{"client_msg_id":"c71e8e46-cb34-4134-8b81-0d497bf41833","type":"message","text":"Why it hasn't been released?","user":"U7YD3DKL2","ts":"1614284448.103300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GC89L","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Why it hasn't been released?"}]}]}]},{"client_msg_id":"6a42ff36-cc6a-429b-9cbd-53c9daf1f8f2","type":"message","text":"Not 100% positive cause I’m still hazy on semver but I think the struct/constructor changes means it is breaking. So it will have to wait until v0.12.","user":"UH9KWTTD3","ts":"1614284508.104100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"leEj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not 100% positive cause I’m still hazy on semver but I think the struct/constructor changes means it is breaking. So it will have to wait until v0.12."}]}]}]},{"client_msg_id":"8a628d46-354c-41a4-acc5-b001c0d6845c","type":"message","text":"Even though the highest level APIs like `onehotbatch` are the same","user":"UH9KWTTD3","ts":"1614284564.104400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5PB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Even though the highest level APIs like "},{"type":"text","text":"onehotbatch","style":{"code":true}},{"type":"text","text":" are the same"}]}]}]},{"client_msg_id":"ffe91dcc-ed16-4e00-8b2d-73c9cd1863fd","type":"message","text":"<https://github.com/FluxML/Flux.jl/compare/v0.11.6...master>","user":"UH9KWTTD3","ts":"1614284665.104600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gkT","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/FluxML/Flux.jl/compare/v0.11.6...master"}]}]}],"thread_ts":"1614284665.104600","reply_count":1,"reply_users_count":1,"latest_reply":"1614285221.104700","reply_users":["UMY1LV01G"],"subscribed":false},{"client_msg_id":"5f19a4fe-0d5f-497c-b364-aac3a556e669","type":"message","text":"I posted a question regarding training only part of a matrix in <#C690QRAA3|machine-learning> I'm using Flux. If anyone can answer or point me in the right direction, I'd appreciate it. Thanks.","user":"U01L0KU0SDV","ts":"1614311593.107400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jGCy5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I posted a question regarding training only part of a matrix in "},{"type":"channel","channel_id":"C690QRAA3"},{"type":"text","text":" I'm using Flux. If anyone can answer or point me in the right direction, I'd appreciate it. Thanks."}]}]}]},{"type":"message","text":"Do we have performant implicit layers (the OptNet layer is one example but more generally): z_i+1 is such that f(z_i, z_i+1) = 0 where the f is defined by weights somehow?","user":"U9MD78Z9N","ts":"1614554176.108000","team":"T68168MUP"},{"client_msg_id":"b93f451f-b086-45bd-b504-e11b646fd6a9","type":"message","text":"That's just a Steady state problem","user":"U69BL50BF","ts":"1614554291.108700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xqywW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That's just a Steady state problem"}]}]}]},{"type":"message","text":"Yes. Sorry that i had a different name than you. However seaching for \"Steady State\" layers in the Flux eco system i didn't find anything either","user":"U9MD78Z9N","ts":"1614554418.108800","team":"T68168MUP","edited":{"user":"U9MD78Z9N","ts":"1614555958.000000"}},{"client_msg_id":"c63cfb5e-d772-43a4-bc0a-97715e88ad0d","type":"message","text":"<https://discourse.julialang.org/t/funding-community-projects/56288|https://discourse.julialang.org/t/funding-community-projects/56288>","user":"UDGT4PM41","ts":"1614644606.109400","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"Funding Community Projects","title_link":"https://discourse.julialang.org/t/funding-community-projects/56288","text":"Hi fellow Julians! Many of you probably know that the Julia project uses the NumFOCUS, an US non-profit organisation as it’s fiscal sponsor. This means that income for the project is collected in accounts of that organisation, and our expenses are paid though there. The primary source of income is via sponsorship of JuliaCon, although individual sponsorships are now significant as well. Major expenses are again for JuliaCon, as well for our summer-of-code projects. As a result of careful stewa...","fallback":"JuliaLang: Funding Community Projects","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","fields":[{"title":"Reading time","value":"1 mins :clock2:","short":true},{"title":"Likes","value":"14 :heart:","short":true}],"ts":1614638394,"from_url":"https://discourse.julialang.org/t/funding-community-projects/56288","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/funding-community-projects/56288"}],"blocks":[{"type":"rich_text","block_id":"krm","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://discourse.julialang.org/t/funding-community-projects/56288","text":"https://discourse.julialang.org/t/funding-community-projects/56288"}]}]}]},{"client_msg_id":"58ee4578-736f-4948-84a1-43772de4725a","type":"message","text":"when I use the train! function, It works well, minimizing the loss function ecc..., but when I then display the params, they are the same as before the training step.","user":"U01FTFACYJ0","ts":"1614682564.113500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PSk0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"when I use the train! function, It works well, minimizing the loss function ecc..., but when I then display the params, they are the same as before the training step."}]}]}]},{"client_msg_id":"07a315d2-38a2-4bff-b08c-a51fc8e6a5c4","type":"message","text":"swift for tensorflow is dead\n\n<https://www.reddit.com/r/programming/comments/lik6fa/swift_for_tensorflow_shuts_down/>\n\nFlux is the only contender now","user":"URE70HE7R","ts":"1614719174.114400","team":"T68168MUP","attachments":[{"service_name":"reddit","title":"Swift for TensorFlow Shuts Down","title_link":"https://www.reddit.com/r/programming/comments/lik6fa/swift_for_tensorflow_shuts_down/","text":"Posted in r/programming by u/dm13450 • 15 points and 5 comments","fallback":"reddit: Swift for TensorFlow Shuts Down","thumb_url":"https://b.thumbs.redditmedia.com/kSx1DvW50gydn_aZwumYbs8UKpllySASXq8mtC7jvFM.jpg","from_url":"https://www.reddit.com/r/programming/comments/lik6fa/swift_for_tensorflow_shuts_down/","thumb_width":140,"thumb_height":140,"service_icon":"http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png","id":1,"original_url":"https://www.reddit.com/r/programming/comments/lik6fa/swift_for_tensorflow_shuts_down/"}],"blocks":[{"type":"rich_text","block_id":"0TJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"swift for tensorflow is dead\n\n"},{"type":"link","url":"https://www.reddit.com/r/programming/comments/lik6fa/swift_for_tensorflow_shuts_down/"},{"type":"text","text":"\n\nFlux is the only contender now"}]}]}]},{"client_msg_id":"e1179335-8215-40e3-b566-77c82ea11da6","type":"message","text":"hi guys, someone asked me at worked how does TF compares to Flux speed-wise. Is there like a comparison somewhere?","user":"U013V2CFZAN","ts":"1614719627.115900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ejd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hi guys, someone asked me at worked how does TF compares to Flux speed-wise. Is there like a comparison somewhere?"}]}]}]},{"client_msg_id":"db1f74f6-3e8a-40d5-99cd-6afc70a7ce9c","type":"message","text":"well, there’s no way to compare, one is python, the other is julian","user":"URE70HE7R","ts":"1614719696.116400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6RU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"well, there’s no way to compare, one is python, the other is julian"}]}]}]},{"client_msg_id":"3afa9f83-d6f9-4e7c-b5e0-c36bd0491593","type":"message","text":"I’m seeking some comment about how to handle batches and resampling in MLJFLux. See this issue: <https://github.com/FluxML/MLJFlux.jl/issues/97> , thanks. <@UMY1LV01G> <@U010XUS4MT7>","user":"UD0SQV5LL","ts":"1614722147.118100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dxv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’m seeking some comment about how to handle batches and resampling in MLJFLux. See this issue: "},{"type":"link","url":"https://github.com/FluxML/MLJFlux.jl/issues/97"},{"type":"text","text":" , thanks. "},{"type":"user","user_id":"UMY1LV01G"},{"type":"text","text":" "},{"type":"user","user_id":"U010XUS4MT7"}]}]}]},{"client_msg_id":"ba436d15-1fd0-4af4-ba8d-743244ec3ce4","type":"message","text":"when I try to implement my_custom_training from the documentation I get an error \"Params\" not defined when I use ps = Params(ps), how can I solve that?","user":"U01FTFACYJ0","ts":"1614969666.121100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zTI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"when I try to implement my_custom_training from the documentation I get an error \"Params\" not defined when I use ps = Params(ps), how can I solve that?"}]}]}],"thread_ts":"1614969666.121100","reply_count":1,"reply_users_count":1,"latest_reply":"1614969991.121300","reply_users":["UMY1LV01G"],"subscribed":false},{"type":"message","text":"Hi, I believed I met with a bp issue. When I tried to train a CNN with `MaxPool` layer, I would got  an error bellow. After removing `MaxPool` layer, everything is fine.  I thought the dimension of my `CuArray` didn't violate the standard here.","files":[{"id":"F01R3M48N2U","created":1615003546,"timestamp":1615003546,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01977X150R","editable":false,"size":75573,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01R3M48N2U/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01R3M48N2U/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01R3M48N2U-5227ee12ee/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01R3M48N2U-5227ee12ee/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01R3M48N2U-5227ee12ee/image_360.png","thumb_360_w":360,"thumb_360_h":13,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01R3M48N2U-5227ee12ee/image_480.png","thumb_480_w":480,"thumb_480_h":18,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01R3M48N2U-5227ee12ee/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01R3M48N2U-5227ee12ee/image_720.png","thumb_720_w":720,"thumb_720_h":26,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01R3M48N2U-5227ee12ee/image_800.png","thumb_800_w":800,"thumb_800_h":29,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01R3M48N2U-5227ee12ee/image_960.png","thumb_960_w":960,"thumb_960_h":35,"thumb_1024":"https://files.slack.com/files-tmb/T68168MUP-F01R3M48N2U-5227ee12ee/image_1024.png","thumb_1024_w":1024,"thumb_1024_h":37,"original_w":2438,"original_h":89,"thumb_tiny":"AwABADC53paTvS0igooooAWlFJSigD//2Q==","permalink":"https://julialang.slack.com/files/U01977X150R/F01R3M48N2U/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01R3M48N2U-daef57622f","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"1lm7J","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I believed I met with a bp issue. When I tried to train a CNN with "},{"type":"text","text":"MaxPool ","style":{"code":true}},{"type":"text","text":"layer, I would got  an error bellow. After removing "},{"type":"text","text":"MaxPool ","style":{"code":true}},{"type":"text","text":"layer, everything is fine.  I thought the dimension of my "},{"type":"text","text":"CuArray ","style":{"code":true}},{"type":"text","text":"didn't violate the standard here."}]}]}],"user":"U01977X150R","display_as_bot":false,"ts":"1615003705.125900"},{"client_msg_id":"5845ccc6-46e8-4c62-b8d4-d58570643e11","type":"message","text":"Th issue is that the eltypes don't match, so it will promote the second argument to float64 and be slow.","user":"UC4QQPG4A","ts":"1615013437.127300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8497","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Th issue is that the eltypes don't match, so it will promote the second argument to float64 and be slow."}]}]}],"thread_ts":"1615013437.127300","reply_count":1,"reply_users_count":1,"latest_reply":"1615013934.127400","reply_users":["U01977X150R"],"subscribed":false},{"client_msg_id":"67ea352b-7e6a-4fcb-8b68-39cc856234ac","type":"message","text":"Has anyone done multi-class segmentation and written a loss function for it that works on the GPU? The model output is a `(h, w, n, b)`-array with `n` the number of classes and `b` the batch size. Each slice in the class dimension corresponds to a one-hot vector for that pixel.","user":"U010XUS4MT7","ts":"1615125971.129900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"BGmH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Has anyone done multi-class segmentation and written a loss function for it that works on the GPU? The model output is a "},{"type":"text","text":"(h, w, n, b)","style":{"code":true}},{"type":"text","text":"-array with "},{"type":"text","text":"n","style":{"code":true}},{"type":"text","text":" the number of classes and "},{"type":"text","text":"b","style":{"code":true}},{"type":"text","text":" the batch size. Each slice in the class dimension corresponds to a one-hot vector for that pixel."}]}]}]},{"client_msg_id":"5cc894b4-b9db-42a4-86f1-078fbd7ebe0c","type":"message","text":"When differentiating a Flux Model wrt to a loss function using the RNG (in my case for an Monte Carlo estimation inside the loss), the `Flux.gradient` / `Flux.Optim.update!` mechanisms seem not to resample the random numbers. This results in optimizing over a fixed set of random numbers, which is not what I want. How would I go about resampling the random variables in each training iteration?","user":"UBGQUAG8K","ts":"1615482684.135000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DJtFs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"When differentiating a Flux Model wrt to a loss function using the RNG (in my case for an Monte Carlo estimation inside the loss), the "},{"type":"text","text":"Flux.gradient","style":{"code":true}},{"type":"text","text":" / "},{"type":"text","text":"Flux.Optim.update!","style":{"code":true}},{"type":"text","text":" mechanisms seem not to resample the random numbers. This results in optimizing over a fixed set of random numbers, which is not what I want. How would I go about resampling the random variables in each training iteration?"}]}]}]},{"client_msg_id":"27d4294d-3952-400d-9b32-09dd45a198b0","type":"message","text":"Say for example my loss is `loss(x) = (randn() - x)^2`  (for an usupervised setting). Then I’d like `x` to converge to 0 (when using SGD). Instead now `randn()` gets sampled only once and then x converges to that sample…","user":"UBGQUAG8K","ts":"1615483028.137600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"z2/4c","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Say for example my loss is "},{"type":"text","text":"loss(x) = (randn() - x)^2","style":{"code":true}},{"type":"text","text":"  (for an usupervised setting). Then I’d like "},{"type":"text","text":"x","style":{"code":true}},{"type":"text","text":" to converge to 0 (when using SGD). Instead now "},{"type":"text","text":"randn()","style":{"code":true}},{"type":"text","text":" gets sampled only once and then x converges to that sample…"}]}]}]},{"client_msg_id":"42bb7b11-6681-4341-a0de-d3013b71fa4a","type":"message","text":"looking at <https://github.com/FluxML/model-zoo/blob/master/vision/vae_mnist/vae_mnist.jl> its supposed to work out of the box, I'll try creating a simple testcase","user":"UBGQUAG8K","ts":"1615487517.138100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KKnHF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"looking at "},{"type":"link","url":"https://github.com/FluxML/model-zoo/blob/master/vision/vae_mnist/vae_mnist.jl"},{"type":"text","text":" its supposed to work out of the box, I'll try creating a simple testcase"}]}]}]},{"client_msg_id":"bc0464fa-49fb-4fda-803d-e87cf6ccc9e5","type":"message","text":"I am trying to understand the correct way to do a 1D convolution over a signal. My best guess would be something like this:\n```x = rand(1000, 1, 1, 32)\nConv((20, 1), 1=&gt;6, relu)```\nfor a signal that is recorded every second for 1000 seconds, with a batch size of 32.\n\nAnd then if I am recording more than one signal with different sensors my best guess is:\n```x = rand(1000, 1, 4, 32)\nConv((20, 1), 4=&gt;6, relu)```\nIs this the correct way?","user":"UAPLG1B7H","ts":"1615671320.141300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VaNX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am trying to understand the correct way to do a 1D convolution over a signal. My best guess would be something like this:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"x = rand(1000, 1, 1, 32)\nConv((20, 1), 1=>6, relu)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"for a signal that is recorded every second for 1000 seconds, with a batch size of 32.\n\nAnd then if I am recording more than one signal with different sensors my best guess is:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"x = rand(1000, 1, 4, 32)\nConv((20, 1), 4=>6, relu)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Is this the correct way?"}]}]}]},{"client_msg_id":"fb1c08ba-81d4-4b0c-977f-c18b68643ed8","type":"message","text":"Reposting from <#C6A044SQH|helpdesk> sorry if it's improper:\n\nI'm looking into using <https://github.com/yuehhua/GeometricFlux.jl|https://github.com/yuehhua/GeometricFlux.jl>. I notice that as per docs page <https://yuehhua.github.io/GeometricFlux.jl/dev/basics/passgraph/|https://yuehhua.github.io/GeometricFlux.jl/dev/basics/passgraph/>, to pass a variable graph you need to define the `GCNConv` layer by passing in a `FeaturedGraph`. However, the `FeaturedGraph` is constructed by taking in an adjacency matrix, meaning that you need to know the graph structure before defining the GNN. Am I missing something and does functionality exist so that a given GNN can take in more than one kind of graph structure?","user":"UNB36FC95","ts":"1615824434.143300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"azN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Reposting from "},{"type":"channel","channel_id":"C6A044SQH"},{"type":"text","text":" sorry if it's improper:\n\nI'm looking into using "},{"type":"link","url":"https://github.com/yuehhua/GeometricFlux.jl","text":"https://github.com/yuehhua/GeometricFlux.jl"},{"type":"text","text":". I notice that as per docs page "},{"type":"link","url":"https://yuehhua.github.io/GeometricFlux.jl/dev/basics/passgraph/","text":"https://yuehhua.github.io/GeometricFlux.jl/dev/basics/passgraph/"},{"type":"text","text":", to pass a variable graph you need to define the "},{"type":"text","text":"GCNConv","style":{"code":true}},{"type":"text","text":" layer by passing in a "},{"type":"text","text":"FeaturedGraph","style":{"code":true}},{"type":"text","text":". However, the "},{"type":"text","text":"FeaturedGraph","style":{"code":true}},{"type":"text","text":" is constructed by taking in an adjacency matrix, meaning that you need to know the graph structure before defining the GNN. Am I missing something and does functionality exist so that a given GNN can take in more than one kind of graph structure?"}]}]}]},{"client_msg_id":"8d3d74c3-d07a-4c82-aa62-177d8f2457d9","type":"message","text":"Can I compute higher order derivatives with Flux/Zygote? E.g. when I need the derivative of the NN output wrt. the input in my loss function.","user":"UBGQUAG8K","ts":"1615833112.145200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8Sav6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can I compute higher order derivatives with Flux/Zygote? E.g. when I need the derivative of the NN output wrt. the input in my loss function."}]}]}]},{"client_msg_id":"264a6b57-6bd4-4533-b998-9965c303e9d7","type":"message","text":"Going for the straightforward `g = gradient(model-&gt;gradient(x-&gt;loss(model, x), x) |&gt; sum, model)` leads to an `Mutating arrays is not supported` error","user":"UBGQUAG8K","ts":"1615833260.146400","team":"T68168MUP","edited":{"user":"UBGQUAG8K","ts":"1615833345.000000"},"blocks":[{"type":"rich_text","block_id":"=zd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Going for the straightforward "},{"type":"text","text":"g = gradient(model->gradient(x->loss(model, x), x) |> sum, model)","style":{"code":true}},{"type":"text","text":" leads to an "},{"type":"text","text":"Mutating arrays is not supported","style":{"code":true}},{"type":"text","text":" error"}]}]}]},{"client_msg_id":"d4dee19f-d35a-4478-9995-238b10d9bb84","type":"message","text":"A general question.\nShould we work on function NNlib to support higher (read at least 2nd order) derivatives in possible? The questions asking for this features are increasing.","user":"U6YRZ18GZ","ts":"1615878507.001500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Kiq8Y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"A general question.\nShould we work on function NNlib to support higher (read at least 2nd order) derivatives in possible? The questions asking for this features are increasing."}]}]}]},{"client_msg_id":"041e5906-bfcd-49b0-bbd6-2788a2ae97db","type":"message","text":"So higher order really depends on the rules being reliably AD'd through. Mutation in the backwards pass would still need to be hidden away","user":"UC4QQPG4A","ts":"1615887458.002700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sE6Ou","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So higher order really depends on the rules being reliably AD'd through. Mutation in the backwards pass would still need to be hidden away"}]}]}]},{"client_msg_id":"f83cad77-0a02-4136-9602-53ca9364b6c5","type":"message","text":"Diffractor should help with some, but it wouldn't solve things of that nature","user":"UC4QQPG4A","ts":"1615887480.003500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pdVb6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Diffractor should help with some, but it wouldn't solve things of that nature"}]}]}]},{"client_msg_id":"218684ce-f0d8-4e67-84a1-d606af1a0670","type":"message","text":"There's also <https://github.com/FluxML/Zygote.jl/pull/876|https://github.com/FluxML/Zygote.jl/pull/876>\n\nIt needs tests though","user":"UC4QQPG4A","ts":"1615887574.004300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RR8z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"There's also "},{"type":"link","url":"https://github.com/FluxML/Zygote.jl/pull/876","text":"https://github.com/FluxML/Zygote.jl/pull/876"},{"type":"text","text":"\n\nIt needs tests though"}]}]}]},{"client_msg_id":"d30f3acf-216e-42b4-8175-5102fc4770b2","type":"message","text":"Any way to do double differentiation with flux?\n```g(x1, x2) = 2*x1^2 + x2^2\njacobian(x1,x2) = gradient(g, x1, x2)```\nworks\n```hessian(x1, x2) = gradient(jacobian, x1, x2)```\ndoesn't work....","user":"U01AJUF2GEP","ts":"1615898876.005900","team":"T68168MUP","edited":{"user":"U01AJUF2GEP","ts":"1615898925.000000"},"blocks":[{"type":"rich_text","block_id":"dwj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Any way to do double differentiation with flux?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"g(x1, x2) = 2*x1^2 + x2^2\njacobian(x1,x2) = gradient(g, x1, x2)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"works\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"hessian(x1, x2) = gradient(jacobian, x1, x2)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"doesn't work...."}]}]}]},{"client_msg_id":"39a74d0f-dfa9-4517-b50a-56be26f43f19","type":"message","text":"```julia&gt; gradient(1, 2) do x, y\n         gradient(x, y) do x1, y1\n           g(x1, y1)\n         end |&gt; sum\n       end\n(4, 2)```\nworks like so","user":"UC4QQPG4A","ts":"1615901415.006400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"CPg","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> gradient(1, 2) do x, y\n         gradient(x, y) do x1, y1\n           g(x1, y1)\n         end |> sum\n       end\n(4, 2)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"works like so"}]}]}]},{"client_msg_id":"99d81496-a87f-4f5a-b546-c8188e10a1e0","type":"message","text":"hello! I was wondering, is it possible to get the model from the params?","user":"U01FTFACYJ0","ts":"1615912283.009500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HdJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hello! I was wondering, is it possible to get the model from the params?"}]}]}]},{"client_msg_id":"2a58fd46-024a-42be-b599-0fa3f9292998","type":"message","text":"Looking at the Zygote docs I got the impression that `pullback` merely runs a forward pass and constructs the pullback function (which once generated should be cached right?). Why does `pullback(f, x)` need more time than `f(x)` even after repeated runs?","user":"UBGQUAG8K","ts":"1616026820.002700","team":"T68168MUP","edited":{"user":"UBGQUAG8K","ts":"1616027470.000000"},"blocks":[{"type":"rich_text","block_id":"EFC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Looking at the Zygote docs I got the impression that "},{"type":"text","text":"pullback","style":{"code":true}},{"type":"text","text":" merely runs a forward pass and constructs the pullback function (which once generated should be cached right?). Why does "},{"type":"text","text":"pullback(f, x)","style":{"code":true}},{"type":"text","text":" need more time than "},{"type":"text","text":"f(x)","style":{"code":true}},{"type":"text","text":" even after repeated runs?"}]}]}]},{"type":"message","text":"Getting this error when I try to remove weights below some certain threshold.","files":[{"id":"F01RHFU1TQW","created":1616078785,"timestamp":1616078785,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01FTFACYJ0","editable":false,"size":34595,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01RHFU1TQW/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01RHFU1TQW/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01RHFU1TQW-41049cb3c4/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01RHFU1TQW-41049cb3c4/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01RHFU1TQW-41049cb3c4/image_360.png","thumb_360_w":360,"thumb_360_h":31,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01RHFU1TQW-41049cb3c4/image_480.png","thumb_480_w":480,"thumb_480_h":42,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01RHFU1TQW-41049cb3c4/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01RHFU1TQW-41049cb3c4/image_720.png","thumb_720_w":720,"thumb_720_h":63,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01RHFU1TQW-41049cb3c4/image_800.png","thumb_800_w":800,"thumb_800_h":70,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01RHFU1TQW-41049cb3c4/image_960.png","thumb_960_w":960,"thumb_960_h":84,"thumb_1024":"https://files.slack.com/files-tmb/T68168MUP-F01RHFU1TQW-41049cb3c4/image_1024.png","thumb_1024_w":1024,"thumb_1024_h":90,"original_w":1589,"original_h":139,"thumb_tiny":"AwAEADClRSCloGJSUUUCCiiigD//2Q==","permalink":"https://julialang.slack.com/files/U01FTFACYJ0/F01RHFU1TQW/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01RHFU1TQW-4df4efc334","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"otf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Getting this error when I try to remove weights below some certain threshold."}]}]}],"user":"U01FTFACYJ0","display_as_bot":false,"ts":"1616078877.005000","thread_ts":"1616078877.005000","reply_count":4,"reply_users_count":3,"latest_reply":"1616080508.006100","reply_users":["U01FTFACYJ0","UC4QQPG4A","U68A3ASP9"],"subscribed":false},{"client_msg_id":"b18150cf-b551-4fad-9891-b393ab1a82cf","type":"message","text":"Good morning! I wanted to ask, how can I compute the second derivative of the loss(x,y) with respect each parameter of my neural net ?","user":"U01FTFACYJ0","ts":"1616318442.010400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hsJaw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Good morning! I wanted to ask, how can I compute the second derivative of the loss(x,y) with respect each parameter of my neural net ?"}]}]}]},{"client_msg_id":"f7b1a9fa-1567-4477-96c3-85b9e636369c","type":"message","text":"Hey folks, I just had an informational question. What is actually the input and output signature of an LSTM layer in flux. For example in pytorch, I know that I can define an LSTM layer. Then the input is of size is in pytorch: `lstm(x, h0, c0)`, which when expanded out is\n```lstm(x::[batch_size, sequence_length, input_size], h0::[batch_size, num_layers * num_directions, hidden_size], c0::[batch_size, num_layers * num_directions, hidden_size])```\nThe output shape for pytorch is of equal detail. When I look at the Flux LSTM docs, the signature for the LSTM is\n\n```LSTM(in::Integer, out::Integer, σ = tanh)```\nSo I was just trying to reconcile my pytorch models with flux models, but I am not clear on the actual way that flux manages these hidden states and inputs.\nI think the tutorial on writing recurrent models is helpful, but I that tutorial does not show an end-to-end model and training, <https://fluxml.ai/Flux.jl/v0.3/models/recurrence.html>","user":"UDDSTBX19","ts":"1616348039.018400","team":"T68168MUP","edited":{"user":"UDDSTBX19","ts":"1616348072.000000"},"blocks":[{"type":"rich_text","block_id":"8mFtn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey folks, I just had an informational question. What is actually the input and output signature of an LSTM layer in flux. For example in pytorch, I know that I can define an LSTM layer. Then the input is of size is in pytorch: "},{"type":"text","text":"lstm(x, h0, c0)","style":{"code":true}},{"type":"text","text":", which when expanded out is\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"lstm(x::[batch_size, sequence_length, input_size], h0::[batch_size, num_layers * num_directions, hidden_size], c0::[batch_size, num_layers * num_directions, hidden_size])"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nThe output shape for pytorch is of equal detail. When I look at the Flux LSTM docs, the signature for the LSTM is\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"LSTM(in::Integer, out::Integer, σ = tanh)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nSo I was just trying to reconcile my pytorch models with flux models, but I am not clear on the actual way that flux manages these hidden states and inputs.\nI think the tutorial on writing recurrent models is helpful, but I that tutorial does not show an end-to-end model and training, "},{"type":"link","url":"https://fluxml.ai/Flux.jl/v0.3/models/recurrence.html"}]}]}]},{"client_msg_id":"2f8f62d6-b9e0-497a-8efe-b84b19202cab","type":"message","text":"Err, that is from v0.3","user":"UMY1LV01G","ts":"1616351693.018900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iTlG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Err, that is from v0.3"}]}]}]},{"client_msg_id":"afcf0693-c153-4609-850d-f03a8bc798f8","type":"message","text":"Look at <https://fluxml.ai/Flux.jl/dev/|https://fluxml.ai/Flux.jl/dev/> instead","user":"UMY1LV01G","ts":"1616351773.019500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7+Cz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Look at "},{"type":"link","url":"https://fluxml.ai/Flux.jl/dev/","text":"https://fluxml.ai/Flux.jl/dev/"},{"type":"text","text":" instead"}]}]}]},{"client_msg_id":"91a1c5d2-4a97-4ad3-a974-c21baeef8157","type":"message","text":"<@UMY1LV01G> Oh thanks. I will take a look at those docs instead. I can see that they look a little better. I was still a little confused about this point though.\n```using Flux\n\nrnn = Flux.RNNCell(2, 5)\n\nx = rand(Float32, 2) # dummy data\nh = rand(Float32, 5)  # initial hidden state\n\nh, y = rnn(h, x)```\nSo where does `y` come from, which seems to be the prediction for that time step. Is `y` simply a `Dense`  layer that takes in `h` and outputs the predicted value of the sequence?","user":"UDDSTBX19","ts":"1616360653.021700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"awa","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UMY1LV01G"},{"type":"text","text":" Oh thanks. I will take a look at those docs instead. I can see that they look a little better. I was still a little confused about this point though.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Flux\n\nrnn = Flux.RNNCell(2, 5)\n\nx = rand(Float32, 2) # dummy data\nh = rand(Float32, 5)  # initial hidden state\n\nh, y = rnn(h, x)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"So where does "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" come from, which seems to be the prediction for that time step. Is "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" simply a "},{"type":"text","text":"Dense","style":{"code":true}},{"type":"text","text":"  layer that takes in "},{"type":"text","text":"h","style":{"code":true}},{"type":"text","text":" and outputs the predicted value of the sequence?"}]}]}]},{"client_msg_id":"dad13d10-ff30-4ef7-a98f-c5b29fb19c39","type":"message","text":"You said it yourself, y is the output","user":"UMY1LV01G","ts":"1616402560.022300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Rk0lu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You said it yourself, y is the output"}]}]}]},{"client_msg_id":"76bca2ac-30d7-4e95-b8e8-733c20054300","type":"message","text":"Morning! I'm trying to prune my NN removing (freezing) the smallest weights with the delete! function. But I noticed that after the pruning step the network accuracy doesn't increase it seems like all the parameters are frozen and the network is not able to learn anymore.\n\n`function one_shot_pruning(model,epochs::Tuple{Int64,Int64},loss::Function,accuracy::Function,data,opt,threshold::Real,X,Y, is_percentage::Bool = false)`\n    `evalcb = () -&gt; println(\"removed edges : \",compute_zero_entries(model), \"\\n  accuracy : \",accuracy(X,Y,model))  #exit training loop`    \n    `@Flux.epochs epochs[1] Flux.train!(loss, Flux.params(model), data, opt,cb = throttle(evalcb, 10))`    \n    `println(\"\\nPRUNING \",'='^40,\"\\n\")`\n    `model = cpu(model)`\n    `if(is_percentage)`\n      `threshold = compute_percentile_threshold(model,threshold)`\n    `end`\n    `ps =prune(model,threshold)  #PRUNING STEP`\n    `model = gpu(model)`\n    `@Flux.epochs epochs[2] Flux.train!(loss, ps, data, opt,cb = throttle(evalcb, 10))`    \n    `#@Flux.epochs epochs[2] my_custom_train!(loss,ps,data,opt,X,Y)`\n    `return model`\n  `end`\n\n\n\n  `function prune(model,threshold::Real)`\n    `for current_layer ∈ model[1:end-1]  #assume last layer softams, we skip this`\n      `current_weights = Flux.params(current_layer)[1]  #current weights is a matrix`\n      `mask = broadcast(abs,current_weights).&lt;= threshold`\n      `current_weights = current_weights[mask] .= 0`\n      `for p in current_weights`\n        `if p == 0`\n          `delete!(Flux.params(model), p)`\n        `end` \n      `end`\n    `end`\n    `return Flux.params(model)`   \n  `end`","user":"U01FTFACYJ0","ts":"1616406406.027100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yGy34","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Morning! I'm trying to prune my NN removing (freezing) the smallest weights with the delete! function. But I noticed that after the pruning step the network accuracy doesn't increase it seems like all the parameters are frozen and the network is not able to learn anymore.\n\n"},{"type":"text","text":"function one_shot_pruning(model,epochs::Tuple{Int64,Int64},loss::Function,accuracy::Function,data,opt,threshold::Real,X,Y, is_percentage::Bool = false)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    evalcb = () -> println(\"removed edges : \",compute_zero_entries(model), \"\\n  accuracy : \",accuracy(X,Y,model))  #exit training loop    ","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    @Flux.epochs epochs[1] Flux.train!(loss, Flux.params(model), data, opt,cb = throttle(evalcb, 10))    ","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    println(\"\\nPRUNING \",'='^40,\"\\n\")","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    model = cpu(model)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    if(is_percentage)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"      threshold = compute_percentile_threshold(model,threshold)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    end","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    ps =prune(model,threshold)  #PRUNING STEP","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    model = gpu(model)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    @Flux.epochs epochs[2] Flux.train!(loss, ps, data, opt,cb = throttle(evalcb, 10))    ","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    #@Flux.epochs epochs[2] my_custom_train!(loss,ps,data,opt,X,Y)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    return model","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"  end","style":{"code":true}},{"type":"text","text":"\n\n\n\n"},{"type":"text","text":"  function prune(model,threshold::Real)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    for current_layer ∈ model[1:end-1]  #assume last layer softams, we skip this","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"      current_weights = Flux.params(current_layer)[1]  #current weights is a matrix","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"      mask = broadcast(abs,current_weights).<= threshold","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"      current_weights = current_weights[mask] .= 0","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"      for p in current_weights","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"        if p == 0","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"          delete!(Flux.params(model), p)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"        end ","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"      end","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    end","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    return Flux.params(model)   ","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"  end","style":{"code":true}}]}]}]},{"client_msg_id":"fc1cf1fe-5a88-486f-80c0-5c6da3227c75","type":"message","text":"Anyone seen an implementation of cyclical momentum/Leslie Smith’s 1cycle learning method (<https://arxiv.org/abs/1803.09820>) I saw this tweet that they have it in <http://fast.ai|fast.ai> (<https://twitter.com/jeremyphoward/status/981928159879749632?lang=en>) and was wondering if there is anything similar in Flux","user":"UAPLG1B7H","ts":"1616456334.029700","team":"T68168MUP","attachments":[{"service_name":"arXiv.org","title":"A disciplined approach to neural network hyper-parameters: Part 1...","title_link":"https://arxiv.org/abs/1803.09820","text":"Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring...","fallback":"arXiv.org: A disciplined approach to neural network hyper-parameters: Part 1...","from_url":"https://arxiv.org/abs/1803.09820","service_icon":"https://static.arxiv.org/static/browse/0.3.2.6/images/icons/favicon.ico","id":1,"original_url":"https://arxiv.org/abs/1803.09820"},{"fallback":"<https://twitter.com/jeremyphoward|@jeremyphoward>: With the addition of cyclical momentum (thanks to <https://twitter.com/GuggerSylvain|@GuggerSylvain>) fastai is now the first library to fully integrate Leslie Smith's 1cycle learning method, which means you can train your model 5x faster\n<https://arxiv.org/abs/1803.09820>\n<https://github.com/fastai/fastai/pull/292/files> <https://pbs.twimg.com/media/DaCCatdUQAASqqs.jpg>","ts":1522944891,"author_name":"Jeremy Howard","author_link":"https://twitter.com/jeremyphoward/status/981928159879749632","author_icon":"https://pbs.twimg.com/profile_images/1279600070145437696/eocLhSLu_normal.jpg","author_subname":"@jeremyphoward","text":"With the addition of cyclical momentum (thanks to <https://twitter.com/GuggerSylvain|@GuggerSylvain>) fastai is now the first library to fully integrate Leslie Smith's 1cycle learning method, which means you can train your model 5x faster\n<https://arxiv.org/abs/1803.09820>\n<https://github.com/fastai/fastai/pull/292/files> <https://pbs.twimg.com/media/DaCCatdUQAASqqs.jpg>","service_name":"twitter","service_url":"https://twitter.com/","from_url":"https://twitter.com/jeremyphoward/status/981928159879749632?lang=en","image_url":"https://pbs.twimg.com/media/DaCCatdUQAASqqs.jpg","image_width":751,"image_height":439,"image_bytes":31179,"id":2,"original_url":"https://twitter.com/jeremyphoward/status/981928159879749632?lang=en","footer":"Twitter","footer_icon":"https://a.slack-edge.com/80588/img/services/twitter_pixel_snapped_32.png"}],"blocks":[{"type":"rich_text","block_id":"S8v0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Anyone seen an implementation of cyclical momentum/Leslie Smith’s 1cycle learning method ("},{"type":"link","url":"https://arxiv.org/abs/1803.09820"},{"type":"text","text":") I saw this tweet that they have it in "},{"type":"link","url":"http://fast.ai","text":"fast.ai"},{"type":"text","text":" ("},{"type":"link","url":"https://twitter.com/jeremyphoward/status/981928159879749632?lang=en"},{"type":"text","text":") and was wondering if there is anything similar in Flux"}]}]}],"thread_ts":"1616456334.029700","reply_count":3,"reply_users_count":2,"latest_reply":"1616457841.032700","reply_users":["UH9KWTTD3","UAPLG1B7H"],"subscribed":false},{"client_msg_id":"5a015ce6-2071-47aa-a540-0b7fd5e71c5a","type":"message","text":"How can I add a final softmax computation defining the net in this way ?\n\n```function linear(in, out)\n  W = randn(out, in)\n  b = randn(out)\n  x -&gt; W * x .+ b\nend\n\nlinear1 = linear(5, 3) # we can access linear1.W etc\nlinear2 = linear(3, 2)\n\nmodel(x) = linear2(σ.(linear1(x)))\n\nmodel(rand(5))```\n","user":"U01FTFACYJ0","ts":"1616576556.034300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ry=Jn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I add a final softmax computation defining the net in this way ?\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function linear(in, out)\n  W = randn(out, in)\n  b = randn(out)\n  x -> W * x .+ b\nend\n\nlinear1 = linear(5, 3) # we can access linear1.W etc\nlinear2 = linear(3, 2)\n\nmodel(x) = linear2(σ.(linear1(x)))\n\nmodel(rand(5))"}]},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"1d01a3dd-f91f-4bb3-8c62-9d9223a28630","type":"message","text":"`softmax(model(x))` not good?","user":"UC4QQPG4A","ts":"1616580411.034900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KSYQa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"softmax(model(x))","style":{"code":true}},{"type":"text","text":" not good?"}]}]}],"thread_ts":"1616580411.034900","reply_count":2,"reply_users_count":1,"latest_reply":"1616581579.035200","reply_users":["U01FTFACYJ0"],"is_locked":false,"subscribed":false},{"client_msg_id":"8b36322b-3a04-4456-9cce-ee815bb8fac3","type":"message","text":"How can I remove the bias from a Dense layer? So far I haven’t been able to trigger the “bias” keyword that appears in the documentation:\n```Dense(in, out, σ=identity; bias=true, init=glorot_uniform)```\n","user":"U01QV28FEHL","ts":"1616597338.038600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9K5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I remove the bias from a Dense layer? So far I haven’t been able to trigger the “bias” keyword that appears in the documentation:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Dense(in, out, σ=identity; bias=true, init=glorot_uniform)"}]},{"type":"rich_text_section","elements":[]}]}],"thread_ts":"1616597338.038600","reply_count":2,"reply_users_count":1,"latest_reply":"1616598318.038900","reply_users":["UH9KWTTD3"],"is_locked":false,"subscribed":false},{"client_msg_id":"07bfe02b-2ee4-4abc-b7ba-45955cd64208","type":"message","text":"Hi, just met with a question, seems that the LSTM of Flux only support 1-dimension input. It's okay to take `[t1 t2 t3]` as a time-series data. Is that possible to input an array of array like `[[t11 t12] [t21 t22] [t31 t32]]` to a LSTM of Flux?","user":"U01977X150R","ts":"1616684673.044300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HOPp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, just met with a question, seems that the LSTM of Flux only support 1-dimension input. It's okay to take"},{"type":"text","text":" [t1 t2 t3] ","style":{"code":true}},{"type":"text","text":"as a time-series data. Is that possible to input an array of array like "},{"type":"text","text":"[[t11 t12] [t21 t22] [t31 t32]]","style":{"code":true}},{"type":"text","text":" to a LSTM of Flux?"}]}]}]},{"client_msg_id":"f2ed2c0e-8752-4535-a1ee-6d4158aeb38e","type":"message","text":"That was removed recently iirc","user":"UC4QQPG4A","ts":"1616685167.044800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sxy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That was removed recently iirc"}]}]}]},{"client_msg_id":"3C923EEA-24FB-462D-8383-B288ACF2274D","type":"message","text":"Is there a zoom call about ml and ad development?","user":"URE70HE7R","ts":"1616686017.045500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"h70b","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a zoom call about ml and ad development?"}]}]}]},{"client_msg_id":"32798961-3062-46b6-acc7-434f601eab1e","type":"message","text":"Yes <https://mit.zoom.us/j/96251790289?pwd=DbktOdGJGWHpUTHVmVGl3dEpZNGZTUT09>","user":"UC4QQPG4A","ts":"1616686083.045800","team":"T68168MUP","edited":{"user":"UC4QQPG4A","ts":"1616686156.000000"},"blocks":[{"type":"rich_text","block_id":"svO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes "},{"type":"link","url":"https://mit.zoom.us/j/96251790289?pwd=DbktOdGJGWHpUTHVmVGl3dEpZNGZTUT09"}]}]}]},{"client_msg_id":"3E5B3B9C-8E0F-42E1-A28D-DBEE8307FCD5","type":"message","text":"I need to report a bug about Julia/flux/zygote","user":"URE70HE7R","ts":"1616686113.046500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GqT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I need to report a bug about Julia/flux/zygote"}]}]}]},{"client_msg_id":"f0b81deb-611e-4e02-b8e8-d323d862d890","type":"message","text":"Yes this is the place","user":"UC4QQPG4A","ts":"1616686160.047200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"BMZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes this is the place"}]}]}]},{"client_msg_id":"8d5d17c3-2ae3-4104-9d3e-9662ca5696f7","type":"message","text":"It happens now :slightly_smiling_face:","user":"UC4QQPG4A","ts":"1616686210.048000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"n0qW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It happens now "},{"type":"emoji","name":"slightly_smiling_face"}]}]}]},{"client_msg_id":"414b8097-2925-4e9e-bb43-714c4592ec0d","type":"message","text":"<@UC4QQPG4A> Julia/Zygote leaks memory","user":"URE70HE7R","ts":"1616686233.048300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/RUC","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UC4QQPG4A"},{"type":"text","text":" Julia/Zygote leaks memory"}]}]}]},{"client_msg_id":"8045a2d9-0abc-4a20-b3cc-1b89ccd86d9b","type":"message","text":"i can tell you more in the call","user":"URE70HE7R","ts":"1616686246.048700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lp9H","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"i can tell you more in the call"}]}]}]},{"client_msg_id":"45944882-bd47-4e6b-9dbc-135c2e5b474e","type":"message","text":"You can join the call I linked to now :slightly_smiling_face:","user":"UC4QQPG4A","ts":"1616686275.049300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uGjp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You can join the call I linked to now "},{"type":"emoji","name":"slightly_smiling_face"}]}]}]},{"client_msg_id":"220bdb84-d669-4215-8cc9-934a2082b88a","type":"message","text":"the meeting has a passcode","user":"URE70HE7R","ts":"1616686288.049500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"i4S","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"the meeting has a passcode"}]}]}]},{"client_msg_id":"96848510-71ee-49b4-bf84-611b6a7607e4","type":"message","text":"<https://mit.zoom.us/j/96251790289?pwd=bktOdGJGWHpUTHVmVGl3dEpZNGZTUT09>","user":"UC4QQPG4A","ts":"1616686316.049800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0/Ywn","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://mit.zoom.us/j/96251790289?pwd=bktOdGJGWHpUTHVmVGl3dEpZNGZTUT09"}]}]}]},{"client_msg_id":"55fae273-1377-46b9-916f-9340af6a028a","type":"message","text":"Oops, fixed","user":"UC4QQPG4A","ts":"1616686322.050000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5Iz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oops, fixed"}]}]}]},{"client_msg_id":"1474a4d6-b245-4319-ba61-66416e9ef968","type":"message","text":"ok, i’m in","user":"URE70HE7R","ts":"1616686333.050300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"z6Mh2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ok, i’m in"}]}]}]},{"client_msg_id":"ccd5fdf1-c45a-4b77-a86f-8bd2caa2cb0b","type":"message","text":"Hi I'm trying to train a LSTM to predict a value based upon last 5 records for obs`i`.  I just use the sum of mse as my loss function and `trX`  is my records, `trY` is my labels in training sets.\n```function LSTMTest()\nlt = Chain(LSTM(5,1),Dense(1,1)) |&gt;gpu\n    function trial(i)\n        pre = lt(trX[i])\n        return pre\n    end  \nend\n\ntx = ty =  [i for i=1:10];\ntrainData = Flux.Data.DataLoader(tx,ty,shuffle=true,batchsize=10)\nm = LSTMTest()\nloss(x,y) = sum(mse.(m.(x),trY[y]))```\nHowever, when training this very simple LSTM, I got this warning which confuse me a lot. I was wondering whether I missed anything? Many thanks.\n```┌ Warning: calls to Base intrinsics might be GPU incompatible\n│   exception = (GPUCompiler.MethodSubstitutionWarning(log(x::Float64) in Base.Math at special/log.jl:253, log(x::Float64) in CUDA at /home/zq/.julia/packages/CUDA/wTQsK/src/device/intrinsics/math.jl:72), Base.StackTraces.StackFrame[log at log.jl:253, broadcast_kernel at broadcast.jl:59])\n└ @ GPUCompiler /home/zq/.julia/packages/GPUCompiler/uTpNx/src/irgen.jl:68\nGPU compilation of kernel broadcast_kernel(CUDA.CuKernelContext, CuDeviceArray{ForwardDiff.Dual{Nothing,Float32,2},1,1}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},Zygote.var\"#1079#1082\"{typeof(mse)},Tuple{Base.Broadcast.Extruded{Array{CuArray{Float32,1},1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}}}}, Int64) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},Zygote.var\"#1079#1082\"{typeof(mse)},Tuple{Base.Broadcast.Extruded{Array{CuArray{Float32,1},1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}}}}, which is not isbits:\n  .args is of type Tuple{Base.Broadcast.Extruded{Array{CuArray{Float32,1},1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}}} which is not isbits.\n    .1 is of type Base.Broadcast.Extruded{Array{CuArray{Float32,1},1},Tuple{Bool},Tuple{Int64}} which is not isbits.\n      .x is of type Array{CuArray{Float32,1},1} which is not isbits.```","user":"U01977X150R","ts":"1616734521.056300","team":"T68168MUP","edited":{"user":"U01977X150R","ts":"1616735188.000000"},"blocks":[{"type":"rich_text","block_id":"mK2R","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi I'm trying to train a LSTM to predict a value based upon last 5 records for obs"},{"type":"text","text":"i","style":{"code":true}},{"type":"text","text":".  I just use the sum of mse as my loss function and "},{"type":"text","text":"trX","style":{"code":true}},{"type":"text","text":"  is my records, "},{"type":"text","text":"trY","style":{"code":true}},{"type":"text","text":" is my labels in training sets.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function LSTMTest()\nlt = Chain(LSTM(5,1),Dense(1,1)) |>gpu\n    function trial(i)\n        pre = lt(trX[i])\n        return pre\n    end  \nend\n\ntx = ty =  [i for i=1:10];\ntrainData = Flux.Data.DataLoader(tx,ty,shuffle=true,batchsize=10)\nm = LSTMTest()\nloss(x,y) = sum(mse.(m.(x),trY[y]))"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"However, when training this very simple LSTM, I got this warning which confuse me a lot. I was wondering whether I missed anything? Many thanks.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"┌ Warning: calls to Base intrinsics might be GPU incompatible\n│   exception = (GPUCompiler.MethodSubstitutionWarning(log(x::Float64) in Base.Math at special/log.jl:253, log(x::Float64) in CUDA at /home/zq/.julia/packages/CUDA/wTQsK/src/device/intrinsics/math.jl:72), Base.StackTraces.StackFrame[log at log.jl:253, broadcast_kernel at broadcast.jl:59])\n└ @ GPUCompiler /home/zq/.julia/packages/GPUCompiler/uTpNx/src/irgen.jl:68\nGPU compilation of kernel broadcast_kernel(CUDA.CuKernelContext, CuDeviceArray{ForwardDiff.Dual{Nothing,Float32,2},1,1}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},Zygote.var\"#1079#1082\"{typeof(mse)},Tuple{Base.Broadcast.Extruded{Array{CuArray{Float32,1},1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}}}}, Int64) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},Zygote.var\"#1079#1082\"{typeof(mse)},Tuple{Base.Broadcast.Extruded{Array{CuArray{Float32,1},1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}}}}, which is not isbits:\n  .args is of type Tuple{Base.Broadcast.Extruded{Array{CuArray{Float32,1},1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}}} which is not isbits.\n    .1 is of type Base.Broadcast.Extruded{Array{CuArray{Float32,1},1},Tuple{Bool},Tuple{Int64}} which is not isbits.\n      .x is of type Array{CuArray{Float32,1},1} which is not isbits."}]}]}],"thread_ts":"1616734521.056300","reply_count":1,"reply_users_count":1,"latest_reply":"1616735982.056500","reply_users":["UMY1LV01G"],"is_locked":false,"subscribed":false},{"client_msg_id":"7ad3d8a6-af2f-46bf-b4b2-3e3766921f67","type":"message","text":"Hi I'm still training some LSTM models. And I just find the LSTM of Flux might failed to calculate the gradient when I vectorize my data. Here's a toy with 5 series of 5 labels to predict 2 corresponding responses.\n```trainX = [rand(5,5)[i,:] for i=1:5]|&gt;gpu\ntrainY = [rand(5,2)[i,:] for i=1:5]|&gt;gpu\nmyModel  = Flux.LSTM(5,2) |&gt;gpu\nparams = Flux.params(myModel)    \nopt =  ADAM()\nloss(x,y) =sum(mse.(trainY[y],myModel.(trainX[x])))\nx1 = y1 = [i for i=1:5]\ntrainData = Flux.Data.DataLoader(x1,y1,shuffle=true,batchsize=5)  \n\nFlux.train!(loss,params,trainData,ADAM())```\nThen I would got following error.\n\n```ArgumentError: NamedTuple{(:σ, :Wi, :Wh, :b, :h, :c),Tuple{Nothing,LinearAlgebra.Transpose{Float32,CuArray{Float32,2}},LinearAlgebra.Transpose{Float32,CuArray{Float32,2}},CuArray{Float32,1},Nothing,Nothing}} keys must be a subset of NamedTuple{(:Wi, :Wh, :b, :h, :c),NTuple{5,Nothing}} keys```\nThe model works fine if I replace the LSTM with RNN cell and I must miss something. Many thanks.","user":"U01977X150R","ts":"1616991525.063200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2fw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi I'm still training some LSTM models. And I just find the LSTM of Flux might failed to calculate the gradient when I vectorize my data. Here's a toy with 5 series of 5 labels to predict 2 corresponding responses.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"trainX = [rand(5,5)[i,:] for i=1:5]|>gpu\ntrainY = [rand(5,2)[i,:] for i=1:5]|>gpu\nmyModel  = Flux.LSTM(5,2) |>gpu\nparams = Flux.params(myModel)    \nopt =  ADAM()\nloss(x,y) =sum(mse.(trainY[y],myModel.(trainX[x])))\nx1 = y1 = [i for i=1:5]\ntrainData = Flux.Data.DataLoader(x1,y1,shuffle=true,batchsize=5)  \n\nFlux.train!(loss,params,trainData,ADAM())"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Then I would got following error.\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ArgumentError: NamedTuple{(:σ, :Wi, :Wh, :b, :h, :c),Tuple{Nothing,LinearAlgebra.Transpose{Float32,CuArray{Float32,2}},LinearAlgebra.Transpose{Float32,CuArray{Float32,2}},CuArray{Float32,1},Nothing,Nothing}} keys must be a subset of NamedTuple{(:Wi, :Wh, :b, :h, :c),NTuple{5,Nothing}} keys"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"The model works fine if I replace the LSTM with RNN cell and I must miss something. Many thanks."}]}]}],"thread_ts":"1616991525.063200","reply_count":1,"reply_users_count":1,"latest_reply":"1616991564.063300","reply_users":["U01977X150R"],"is_locked":false,"subscribed":false},{"client_msg_id":"5861fc06-7ad8-4fb9-8948-317047190b23","type":"message","text":"I’m thinking of playing with the Vision Transformer for a class project. It probably isn’t going to be very practical to try to do it in Julia, right?\nEdit: it looks like there are pretrained ViT pytorch models, so maybe I can just use pytorch.jl to load them?","user":"US8V7JSKB","ts":"1617133858.066300","team":"T68168MUP","edited":{"user":"US8V7JSKB","ts":"1617135262.000000"},"blocks":[{"type":"rich_text","block_id":"ov=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’m thinking of playing with the Vision Transformer for a class project. It probably isn’t going to be very practical to try to do it in Julia, right?\nEdit: it looks like there are pretrained ViT pytorch models, so maybe I can just use pytorch.jl to load them?"}]}]}],"thread_ts":"1617133858.066300","reply_count":1,"reply_users_count":1,"latest_reply":"1617135499.066500","reply_users":["UH9KWTTD3"],"is_locked":false,"subscribed":false},{"type":"message","text":"hey. does somebody also have problems with BSON + Flux in julia 1.6? I'm getting errors that i hadn't before","user":"UBQ05LZ52","ts":"1617297411.070000","team":"T68168MUP"},{"client_msg_id":"f93929b3-094e-427a-bb1d-16dca0aa3c17","type":"message","text":"Do you have reproducers?","user":"UC4QQPG4A","ts":"1617297609.070300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OGkmd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do you have reproducers?"}]}]}],"thread_ts":"1617297609.070300","reply_count":7,"reply_users_count":1,"latest_reply":"1617298502.072600","reply_users":["UBQ05LZ52"],"is_locked":false,"subscribed":false},{"client_msg_id":"bee120f4-f4c8-433a-84c3-01003ad45df1","type":"message","text":"Is it specifically with 1.6 (ie are you able to run things fine on 1.5?)","user":"UC4QQPG4A","ts":"1617297663.071300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yPo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is it specifically with 1.6 (ie are you able to run things fine on 1.5?)"}]}]}]},{"type":"message","subtype":"thread_broadcast","text":"on a somewhat different, though related, topic: I was just looking at <https://thinc.ai/docs> and was very taken by how it’s apparently possible to “switch between PyTorch, TensorFlow and MXNet models without changing your application, or even create mutant hybrids using zero-copy array interchange.” I know this isn’t currently possible in Julia, but is _theoretically_ possible?","user":"US8V7JSKB","ts":"1617351365.077900","thread_ts":"1617133858.066300","root":{"client_msg_id":"5861fc06-7ad8-4fb9-8948-317047190b23","type":"message","text":"I’m thinking of playing with the Vision Transformer for a class project. It probably isn’t going to be very practical to try to do it in Julia, right?\nEdit: it looks like there are pretrained ViT pytorch models, so maybe I can just use pytorch.jl to load them?","user":"US8V7JSKB","ts":"1617133858.066300","team":"T68168MUP","edited":{"user":"US8V7JSKB","ts":"1617135262.000000"},"blocks":[{"type":"rich_text","block_id":"ov=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’m thinking of playing with the Vision Transformer for a class project. It probably isn’t going to be very practical to try to do it in Julia, right?\nEdit: it looks like there are pretrained ViT pytorch models, so maybe I can just use pytorch.jl to load them?"}]}]}],"thread_ts":"1617133858.066300","reply_count":9,"reply_users_count":4,"latest_reply":"1617351365.077900","reply_users":["UH9KWTTD3","US8V7JSKB","U6YRZ18GZ","U69F2VCFJ"],"is_locked":false,"subscribed":false},"attachments":[{"service_name":"Thinc","title":"Introduction · Thinc  · A refreshing functional take on deep learning","title_link":"https://thinc.ai/docs","text":"Thinc is a lightweight type-checked deep learning library for composing models, with support for layers defined in frameworks like PyTorch and TensorFlow.","fallback":"Thinc: Introduction · Thinc  · A refreshing functional take on deep learning","image_url":"https://thinc.ai/static/social-ae90868b0b884a7e9c7f0be4ae73342d.jpg","from_url":"https://thinc.ai/docs","image_width":476,"image_height":250,"image_bytes":224483,"service_icon":"https://thinc.ai/icons/icon-48x48.png?v=955d2b4f3777d59a83a3c1b75d52903f","id":1,"original_url":"https://thinc.ai/docs"}],"blocks":[{"type":"rich_text","block_id":"F7=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"on a somewhat different, though related, topic: I was just looking at "},{"type":"link","url":"https://thinc.ai/docs"},{"type":"text","text":" and was very taken by how it’s apparently possible to “switch between PyTorch, TensorFlow and MXNet models without changing your application, or even create mutant hybrids using zero-copy array interchange.” I know this isn’t currently possible in Julia, but is "},{"type":"text","text":"theoretically","style":{"italic":true}},{"type":"text","text":" possible?"}]}]}],"client_msg_id":"f90c1bfa-bffc-4ea5-b329-213d1d70cc5b"}]}