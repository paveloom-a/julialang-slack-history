{"cursor": 1, "messages": [{"client_msg_id":"82cbac36-6aa2-4d9d-8e67-e7dc1151dfb0","type":"message","text":"I think this used to be possible but I am getting \"Can't differentiate foreigncall expression\" now?","user":"UDVQM5U1Z","ts":"1613677424.155600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zhT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think this used to be possible but I am getting \"Can't differentiate foreigncall expression\" now?"}]}]}],"thread_ts":"1613677424.155600","reply_count":9,"reply_users_count":2,"latest_reply":"1613678608.158300","reply_users":["UH9KWTTD3","UDVQM5U1Z"],"subscribed":false},{"client_msg_id":"e8db1518-5a59-4900-a896-733a6d88cdbc","type":"message","text":"Hi I'm try the `ExpDecay` of Flux and seems it doesn't work.\n`opt = Flux.Optimiser(ExpDecay(0.001, 0.1, 1, 1e-4), Descent())`\n`for i=1:10`\n    `println(opt.os[2].eta)`\n    `Flux.train!(loss,θ,trainData,opt)`     \n`end`\nHere's my toy example. I thought the learning rate should decrease by 0.1 after each training following the document, but the toy return me same learning rate for 10 training. Am I missing anything here?","user":"U01977X150R","ts":"1613872391.004000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dxd3z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi I'm try the "},{"type":"text","text":"ExpDecay","style":{"code":true}},{"type":"text","text":" of Flux and seems it doesn't work.\n"},{"type":"text","text":"opt = Flux.Optimiser(ExpDecay(0.001, 0.1, 1, 1e-4), Descent())","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"for i=1:10","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    println(opt.os[2].eta)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    Flux.train!(loss,θ,trainData,opt)     ","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"end","style":{"code":true}},{"type":"text","text":"\nHere's my toy example. I thought the learning rate should decrease by 0.1 after each training following the document, but the toy return me same learning rate for 10 training. Am I missing anything here?"}]}]}],"thread_ts":"1613872391.004000","reply_count":3,"reply_users_count":2,"latest_reply":"1613874113.005400","reply_users":["UH9KWTTD3","UMY1LV01G"],"subscribed":false},{"client_msg_id":"610b85f4-b57c-46e8-8221-b2cb304af9df","type":"message","text":"I’m looking at implementing a “data front-end” for the MLJ implementation of some Flux models (<https://alan-turing-institute.github.io/MLJ.jl/dev/adding_models_for_general_use/#Implementing-a-data-front-end-1>). Doing so will avoid some copying that currently happens when one retrains a model after changing hyper-parameters (among other things). Currently we use `Flux.train!` which assumes features/inputs X and  labels/target y observations are “zipped”. So, in a simple example, `train!` is called on `[(X1, y1), (X2, y2), …]`  This “conflation” of inputs and target occurs nowhere else I know of in the MLJ ecosystem, and is currently a deal-breaker as far as a data front-end for MLJFlux. My question: *is there an essential performance reason for zipping the data?* Or can I just keep the two steams separate and replace `Flux.train!` with a custom version. <@UMY1LV01G> <@U8RHPM4KF>","user":"UD0SQV5LL","ts":"1614056466.020600","team":"T68168MUP","edited":{"user":"UD0SQV5LL","ts":"1614056813.000000"},"blocks":[{"type":"rich_text","block_id":"5V7v","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’m looking at implementing a “data front-end” for the MLJ implementation of some Flux models ("},{"type":"link","url":"https://alan-turing-institute.github.io/MLJ.jl/dev/adding_models_for_general_use/#Implementing-a-data-front-end-1"},{"type":"text","text":"). Doing so will avoid some copying that currently happens when one retrains a model after changing hyper-parameters (among other things). Currently we use "},{"type":"text","text":"Flux.train!","style":{"code":true}},{"type":"text","text":" which assumes features/inputs X and  labels/target y observations are “zipped”. So, in a simple example, "},{"type":"text","text":"train!","style":{"code":true}},{"type":"text","text":" is called on "},{"type":"text","text":"[(X1, y1), (X2, y2), …]","style":{"code":true}},{"type":"text","text":"  This “conflation” of inputs and target occurs nowhere else I know of in the MLJ ecosystem, and is currently a deal-breaker as far as a data front-end for MLJFlux. My question: "},{"type":"text","text":"is there an essential performance reason for zipping the data?","style":{"bold":true}},{"type":"text","text":" Or can I just keep the two steams separate and replace "},{"type":"text","text":"Flux.train!","style":{"code":true}},{"type":"text","text":" with a custom version. "},{"type":"user","user_id":"UMY1LV01G"},{"type":"text","text":" "},{"type":"user","user_id":"U8RHPM4KF"}]}]}]},{"client_msg_id":"418c13df-5271-4931-8525-604d94376b42","type":"message","text":"You can definitely replace that with a custom version. The thinking behind this design is that rather than thinking of the “minibatches” as zipped data and labels, it is a tuple of arguments to a function that is to be differentiated. There is no requirement for that, but outside of straightforward dl examples, we prolly want arguments to the objective function other than the data and labels","user":"UC4QQPG4A","ts":"1614059231.023400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"c/=7w","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You can definitely replace that with a custom version. The thinking behind this design is that rather than thinking of the “minibatches” as zipped data and labels, it is a tuple of arguments to a function that is to be differentiated. There is no requirement for that, but outside of straightforward dl examples, we prolly want arguments to the objective function other than the data and labels"}]}]}]},{"client_msg_id":"25db3af1-f515-4902-be8a-3261dc422629","type":"message","text":"`Flux.train!` is more of an end-user function than anything else and hardly the pinnacle of performance optimization, so I think it's safe to circumvent :slightly_smiling_face:","user":"UMY1LV01G","ts":"1614065379.025000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fc0Tn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Flux.train!","style":{"code":true}},{"type":"text","text":" is more of an end-user function than anything else and hardly the pinnacle of performance optimization, so I think it's safe to circumvent "},{"type":"emoji","name":"slightly_smiling_face"}]}]}]},{"client_msg_id":"1faf0dab-cf0b-473d-b2ef-0090166007ab","type":"message","text":"<@UC4QQPG4A>'s point about carrying through data that isn't strictly model inputs or targets is important though. One reason something like <http://Fast.ai|Fast.ai> is so frustrating to extend is because it doesn't allow for this and forces you to sneak metadata through in unique ways.","user":"UMY1LV01G","ts":"1614065471.026600","team":"T68168MUP","edited":{"user":"UMY1LV01G","ts":"1614065485.000000"},"blocks":[{"type":"rich_text","block_id":"lPuo","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UC4QQPG4A"},{"type":"text","text":"'s point about carrying through data that isn't strictly model inputs or targets is important though. One reason something like "},{"type":"link","url":"http://Fast.ai","text":"Fast.ai"},{"type":"text","text":" is so frustrating to extend is because it doesn't allow for this and forces you to sneak metadata through in unique ways."}]}]}]},{"client_msg_id":"9e0303c7-5be7-438c-b987-2c38ad705bba","type":"message","text":"Not relying on `zip` also opens up the possibility to make full use of smarter data containers like DataLoaders.jl (which supports random access loading on demand)","user":"UMY1LV01G","ts":"1614065663.028100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HTc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not relying on "},{"type":"text","text":"zip","style":{"code":true}},{"type":"text","text":" also opens up the possibility to make full use of smarter data containers like DataLoaders.jl (which supports random access loading on demand)"}]}]}]},{"client_msg_id":"84ab4094-d0c3-43f8-828a-213499e07644","type":"message","text":"I think it makes sense to rely on `Flux.train!` for the simple reason that any clever tricks you might want to do would end up as implementation details of the dataloader itself, not of the loop specifically. I think <https://github.com/FluxML/Flux.jl/pull/1471> is a net positive. It actually makes the callback system more robust, to mess with the loop outside of the cases we serve usually","user":"UC4QQPG4A","ts":"1614066298.032600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WLz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think it makes sense to rely on "},{"type":"text","text":"Flux.train!","style":{"code":true}},{"type":"text","text":" for the simple reason that any clever tricks you might want to do would end up as implementation details of the dataloader itself, not of the loop specifically. I think "},{"type":"link","url":"https://github.com/FluxML/Flux.jl/pull/1471"},{"type":"text","text":" is a net positive. It actually makes the callback system more robust, to mess with the loop outside of the cases we serve usually"}]}]}]},{"client_msg_id":"4a85d3eb-9e60-47cf-a9a4-a67dbca692d3","type":"message","text":"I think gradient accumulation is a good counterexample that is not an implementation detail of the dataloader. I do wish we collectively had something better than callbacks for customizing the train loop though. There are essentially `O(LOC in training loop)` possible extension points, and it's a slippery slope to something like <https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html> (which has what, 30+ hooks and methods?)","user":"UMY1LV01G","ts":"1614067536.038000","team":"T68168MUP","edited":{"user":"UMY1LV01G","ts":"1614067607.000000"},"blocks":[{"type":"rich_text","block_id":"NqQs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think gradient accumulation is a good counterexample that is not an implementation detail of the dataloader. I do wish we collectively had something better than callbacks for customizing the train loop though. There are essentially "},{"type":"text","text":"O(LOC in training loop)","style":{"code":true}},{"type":"text","text":" possible extension points, and it's a slippery slope to something like "},{"type":"link","url":"https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html"},{"type":"text","text":" (which has what, 30+ hooks and methods?)"}]}]}]},{"client_msg_id":"1a68f22d-5bbf-4b90-aee1-f91374f31f92","type":"message","text":"Gradient accumulation can happen at the Zygote level.","user":"UC4QQPG4A","ts":"1614081995.039300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ZLW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Gradient accumulation can happen at the Zygote level."}]}]}]},{"client_msg_id":"ca0298a2-b201-4e20-b622-a63cd7a73ec2","type":"message","text":"I don’t think there is a replacement for hooks that DL people want to use, currently. Callbacks are convenient for sequential models.","user":"UC4QQPG4A","ts":"1614082079.040400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/qhr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I don’t think there is a replacement for hooks that DL people want to use, currently. Callbacks are convenient for sequential models."}]}]}]},{"client_msg_id":"287b032e-1dc4-42d4-90a0-35039d1db4d3","type":"message","text":"I continue to dislike `Flux.train!` . Most people should implement custom training loops.","user":"U6A936746","ts":"1614085038.042300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"905I","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I continue to dislike "},{"type":"text","text":"Flux.train!","style":{"code":true}},{"type":"text","text":" . Most people should implement custom training loops."}]}]}],"reactions":[{"name":"+1","users":["UDXST8ARK"],"count":1}]},{"client_msg_id":"84fdb87d-bb23-49be-b55c-e186a95b5322","type":"message","text":"Callbacks seems like they are just a more limited and confusing way of doing it, esp since as Brian says, there are basically desire for them at every possibly step, and futher: also with every possible piece of intermediate state","user":"U6A936746","ts":"1614085132.043800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nBsAx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Callbacks seems like they are just a more limited and confusing way of doing it, esp since as Brian says, there are basically desire for them at every possibly step, and futher: also with every possible piece of intermediate state"}]}]}]},{"client_msg_id":"f7ddfad1-be37-4f2e-a8d9-84291b5406a6","type":"message","text":"Also the way train does batching and epochs with iterator constructs is just kinds of confusing to me. A custom loop is much clearer.","user":"U6A936746","ts":"1614085173.044700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RSW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Also the way train does batching and epochs with iterator constructs is just kinds of confusing to me. A custom loop is much clearer."}]}]}]},{"client_msg_id":"0f3f3b59-8cc6-45d7-853d-5723cca3f2b2","type":"message","text":"Maybe it's a silly idea, but wouldn't something basic like\n\n`function optimize!(loss, params, data, opt)\n    loss_value, back = pullback(() -&gt; loss(data), params)\n    grads = back(one(loss_value))\n    update!(opt, params, grads)\n    return loss_value, grads # for diagnostics\nend`\n\nbe a decent compromise? This way a custom loop is essentially as easy to write as a call to `train!` (It is just a one-line for loop)","user":"U6BJ9E351","ts":"1614087481.055600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"++J","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Maybe it's a silly idea, but wouldn't something basic like\n\n"},{"type":"text","text":"function optimize!(loss, params, data, opt)\n","style":{"code":true}},{"type":"text","text":"    loss_value, back = pullback(() -> loss(data), params)\n","style":{"code":true}},{"type":"text","text":"    grads = back(one(loss_value))\n","style":{"code":true}},{"type":"text","text":"    update!(opt, params, grads)\n","style":{"code":true}},{"type":"text","text":"    return loss_value, grads # for diagnostics\n","style":{"code":true}},{"type":"text","text":"end\n","style":{"code":true}},{"type":"text","text":"\nbe a decent compromise? This way a custom loop is essentially as easy to write as a call to "},{"type":"text","text":"train!","style":{"code":true}},{"type":"text","text":" (It is just a one-line for loop)"}]}]}]},{"client_msg_id":"92f74205-2553-404c-bb77-13810e206c04","type":"message","text":"See <https://github.com/FluxML/Flux.jl/pull/1471|https://github.com/FluxML/Flux.jl/pull/1471> which is doing basically that","user":"UC4QQPG4A","ts":"1614088155.056100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OT5n","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"See "},{"type":"link","url":"https://github.com/FluxML/Flux.jl/pull/1471","text":"https://github.com/FluxML/Flux.jl/pull/1471"},{"type":"text","text":" which is doing basically that"}]}]}],"reactions":[{"name":"+1","users":["U6BJ9E351"],"count":1}]},{"client_msg_id":"d1a4c9ec-b6e1-48b6-9f38-cd5105707bc8","type":"message","text":"That and <https://github.com/FluxML/Flux.jl/pull/1017|https://github.com/FluxML/Flux.jl/pull/1017>","user":"UC4QQPG4A","ts":"1614088263.056700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KW/FJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That and "},{"type":"link","url":"https://github.com/FluxML/Flux.jl/pull/1017","text":"https://github.com/FluxML/Flux.jl/pull/1017"}]}]}]},{"client_msg_id":"21d27f95-ba09-41c3-b2f3-f457575008d9","type":"message","text":"It makes sense for us to fix our dataloaders to act for tuples of arguments approach. Further, we should think about the needs beyond strict sequential computer vision tasks. Something that would consolidate a lot of data management repos in the wild","user":"UC4QQPG4A","ts":"1614088415.059200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"c4q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It makes sense for us to fix our dataloaders to act for tuples of arguments approach. Further, we should think about the needs beyond strict sequential computer vision tasks. Something that would consolidate a lot of data management repos in the wild"}]}]}]},{"client_msg_id":"4FA26759-CF92-4BE8-BF98-79B078338344","type":"message","text":"I don’t think <https://github.com/FluxML/Flux.jl/pull/1471|https://github.com/FluxML/Flux.jl/pull/1471> does what <@U6BJ9E351> suggested. The proposed `optimize!` is the same as the `step!` suggested in the comments of the PR. I think that’s something we all want.\n\n1471 could use `step!` but right now it looks like it is using the callback approach. I’m with Lyndon and Brian on this.","user":"UH9KWTTD3","ts":"1614093592.062700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kboqM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I don’t think "},{"type":"link","url":"https://github.com/FluxML/Flux.jl/pull/1471","text":"https://github.com/FluxML/Flux.jl/pull/1471"},{"type":"text","text":" does what "},{"type":"user","user_id":"U6BJ9E351"},{"type":"text","text":" suggested. The proposed "},{"type":"text","text":"optimize!","style":{"code":true}},{"type":"text","text":" is the same as the "},{"type":"text","text":"step!","style":{"code":true}},{"type":"text","text":" suggested in the comments of the PR. I think that’s something we all want.\n"},{"type":"text","text":"\n1471 could use "},{"type":"text","text":"step!","style":{"code":true}},{"type":"text","text":" but right now it looks like it is using the callback approach. I’m with Lyndon and Brian on this."}]}]}]},{"client_msg_id":"471C1B7A-07FF-4434-A66E-1FF2F7C7DD00","type":"message","text":"1017 looks pretty good though","user":"UH9KWTTD3","ts":"1614093648.063000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Phgz+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"1017 looks pretty good though"}]}]}]},{"client_msg_id":"b943d800-eefd-4b0b-a269-15452b44ddc6","type":"message","text":"I mean, the second one is the one he wants but the first one manages the callbacks. It's weird to say one is okay and the other isn't, because even without the hooks, passing the stuff into callbacks is strictly better than the current status","user":"UC4QQPG4A","ts":"1614095769.065200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YY/Tm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I mean, the second one is the one he wants but the first one manages the callbacks. It's weird to say one is okay and the other isn't, because even without the hooks, passing the stuff into callbacks is strictly better than the current status"}]}]}]},{"client_msg_id":"F4233BBD-D010-4B96-9E0D-7DFA8E2C8362","type":"message","text":"One expands the functionality of callbacks and the other collapses the forward pass, backwards pass, and parameter update into a single function. I like the latter but not the former.\n\nExpanding callbacks is only strictly better from the perspective of “can I do X with a callback.” It doesn’t address whether callbacks are clearest or most intuitive way to do X.","user":"UH9KWTTD3","ts":"1614096755.069000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/Vw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"One expands the functionality of callbacks and the other collapses the forward pass, backwards pass, and parameter update into a single function. I like the latter but not the former.\n"},{"type":"text","text":"\nExpanding callbacks is only strictly better from the perspective of “can I do X with a callback.” It doesn’t address whether callbacks are clearest or most intuitive way to do X."}]}]}]},{"client_msg_id":"3942994d-5d31-4917-8318-bd9852cc6f46","type":"message","text":"Sure, but isn't the effect restricting the usability of the callbacks for the tasks they are meant for? Today you can't do basic things like printing the current running loss, stop on nan loss/ gradients, debug gradients easily, or hook into a logger neatly.\n\nWhen they try to do non trivial tasks, we should absolutely guide them to the loop, but before that, :shrug:.","user":"UC4QQPG4A","ts":"1614101276.079300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eXAG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Sure, but isn't the effect restricting the usability of the callbacks for the tasks they are meant for? Today you can't do basic things like printing the current running loss, stop on nan loss/ gradients, debug gradients easily, or hook into a logger neatly.\n\nWhen they try to do non trivial tasks, we should absolutely guide them to the loop, but before that, "},{"type":"emoji","name":"shrug"},{"type":"text","text":"."}]}]}]},{"client_msg_id":"20e9ba4b-3454-4797-a7a8-47a6a52b87bd","type":"message","text":"The million-dollar question is where we draw the proverbial line in the sand. 1471 is a pretty decent 80/20 implementation, but do we freeze it there and clamp down on any further expansion?","user":"UMY1LV01G","ts":"1614101648.080600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"g9g","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The million-dollar question is where we draw the proverbial line in the sand. 1471 is a pretty decent 80/20 implementation, but do we freeze it there and clamp down on any further expansion?"}]}]}]},{"client_msg_id":"568928fd-4417-4c69-96d9-a17fca60d29a","type":"message","text":"Perhaps, but that can be left for the future. 1471 (minus the hooks) is not the 80/20 of anything. We have had callbacks for a while, and this is strictly improving that. Intentionally kneecapping them doesn't make sense. I'm assuming the consensus would be to deprecate them entirely?","user":"UC4QQPG4A","ts":"1614102997.092300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"D04","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Perhaps, but that can be left for the future. 1471 (minus the hooks) is not the 80/20 of anything. We have had callbacks for a while, and this is strictly improving that. Intentionally kneecapping them doesn't make sense. I'm assuming the consensus would be to deprecate them entirely?"}]}]}]},{"type":"message","text":"What's the meeting code for the ML and AD Development/Usage call?","user":"U9MD78Z9N","ts":"1614270806.095200","team":"T68168MUP"},{"client_msg_id":"c7814180-a072-47d3-a44b-1b47c5a3ff68","type":"message","text":"All the details should be on <https://www.google.com/calendar/event?eid=NDY0YW51c3JhdWtpcHZxam91YzFpazh2bjdfMjAyMTAyMjVUMTYzMDAwWiBqdWxpYWxhbmcub3JnX2tvbWF1YXFldDE0ZW9nOW9pdjNwNm83cG1nQGc&amp;ctz=America/Vancouver|https://www.google.com/calendar/event?eid=NDY0YW51c3JhdWtpcHZxam91YzFpazh2bjdfMjAyMT[…]X2tvbWF1YXFldDE0ZW9nOW9pdjNwNm83cG1nQGc>","user":"UMY1LV01G","ts":"1614271756.095500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TcfU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"All the details should be on "},{"type":"link","url":"https://www.google.com/calendar/event?eid=NDY0YW51c3JhdWtpcHZxam91YzFpazh2bjdfMjAyMTAyMjVUMTYzMDAwWiBqdWxpYWxhbmcub3JnX2tvbWF1YXFldDE0ZW9nOW9pdjNwNm83cG1nQGc&ctz=America/Vancouver","text":"https://www.google.com/calendar/event?eid=NDY0YW51c3JhdWtpcHZxam91YzFpazh2bjdfMjAyMT[…]X2tvbWF1YXFldDE0ZW9nOW9pdjNwNm83cG1nQGc"}]}]}]},{"client_msg_id":"73b3c2c7-38b6-43cd-859e-8aad78a6d4bb","type":"message","text":"Can you not join through <https://https>:&lt;//www.google.com/url?q=https://mit.zoom.us/j/96251790289&gt; directly?","user":"UMY1LV01G","ts":"1614271777.095900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hd+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can you not join through "},{"type":"link","url":"https://https"},{"type":"text","text":":"},{"type":"link","url":"//www.google.com/url?q=https://mit.zoom.us/j/96251790289"},{"type":"text","text":" directly?"}]}]}]},{"client_msg_id":"c24d835c-ae36-4489-8b57-0ea5d94e2438","type":"message","text":"he's on.","user":"U69BL50BF","ts":"1614271789.096200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ehn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"he's on."}]}]}]},{"client_msg_id":"c694143d-46b2-441e-9dc7-28aa32d2b4ad","type":"message","text":"Ah I see you already got on","user":"UMY1LV01G","ts":"1614271792.096400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wBUP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah I see you already got on"}]}]}]},{"client_msg_id":"f12f76c0-a7ed-450e-b7ef-84b7a86763f3","type":"message","text":"That's what I get for checking  <#C6G240ENA|autodiff> after posting","user":"UMY1LV01G","ts":"1614271832.096900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KqnOL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That's what I get for checking  "},{"type":"channel","channel_id":"C6G240ENA"},{"type":"text","text":" after posting"}]}]}]},{"client_msg_id":"eabb82d4-23cb-4bad-92ea-e68f95c8905e","type":"message","text":"Short progress update on the FastAI.jl development: it's not released yet, but large parts are already working and I've set up documentation, so check it out if you're interested: <https://lorenzoh.github.io/FastAI.jl/dev/README.html>\n\nI usually post over on the Zulip channel, but figured some people who might be interested are only active here. Let me know what you think!","user":"U010XUS4MT7","ts":"1614282534.099400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"03xd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Short progress update on the FastAI.jl development: it's not released yet, but large parts are already working and I've set up documentation, so check it out if you're interested: "},{"type":"link","url":"https://lorenzoh.github.io/FastAI.jl/dev/README.html"},{"type":"text","text":"\n\nI usually post over on the Zulip channel, but figured some people who might be interested are only active here. Let me know what you think!"}]}]}],"reactions":[{"name":"+1","users":["UGD4K0Z25"],"count":1}]},{"client_msg_id":"04f77148-53d9-474f-8721-171b9779cee8","type":"message","text":"Follow <https://julialang.zulipchat.com/#narrow/stream/237432-ml-ecosystem-coordination/topic/FastAI.2Ejl.20development/near/227827979> for more frequent updates","user":"U010XUS4MT7","ts":"1614282979.100000","team":"T68168MUP","attachments":[{"service_name":"Zulip","title":"JuliaLang","title_link":"https://julialang.zulipchat.com/#narrow/stream/237432-ml-ecosystem-coordination/topic/FastAI.2Ejl.20development/near/227827979","text":"This is the Zulip server for the Julia programming language community. We ask anyone joining to adhere to the Julia Code of Conduct. | To learn more about Julia, check out <https://julialang.org/>, or just come ask us here! | You can reach out to the admins of this Zulip by sending a direct message to @zulip-admins.","fallback":"Zulip: JuliaLang","thumb_url":"https://zulip-avatars.s3.amazonaws.com/7178/realm/icon.png?version=6","from_url":"https://julialang.zulipchat.com/#narrow/stream/237432-ml-ecosystem-coordination/topic/FastAI.2Ejl.20development/near/227827979","thumb_width":100,"thumb_height":100,"id":1,"original_url":"https://julialang.zulipchat.com/#narrow/stream/237432-ml-ecosystem-coordination/topic/FastAI.2Ejl.20development/near/227827979"}],"blocks":[{"type":"rich_text","block_id":"vaPg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Follow "},{"type":"link","url":"https://julialang.zulipchat.com/#narrow/stream/237432-ml-ecosystem-coordination/topic/FastAI.2Ejl.20development/near/227827979"},{"type":"text","text":" for more frequent updates"}]}]}]},{"client_msg_id":"27cefc8b-f685-459b-93be-f89d57efed89","type":"message","text":"```julia&gt; Flux.OneHotArray\nERROR: UndefVarError: OneHotArray not defined```","user":"U7YD3DKL2","ts":"1614283926.100800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gcME","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> Flux.OneHotArray\nERROR: UndefVarError: OneHotArray not defined"}]}]}]},{"client_msg_id":"4b9ce7b4-9e03-4e0b-b218-03fc53267f00","type":"message","text":"Where is OneHotArray defined?","user":"U7YD3DKL2","ts":"1614283931.101100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ydvpW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Where is OneHotArray defined?"}]}]}]},{"client_msg_id":"8b64941d-19b6-4149-a955-34fadc2f48c1","type":"message","text":"Are you using master?","user":"UH9KWTTD3","ts":"1614284377.101300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"cDm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Are you using master?"}]}]}]},{"client_msg_id":"a09ca423-f64b-48ce-960d-296a4f228b70","type":"message","text":"No. The last released version.","user":"U7YD3DKL2","ts":"1614284389.101800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lUUuf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"No. The last released version."}]}]}]},{"client_msg_id":"5123929f-f489-41ba-afff-08f3c05c2587","type":"message","text":"`OneHotArray` is still unreleased I think","user":"UH9KWTTD3","ts":"1614284390.101900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wg7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"OneHotArray","style":{"code":true}},{"type":"text","text":" is still unreleased I think"}]}]}]},{"client_msg_id":"884fe263-4084-44c0-88a3-9d300b22930e","type":"message","text":"But I just checked that PR was included since 0.11.4. <https://github.com/FluxML/Flux.jl/releases/tag/v0.11.4>","user":"U7YD3DKL2","ts":"1614284416.102600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"M2BFN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But I just checked that PR was included since 0.11.4. "},{"type":"link","url":"https://github.com/FluxML/Flux.jl/releases/tag/v0.11.4"}]}]}]},{"client_msg_id":"ff70017c-a595-439f-8096-9d4011ea789e","type":"message","text":"The release notes on Github are all screwed up. I’ve already told <@UC4QQPG4A> about it, and he’s working on fixing it for future releases.","user":"UH9KWTTD3","ts":"1614284431.102900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+axp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The release notes on Github are all screwed up. I’ve already told "},{"type":"user","user_id":"UC4QQPG4A"},{"type":"text","text":" about it, and he’s working on fixing it for future releases."}]}]}]},{"client_msg_id":"b59ae55f-153c-4276-b4b1-2a7a49263d03","type":"message","text":"Oh","user":"U7YD3DKL2","ts":"1614284442.103100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mLVQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oh"}]}]}]},{"client_msg_id":"c71e8e46-cb34-4134-8b81-0d497bf41833","type":"message","text":"Why it hasn't been released?","user":"U7YD3DKL2","ts":"1614284448.103300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GC89L","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Why it hasn't been released?"}]}]}]},{"client_msg_id":"6a42ff36-cc6a-429b-9cbd-53c9daf1f8f2","type":"message","text":"Not 100% positive cause I’m still hazy on semver but I think the struct/constructor changes means it is breaking. So it will have to wait until v0.12.","user":"UH9KWTTD3","ts":"1614284508.104100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"leEj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not 100% positive cause I’m still hazy on semver but I think the struct/constructor changes means it is breaking. So it will have to wait until v0.12."}]}]}]},{"client_msg_id":"8a628d46-354c-41a4-acc5-b001c0d6845c","type":"message","text":"Even though the highest level APIs like `onehotbatch` are the same","user":"UH9KWTTD3","ts":"1614284564.104400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5PB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Even though the highest level APIs like "},{"type":"text","text":"onehotbatch","style":{"code":true}},{"type":"text","text":" are the same"}]}]}]},{"client_msg_id":"ffe91dcc-ed16-4e00-8b2d-73c9cd1863fd","type":"message","text":"<https://github.com/FluxML/Flux.jl/compare/v0.11.6...master>","user":"UH9KWTTD3","ts":"1614284665.104600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gkT","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/FluxML/Flux.jl/compare/v0.11.6...master"}]}]}],"thread_ts":"1614284665.104600","reply_count":1,"reply_users_count":1,"latest_reply":"1614285221.104700","reply_users":["UMY1LV01G"],"subscribed":false},{"client_msg_id":"5f19a4fe-0d5f-497c-b364-aac3a556e669","type":"message","text":"I posted a question regarding training only part of a matrix in <#C690QRAA3|machine-learning> I'm using Flux. If anyone can answer or point me in the right direction, I'd appreciate it. Thanks.","user":"U01L0KU0SDV","ts":"1614311593.107400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jGCy5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I posted a question regarding training only part of a matrix in "},{"type":"channel","channel_id":"C690QRAA3"},{"type":"text","text":" I'm using Flux. If anyone can answer or point me in the right direction, I'd appreciate it. Thanks."}]}]}]},{"type":"message","text":"Do we have performant implicit layers (the OptNet layer is one example but more generally): z_i+1 is such that f(z_i, z_i+1) = 0 where the f is defined by weights somehow?","user":"U9MD78Z9N","ts":"1614554176.108000","team":"T68168MUP"},{"client_msg_id":"b93f451f-b086-45bd-b504-e11b646fd6a9","type":"message","text":"That's just a Steady state problem","user":"U69BL50BF","ts":"1614554291.108700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xqywW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That's just a Steady state problem"}]}]}]},{"type":"message","text":"Yes. Sorry that i had a different name than you. However seaching for \"Steady State\" layers in the Flux eco system i didn't find anything either","user":"U9MD78Z9N","ts":"1614554418.108800","team":"T68168MUP","edited":{"user":"U9MD78Z9N","ts":"1614555958.000000"}},{"client_msg_id":"c63cfb5e-d772-43a4-bc0a-97715e88ad0d","type":"message","text":"<https://discourse.julialang.org/t/funding-community-projects/56288|https://discourse.julialang.org/t/funding-community-projects/56288>","user":"UDGT4PM41","ts":"1614644606.109400","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"Funding Community Projects","title_link":"https://discourse.julialang.org/t/funding-community-projects/56288","text":"Hi fellow Julians! Many of you probably know that the Julia project uses the NumFOCUS, an US non-profit organisation as it’s fiscal sponsor. This means that income for the project is collected in accounts of that organisation, and our expenses are paid though there. The primary source of income is via sponsorship of JuliaCon, although individual sponsorships are now significant as well. Major expenses are again for JuliaCon, as well for our summer-of-code projects. As a result of careful stewa...","fallback":"JuliaLang: Funding Community Projects","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","fields":[{"title":"Reading time","value":"1 mins :clock2:","short":true},{"title":"Likes","value":"14 :heart:","short":true}],"ts":1614638394,"from_url":"https://discourse.julialang.org/t/funding-community-projects/56288","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/funding-community-projects/56288"}],"blocks":[{"type":"rich_text","block_id":"krm","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://discourse.julialang.org/t/funding-community-projects/56288","text":"https://discourse.julialang.org/t/funding-community-projects/56288"}]}]}]},{"client_msg_id":"58ee4578-736f-4948-84a1-43772de4725a","type":"message","text":"when I use the train! function, It works well, minimizing the loss function ecc..., but when I then display the params, they are the same as before the training step.","user":"U01FTFACYJ0","ts":"1614682564.113500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PSk0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"when I use the train! function, It works well, minimizing the loss function ecc..., but when I then display the params, they are the same as before the training step."}]}]}]},{"client_msg_id":"07a315d2-38a2-4bff-b08c-a51fc8e6a5c4","type":"message","text":"swift for tensorflow is dead\n\n<https://www.reddit.com/r/programming/comments/lik6fa/swift_for_tensorflow_shuts_down/>\n\nFlux is the only contender now","user":"URE70HE7R","ts":"1614719174.114400","team":"T68168MUP","attachments":[{"service_name":"reddit","title":"Swift for TensorFlow Shuts Down","title_link":"https://www.reddit.com/r/programming/comments/lik6fa/swift_for_tensorflow_shuts_down/","text":"Posted in r/programming by u/dm13450 • 15 points and 5 comments","fallback":"reddit: Swift for TensorFlow Shuts Down","thumb_url":"https://b.thumbs.redditmedia.com/kSx1DvW50gydn_aZwumYbs8UKpllySASXq8mtC7jvFM.jpg","from_url":"https://www.reddit.com/r/programming/comments/lik6fa/swift_for_tensorflow_shuts_down/","thumb_width":140,"thumb_height":140,"service_icon":"http://www.redditstatic.com/desktop2x/img/favicon/apple-icon-57x57.png","id":1,"original_url":"https://www.reddit.com/r/programming/comments/lik6fa/swift_for_tensorflow_shuts_down/"}],"blocks":[{"type":"rich_text","block_id":"0TJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"swift for tensorflow is dead\n\n"},{"type":"link","url":"https://www.reddit.com/r/programming/comments/lik6fa/swift_for_tensorflow_shuts_down/"},{"type":"text","text":"\n\nFlux is the only contender now"}]}]}]},{"client_msg_id":"e1179335-8215-40e3-b566-77c82ea11da6","type":"message","text":"hi guys, someone asked me at worked how does TF compares to Flux speed-wise. Is there like a comparison somewhere?","user":"U013V2CFZAN","ts":"1614719627.115900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ejd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hi guys, someone asked me at worked how does TF compares to Flux speed-wise. Is there like a comparison somewhere?"}]}]}]},{"client_msg_id":"db1f74f6-3e8a-40d5-99cd-6afc70a7ce9c","type":"message","text":"well, there’s no way to compare, one is python, the other is julian","user":"URE70HE7R","ts":"1614719696.116400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6RU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"well, there’s no way to compare, one is python, the other is julian"}]}]}]},{"client_msg_id":"3afa9f83-d6f9-4e7c-b5e0-c36bd0491593","type":"message","text":"I’m seeking some comment about how to handle batches and resampling in MLJFLux. See this issue: <https://github.com/FluxML/MLJFlux.jl/issues/97> , thanks. <@UMY1LV01G> <@U010XUS4MT7>","user":"UD0SQV5LL","ts":"1614722147.118100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dxv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’m seeking some comment about how to handle batches and resampling in MLJFLux. See this issue: "},{"type":"link","url":"https://github.com/FluxML/MLJFlux.jl/issues/97"},{"type":"text","text":" , thanks. "},{"type":"user","user_id":"UMY1LV01G"},{"type":"text","text":" "},{"type":"user","user_id":"U010XUS4MT7"}]}]}]},{"client_msg_id":"ba436d15-1fd0-4af4-ba8d-743244ec3ce4","type":"message","text":"when I try to implement my_custom_training from the documentation I get an error \"Params\" not defined when I use ps = Params(ps), how can I solve that?","user":"U01FTFACYJ0","ts":"1614969666.121100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zTI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"when I try to implement my_custom_training from the documentation I get an error \"Params\" not defined when I use ps = Params(ps), how can I solve that?"}]}]}],"thread_ts":"1614969666.121100","reply_count":1,"reply_users_count":1,"latest_reply":"1614969991.121300","reply_users":["UMY1LV01G"],"subscribed":false},{"type":"message","text":"Hi, I believed I met with a bp issue. When I tried to train a CNN with `MaxPool` layer, I would got  an error bellow. After removing `MaxPool` layer, everything is fine.  I thought the dimension of my `CuArray` didn't violate the standard here.","files":[{"id":"F01R3M48N2U","created":1615003546,"timestamp":1615003546,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01977X150R","editable":false,"size":75573,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01R3M48N2U/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01R3M48N2U/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01R3M48N2U-5227ee12ee/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01R3M48N2U-5227ee12ee/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01R3M48N2U-5227ee12ee/image_360.png","thumb_360_w":360,"thumb_360_h":13,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01R3M48N2U-5227ee12ee/image_480.png","thumb_480_w":480,"thumb_480_h":18,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01R3M48N2U-5227ee12ee/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01R3M48N2U-5227ee12ee/image_720.png","thumb_720_w":720,"thumb_720_h":26,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01R3M48N2U-5227ee12ee/image_800.png","thumb_800_w":800,"thumb_800_h":29,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01R3M48N2U-5227ee12ee/image_960.png","thumb_960_w":960,"thumb_960_h":35,"thumb_1024":"https://files.slack.com/files-tmb/T68168MUP-F01R3M48N2U-5227ee12ee/image_1024.png","thumb_1024_w":1024,"thumb_1024_h":37,"original_w":2438,"original_h":89,"thumb_tiny":"AwABADC53paTvS0igooooAWlFJSigD//2Q==","permalink":"https://julialang.slack.com/files/U01977X150R/F01R3M48N2U/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01R3M48N2U-daef57622f","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"1lm7J","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I believed I met with a bp issue. When I tried to train a CNN with "},{"type":"text","text":"MaxPool ","style":{"code":true}},{"type":"text","text":"layer, I would got  an error bellow. After removing "},{"type":"text","text":"MaxPool ","style":{"code":true}},{"type":"text","text":"layer, everything is fine.  I thought the dimension of my "},{"type":"text","text":"CuArray ","style":{"code":true}},{"type":"text","text":"didn't violate the standard here."}]}]}],"user":"U01977X150R","display_as_bot":false,"ts":"1615003705.125900"},{"client_msg_id":"5845ccc6-46e8-4c62-b8d4-d58570643e11","type":"message","text":"Th issue is that the eltypes don't match, so it will promote the second argument to float64 and be slow.","user":"UC4QQPG4A","ts":"1615013437.127300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8497","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Th issue is that the eltypes don't match, so it will promote the second argument to float64 and be slow."}]}]}],"thread_ts":"1615013437.127300","reply_count":1,"reply_users_count":1,"latest_reply":"1615013934.127400","reply_users":["U01977X150R"],"subscribed":false},{"client_msg_id":"67ea352b-7e6a-4fcb-8b68-39cc856234ac","type":"message","text":"Has anyone done multi-class segmentation and written a loss function for it that works on the GPU? The model output is a `(h, w, n, b)`-array with `n` the number of classes and `b` the batch size. Each slice in the class dimension corresponds to a one-hot vector for that pixel.","user":"U010XUS4MT7","ts":"1615125971.129900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"BGmH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Has anyone done multi-class segmentation and written a loss function for it that works on the GPU? The model output is a "},{"type":"text","text":"(h, w, n, b)","style":{"code":true}},{"type":"text","text":"-array with "},{"type":"text","text":"n","style":{"code":true}},{"type":"text","text":" the number of classes and "},{"type":"text","text":"b","style":{"code":true}},{"type":"text","text":" the batch size. Each slice in the class dimension corresponds to a one-hot vector for that pixel."}]}]}]}]}