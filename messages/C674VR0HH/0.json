{"cursor": 4, "messages": [{"client_msg_id":"82af0166-9839-4172-aec4-06fb6051421d","type":"message","text":"Is there any chance to read a tsv file with multiple `delim` s using CSV.jl?\n\nThis file <https://ec.europa.eu/eurostat/estat-navtree-portlet-prod/BulkDownloadListing?file=data/nama_10r_2hhinc.tsv.gz> from Eurostat uses `\\t` ,   `\\t` and `,`  in one file.","user":"U836PQXSN","ts":"1615562563.361000","team":"T68168MUP","edited":{"user":"U836PQXSN","ts":"1615562577.000000"},"blocks":[{"type":"rich_text","block_id":"jLQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there any chance to read a tsv file with multiple "},{"type":"text","text":"delim","style":{"code":true}},{"type":"text","text":" s using CSV.jl?\n\nThis file "},{"type":"link","url":"https://ec.europa.eu/eurostat/estat-navtree-portlet-prod/BulkDownloadListing?file=data/nama_10r_2hhinc.tsv.gz"},{"type":"text","text":" from Eurostat uses "},{"type":"text","text":"\\t","style":{"code":true}},{"type":"text","text":" ,  "},{"type":"text","text":" \\t","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":",","style":{"code":true}},{"type":"text","text":"  in one file."}]}]}],"thread_ts":"1615562563.361000","reply_count":1,"reply_users_count":1,"latest_reply":"1615562719.361200","reply_users":["U836PQXSN"],"subscribed":false},{"client_msg_id":"eb703cc1-1ec0-4d6f-93a2-2d6566c5ad18","type":"message","text":"Could interested people have a look at <https://github.com/JuliaData/DataFrames.jl/pull/2649> so that we do not regret the design after we implement it? (we are now getting more restrictive with breaking things so experimenting is risky :smile:) Thank you!","user":"U8JAMQGQY","ts":"1615563217.362600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TcH0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Could interested people have a look at "},{"type":"link","url":"https://github.com/JuliaData/DataFrames.jl/pull/2649"},{"type":"text","text":" so that we do not regret the design after we implement it? (we are now getting more restrictive with breaking things so experimenting is risky "},{"type":"emoji","name":"smile"},{"type":"text","text":") Thank you!"}]}]}]},{"client_msg_id":"2563e67e-c56e-401e-88c5-d2c327e6ac31","type":"message","text":"In Julia do changes on a copy of a variable get applied to the original automatically?\nIn python I would generally assign data to a new variable in case I was going to experiment so as to not affect the original.\n\nI just tried doing that in Julia and I now see that my original is showing up just as my copy.\nFor reference, I was trying to remove the \"T\" from the DateTime Format and my column's data changed from datetime to Any.","user":"U01QJ915TFD","ts":"1615574484.366100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"AE6mV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In Julia do changes on a copy of a variable get applied to the original automatically?\nIn python I would generally assign data to a new variable in case I was going to experiment so as to not affect the original.\n\nI just tried doing that in Julia and I now see that my original is showing up just as my copy.\nFor reference, I was trying to remove the \"T\" from the DateTime Format and my column's data changed from datetime to Any."}]}]}],"thread_ts":"1615574484.366100","reply_count":37,"reply_users_count":4,"latest_reply":"1615577198.374000","reply_users":["UBF9YRB6H","U01QJ915TFD","USU9FRPEU","UH24GRBLL"],"subscribed":false},{"type":"message","text":"How can I remove the T from the middle of my DateTime rows?","files":[{"id":"F01R4JRBAAF","created":1615580327,"timestamp":1615580327,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01QJ915TFD","editable":false,"size":50464,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01R4JRBAAF/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01R4JRBAAF/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01R4JRBAAF-b877d7adbb/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01R4JRBAAF-b877d7adbb/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01R4JRBAAF-b877d7adbb/image_360.png","thumb_360_w":360,"thumb_360_h":254,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01R4JRBAAF-b877d7adbb/image_480.png","thumb_480_w":480,"thumb_480_h":339,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01R4JRBAAF-b877d7adbb/image_160.png","original_w":658,"original_h":465,"thumb_tiny":"AwAhADCruI70u7nPP502lFMALE96Snd6P896AG0HHalNKelADaUdqSlFAC9//wBdL/nvSZGaMj/IoAG6fjQen/66QkUEigBKKKKACiiigAooooA//9k=","permalink":"https://julialang.slack.com/files/U01QJ915TFD/F01R4JRBAAF/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01R4JRBAAF-df04c37dda","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"2M6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I remove the T from the middle of my DateTime rows?"}]}]}],"user":"U01QJ915TFD","display_as_bot":false,"ts":"1615580400.376200"},{"client_msg_id":"02c1b767-fcab-4040-a616-31a0d607e734","type":"message","text":"That’s the default of how `DateTime` objects are printed. If you’d rather transform it to a custom-formatted string column, you could do something like:\n```df.dt = Dates.format.(dt.dt, \"yyyy-mm-dd HH:MM:SS.s\")```","user":"U681ELA87","ts":"1615580706.378300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"B8X/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That’s the default of how "},{"type":"text","text":"DateTime","style":{"code":true}},{"type":"text","text":" objects are printed. If you’d rather transform it to a custom-formatted string column, you could do something like:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"df.dt = Dates.format.(dt.dt, \"yyyy-mm-dd HH:MM:SS.s\")"}]}]}],"thread_ts":"1615580706.378300","reply_count":2,"reply_users_count":2,"latest_reply":"1615583508.385800","reply_users":["U01QJ915TFD","UAGUENL2Y"],"subscribed":false,"reactions":[{"name":"slightly_smiling_face","users":["U01QJ915TFD"],"count":1}]},{"client_msg_id":"1864bc9e-8a69-46d3-8dad-20585cfa7482","type":"message","text":"the `T` is part of ISO 8601, the objectively best datetime format standard","user":"UH24GRBLL","ts":"1615580736.378800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FbdJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"the "},{"type":"text","text":"T","style":{"code":true}},{"type":"text","text":" is part of ISO 8601, the objectively best datetime format standard"}]}]}],"thread_ts":"1615580736.378800","reply_count":5,"reply_users_count":2,"latest_reply":"1615582668.381300","reply_users":["U01QJ915TFD","UH24GRBLL"],"subscribed":false,"reactions":[{"name":"slightly_smiling_face","users":["U01QJ915TFD","U82LX4ACB"],"count":2},{"name":"+1","users":["U01FKQQ7J0J"],"count":1}]},{"client_msg_id":"35f4ff51-5238-409f-9d03-481b1988837f","type":"message","text":"it's also just a representation thing","user":"UH24GRBLL","ts":"1615580784.379100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"V+B","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it's also just a representation thing"}]}]}]},{"type":"message","text":"a final conversion from dataframe to Array is usually ugly because I want to be in column-major for numerical reasons.\n\nIs there an elegant way to convert to an Array and have it be column-major?","user":"U90JR0C80","ts":"1615581633.379800","team":"T68168MUP","attachments":[{"fallback":"[March 12th, 2021 3:21 PM] jessebett: saving and loading csv data with DataFrames keeps things row-major.\nafter manipulating the data, e.g. for selection, I am prepared to do numerical work where I want my data as an Array stored column-major.\n\nIs there an elegant way to do this transformation? Currently converting to matrix, transposing, and collecting from the LinearAlgebra.Adjoint type.","ts":"1615580474.019200","author_id":"U90JR0C80","author_subname":"Jesse Bettencourt","channel_id":"C6A044SQH","channel_name":"helpdesk","is_msg_unfurl":true,"is_thread_root_unfurl":true,"text":"saving and loading csv data with DataFrames keeps things row-major.\nafter manipulating the data, e.g. for selection, I am prepared to do numerical work where I want my data as an Array stored column-major.\n\nIs there an elegant way to do this transformation? Currently converting to matrix, transposing, and collecting from the LinearAlgebra.Adjoint type.","author_name":"Jesse Bettencourt","author_link":"https://julialang.slack.com/team/U90JR0C80","author_icon":"https://avatars.slack-edge.com/2018-01-30/307291696386_d856e2350ce251cee88a_48.jpg","mrkdwn_in":["text"],"color":"D0D0D0","from_url":"https://julialang.slack.com/archives/C6A044SQH/p1615580474019200?thread_ts=1615580474019200&cid=C6A044SQH","is_share":true,"footer":"Thread in #helpdesk"}],"blocks":[{"type":"rich_text","block_id":"enyK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"a final conversion from dataframe to Array is usually ugly because I want to be in column-major for numerical reasons.\n\nIs there an elegant way to convert to an Array and have it be column-major?"}]}]}],"thread_ts":"1615581633.379800","reply_count":2,"reply_users_count":1,"latest_reply":"1615581969.380100","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"f2e29bfd-3c26-4f76-8638-baf7e5c4eea6","type":"message","text":"How can I set my DateTime rows to only include rows between between two times? The file I'm working with has data from 4:00AM to 8:00PM(20:00) but I only want to have data from 9:30AM to 4:00PM on my dataframe.","user":"U01QJ915TFD","ts":"1615590197.395100","team":"T68168MUP","edited":{"user":"U01QJ915TFD","ts":"1615590246.000000"},"blocks":[{"type":"rich_text","block_id":"gTV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I set my DateTime rows to only include rows between between two times? The file I'm working with has data from 4:00AM to 8:00PM(20:00) but I only want to have data from 9:30AM to 4:00PM on my dataframe."}]}]}],"thread_ts":"1615590197.395100","reply_count":1,"reply_users_count":1,"latest_reply":"1615590364.395300","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"0a205e6b-8499-41c3-a719-57251ca7c83f","type":"message","text":"<@U681ELA87> was typing this up in slack here before I realized it might be better to preserve in GitHub <https://github.com/JuliaData/Arrow.jl/issues/88#issuecomment-798806038>\n\na Slack-amenable tangent to that, though - I wonder if there's a better way to support/replace the `Foo`/`_FooArrow` pattern I'm demo'ing in that comment. Instead of:\n\n```struct _FooArrow ... end\n\nFoo(::_FooArrow) = ...\n\nArrow.ArrowTypes.registertype!(Foo, _FooArrow)\n\nArrow.ArrowTypes.arrowconvert(::Type{_FooArrow}, f::Foo) = ...```\nIt'd be nice if I could just define\n\n```toarrow(::Foo)::NamedTuple = ...\nfromarrow(Type{&lt;:Foo}, ::NamedTuple) = ...```\nwithout needing to define a `_FooArrow` type at all or dynamically register stuff\n\nhaven't thought too hard about it though","user":"U674T0Y9Z","ts":"1615681523.427700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"n5vVH","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U681ELA87"},{"type":"text","text":" was typing this up in slack here before I realized it might be better to preserve in GitHub "},{"type":"link","url":"https://github.com/JuliaData/Arrow.jl/issues/88#issuecomment-798806038"},{"type":"text","text":"\n\na Slack-amenable tangent to that, though - I wonder if there's a better way to support/replace the "},{"type":"text","text":"Foo","style":{"code":true}},{"type":"text","text":"/"},{"type":"text","text":"_FooArrow","style":{"code":true}},{"type":"text","text":" pattern I'm demo'ing in that comment. Instead of:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"struct _FooArrow ... end\n\nFoo(::_FooArrow) = ...\n\nArrow.ArrowTypes.registertype!(Foo, _FooArrow)\n\nArrow.ArrowTypes.arrowconvert(::Type{_FooArrow}, f::Foo) = ..."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nIt'd be nice if I could just define\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"toarrow(::Foo)::NamedTuple = ...\nfromarrow(Type{<:Foo}, ::NamedTuple) = ..."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nwithout needing to define a "},{"type":"text","text":"_FooArrow","style":{"code":true}},{"type":"text","text":" type at all or dynamically register stuff\n\nhaven't thought too hard about it though"}]}]}]},{"client_msg_id":"cd2a8008-c348-4ca1-904d-c2cbe7ef2d85","type":"message","text":"Yes, I've been slowly forming my thoughts around this over the last week. I would add a `ArrowTypes.lower` function (like `JSON.lower`) that would allow a hook into the serialization process to say how your custom struct should be serialized. You would overload `ArrowTypes.lower(::MyCustomStruct)` and it would need to return standard arrow supported structs.","user":"U681ELA87","ts":"1615681710.429600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wapF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes, I've been slowly forming my thoughts around this over the last week. I would add a "},{"type":"text","text":"ArrowTypes.lower","style":{"code":true}},{"type":"text","text":" function (like "},{"type":"text","text":"JSON.lower","style":{"code":true}},{"type":"text","text":") that would allow a hook into the serialization process to say how your custom struct should be serialized. You would overload "},{"type":"text","text":"ArrowTypes.lower(::MyCustomStruct)","style":{"code":true}},{"type":"text","text":" and it would need to return standard arrow supported structs."}]}]}],"thread_ts":"1615681710.429600","reply_count":1,"reply_users_count":1,"latest_reply":"1615681830.430600","reply_users":["U674T0Y9Z"],"subscribed":false},{"client_msg_id":"8d32632d-cf05-4c50-a996-bbc3446bee2a","type":"message","text":"What I haven't quite worked out the details on is still serializing the metadata of the pre-lowered struct so you can still get it back out automatically..","user":"U681ELA87","ts":"1615681758.430500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"M4V8B","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What I haven't quite worked out the details on is still serializing the metadata of the pre-lowered struct so you can still get it back out automatically.."}]}]}],"thread_ts":"1615681758.430500","reply_count":1,"reply_users_count":1,"latest_reply":"1615682229.430800","reply_users":["U674T0Y9Z"],"subscribed":false},{"client_msg_id":"892e951a-5673-4378-97e0-34d2d030910c","type":"message","text":"<https://h2oai.github.io/db-benchmark/|https://h2oai.github.io/db-benchmark/>","user":"UKG4WF8PJ","ts":"1615689460.433200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5qfr2","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://h2oai.github.io/db-benchmark/","text":"https://h2oai.github.io/db-benchmark/"}]}]}]},{"client_msg_id":"aa72022e-cc0c-400b-a690-e9a6d5ca564a","type":"message","text":"<https://www.ritchievink.com/blog/2021/02/28/i-wrote-one-of-the-fastest-dataframe-libraries/>","user":"UDGT4PM41","ts":"1615699697.434400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FGe","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://www.ritchievink.com/blog/2021/02/28/i-wrote-one-of-the-fastest-dataframe-libraries/"}]}]}],"thread_ts":"1615699697.434400","reply_count":1,"reply_users_count":1,"latest_reply":"1615700380.434500","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"fd4560a3-453c-4ed0-867a-2bfd99fdcc72","type":"message","text":"What’s the best way to read Parquet files from Google Cloud Storage?","user":"U01GXNFKY6R","ts":"1615711455.437000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eGHd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What’s the best way to read Parquet files from Google Cloud Storage?"}]}]}]},{"client_msg_id":"93ef6318-1ffc-49ac-b4a3-6301f17d1cf0","type":"message","text":"I have a DataFrame with a column of tuples, each containing three elements. How can I split that up into three columns where each one has the corresponding tuple elements?","user":"U011V2YN59N","ts":"1615749713.444300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"44J","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a DataFrame with a column of tuples, each containing three elements. How can I split that up into three columns where each one has the corresponding tuple elements?"}]}]}]},{"client_msg_id":"37e15e1c-4265-4863-9c6e-ee39f124fb3d","type":"message","text":"I can't seem to find a nice way to do it with the dataframes api","user":"U011V2YN59N","ts":"1615749755.445100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"NLH1+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I can't seem to find a nice way to do it with the dataframes api"}]}]}]},{"client_msg_id":"c893e776-7a59-499e-a655-7d670234d91c","type":"message","text":"```julia&gt; using DataFrames\n\njulia&gt; df = DataFrame(a = [(1, 2, 3), (4, 5, 6)])\n2×1 DataFrame\n Row │ a         \n     │ Tuple…    \n─────┼───────────\n   1 │ (1, 2, 3)\n   2 │ (4, 5, 6)\n\njulia&gt; transform(df, :a =&gt; identity =&gt; AsTable)\n2×4 DataFrame\n Row │ a          x1     x2     x3    \n     │ Tuple…     Int64  Int64  Int64 \n─────┼────────────────────────────────\n   1 │ (1, 2, 3)      1      2      3\n   2 │ (4, 5, 6)      4      5      6```\nTo give them informative names, change `identity` to be a function returning a named tuple or a DataFrame","user":"UBF9YRB6H","ts":"1615749845.446000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/mJ+0","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> using DataFrames\n\njulia> df = DataFrame(a = [(1, 2, 3), (4, 5, 6)])\n2×1 DataFrame\n Row │ a         \n     │ Tuple…    \n─────┼───────────\n   1 │ (1, 2, 3)\n   2 │ (4, 5, 6)\n\njulia> transform(df, :a => identity => AsTable)\n2×4 DataFrame\n Row │ a          x1     x2     x3    \n     │ Tuple…     Int64  Int64  Int64 \n─────┼────────────────────────────────\n   1 │ (1, 2, 3)      1      2      3\n   2 │ (4, 5, 6)      4      5      6"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nTo give them informative names, change "},{"type":"text","text":"identity","style":{"code":true}},{"type":"text","text":" to be a function returning a named tuple or a DataFrame"}]}]}]},{"client_msg_id":"4c881f55-8ef9-45b3-b00d-f77ffa96b91e","type":"message","text":"ah wow that is so much nicer than my mess of `map`  and `zip`","user":"U011V2YN59N","ts":"1615749880.446500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Q+r","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ah wow that is so much nicer than my mess of "},{"type":"text","text":"map","style":{"code":true}},{"type":"text","text":"  and "},{"type":"text","text":"zip","style":{"code":true}}]}]}],"reactions":[{"name":"+1","users":["UBF9YRB6H"],"count":1}]},{"client_msg_id":"1397af91-79bb-4096-83e5-b4ec0f921017","type":"message","text":"thanks!","user":"U011V2YN59N","ts":"1615749890.446800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"c+s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"thanks!"}]}]}]},{"client_msg_id":"6b784b2e-2280-4ecf-a053-c811949a1d00","type":"message","text":"hmm I am getting this error with that code","user":"U011V2YN59N","ts":"1615750127.447100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6sqT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hmm I am getting this error with that code"}]}]}]},{"client_msg_id":"2d1ee8a4-5727-473f-bf15-18dadc62d96a","type":"message","text":"```ERROR: ArgumentError: keys of the returned elements must be identical\nStacktrace:\n [1] _expand_to_table(res::Vector{String})\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:380\n [2] select_transform!(nc::Union{Function, Pair{var\"#s267\", var\"#s266\"} where {var\"#s267\"&lt;:Union{Int64, AsTable, AbstractVector{Int64}}, var\"#s266\"&lt;:(Pair{var\"#s164\", var\"#s163\"} where {var\"#s164\"&lt;:Union{Function, Type}, var\"#s163\"&lt;:Union{DataType, Symbol, AbstractVector{Symbol}}})}, Type}, df::DataFrame, newdf::DataFrame, transformed_cols::Set{Symbol}, copycols::Bool, allow_resizing_newdf::Base.RefValue{Bool})\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:522\n [3] _manipulate(df::DataFrame, normalized_cs::Any, copycols::Bool, keeprows::Bool)\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:1279\n [4] manipulate(::DataFrame, ::Any, ::Vararg{Any, N} where N; copycols::Bool, keeprows::Bool, renamecols::Bool)\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:1209\n [5] #select#384\n   @ ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:847 [inlined]\n [6] #transform#386\n   @ ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:913 [inlined]```","user":"U011V2YN59N","ts":"1615750146.447800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"H6l","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: ArgumentError: keys of the returned elements must be identical\nStacktrace:\n [1] _expand_to_table(res::Vector{String})\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:380\n [2] select_transform!(nc::Union{Function, Pair{var\"#s267\", var\"#s266\"} where {var\"#s267\"<:Union{Int64, AsTable, AbstractVector{Int64}}, var\"#s266\"<:(Pair{var\"#s164\", var\"#s163\"} where {var\"#s164\"<:Union{Function, Type}, var\"#s163\"<:Union{DataType, Symbol, AbstractVector{Symbol}}})}, Type}, df::DataFrame, newdf::DataFrame, transformed_cols::Set{Symbol}, copycols::Bool, allow_resizing_newdf::Base.RefValue{Bool})\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:522\n [3] _manipulate(df::DataFrame, normalized_cs::Any, copycols::Bool, keeprows::Bool)\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:1279\n [4] manipulate(::DataFrame, ::Any, ::Vararg{Any, N} where N; copycols::Bool, keeprows::Bool, renamecols::Bool)\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:1209\n [5] #select#384\n   @ ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:847 [inlined]\n [6] #transform#386\n   @ ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:913 [inlined]"}]}]}]},{"client_msg_id":"d201c99b-5b62-4078-85f6-ed665ff5583a","type":"message","text":"Do you have a mix of tuples and named tuples? Or some tuples that have 4 elements?","user":"UBF9YRB6H","ts":"1615750564.448400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RLU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do you have a mix of tuples and named tuples? Or some tuples that have 4 elements?"}]}]}]},{"client_msg_id":"33cc3e7f-5886-4b2b-b929-ae2116c852f3","type":"message","text":"ah I see what was happening, my tuples weren't be parsed from the CSV properly.","user":"U011V2YN59N","ts":"1615750777.448800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yy/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ah I see what was happening, my tuples weren't be parsed from the CSV properly."}]}]}]},{"client_msg_id":"2c023030-8b94-48ac-a652-0b63cf64acff","type":"message","text":"I am really trying to get a Pandas dataframe where one column is full of tuples","user":"U011V2YN59N","ts":"1615750794.449300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iYd5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am really trying to get a Pandas dataframe where one column is full of tuples"}]}]}]},{"client_msg_id":"ccd93f57-56fa-48e9-883f-0c0f9c4d91aa","type":"message","text":"from python to julia","user":"U011V2YN59N","ts":"1615750799.449500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uzLMm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"from python to julia"}]}]}]},{"client_msg_id":"f49a5572-fdc0-433a-805f-7b7de4cc6a74","type":"message","text":"Pandas.read_csv does not parse tuples though","user":"U011V2YN59N","ts":"1615750811.449800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"EeGo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Pandas.read_csv does not parse tuples though"}]}]}]},{"client_msg_id":"899013a1-90e5-451a-92a8-21281f661f8b","type":"message","text":"(Pandas being `Pandas.jl`)","user":"U011V2YN59N","ts":"1615750837.450200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qTPd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(Pandas being "},{"type":"text","text":"Pandas.jl","style":{"code":true}},{"type":"text","text":")"}]}]}]},{"client_msg_id":"bf8d2b46-bcbb-4872-853c-4f674690b29c","type":"message","text":"thanks!","user":"U011V2YN59N","ts":"1615750940.450600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eGebC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"thanks!"}]}]}]},{"client_msg_id":"33acb2e6-d817-44dc-ab76-b6e8e7546b86","type":"message","text":"dataframes qn: Why is it that\n```checkEntry(col) = map(cell-&gt;(cell == \"&lt;5\" ? rand([1,2,3,4]) : cell), col)\nmapcols!(checkEntry, df)```\nworks, but\n```for col in eachcol(df)\n  col = map(checkEntry, col)\nend```\ndoesn’t?\n(The first is obviously much nicer, but I’d expected the latter to work too.)","user":"US8V7JSKB","ts":"1615792371.454700","team":"T68168MUP","edited":{"user":"US8V7JSKB","ts":"1615792422.000000"},"blocks":[{"type":"rich_text","block_id":"/hS+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"dataframes qn: Why is it that\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"checkEntry(col) = map(cell->(cell == \"<5\" ? rand([1,2,3,4]) : cell), col)\nmapcols!(checkEntry, df)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"works, but\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"for col in eachcol(df)\n  col = map(checkEntry, col)\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"doesn’t?\n(The first is obviously much nicer, but I’d expected the latter to work too.)"}]}]}]},{"client_msg_id":"237c5f8f-4177-46fc-9204-0d892cbdf536","type":"message","text":"Trying to get the rows with the top N items from some column. Can anyone think of a better way than this?\n\n```julia&gt; df = DataFrame(x = 1:10, y = rand(10));\n\njulia&gt; function findmaxn(v, n)\n           srt = invperm(sortperm(v,rev=true))\n           findall(&lt;=(n), srt)\n       end\nfindmaxn (generic function with 1 method)\n\njulia&gt; df[findmaxn(df.y, 3), :]\n3×2 DataFrame\n Row │ x      y        \n     │ Int64  Float64  \n─────┼─────────────────\n   1 │     3  0.555818\n   2 │     7  0.818728\n   3 │     8  0.792478```","user":"U8JP5B9T2","ts":"1615822342.456800","team":"T68168MUP","edited":{"user":"U8JP5B9T2","ts":"1615822368.000000"},"blocks":[{"type":"rich_text","block_id":"=ty","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Trying to get the rows with the top N items from some column. Can anyone think of a better way than this?\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> df = DataFrame(x = 1:10, y = rand(10));\n\njulia> function findmaxn(v, n)\n           srt = invperm(sortperm(v,rev=true))\n           findall(<=(n), srt)\n       end\nfindmaxn (generic function with 1 method)\n\njulia> df[findmaxn(df.y, 3), :]\n3×2 DataFrame\n Row │ x      y        \n     │ Int64  Float64  \n─────┼─────────────────\n   1 │     3  0.555818\n   2 │     7  0.818728\n   3 │     8  0.792478"}]}]}],"thread_ts":"1615822342.456800","reply_count":4,"reply_users_count":2,"latest_reply":"1615822660.458100","reply_users":["U67431ELR","U8JP5B9T2"],"subscribed":false},{"client_msg_id":"258894bb-23cb-4523-b26f-e2047fcd9ae6","type":"message","text":"in particular, it seems like there should be something like `findmaxn` somewhere","user":"U8JP5B9T2","ts":"1615822388.457400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"aOSj0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"in particular, it seems like there should be something like "},{"type":"text","text":"findmaxn","style":{"code":true}},{"type":"text","text":" somewhere"}]}]}]},{"client_msg_id":"02e14eb2-d3ae-4c7d-8b3a-20ff931123c6","type":"message","text":"Is there a way to check if a high-resolution timestamp is contained within a low-resolution one?\n```using Dates\nt_month = floor(Dates.now(), Dates.Month)\nt_sec = floor(Dates.now(), Dates.Second)\nt_sec in t_month == true # method error```","user":"UB2QSHWPN","ts":"1615822928.459900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tlwq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to check if a high-resolution timestamp is contained within a low-resolution one?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Dates\nt_month = floor(Dates.now(), Dates.Month)\nt_sec = floor(Dates.now(), Dates.Second)\nt_sec in t_month == true # method error"}]}]}],"thread_ts":"1615822928.459900","reply_count":1,"reply_users_count":1,"latest_reply":"1615823034.460000","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"de8e5604-a952-4497-8070-8166c472a03d","type":"message","text":"hi guys! is there an equivalent of <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html> ? I’m trying to put something similar together than quantopian’s alphalens, and would need to bin the data effectively. I wrote my own function to do it (as I couldn’t find anything similar), but would love to compare it to some kind of official implementation","user":"U01N351DMT9","ts":"1615903158.002300","team":"T68168MUP","edited":{"user":"U01N351DMT9","ts":"1615903162.000000"},"blocks":[{"type":"rich_text","block_id":"yjw2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hi guys! is there an equivalent of "},{"type":"link","url":"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html"},{"type":"text","text":" ? I’m trying to put something similar together than quantopian’s alphalens, and would need to bin the data effectively. I wrote my own function to do it (as I couldn’t find anything similar), but would love to compare it to some kind of official implementation"}]}]}],"thread_ts":"1615903158.002300","reply_count":9,"reply_users_count":2,"latest_reply":"1615904212.005600","reply_users":["U01N351DMT9","U67431ELR"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"if there’s no such function, I’d love to collaborate with someone who is more experienced with Julia to make this happen as part of any package!","user":"U01N351DMT9","ts":"1615903350.004000","thread_ts":"1615903158.002300","root":{"client_msg_id":"de8e5604-a952-4497-8070-8166c472a03d","type":"message","text":"hi guys! is there an equivalent of <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html> ? I’m trying to put something similar together than quantopian’s alphalens, and would need to bin the data effectively. I wrote my own function to do it (as I couldn’t find anything similar), but would love to compare it to some kind of official implementation","user":"U01N351DMT9","ts":"1615903158.002300","team":"T68168MUP","edited":{"user":"U01N351DMT9","ts":"1615903162.000000"},"blocks":[{"type":"rich_text","block_id":"yjw2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hi guys! is there an equivalent of "},{"type":"link","url":"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html"},{"type":"text","text":" ? I’m trying to put something similar together than quantopian’s alphalens, and would need to bin the data effectively. I wrote my own function to do it (as I couldn’t find anything similar), but would love to compare it to some kind of official implementation"}]}]}],"thread_ts":"1615903158.002300","reply_count":9,"reply_users_count":2,"latest_reply":"1615904212.005600","reply_users":["U01N351DMT9","U67431ELR"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"8IDTe","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"if there’s no such function, I’d love to collaborate with someone who is more experienced with Julia to make this happen as part of any package!"}]}]}],"client_msg_id":"00db99f9-c6d7-4284-9ef0-f12f9553dd34"},{"type":"message","subtype":"thread_broadcast","text":"nothing! my own crappy implementation also uses it. but I couldn’t find a code snippet to easily use it with dataframes’ groupby / combine methods. do you have a direction to point me towards?","user":"U01N351DMT9","ts":"1615903618.004500","thread_ts":"1615903158.002300","root":{"client_msg_id":"de8e5604-a952-4497-8070-8166c472a03d","type":"message","text":"hi guys! is there an equivalent of <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html> ? I’m trying to put something similar together than quantopian’s alphalens, and would need to bin the data effectively. I wrote my own function to do it (as I couldn’t find anything similar), but would love to compare it to some kind of official implementation","user":"U01N351DMT9","ts":"1615903158.002300","team":"T68168MUP","edited":{"user":"U01N351DMT9","ts":"1615903162.000000"},"blocks":[{"type":"rich_text","block_id":"yjw2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hi guys! is there an equivalent of "},{"type":"link","url":"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html"},{"type":"text","text":" ? I’m trying to put something similar together than quantopian’s alphalens, and would need to bin the data effectively. I wrote my own function to do it (as I couldn’t find anything similar), but would love to compare it to some kind of official implementation"}]}]}],"thread_ts":"1615903158.002300","reply_count":9,"reply_users_count":2,"latest_reply":"1615904212.005600","reply_users":["U01N351DMT9","U67431ELR"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"+7/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"nothing! my own crappy implementation also uses it. but I couldn’t find a code snippet to easily use it with dataframes’ groupby / combine methods. do you have a direction to point me towards?"}]}]}],"client_msg_id":"7924922b-d1b9-415a-bbeb-2eb77f1b7cd2"},{"client_msg_id":"3034bcb7-8669-4da2-a73e-e3d69e6fb222","type":"message","text":"How would you merge 2 dataframes in the following situation? :\nDataFrame A has 2 columns, DataFrame B has 3 columns but both DataFrame A and B share some rows in their first column.\n\nAfter merging, I'd like to have a DataFrame with a total of 4 columns which only keeps the rows that DataFrame A and B have in common in their first column.","user":"U01QJ915TFD","ts":"1615916916.011400","team":"T68168MUP","edited":{"user":"U01QJ915TFD","ts":"1615917014.000000"},"blocks":[{"type":"rich_text","block_id":"Tuu2G","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How would you merge 2 dataframes in the following situation? :\nDataFrame A has 2 columns, DataFrame B has 3 columns but both DataFrame A and B share some rows in their first column.\n\nAfter merging, I'd like to have a DataFrame with a total of 4 columns which only keeps the rows that DataFrame A and B have in common in their first column."}]}]}],"thread_ts":"1615916916.011400","reply_count":2,"reply_users_count":2,"latest_reply":"1615917490.012300","reply_users":["U8JAMQGQY","U01QJ915TFD"],"subscribed":false},{"client_msg_id":"c1c57145-31dd-48d9-b475-af50b054bae3","type":"message","text":"Hello hello data peeps","user":"U6QGE7S86","ts":"1615944507.000300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YIvMt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello hello data peeps"}]}]}]},{"client_msg_id":"d5b861fe-e0ee-4a26-9b14-6903d759dc61","type":"message","text":"Are there more recent benchmarks vs other tabular readers than <https://h2oai.github.io/db-benchmark/>?","user":"U6QGE7S86","ts":"1615944526.000700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gcC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Are there more recent benchmarks vs other tabular readers than "},{"type":"link","url":"https://h2oai.github.io/db-benchmark/"},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"1e0685f3-8260-45ca-9d77-5ab1d4122e52","type":"message","text":"They're using 1.5.3 Julia so I'm guessing there will be some awesome speedups for their new round of benchmarking.","user":"U6QGE7S86","ts":"1615944547.001200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sCsp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"They're using 1.5.3 Julia so I'm guessing there will be some awesome speedups for their new round of benchmarking."}]}]}]},{"client_msg_id":"62c081a1-ffe8-4bb8-a54c-98ec3418e332","type":"message","text":"The speedups probably won't come from 1.6, but rather through multithreading in grouping and combining","user":"UBF9YRB6H","ts":"1615945131.001800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mI4p","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The speedups probably won't come from 1.6, but rather through multithreading in grouping and combining"}]}]}],"reactions":[{"name":"+1::skin-tone-5","users":["U6QGE7S86"],"count":1}]},{"client_msg_id":"d7c0ac02-fff0-4745-9304-ec4e1d481d7c","type":"message","text":"Ah, also found <https://bkamins.github.io/julialang/2021/03/05/pooledarrays.html>.","user":"U6QGE7S86","ts":"1615945175.002100","team":"T68168MUP","attachments":[{"service_name":"Blog by Bogumił Kamiński","title":"What is new in PooledArrays.jl?","title_link":"https://bkamins.github.io/julialang/2021/03/05/pooledarrays.html","text":"Introduction","fallback":"Blog by Bogumił Kamiński: What is new in PooledArrays.jl?","ts":1614941097,"from_url":"https://bkamins.github.io/julialang/2021/03/05/pooledarrays.html","service_icon":"https://bkamins.github.io/favicon.ico","id":1,"original_url":"https://bkamins.github.io/julialang/2021/03/05/pooledarrays.html"}],"blocks":[{"type":"rich_text","block_id":"SBd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah, also found "},{"type":"link","url":"https://bkamins.github.io/julialang/2021/03/05/pooledarrays.html"},{"type":"text","text":"."}]}]}]},{"client_msg_id":"34c2b1f1-3f25-4fd4-934e-a2d5886a4ecb","type":"message","text":"could we add a little utility to Arrow.jl that is the equivalent of\n\n```function f(data)\n           io = IOBuffer()\n           Arrow.write(io, data)\n           seekstart(io)\n           return Arrow.Table(io)\n       end```\neven if unexported (name bikeshed? :grin:). I feel like this isn't useful in \"real\" code but I copy-paste it in to my REPL all the time for interactive testing","user":"U674T0Y9Z","ts":"1615946006.005700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LWXx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"could we add a little utility to Arrow.jl that is the equivalent of\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function f(data)\n           io = IOBuffer()\n           Arrow.write(io, data)\n           seekstart(io)\n           return Arrow.Table(io)\n       end"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\neven if unexported (name bikeshed? "},{"type":"emoji","name":"grin"},{"type":"text","text":"). I feel like this isn't useful in \"real\" code but I copy-paste it in to my REPL all the time for interactive testing"}]}]}],"thread_ts":"1615946006.005700","reply_count":3,"reply_users_count":2,"latest_reply":"1615947403.007900","reply_users":["U681ELA87","U674T0Y9Z"],"subscribed":false},{"client_msg_id":"3825e63d-4c0c-4c60-8d94-99e26b8d22db","type":"message","text":"Is there any precedent for using Arrow.jl with image data? I'm realizing that several of the tasks I'm working on could be rephrased into pure data handling issues. In particular the LZ4 or ZStd support could be particular useful.","user":"USU9FRPEU","ts":"1616000902.013900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SBK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there any precedent for using Arrow.jl with image data? I'm realizing that several of the tasks I'm working on could be rephrased into pure data handling issues. In particular the LZ4 or ZStd support could be particular useful."}]}]}],"thread_ts":"1616000902.013900","reply_count":20,"reply_users_count":3,"latest_reply":"1616001950.020500","reply_users":["U9VG1AYSG","USU9FRPEU","U681ELA87"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"One thing I'm missing at the moment is a byte or bit shuffle such as implemented by Blosc:\n<https://github.com/Blosc/c-blosc/blob/5352508a420bec865c57cda116a5e5303df898d2/blosc/shuffle-generic.h#L32-L52>\n\nThat seems to have an impact on how well LZ4 compression works.","user":"USU9FRPEU","ts":"1616001188.015500","thread_ts":"1616000902.013900","root":{"client_msg_id":"3825e63d-4c0c-4c60-8d94-99e26b8d22db","type":"message","text":"Is there any precedent for using Arrow.jl with image data? I'm realizing that several of the tasks I'm working on could be rephrased into pure data handling issues. In particular the LZ4 or ZStd support could be particular useful.","user":"USU9FRPEU","ts":"1616000902.013900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SBK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there any precedent for using Arrow.jl with image data? I'm realizing that several of the tasks I'm working on could be rephrased into pure data handling issues. In particular the LZ4 or ZStd support could be particular useful."}]}]}],"thread_ts":"1616000902.013900","reply_count":20,"reply_users_count":3,"latest_reply":"1616001950.020500","reply_users":["U9VG1AYSG","USU9FRPEU","U681ELA87"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"Kbni8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"One thing I'm missing at the moment is a byte or bit shuffle such as implemented by Blosc:\n"},{"type":"link","url":"https://github.com/Blosc/c-blosc/blob/5352508a420bec865c57cda116a5e5303df898d2/blosc/shuffle-generic.h#L32-L52"},{"type":"text","text":"\n\nThat seems to have an impact on how well LZ4 compression works."}]}]}],"client_msg_id":"7b4003a7-d865-4974-a9df-999e5e52068c"},{"type":"message","subtype":"thread_broadcast","text":"Perhaps what I'm really asking about is reusing some of the components that Arrow uses to do other things. For example, the compression codecs interface has come to my attention.","user":"USU9FRPEU","ts":"1616001614.018000","thread_ts":"1616000902.013900","root":{"client_msg_id":"3825e63d-4c0c-4c60-8d94-99e26b8d22db","type":"message","text":"Is there any precedent for using Arrow.jl with image data? I'm realizing that several of the tasks I'm working on could be rephrased into pure data handling issues. In particular the LZ4 or ZStd support could be particular useful.","user":"USU9FRPEU","ts":"1616000902.013900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SBK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there any precedent for using Arrow.jl with image data? I'm realizing that several of the tasks I'm working on could be rephrased into pure data handling issues. In particular the LZ4 or ZStd support could be particular useful."}]}]}],"thread_ts":"1616000902.013900","reply_count":20,"reply_users_count":3,"latest_reply":"1616001950.020500","reply_users":["U9VG1AYSG","USU9FRPEU","U681ELA87"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"PJG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Perhaps what I'm really asking about is reusing some of the components that Arrow uses to do other things. For example, the compression codecs interface has come to my attention."}]}]}],"client_msg_id":"91ed8538-b964-4f55-bc78-68bb460d05d5"},{"client_msg_id":"428016db-09df-45ca-9bbf-69c46e8c6a67","type":"message","text":"I'm also looking around for any 12-bit integer support, especially in a packed format where two integers are packed into three bytes. I have this mostly figured out for my purposes. Before I go about creating a package for this, I wanted to check if there was any existing work.\n\nI've built some utilities on top of <https://github.com/rfourquet/BitIntegers.jl> and SIMD.jl to either simulate 12-bit integer arrays or unpack them quickly into `UInt16` .","user":"USU9FRPEU","ts":"1616001943.020400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ng6A1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm also looking around for any 12-bit integer support, especially in a packed format where two integers are packed into three bytes. I have this mostly figured out for my purposes. Before I go about creating a package for this, I wanted to check if there was any existing work.\n\nI've built some utilities on top of "},{"type":"link","url":"https://github.com/rfourquet/BitIntegers.jl"},{"type":"text","text":" and SIMD.jl to either simulate 12-bit integer arrays or unpack them quickly into "},{"type":"text","text":"UInt16","style":{"code":true}},{"type":"text","text":" ."}]}]}]},{"client_msg_id":"4f9ea39f-845d-4619-bfec-41620b02b268","type":"message","text":"Is there a way to add missing rows? I understand there are functions to backfill or even forward fill missing values but how can I add rows that are missing altogether ?\nHow would I add rows for the missing seconds in the example DateTime column below?\n\nHere's an example:\n\nDATETIME\n9:30:01\n9:30:05","user":"U01QJ915TFD","ts":"1616101028.026500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=2U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to add missing rows? I understand there are functions to backfill or even forward fill missing values but how can I add rows that are missing altogether ?\nHow would I add rows for the missing seconds in the example DateTime column below?\n\nHere's an example:\n\nDATETIME\n9:30:01\n9:30:05"}]}]}]},{"client_msg_id":"8da9c75a-b4cc-4f99-8789-6a5088907e80","type":"message","text":"I'm not entirely sure if what I'm wanting to do above is the best approach. Another approach I just thought of is instead joining two dataframes, one with no missing DateTime rows being one of the dataframes. In doing so, would this automatically maintain all the rows from the complete dataframe and thus now have missing values instead of just missing rows for the other incomplete dataframe?","user":"U01QJ915TFD","ts":"1616101944.030700","team":"T68168MUP","edited":{"user":"U01QJ915TFD","ts":"1616102035.000000"},"blocks":[{"type":"rich_text","block_id":"KFAx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm not entirely sure if what I'm wanting to do above is the best approach. Another approach I just thought of is instead joining two dataframes, one with no missing DateTime rows being one of the dataframes. In doing so, would this automatically maintain all the rows from the complete dataframe and thus now have missing values instead of just missing rows for the other incomplete dataframe?"}]}]}]},{"client_msg_id":"3749f949-3fd5-44ac-8cf5-45ce4ba65731","type":"message","text":"How can I roll back the time 5 hours in my DateTime column?\nFor example:\nfrom this *2020-01-12T14:30:00.016*  to  *2020-01-12T09:30:00.016*\n\nBased on the docs, DateTime is unaware or naive when it comes to time zones. It points to using TimeZones.jl. But I'm having trouble doing this.","user":"U01QJ915TFD","ts":"1616108146.034300","team":"T68168MUP","edited":{"user":"U01QJ915TFD","ts":"1616108175.000000"},"blocks":[{"type":"rich_text","block_id":"BXs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I roll back the time 5 hours in my DateTime column?\nFor example:\nfrom this "},{"type":"text","text":"2020-01-12T14:30:00.016","style":{"bold":true}},{"type":"text","text":"  to  "},{"type":"text","text":"2020-01-12T09:30:00.016","style":{"bold":true}},{"type":"text","text":"\n\nBased on the docs, DateTime is unaware or naive when it comes to time zones. It points to using TimeZones.jl. But I'm having trouble doing this."}]}]}],"thread_ts":"1616108146.034300","reply_count":1,"reply_users_count":1,"latest_reply":"1616109245.034600","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"e3bb2169-fd13-4c67-b025-0d7e66f95e52","type":"message","text":"Any idea on why the below code would produce an error?\n\n\n`df_comb = leftjoin(df_master, df_msft, df_fb, df_pypl, df_ba, df_cvx, df_goog, df_jpm, df_nvda, df_TSLA, on = :DateTime, makeunique=true)`\n\n\nHere's the error:\n\n`ERROR: MethodError: no method matching leftjoin(::DataFrame, ::DataFrame, ::DataFrame, ::DataFrame, ::DataFrame,`\n`::DataFrame, ::DataFrame, ::DataFrame, ::DataFrame, ::DataFrame; on=:DateTime, makeunique=true)`","user":"U01QJ915TFD","ts":"1616126515.040500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sC=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Any idea on why the below code would produce an error?\n\n\n"},{"type":"text","text":"df_comb = leftjoin(df_master, df_msft, df_fb, df_pypl, df_ba, df_cvx, df_goog, df_jpm, df_nvda, df_TSLA, on = :DateTime, makeunique=true)","style":{"code":true}},{"type":"text","text":"\n\n\nHere's the error:\n\n"},{"type":"text","text":"ERROR: MethodError: no method matching leftjoin(::DataFrame, ::DataFrame, ::DataFrame, ::DataFrame, ::DataFrame,","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"::DataFrame, ::DataFrame, ::DataFrame, ::DataFrame, ::DataFrame; on=:DateTime, makeunique=true)","style":{"code":true}}]}]}]},{"client_msg_id":"f7db74fd-71b3-4af5-b873-a4d91513688c","type":"message","text":"Not getting the error with `innerjoin` though so I wonder how am I implementing `leftjoin` wrong?","user":"U01QJ915TFD","ts":"1616127193.041700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Kg4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not getting the error with "},{"type":"text","text":"innerjoin","style":{"code":true}},{"type":"text","text":" though so I wonder how am I implementing "},{"type":"text","text":"leftjoin","style":{"code":true}},{"type":"text","text":" wrong?"}]}]}]},{"client_msg_id":"1383fa15-e254-4608-9e02-6da23c0a956a","type":"message","text":"Letfjoin doesn't allow for more than 2 arguments. Remember to check the docs with `? leftjoin`","user":"UBF9YRB6H","ts":"1616131288.042600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WoLm8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Letfjoin doesn't allow for more than 2 arguments. Remember to check the docs with "},{"type":"text","text":"? leftjoin","style":{"code":true}}]}]}]},{"client_msg_id":"4F1F4582-52E2-4D2D-9AFF-A08C14837A55","type":"message","text":"<@UBF9YRB6H> Thanks. ","user":"U01QJ915TFD","ts":"1616145444.044000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fPEYI","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UBF9YRB6H"},{"type":"text","text":" Thanks. "}]}]}]},{"client_msg_id":"1145550f-8363-4c29-856a-79db95bf67b9","type":"message","text":"Is there a simple way to select columns by a property in a `DataFrame`? For example, I would like to select all columns with no missing entries","user":"U6BJ9E351","ts":"1616146653.044900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9Iz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a simple way to select columns by a property in a "},{"type":"text","text":"DataFrame","style":{"code":true}},{"type":"text","text":"? For example, I would like to select all columns with no missing entries"}]}]}],"thread_ts":"1616146653.044900","reply_count":4,"reply_users_count":2,"latest_reply":"1616146885.046100","reply_users":["U67431ELR","U6BJ9E351"],"subscribed":false},{"client_msg_id":"f524b58b-c30c-44fc-82fc-69d316decfef","type":"message","text":"I have (lots of) csv files with a `Vector{Float64}` column a la `1, 2, \"[1.0, 2.0, 3.0]\"` . Is there a good way to parse this into a `DataFrame`? Since this is a one-shot script i’ve tried the hacky\n```Base.parse(::Type{Vector{Float64}}, s::String) = eval(Meta.parse(s))\ndf = CSV.read(f, DataFrame, types=Dict(:correlation=&gt;Vector{Float64}))```\nwhich works but it is very slow…","user":"U01BYANF42K","ts":"1616159455.052500","team":"T68168MUP","edited":{"user":"U01BYANF42K","ts":"1616159618.000000"},"blocks":[{"type":"rich_text","block_id":"39QBH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have (lots of) csv files with a "},{"type":"text","text":"Vector{Float64}","style":{"code":true}},{"type":"text","text":" column a la "},{"type":"text","text":"1, 2, \"[1.0, 2.0, 3.0]\"","style":{"code":true}},{"type":"text","text":" . Is there a good way to parse this into a "},{"type":"text","text":"DataFrame","style":{"code":true}},{"type":"text","text":"? Since this is a one-shot script i’ve tried the hacky\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Base.parse(::Type{Vector{Float64}}, s::String) = eval(Meta.parse(s))\ndf = CSV.read(f, DataFrame, types=Dict(:correlation=>Vector{Float64}))"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"which works but it is very slow…"}]}]}]},{"client_msg_id":"F12BA2FD-D0EA-48C5-B469-00E23BB6BE16","type":"message","text":"How can you use leftjoin with multiple data frames? For working with just a few, leftjoining two at a time would work but  if I want to leftjoin 100 data frames, how can I join them faster?","user":"U01QJ915TFD","ts":"1616159976.057600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KqGp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can you use leftjoin with multiple data frames? For working with just a few, leftjoining two at a time would work but  if I want to leftjoin 100 data frames, how can I join them faster?"}]}]}]},{"client_msg_id":"18c20856-a045-4896-8280-eacbab566be4","type":"message","text":"I know there used to be some talk about adding Pandas-style row indices to `DataFrames.jl`, but it seems that has now been abandoned. Are there any other packages out there that provide this functionality?","user":"UENHZ1M08","ts":"1616172266.066000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Xtx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I know there used to be some talk about adding Pandas-style row indices to "},{"type":"text","text":"DataFrames.jl","style":{"code":true}},{"type":"text","text":", but it seems that has now been abandoned. Are there any other packages out there that provide this functionality?"}]}]}],"thread_ts":"1616172266.066000","reply_count":9,"reply_users_count":3,"latest_reply":"1616174925.067900","reply_users":["UBF9YRB6H","U681ELA87","UENHZ1M08"],"subscribed":false}]}