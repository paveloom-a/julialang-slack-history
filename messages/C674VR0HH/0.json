{"cursor": 1, "messages": [{"client_msg_id":"31e6bc4e-6b24-4755-8161-450af7b187b9","type":"message","text":"Suppose you’re trying to load data from multiple sources (e.g. , reddit, nyt, etc) into a SQLite DB. Would you store the date/time info in the sqlite db as text or as a number (i.e., as either the integer or real storage class in sqlite)?","user":"US8V7JSKB","ts":"1610569648.426000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1HAN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Suppose you’re trying to load data from multiple sources (e.g. , reddit, nyt, etc) into a SQLite DB. Would you store the date/time info in the sqlite db as text or as a number (i.e., as either the integer or real storage class in sqlite)?"}]}]}],"thread_ts":"1610569648.426000","reply_count":6,"reply_users_count":2,"latest_reply":"1610570546.432900","reply_users":["UH24GRBLL","US8V7JSKB"],"subscribed":false},{"client_msg_id":"673ffe2b-42b0-41f1-adef-40332cd09e83","type":"message","text":"What's the difference between the JuliaIO and JuliaData orgs?","user":"U6C1MMAAJ","ts":"1610570149.426900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FeI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What's the difference between the JuliaIO and JuliaData orgs?"}]}]}]},{"client_msg_id":"f79af1d5-981d-49d0-b4a6-9a2f7fd2fce5","type":"message","text":"Should we move HDF5.jl over to JuliaData? There seems to be overlap.","user":"U6C1MMAAJ","ts":"1610570155.427200","team":"T68168MUP","edited":{"user":"U6C1MMAAJ","ts":"1610570222.000000"},"blocks":[{"type":"rich_text","block_id":"K0/0R","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Should we move HDF5.jl over to JuliaData? There seems to be overlap."}]}]}]},{"client_msg_id":"fbab8fea-06af-4f40-9bd0-0e0c72b23368","type":"message","text":"JuliaData is primarily (but not exclusively) concerned with tabular data.\nJuliaIO is concerned with loading and savings all formats.\nHDF5 is nontabular so seems no strong argument to move.\nIt could join the handful of nontabular formats in JuliaData but I don't see a reason why it should","user":"U6A936746","ts":"1610570327.430900","team":"T68168MUP","edited":{"user":"U6A936746","ts":"1610570349.000000"},"blocks":[{"type":"rich_text","block_id":"wCSpi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"JuliaData is primarily (but not exclusively) concerned with tabular data.\nJuliaIO is concerned with loading and savings all formats.\nHDF5 is nontabular so seems no strong argument to move.\nIt could join the handful of nontabular formats in JuliaData but I don't see a reason why it should"}]}]}],"thread_ts":"1610570327.430900","reply_count":2,"reply_users_count":1,"latest_reply":"1610570375.431700","reply_users":["U6C1MMAAJ"],"subscribed":false},{"client_msg_id":"d641018a-112b-458e-9aae-09f32f4cd2e8","type":"message","text":"yeah, it’s not a strict division, but that’s the general idea.","user":"U681ELA87","ts":"1610570434.432400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xbZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yeah, it’s not a strict division, but that’s the general idea."}]}]}]},{"client_msg_id":"17b8812c-d33f-49d7-b6ba-3b196cc63cf1","type":"message","text":"Is SQLite.jl currently mature enough that it’d be safe for users who may not be skilled enough to dive into the source code to resolve any possible issues to use it? I’ve been trying to read up on SQLite.jl on the julia discourse before committing to using julia for my ETL, and it wasn’t clear if it’s mature / well-tested enough. (This isn’t a knock on SQLite.jl or anything like that, and a lot of great and hard work has clearly gone into it; I’m just trying to get a sense for its viability for a normal user.)","user":"US8V7JSKB","ts":"1610587628.005700","team":"T68168MUP","edited":{"user":"US8V7JSKB","ts":"1610587699.000000"},"blocks":[{"type":"rich_text","block_id":"n4t","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is SQLite.jl currently mature enough that it’d be safe for users who may not be skilled enough to dive into the source code to resolve any possible issues to use it? I’ve been trying to read up on SQLite.jl on the julia discourse before committing to using julia for my ETL, and it wasn’t clear if it’s mature / well-tested enough. (This isn’t a knock on SQLite.jl or anything like that, and a lot of great and hard work has clearly gone into it; I’m just trying to get a sense for its viability for a normal user.)"}]}]}],"thread_ts":"1610587628.005700","reply_count":1,"reply_users_count":1,"latest_reply":"1610591485.009400","reply_users":["U8JP5B9T2"],"subscribed":false},{"client_msg_id":"cae0266a-cc97-4a4c-a56d-4359a52eea60","type":"message","text":"I think the upgrade to DBInterface is producing an error on the execute function. DBInterface.execute statements are producing an error about an ambiguity with ODBC.execute. Downgrading to Do interface v2.2.0 fixes the error.","user":"UCAFZ51L3","ts":"1610587776.009300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"87yU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think the upgrade to DBInterface is producing an error on the execute function. DBInterface.execute statements are producing an error about an ambiguity with ODBC.execute. Downgrading to Do interface v2.2.0 fixes the error."}]}]}]},{"client_msg_id":"44f55d2c-7c8d-47c9-ab86-2adc510ecc99","type":"message","text":"I am getting an error when I try `using CSV`,\n`Failed to precompile CSV: CategoricalString not defined`","user":"UAH43TMUN","ts":"1610597930.010900","team":"T68168MUP","edited":{"user":"UAH43TMUN","ts":"1610597999.000000"},"blocks":[{"type":"rich_text","block_id":"+xW61","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am getting an error when I try "},{"type":"text","text":"using CSV","style":{"code":true}},{"type":"text","text":",\n"},{"type":"text","text":"Failed to precompile CSV: CategoricalString not defined","style":{"code":true}}]}]}]},{"client_msg_id":"fca8c4a5-2e74-49f0-afdd-cfa4c7fca580","type":"message","text":"Given where we currently are on `leftjoin` performance, should I have any hope for `leftjoin(df1, df2)` with `size(df1) == (31635463, 7)` and `size(df2) == (564537, 2)`?","user":"U7JQGPGCQ","ts":"1610634257.013200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TXUuH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Given where we currently are on "},{"type":"text","text":"leftjoin","style":{"code":true}},{"type":"text","text":" performance, should I have any hope for "},{"type":"text","text":"leftjoin(df1, df2)","style":{"code":true}},{"type":"text","text":" with "},{"type":"text","text":"size(df1) == (31635463, 7)","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"size(df2) == (564537, 2)","style":{"code":true}},{"type":"text","text":"?"}]}]}],"thread_ts":"1610634257.013200","reply_count":6,"reply_users_count":3,"latest_reply":"1610634926.014800","reply_users":["U67431ELR","U7JQGPGCQ","U6A936746"],"subscribed":false},{"client_msg_id":"d8ff4009-e0e8-464d-950d-17d70d12ae79","type":"message","text":"<@U6A936746> Can I pick your brain about a DataDeps thing?","user":"U8JP5B9T2","ts":"1610656132.017700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"d8O","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U6A936746"},{"type":"text","text":" Can I pick your brain about a DataDeps thing?"}]}]}],"thread_ts":"1610656132.017700","reply_count":7,"reply_users_count":2,"latest_reply":"1610656724.019100","reply_users":["U8JP5B9T2","U6A936746"],"subscribed":false},{"client_msg_id":"cb30e1a3-349a-4ed2-9f9e-b75ecc987ce3","type":"message","text":"It seems that matlab tables are not supported by `MAT.jl` or `MATLAB.jl`. Which do you think will be more performant/reliable:\n:one: Export matlab table as parquet and read that into julia using `Parquet.jl`\n:two: Convert matlab table to struct array and save as .mat file and read that into julia using `MAT.jl`","user":"U017JTQFNEQ","ts":"1610688541.029500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6fc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It seems that matlab tables are not supported by "},{"type":"text","text":"MAT.jl","style":{"code":true}},{"type":"text","text":" or "},{"type":"text","text":"MATLAB.jl","style":{"code":true}},{"type":"text","text":". Which do you think will be more performant/reliable:\n"},{"type":"emoji","name":"one"},{"type":"text","text":" Export matlab table as parquet and read that into julia using "},{"type":"text","text":"Parquet.jl","style":{"code":true}},{"type":"text","text":"\n"},{"type":"emoji","name":"two"},{"type":"text","text":" Convert matlab table to struct array and save as .mat file and read that into julia using "},{"type":"text","text":"MAT.jl","style":{"code":true}}]}]}]},{"client_msg_id":"76b1900a-c3ad-4bd9-a963-1b27cf37b2e4","type":"message","text":"Guten Morgen <#C674VR0HH|data>, does this snippet emit SIMD code? Is there a way for me to verify this?\n```concatenated_col = Vector{String}(undef, df_length)\n@threads for i in 1:df_length\n    @inbounds concatenated_col[i] = @inbounds df.input[i] * @inbounds df.target[i]\nend```\nside note: I have no idea what I’m doing with @inbounds. Is this overkill or are all three instances necessary?","user":"U01GXNFKY6R","ts":"1610698227.030600","team":"T68168MUP","edited":{"user":"U01GXNFKY6R","ts":"1610698276.000000"},"blocks":[{"type":"rich_text","block_id":"r19","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Guten Morgen "},{"type":"channel","channel_id":"C674VR0HH"},{"type":"text","text":", does this snippet emit SIMD code? Is there a way for me to verify this?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"concatenated_col = Vector{String}(undef, df_length)\n@threads for i in 1:df_length\n    @inbounds concatenated_col[i] = @inbounds df.input[i] * @inbounds df.target[i]\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"side note: I have no idea what I’m doing with @inbounds. Is this overkill or are all three instances necessary?"}]}]}],"thread_ts":"1610698227.030600","reply_count":7,"reply_users_count":3,"latest_reply":"1610699924.032400","reply_users":["U8JAMQGQY","U01GXNFKY6R","U67431ELR"],"subscribed":false},{"client_msg_id":"7a9748e1-8188-48ee-88e3-a3358c5561cc","type":"message","text":"How can I get the `n` th row directly for a `Tables.jl` compliant implementation ? I want to directly get `Tables.Row(Tables.rows(table), n)` .","user":"U93U5PMH9","ts":"1610957747.050300","team":"T68168MUP","edited":{"user":"U93U5PMH9","ts":"1610958172.000000"},"blocks":[{"type":"rich_text","block_id":"R0L","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I get the "},{"type":"text","text":"n","style":{"code":true}},{"type":"text","text":" th row directly for a "},{"type":"text","text":"Tables.jl","style":{"code":true}},{"type":"text","text":" compliant implementation ? I want to directly get "},{"type":"text","text":"Tables.Row(Tables.rows(table), n)","style":{"code":true}},{"type":"text","text":" ."}]}]}],"thread_ts":"1610957747.050300","reply_count":1,"reply_users_count":1,"latest_reply":"1610980898.055800","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"6006fa07-b3ec-4aba-9d20-ebe08b8754c4","type":"message","text":"I have used Chain.jl for the fist time on SO :smile: (CC <@UK1BNFHFV>): <https://stackoverflow.com/questions/65789320/how-to-reshape-group-by-and-rename-julia-dataframe/65790242#65790242>","user":"U8JAMQGQY","ts":"1611054244.056000","team":"T68168MUP","attachments":[{"service_name":"Stack Overflow","title":"How to reshape, group by and rename Julia dataframe?","title_link":"https://stackoverflow.com/questions/65789320/how-to-reshape-group-by-and-rename-julia-dataframe/65790242#65790242","text":"I have the following DataFrame : Police Product PV1 PV2 PV3 PM1 PM2 PM3 0 1 AA 10 8 14 150 145 140 1 2 AB 25 4 7 700 650 620 2 3 ...","fallback":"Stack Overflow: How to reshape, group by and rename Julia dataframe?","thumb_url":"https://cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon@2.png?v=73d79a89bded","from_url":"https://stackoverflow.com/questions/65789320/how-to-reshape-group-by-and-rename-julia-dataframe/65790242#65790242","thumb_width":316,"thumb_height":316,"service_icon":"https://cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon.png?v=c78bd457575a","id":1,"original_url":"https://stackoverflow.com/questions/65789320/how-to-reshape-group-by-and-rename-julia-dataframe/65790242#65790242"}],"blocks":[{"type":"rich_text","block_id":"eT4ii","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have used Chain.jl for the fist time on SO "},{"type":"emoji","name":"smile"},{"type":"text","text":" (CC "},{"type":"user","user_id":"UK1BNFHFV"},{"type":"text","text":"): "},{"type":"link","url":"https://stackoverflow.com/questions/65789320/how-to-reshape-group-by-and-rename-julia-dataframe/65790242#65790242"}]}]}],"thread_ts":"1611054244.056000","reply_count":3,"reply_users_count":2,"latest_reply":"1611062107.062200","reply_users":["U01GXNFKY6R","UK1BNFHFV"],"subscribed":false,"reactions":[{"name":"white_check_mark","users":["U01GXNFKY6R"],"count":1},{"name":"heart","users":["UK1BNFHFV","U017JTQFNEQ","UM4TSHKF1"],"count":3},{"name":"clapping","users":["UB197FRCL","U8JP5B9T2","U8T0YV7QC"],"count":3},{"name":"chains","users":["U01GXNFKY6R","U8T0YV7QC"],"count":2}]},{"client_msg_id":"c97e6c28-309d-4a03-8d47-a3698ed8d569","type":"message","text":"<@U681ELA87> Sorry to keep bugging you - I'm having some issues writing arrow files. Details in thread.","user":"U8JP5B9T2","ts":"1611083623.072300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0OR","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U681ELA87"},{"type":"text","text":" Sorry to keep bugging you - I'm having some issues writing arrow files. Details in thread."}]}]}],"thread_ts":"1611083623.072300","reply_count":4,"reply_users_count":2,"latest_reply":"1611086939.073000","reply_users":["U8JP5B9T2","U681ELA87"],"subscribed":false},{"client_msg_id":"5e1f5f92-ea46-43fc-adb9-ac536a90c37e","type":"message","text":"Hi guys\ni am struggling to read some json file which i am not just getting right,\nthe json looks like so\n```  {\n    \"Variable\": \"Returns\",\n    \"Data\":[\n        {\n\t\t\t\t\"item 1.0\": [{\n\t\t\t\t\t\t\"Value\": 0.0127041742286751361161524501,\n\t\t\t\t\t\t\"Date\": \"2019-04-01T00:00:00\"\n\t\t\t\t\t},\n\t\t\t\t\t{\n\t\t\t\t\t\t\"Value\": 0.0071684587813620071684587814,\n\t\t\t\t\t\t\"Date\": \"2019-04-02T00:00:00\"\n\t\t\t\t\t}],\n                \"item 2.0\": [\n                    {\n\t\t\t\t\t\t\"Value\": 0.0,\n\t\t\t\t\t\t\"Date\": \"2019-04-01T00:00:00\"\n\t\t\t\t\t},\n\t\t\t\t\t{\n\t\t\t\t\t\t\"Value\": -0.001779359430604982206405694,\n\t\t\t\t\t\t\"Date\": \"2019-04-02T00:00:00\"\n\t\t\t\t\t}\n                ]\n        }],\n    \"DataWeights\": [\n        {\n\t\t\t\t\"item 1.0\": [{\n\t\t\t\t\t\t\"Value\": 0.0336538461538461538461538462,\n\t\t\t\t\t\t\"Date\": \"2019-04-01T00:00:00\"\n\t\t\t\t\t},\n\t\t\t\t\t{\n\t\t\t\t\t\t\"Value\": 0.0,\n\t\t\t\t\t\t\"Date\": \"2019-04-02T00:00:00\"\n\t\t\t\t\t}],\n                \"item 2.0\": [\n                    {\n\t\t\t\t\t\t\"Value\": -0.0061099796334012219959266802,\n\t\t\t\t\t\t\"Date\": \"2019-04-01T00:00:00\"\n\t\t\t\t\t},\n\t\t\t\t\t{\n\t\t\t\t\t\t\"Value\": 0.0143442622950819672131147541,\n\t\t\t\t\t\t\"Date\": \"2019-04-02T00:00:00\"\n\t\t\t\t\t}\n                ]\n        }]\n  }```\nand I'd like a final output like below\n```4×4 DataFrame\n Row │ Date        Name      Data      Dataweights \n     │ String      String    Float64   Float64     \n─────┼─────────────────────────────────────────────\n   1 │ 2019-04-01  item 1.0   0.0127       0.03365\n   2 │ 2019-04-01  item 2.0   0.0         -0.0061\n   3 │ 2019-04-02  item 1.0   0.0071       0.0\n   4 │ 2019-04-02  item 2.0  -0.00177      0.014\n ```\nthe challenge mostly is on how to pick it date wise from both the weights and data.\nany help or suggestions i will appreciate.","user":"UPH1M2MB2","ts":"1611137031.076500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"exoJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi guys\ni am struggling to read some json file which i am not just getting right,\nthe json looks like so\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"  {\n    \"Variable\": \"Returns\",\n    \"Data\":[\n        {\n\t\t\t\t\"item 1.0\": [{\n\t\t\t\t\t\t\"Value\": 0.0127041742286751361161524501,\n\t\t\t\t\t\t\"Date\": \"2019-04-01T00:00:00\"\n\t\t\t\t\t},\n\t\t\t\t\t{\n\t\t\t\t\t\t\"Value\": 0.0071684587813620071684587814,\n\t\t\t\t\t\t\"Date\": \"2019-04-02T00:00:00\"\n\t\t\t\t\t}],\n                \"item 2.0\": [\n                    {\n\t\t\t\t\t\t\"Value\": 0.0,\n\t\t\t\t\t\t\"Date\": \"2019-04-01T00:00:00\"\n\t\t\t\t\t},\n\t\t\t\t\t{\n\t\t\t\t\t\t\"Value\": -0.001779359430604982206405694,\n\t\t\t\t\t\t\"Date\": \"2019-04-02T00:00:00\"\n\t\t\t\t\t}\n                ]\n        }],\n    \"DataWeights\": [\n        {\n\t\t\t\t\"item 1.0\": [{\n\t\t\t\t\t\t\"Value\": 0.0336538461538461538461538462,\n\t\t\t\t\t\t\"Date\": \"2019-04-01T00:00:00\"\n\t\t\t\t\t},\n\t\t\t\t\t{\n\t\t\t\t\t\t\"Value\": 0.0,\n\t\t\t\t\t\t\"Date\": \"2019-04-02T00:00:00\"\n\t\t\t\t\t}],\n                \"item 2.0\": [\n                    {\n\t\t\t\t\t\t\"Value\": -0.0061099796334012219959266802,\n\t\t\t\t\t\t\"Date\": \"2019-04-01T00:00:00\"\n\t\t\t\t\t},\n\t\t\t\t\t{\n\t\t\t\t\t\t\"Value\": 0.0143442622950819672131147541,\n\t\t\t\t\t\t\"Date\": \"2019-04-02T00:00:00\"\n\t\t\t\t\t}\n                ]\n        }]\n  }"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"and I'd like a final output like below\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"4×4 DataFrame\n Row │ Date        Name      Data      Dataweights \n     │ String      String    Float64   Float64     \n─────┼─────────────────────────────────────────────\n   1 │ 2019-04-01  item 1.0   0.0127       0.03365\n   2 │ 2019-04-01  item 2.0   0.0         -0.0061\n   3 │ 2019-04-02  item 1.0   0.0071       0.0\n   4 │ 2019-04-02  item 2.0  -0.00177      0.014\n "}]},{"type":"rich_text_section","elements":[{"type":"text","text":"the challenge mostly is on how to pick it date wise from both the weights and data.\nany help or suggestions i will appreciate."}]}]}],"thread_ts":"1611137031.076500","reply_count":2,"reply_users_count":2,"latest_reply":"1611192917.000200","reply_users":["U8JAMQGQY","UPH1M2MB2"],"subscribed":false},{"client_msg_id":"656bace2-12d8-4164-bcc0-b031f4f1ec12","type":"message","text":"Is there a version of the arrow implementation status page that has julia?\n<https://arrow.apache.org/docs/status.html>","user":"U6A936746","ts":"1611143646.076900","team":"T68168MUP","edited":{"user":"U6A936746","ts":"1611143689.000000"},"blocks":[{"type":"rich_text","block_id":"eSuR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a version of the arrow implementation status page that has julia?\n"},{"type":"link","url":"https://arrow.apache.org/docs/status.html"}]}]}],"thread_ts":"1611143646.076900","reply_count":3,"reply_users_count":2,"latest_reply":"1611154695.078600","reply_users":["U6A936746","U681ELA87"],"subscribed":false},{"client_msg_id":"9fc6c643-91d1-4d03-9b6e-c02819949e45","type":"message","text":"I have a column with ids and i want a new column that is the ordinalrank of those ids. I can achieve it like this, but i know there must be a 1-liner (right?)\n```dd = DataFrame(a = [1 1 1 5 5 6 6 6 101 10 10 22 5 2 1][:])\ntempd = Dict( a=&gt;b for (a,b) in zip(unique(dd.a), ordinalrank(unique(dd.a))))\ntransform(dd, :a=&gt;ByRow(x -&gt; tempd[x])=&gt;:idx)```","user":"U017JTQFNEQ","ts":"1611158749.080400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"duk/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a column with ids and i want a new column that is the ordinalrank of those ids. I can achieve it like this, but i know there must be a 1-liner (right?)\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"dd = DataFrame(a = [1 1 1 5 5 6 6 6 101 10 10 22 5 2 1][:])\ntempd = Dict( a=>b for (a,b) in zip(unique(dd.a), ordinalrank(unique(dd.a))))\ntransform(dd, :a=>ByRow(x -> tempd[x])=>:idx)"}]}]}],"thread_ts":"1611158749.080400","reply_count":3,"reply_users_count":2,"latest_reply":"1611199846.000600","reply_users":["U8JAMQGQY","U017JTQFNEQ"],"subscribed":false},{"client_msg_id":"5fd056fc-1754-4ba4-b2da-5d563d198723","type":"message","text":"Anyone with access to JuliaData or YAML.jl (<@U8JP5B9T2>?), there are a few very simple PRs that could use merging:\n- <https://github.com/JuliaData/YAML.jl/pull/96> (print empty collections properly)\n- <https://github.com/JuliaData/YAML.jl/pull/109> (fix 1.0 support)\n- <https://github.com/JuliaData/YAML.jl/pull/110> (update CI badge)\n- <https://github.com/JuliaData/YAML.jl/pull/111> (fix writing of single-line strings with dollar signs)","user":"U6NFPDBV1","ts":"1611165449.082800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ukf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Anyone with access to JuliaData or YAML.jl ("},{"type":"user","user_id":"U8JP5B9T2"},{"type":"text","text":"?), there are a few very simple PRs that could use merging:\n- "},{"type":"link","url":"https://github.com/JuliaData/YAML.jl/pull/96"},{"type":"text","text":" (print empty collections properly)\n- "},{"type":"link","url":"https://github.com/JuliaData/YAML.jl/pull/109"},{"type":"text","text":" (fix 1.0 support)\n- "},{"type":"link","url":"https://github.com/JuliaData/YAML.jl/pull/110"},{"type":"text","text":" (update CI badge)\n- "},{"type":"link","url":"https://github.com/JuliaData/YAML.jl/pull/111"},{"type":"text","text":" (fix writing of single-line strings with dollar signs)"}]}]}],"thread_ts":"1611165449.082800","reply_count":14,"reply_users_count":3,"latest_reply":"1611169212.085700","reply_users":["U6NFPDBV1","U681ELA87","U8JP5B9T2"],"subscribed":false},{"client_msg_id":"123fb093-d3b6-49be-a98d-6f4decc61f29","type":"message","text":"Is a `DataFrame` column allowed to be any abstract array or are there restrictions on what should be used as a column?","user":"U68M6ERG8","ts":"1611170052.086400","team":"T68168MUP","edited":{"user":"U68M6ERG8","ts":"1611170066.000000"},"blocks":[{"type":"rich_text","block_id":"VvKBI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is a "},{"type":"text","text":"DataFrame","style":{"code":true}},{"type":"text","text":" column allowed to be any abstract array or are there restrictions on what should be used as a column?"}]}]}],"thread_ts":"1611170052.086400","reply_count":4,"reply_users_count":3,"latest_reply":"1611175191.094000","reply_users":["U9VG1AYSG","U68M6ERG8","U8JAMQGQY"],"subscribed":false},{"client_msg_id":"97E8A79D-8123-411B-A7F2-461E57C7505E","type":"message","text":"I think the internal field is `AbstractVector[]`,","user":"U681ELA87","ts":"1611170159.087700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WYp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think the internal field is "},{"type":"text","text":"AbstractVector[]","style":{"code":true}},{"type":"text","text":","}]}]}],"reactions":[{"name":"white_check_mark","users":["U9VG1AYSG"],"count":1}]},{"client_msg_id":"f65978e8-f5eb-4c3a-80b5-71d56867b4a7","type":"message","text":"Is there a shorthand in the DataFrames mini-language for leaving missing values alone and only applying function to non-missing values? I tried eg\n\n```transform!(df, \"subject\" =&gt; ByRow(skipmissing ∘ (s-&gt; parse(Int, s))) =&gt; \"subject\")```\nbut no joy","user":"U8JP5B9T2","ts":"1611171486.089400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Jiv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a shorthand in the DataFrames mini-language for leaving missing values alone and only applying function to non-missing values? I tried eg\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"transform!(df, \"subject\" => ByRow(skipmissing ∘ (s-> parse(Int, s))) => \"subject\")"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"but no joy"}]}]}],"thread_ts":"1611171486.089400","reply_count":7,"reply_users_count":3,"latest_reply":"1611176199.094800","reply_users":["U68M6ERG8","U8JP5B9T2","U67431ELR"],"subscribed":false},{"client_msg_id":"7997d689-e041-40ab-8f6d-37d69cd9dad5","type":"message","text":"Would be nice to have something like `ByRow(fun, skipmissing=true)`","user":"U8JP5B9T2","ts":"1611171514.089900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IPYj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Would be nice to have something like "},{"type":"text","text":"ByRow(fun, skipmissing=true)","style":{"code":true}}]}]}],"thread_ts":"1611171514.089900","reply_count":5,"reply_users_count":4,"latest_reply":"1611240941.004300","reply_users":["U681ELA87","U8JAMQGQY","UBF9YRB6H","U8JP5B9T2"],"subscribed":false},{"client_msg_id":"51df6636-ee81-4898-9650-5bc091afcde5","type":"message","text":"Or `transform(... , skipmissing::Bool)` (plus things like arrays of `Bool` or `col=&gt;Bool` pairs)","user":"U8JP5B9T2","ts":"1611171644.091700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MFyH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Or "},{"type":"text","text":"transform(... , skipmissing::Bool)","style":{"code":true}},{"type":"text","text":" (plus things like arrays of "},{"type":"text","text":"Bool","style":{"code":true}},{"type":"text","text":" or "},{"type":"text","text":"col=>Bool","style":{"code":true}},{"type":"text","text":" pairs)"}]}]}]},{"client_msg_id":"51905396-ee1f-4062-8d7d-43320ef5c455","type":"message","text":"Are there packages similar to Query.jl but with less \"macro magic\"? I mean manipulation of table-like objects (mostly Vector{NamedTuple}) such as examples at <https://www.queryverse.org/Query.jl/stable/standalonequerycommands/>.","user":"UGTUKUHLN","ts":"1611226985.003400","team":"T68168MUP","edited":{"user":"UGTUKUHLN","ts":"1611227156.000000"},"blocks":[{"type":"rich_text","block_id":"pV0v","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Are there packages similar to Query.jl but with less \"macro magic\"? I mean manipulation of table-like objects (mostly Vector{NamedTuple}) such as examples at "},{"type":"link","url":"https://www.queryverse.org/Query.jl/stable/standalonequerycommands/"},{"type":"text","text":"."}]}]}],"thread_ts":"1611226985.003400","reply_count":42,"reply_users_count":8,"latest_reply":"1611349416.032200","reply_users":["UBF9YRB6H","U01GXNFKY6R","U681ELA87","UGTUKUHLN","U6A936746","UDGT4PM41","U01JRKTSL2U","U68UUUFPS"],"subscribed":false},{"client_msg_id":"643cf3cd-0aa6-43ec-871d-3eaec4899c31","type":"message","text":"DataFrames.jl 0.22.3 patch release is out. It fixes a minor bug in integration with Unitful.jl.","user":"U8JAMQGQY","ts":"1611253712.015100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jQYl/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"DataFrames.jl 0.22.3 patch release is out. It fixes a minor bug in integration with Unitful.jl."}]}]}],"reactions":[{"name":"+1","users":["U67431ELR","UBF9YRB6H","UB197FRCL","U01GXNFKY6R","U0163V6FU1H"],"count":5}]},{"client_msg_id":"3c7edfdd-8cbe-4e36-a2fd-8f6f17f52bc1","type":"message","text":"can somebody merge or comment on my PR to Tables.jl? (<https://github.com/JuliaData/Tables.jl/pull/226>).  It contains some small fixes to improve the usability of the `Tables.Columns` interface wrapper.  It's languished for a while now and it's blocking a package registration for me","user":"U9VG1AYSG","ts":"1611269434.017400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MmW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"can somebody merge or comment on my PR to Tables.jl? ("},{"type":"link","url":"https://github.com/JuliaData/Tables.jl/pull/226"},{"type":"text","text":").  It contains some small fixes to improve the usability of the "},{"type":"text","text":"Tables.Columns","style":{"code":true}},{"type":"text","text":" interface wrapper.  It's languished for a while now and it's blocking a package registration for me"}]}]}]},{"client_msg_id":"1cd32a37-f68b-458b-aefc-fc2f70ca47c6","type":"message","text":"Following the recent discussion on DataFrames.jl documentation quality this week I have posted another how-to guide in my blog: <https://bkamins.github.io/julialang/2021/01/22/transforming.html>","user":"U8JAMQGQY","ts":"1611310955.018400","team":"T68168MUP","attachments":[{"service_name":"Blog by Bogumił Kamiński","title":"Mass transformations of data frames how-to","title_link":"https://bkamins.github.io/julialang/2021/01/22/transforming.html","text":"Introduction","fallback":"Blog by Bogumił Kamiński: Mass transformations of data frames how-to","ts":1611301913,"from_url":"https://bkamins.github.io/julialang/2021/01/22/transforming.html","service_icon":"https://bkamins.github.io/favicon.ico","id":1,"original_url":"https://bkamins.github.io/julialang/2021/01/22/transforming.html"}],"blocks":[{"type":"rich_text","block_id":"K6R","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Following the recent discussion on DataFrames.jl documentation quality this week I have posted another how-to guide in my blog: "},{"type":"link","url":"https://bkamins.github.io/julialang/2021/01/22/transforming.html"}]}]}],"reactions":[{"name":"+1","users":["U01GXNFKY6R","U01K3MC53LY"],"count":2}]},{"client_msg_id":"258a1456-4399-476f-94f0-926581f86e00","type":"message","text":"Next month I will be teaching a workshop with <@URFTMGLQL> on \"Julia for Data Science\".  The audience members should have experience with R or Python but probably not Julia.\n\nI have a Pluto notebook <https://github.com/crsl4/julia-workshop/blob/main/notebooks/consistency.jl> with a vignette on checking values in a data column for consistency with keys in another column.  Part of the purpose here is to show that even if you don't know the higher-level way of doing data manipulation you can write code in a lower-level approach without too much pain.\n\nPlease let me know if\n1. I have misrepresented what goes on in Tables.jl\n2. I have missed an obviously superior approach","user":"UBGRZ7FSP","ts":"1611334183.026200","team":"T68168MUP","edited":{"user":"UBGRZ7FSP","ts":"1611334502.000000"},"blocks":[{"type":"rich_text","block_id":"AmG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Next month I will be teaching a workshop with "},{"type":"user","user_id":"URFTMGLQL"},{"type":"text","text":" on \"Julia for Data Science\".  The audience members should have experience with R or Python but probably not Julia.\n\nI have a Pluto notebook "},{"type":"link","url":"https://github.com/crsl4/julia-workshop/blob/main/notebooks/consistency.jl"},{"type":"text","text":" with a vignette on checking values in a data column for consistency with keys in another column.  Part of the purpose here is to show that even if you don't know the higher-level way of doing data manipulation you can write code in a lower-level approach without too much pain.\n\nPlease let me know if\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have misrepresented what goes on in Tables.jl"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I have missed an obviously superior approach"}]}],"style":"ordered","indent":0}]}],"thread_ts":"1611334183.026200","reply_count":2,"reply_users_count":2,"latest_reply":"1611336121.027600","reply_users":["U681ELA87","UBGRZ7FSP"],"subscribed":false,"reactions":[{"name":"heart","users":["U680THK2S","U6795JH6H","U66M57AN4","UCAFZ51L3","U01C15GH58B","U017JTQFNEQ","U85JBUGGP","UAZP7LJLU"],"count":8}]},{"type":"message","subtype":"channel_join","ts":"1611334190.026400","user":"URFTMGLQL","text":"<@URFTMGLQL> has joined the channel","inviter":"UBGRZ7FSP"},{"client_msg_id":"32096858-f363-45b7-9b13-09ea11bea15e","type":"message","text":"dear god does DataFrames.jl have a lot of unit tests these days","user":"U9VG1AYSG","ts":"1611337585.028100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eRhe","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"dear god does DataFrames.jl have a lot of unit tests these days"}]}]}]},{"client_msg_id":"0e1acbe3-f4f4-4c4e-816e-45959235f2ca","type":"message","text":"Just saw these latest benchmarks done this month and was quite surprised by how slow DataFrames.jl seems to be for joins, its ~10x slower than R dplyr: <https://h2oai.github.io/db-benchmark/>, though is faster than dplyr for groupby. I know the reason data.table wins is because multithreading but still very weird that dplyr beats DataFrames.jl for joins. Why might joins be this slow?","user":"U01EF0QVAB0","ts":"1611343078.030200","team":"T68168MUP","edited":{"user":"U01EF0QVAB0","ts":"1611343094.000000"},"blocks":[{"type":"rich_text","block_id":"GrP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Just saw these latest benchmarks done this month and was quite surprised by how slow DataFrames.jl seems to be for joins, its ~10x slower than R dplyr: "},{"type":"link","url":"https://h2oai.github.io/db-benchmark/"},{"type":"text","text":", though is faster than dplyr for groupby. I know the reason data.table wins is because multithreading but still very weird that dplyr beats DataFrames.jl for joins. Why might joins be this slow?"}]}]}],"thread_ts":"1611343078.030200","reply_count":6,"reply_users_count":4,"latest_reply":"1611350141.032400","reply_users":["U7JQGPGCQ","U9VG1AYSG","U8JP5B9T2","U8JAMQGQY"],"subscribed":false},{"client_msg_id":"5b70f5bd-e1fc-4aec-a1f6-bd429dd95697","type":"message","text":"I am applying a function to a column and I want this function to return two resulting columns... something like\n```function tst_done(vec) \n    z = zeros(Bool, length(vec))\n    for (i, x) in enumerate(vec)\n        if !ismissing(x) \n           z[i] = 1 \n        else \n           z[i] = 0\n        end\n    end\n    return (z = z, v = z)\nend\n\ntransform!(a, :Antigen =&gt; tst_done)```\nwhere it takes the column `Antigen` and the function `tst_done` will theoretically return two vectors. Anyway to do this? I can always do it manually.","user":"U6Z8377N2","ts":"1611369869.033900","team":"T68168MUP","edited":{"user":"U6Z8377N2","ts":"1611369881.000000"},"blocks":[{"type":"rich_text","block_id":"Eihhy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am applying a function to a column and I want this function to return two resulting columns... something like\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function tst_done(vec) \n    z = zeros(Bool, length(vec))\n    for (i, x) in enumerate(vec)\n        if !ismissing(x) \n           z[i] = 1 \n        else \n           z[i] = 0\n        end\n    end\n    return (z = z, v = z)\nend\n\ntransform!(a, :Antigen => tst_done)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"where it takes the column "},{"type":"text","text":"Antigen","style":{"code":true}},{"type":"text","text":" and the function "},{"type":"text","text":"tst_done","style":{"code":true}},{"type":"text","text":" will theoretically return two vectors. Anyway to do this? I can always do it manually."}]}]}],"thread_ts":"1611369869.033900","reply_count":1,"reply_users_count":1,"latest_reply":"1611374854.034200","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"7609aeec-c740-405c-b435-459a46696e0b","type":"message","text":"Happy Sunday y’all! I’m interested in implementing the C Data Interface for Apache Arrow, which should allow us to reuse pyarrow’s memory in Julia in a zero-copy way. I’ve translated the <https://arrow.apache.org/docs/format/CDataInterface.html|Structure Definitions> to Julia using Clang.jl, but I’m not sure where to go from here. Does anyone have ideas?","user":"U01GXNFKY6R","ts":"1611478490.038600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ao5ly","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Happy Sunday y’all! I’m interested in implementing the C Data Interface for Apache Arrow, which should allow us to reuse pyarrow’s memory in Julia in a zero-copy way. I’ve translated the "},{"type":"link","url":"https://arrow.apache.org/docs/format/CDataInterface.html","text":"Structure Definitions"},{"type":"text","text":" to Julia using Clang.jl, but I’m not sure where to go from here. Does anyone have ideas?"}]}]}],"thread_ts":"1611478490.038600","reply_count":2,"reply_users_count":2,"latest_reply":"1611567038.049700","reply_users":["U6A936746","U01GXNFKY6R"],"subscribed":false},{"client_msg_id":"28694823-1612-4c13-8a24-cf2c21f80eda","type":"message","text":"<@U8JAMQGQY> can you help me understand the reasoning for the following\n\n```df = DataFrame(a = [\"1\", \"2\"])\ndf[:, \"a\"]  = parse.(Int64, df[:, \"a\"])```\nthrows an error? but using `!` does not? My intuition would be the opposite: `:` means you can make *more* modifications to the data frame, not fewer, since you are copying everywhere.","user":"UBF9YRB6H","ts":"1611524015.041000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IBgi","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U8JAMQGQY"},{"type":"text","text":" can you help me understand the reasoning for the following\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"df = DataFrame(a = [\"1\", \"2\"])\ndf[:, \"a\"]  = parse.(Int64, df[:, \"a\"])"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nthrows an error? but using "},{"type":"text","text":"!","style":{"code":true}},{"type":"text","text":" does not? My intuition would be the opposite: "},{"type":"text","text":":","style":{"code":true}},{"type":"text","text":" means you can make "},{"type":"text","text":"more","style":{"bold":true}},{"type":"text","text":" modifications to the data frame, not fewer, since you are copying everywhere."}]}]}],"thread_ts":"1611524015.041000","reply_count":11,"reply_users_count":3,"latest_reply":"1611525934.044000","reply_users":["U8JAMQGQY","UBF9YRB6H","U681ELA87"],"subscribed":false},{"client_msg_id":"a1187456-5b4e-4861-ba8c-3135b0f3c594","type":"message","text":":wave: is there a straight forward way to do lag a column by something? I used to do this but `by` has been deprecated:\n```@&gt; begin\n    df\n    sort!([:category, :item, :date])\n    by([:category, :item], \n    lag_sales = :sales =&gt; Base.Fix2(ShiftedArrays.lag, 1))\nend```","user":"UUYRZ3LU8","ts":"1611538189.045900","team":"T68168MUP","edited":{"user":"UUYRZ3LU8","ts":"1611538196.000000"},"blocks":[{"type":"rich_text","block_id":"cNEL","elements":[{"type":"rich_text_section","elements":[{"type":"emoji","name":"wave"},{"type":"text","text":" is there a straight forward way to do lag a column by something? I used to do this but "},{"type":"text","text":"by","style":{"code":true}},{"type":"text","text":" has been deprecated:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@> begin\n    df\n    sort!([:category, :item, :date])\n    by([:category, :item], \n    lag_sales = :sales => Base.Fix2(ShiftedArrays.lag, 1))\nend"}]}]}],"thread_ts":"1611538189.045900","reply_count":7,"reply_users_count":4,"latest_reply":"1611582328.050900","reply_users":["UBF9YRB6H","UUYRZ3LU8","U7JQGPGCQ","U8JAMQGQY"],"subscribed":false},{"client_msg_id":"488870c3-5e5a-4113-83f7-857da731b474","type":"message","text":"is there a way to modify a value in a json file and write back to disk?\n\nedit: `JSON` can do it `JSON3` has immutables","user":"UH8A351DJ","ts":"1611542012.046500","team":"T68168MUP","edited":{"user":"UH8A351DJ","ts":"1611542294.000000"},"blocks":[{"type":"rich_text","block_id":"d3t","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is there a way to modify a value in a json file and write back to disk?\n\nedit: "},{"type":"text","text":"JSON","style":{"code":true}},{"type":"text","text":" can do it "},{"type":"text","text":"JSON3","style":{"code":true}},{"type":"text","text":" has immutables"}]}]}]},{"client_msg_id":"032102fd-2c76-4f74-ac64-e35e937d9cf0","type":"message","text":"I mean, json is a text format, so unless the value you're replacing has the exact same byte width, then it's not _really_ modifying in place. You can get the same \"mutability\" from JSON3 by doing `JSON3.read(json, Dict)` , modify the dict, then write it back out.","user":"U681ELA87","ts":"1611547844.048300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"frC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I mean, json is a text format, so unless the value you're replacing has the exact same byte width, then it's not "},{"type":"text","text":"really","style":{"italic":true}},{"type":"text","text":" modifying in place. You can get the same \"mutability\" from JSON3 by doing "},{"type":"text","text":"JSON3.read(json, Dict)","style":{"code":true}},{"type":"text","text":" , modify the dict, then write it back out."}]}]}],"thread_ts":"1611547844.048300","reply_count":2,"reply_users_count":2,"latest_reply":"1611601805.051300","reply_users":["U6A936746","UH8A351DJ"],"subscribed":false,"reactions":[{"name":"+1","users":["UH8A351DJ","U017JTQFNEQ"],"count":2}]},{"client_msg_id":"73c8c5be-5ea5-49d0-8e66-189f1e9e1475","type":"message","text":"finally merged my package [Shapley.jl](<https://gitlab.com/ExpandingMan/Shapley.jl>) to the general registry, in case anyone is interested.  Got stalled for a while because I needed to make a small change to Tables.jl","user":"U9VG1AYSG","ts":"1611618292.052300","team":"T68168MUP","attachments":[{"service_name":"GitLab","title":"Expanding Man / Shapley.jl","title_link":"https://gitlab.com/ExpandingMan/Shapley.jl","text":"feature importance from game theory in Julia","fallback":"GitLab: Expanding Man / Shapley.jl","thumb_url":"https://assets.gitlab-static.net/assets/gitlab_logo-7ae504fe4f68fdebb3c2034e36621930cd36ea87924c11ff65dbcb8ed50dca58.png","from_url":"https://gitlab.com/ExpandingMan/Shapley.jl","thumb_width":128,"thumb_height":128,"service_icon":"https://assets.gitlab-static.net/assets/touch-icon-iphone-5a9cee0e8a51212e70b90c87c12f382c428870c0ff67d1eb034d884b78d2dae7.png","id":1,"original_url":"https://gitlab.com/ExpandingMan/Shapley.jl"}],"blocks":[{"type":"rich_text","block_id":"hbt1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"finally merged my package [Shapley.jl]("},{"type":"link","url":"https://gitlab.com/ExpandingMan/Shapley.jl"},{"type":"text","text":") to the general registry, in case anyone is interested.  Got stalled for a while because I needed to make a small change to Tables.jl"}]}]}],"reactions":[{"name":"+1","users":["U7PGB5DU3","U01GXNFKY6R","UKG4WF8PJ","U969CNQU9"],"count":4},{"name":"mask-parrot","users":["U01GXNFKY6R"],"count":1}]},{"client_msg_id":"7ac27fee-f83c-4600-a4ec-9b7d43dd481d","type":"message","text":"How does it differ from <https://juliapackages.com/p/shapml>","user":"U01FR784NSW","ts":"1611620005.052800","team":"T68168MUP","attachments":[{"service_name":"Julia Packages","title":"ShapML.jl","title_link":"https://juliapackages.com/p/shapml","text":"A Julia package for interpretable machine learning with stochastic Shapley values","fallback":"Julia Packages: ShapML.jl","thumb_url":"https://juliapackages.com/julia_share.png","from_url":"https://juliapackages.com/p/shapml","thumb_width":1200,"thumb_height":1200,"service_icon":"https://juliapackages.com/julia.ico","id":1,"original_url":"https://juliapackages.com/p/shapml"}],"blocks":[{"type":"rich_text","block_id":"TGBH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How does it differ from "},{"type":"link","url":"https://juliapackages.com/p/shapml"}]}]}]},{"client_msg_id":"35c29a37-a6df-45f9-9c38-893d945aa009","type":"message","text":"well, I had attempted to use ShapML and it didn't have very good compatibility with anything else in the ecosystem.  I was going to make a PR to that to improve this,  but found that it was almost entirely written with DataFrames operations.  So, the main advantage of my package is that it's compatible with the Tables.jl interface, so it should work seamlessly with most stuff in the data ecosystem, particularly MLJ.  It's also likely to perform better under most circumstances (much of the work that went into Shapley.jl was to make sure that I always called `predict` methods on full datasets rather than individual data points, because most such methods are designed to be more efficient this way), but I have not thoroughly benchmarked it, so I could be wrong","user":"U9VG1AYSG","ts":"1611672330.056800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TkXMs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"well, I had attempted to use ShapML and it didn't have very good compatibility with anything else in the ecosystem.  I was going to make a PR to that to improve this,  but found that it was almost entirely written with DataFrames operations.  So, the main advantage of my package is that it's compatible with the Tables.jl interface, so it should work seamlessly with most stuff in the data ecosystem, particularly MLJ.  It's also likely to perform better under most circumstances (much of the work that went into Shapley.jl was to make sure that I always called "},{"type":"text","text":"predict","style":{"code":true}},{"type":"text","text":" methods on full datasets rather than individual data points, because most such methods are designed to be more efficient this way), but I have not thoroughly benchmarked it, so I could be wrong"}]}]}],"thread_ts":"1611672330.056800","reply_count":1,"reply_users_count":1,"latest_reply":"1611768362.107100","reply_users":["U01EF0QVAB0"],"subscribed":false,"reactions":[{"name":"+1","users":["U6A936746","UDGT4PM41","U01C15GH58B","U01EF0QVAB0"],"count":4}]},{"client_msg_id":"23ab8da7-f436-46a0-84bb-1868ebb8b729","type":"message","text":"I am trying to demonstrate the use of the Arrow file format to archive a data table in one language and read it in another.  The simplest case is to write an Arrow file from R using `arrow::write_feather`.  I can read such a file using `Arrow.Table` in Julia but I get an error message from `pyarrow.feather.read_feather` in Python.  My Python is pretty rusty so I'm not sure if I misunderstood the documentation or I have an older version of the package or ...","user":"UBGRZ7FSP","ts":"1611680827.061700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fTz+P","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am trying to demonstrate the use of the Arrow file format to archive a data table in one language and read it in another.  The simplest case is to write an Arrow file from R using "},{"type":"text","text":"arrow::write_feather","style":{"code":true}},{"type":"text","text":".  I can read such a file using "},{"type":"text","text":"Arrow.Table","style":{"code":true}},{"type":"text","text":" in Julia but I get an error message from "},{"type":"text","text":"pyarrow.feather.read_feather","style":{"code":true}},{"type":"text","text":" in Python.  My Python is pretty rusty so I'm not sure if I misunderstood the documentation or I have an older version of the package or ..."}]}]}]},{"client_msg_id":"27d9d888-151f-4369-b1f3-c1455fa22aa0","type":"message","text":"Sorry for asking a Pythonish question on this forum but can anyone point me to where I might see an example of reading a file in the Arrow file format using `pyarrow.feather.read_feather?`","user":"UBGRZ7FSP","ts":"1611680901.063100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"I380","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Sorry for asking a Pythonish question on this forum but can anyone point me to where I might see an example of reading a file in the Arrow file format using "},{"type":"text","text":"pyarrow.feather.read_feather?","style":{"code":true}}]}]}],"thread_ts":"1611680901.063100","reply_count":8,"reply_users_count":2,"latest_reply":"1611683673.068000","reply_users":["U681ELA87","UBGRZ7FSP"],"subscribed":false},{"client_msg_id":"20af31f2-fa7d-4c7f-b366-4801fd0c5238","type":"message","text":"speaking of arrow, has anyone ever tried to make them query-able through apache presto (aws athena)?  I've wanted to switch everything from using CSV to using Arrow, but I can't because my colleagues won't read them (and increasingly I am relying on presto/athena, so it's now a problem for me as well).  Unfortunately parquet seems to be the only binary format that's query-able with these.  Anybody do anything with this?","user":"U9VG1AYSG","ts":"1611682533.066300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3imZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"speaking of arrow, has anyone ever tried to make them query-able through apache presto (aws athena)?  I've wanted to switch everything from using CSV to using Arrow, but I can't because my colleagues won't read them (and increasingly I am relying on presto/athena, so it's now a problem for me as well).  Unfortunately parquet seems to be the only binary format that's query-able with these.  Anybody do anything with this?"}]}]}]},{"client_msg_id":"b8d14b5b-9f83-460c-b903-87029eb8cc29","type":"message","text":"Yeah, I'm not familiar enough with presto/athena to know if it's possible to query arrow files. I still would like to do a deep dive in Parquet.jl to get it up to the same amount of support it has in the c++ part of the apache/arrow project","user":"U681ELA87","ts":"1611683016.067300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"F5wO7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah, I'm not familiar enough with presto/athena to know if it's possible to query arrow files. I still would like to do a deep dive in Parquet.jl to get it up to the same amount of support it has in the c++ part of the apache/arrow project"}]}]}],"reactions":[{"name":"heart","users":["UDXST8ARK","U01C15GH58B","U01GXNFKY6R"],"count":3}]},{"client_msg_id":"E910EB55-AB50-48A7-96B7-ABD31187D18F","type":"message","text":"Hello, I have an array of a custom data structure representing different files with similar content. Now I want to vertically concatenate the multidimensional matrix stored in the field PP, i.e. something like dat[:].PP . I know that [dat[1].PP; dat[2].PP; dat[3].PP] works fine as only the size of the first dimension changes. What would be an efficient way to do that if the array becomes large? Can that be done in one line, or using array comprehension? I am still new to Julia programming and would appreciate a good hint. Thanks!","user":"U01GBPDL7SP","ts":"1611685208.077700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1=C","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello, I have an array of a custom data structure representing different files with similar content. Now I want to vertically concatenate the multidimensional matrix stored in the field PP, i.e. something like dat[:].PP . I know that [dat[1].PP; dat[2].PP; dat[3].PP] works fine as only the size of the first dimension changes. What would be an efficient way to do that if the array becomes large? Can that be done in one line, or using array comprehension? I am still new to Julia programming and would appreciate a good hint. Thanks!"}]}]}]},{"client_msg_id":"4095563b-f321-4d81-b4ef-0b77950a3b70","type":"message","text":"you probably want `reduce(vcat, (d.PP for d \\in dat))`","user":"U9VG1AYSG","ts":"1611685269.078300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sDO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you probably want "},{"type":"text","text":"reduce(vcat, (d.PP for d \\in dat))","style":{"code":true}}]}]}],"reactions":[{"name":"+1","users":["U01GBPDL7SP"],"count":1}]},{"client_msg_id":"EF28806A-BD27-4266-8DB6-06D0DA1278C7","type":"message","text":"Great, works like a charm. Thanks a lot, Michael","user":"U01GBPDL7SP","ts":"1611687796.079500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kMa9f","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Great, works like a charm. Thanks a lot, Michael"}]}]}]},{"client_msg_id":"2bf24142-c919-4ee6-944b-b8e46f16cbae","type":"message","text":"Blogpost about official Julia Arrow support up for consideration: <https://github.com/JuliaLang/www.julialang.org/pull/1129>. The Apache Arrow project just released 3.0.0 this morning, which is the first official release with Julia support included in the project.","user":"U681ELA87","ts":"1611690839.080600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"c28dm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Blogpost about official Julia Arrow support up for consideration: "},{"type":"link","url":"https://github.com/JuliaLang/www.julialang.org/pull/1129"},{"type":"text","text":". The Apache Arrow project just released 3.0.0 this morning, which is the first official release with Julia support included in the project."}]}]}],"reactions":[{"name":"+1","users":["U680THK2S","U6795JH6H","U67431ELR","U9VG1AYSG","UKG4WF8PJ","U6SHSF4R0","U969CNQU9","U011V2YN59N","UBGRZ7FSP","U680T6770","UDXST8ARK","U82LX4ACB","UCZ7VBGUD","U017JTQFNEQ","U01C15GH58B","U01GXNFKY6R","U66QZ3QF3"],"count":17},{"name":"tada","users":["U6795JH6H","U01FAHWCMFF","U67431ELR","U8D0QF5NZ","UCAFZ51L3","U82LX4ACB","U01CQTKB86N","U69KQT9DL","UCZ7VBGUD","U01GXNFKY6R","U66QZ3QF3"],"count":11}]},{"client_msg_id":"691f73e1-a919-49bb-8014-9cdeb1a24ebc","type":"message","text":"Does the 3.0.0 release of Arrow allow R and Pandas to use unsigned integer types as indices into a DictEncoding type?  If not we may want consider altering `PooledArrays.jl` to default to signed integer types for the ref array.","user":"UBGRZ7FSP","ts":"1611698589.085100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IRI3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does the 3.0.0 release of Arrow allow R and Pandas to use unsigned integer types as indices into a DictEncoding type?  If not we may want consider altering "},{"type":"text","text":"PooledArrays.jl","style":{"code":true}},{"type":"text","text":" to default to signed integer types for the ref array."}]}]}]},{"client_msg_id":"22ac5928-3276-49c0-9a5a-5d99df847a75","type":"message","text":"Good question. I doubt they changed things on their side; I thought in Arrow.jl we always convert to signed indices though; are you seeing issues?","user":"U681ELA87","ts":"1611699007.085600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"u+1An","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Good question. I doubt they changed things on their side; I thought in Arrow.jl we always convert to signed indices though; are you seeing issues?"}]}]}]},{"client_msg_id":"84241489-ae55-40ed-8ce0-89200c85ffa5","type":"message","text":"Yes, I read a CSV file then wrote an Arrow file and it ended up with UInt32 indices in the Arrow file.  I can read it using `pyarrow.feather.read_table` which reports","user":"UBGRZ7FSP","ts":"1611700099.089100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"L/L","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes, I read a CSV file then wrote an Arrow file and it ended up with UInt32 indices in the Arrow file.  I can read it using "},{"type":"text","text":"pyarrow.feather.read_table","style":{"code":true}},{"type":"text","text":" which reports"}]}]}]},{"client_msg_id":"b8b0e1f4-ff4d-4247-8f2d-690a0b57ef1b","type":"message","text":"```PyObject pyarrow.Table\nlab: dictionary&lt;values=string, indices=uint32, ordered=0&gt; not null\nsubid: dictionary&lt;values=string, indices=uint32, ordered=0&gt; not null\ntrial_num: int64 not null\ntrial_type: dictionary&lt;values=string, indices=uint32, ordered=0&gt;\nstimulus_num: int64\nlooking_time: double\ntrial_error: bool not null\ntrial_error_type: dictionary&lt;values=string, indices=uint32, ordered=0&gt;\nmethod: dictionary&lt;values=string, indices=uint32, ordered=0&gt; not null\nra: dictionary&lt;values=string, indices=uint32, ordered=0&gt;```\n","user":"UBGRZ7FSP","ts":"1611700115.089300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jz0","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"PyObject pyarrow.Table\nlab: dictionary<values=string, indices=uint32, ordered=0> not null\nsubid: dictionary<values=string, indices=uint32, ordered=0> not null\ntrial_num: int64 not null\ntrial_type: dictionary<values=string, indices=uint32, ordered=0>\nstimulus_num: int64\nlooking_time: double\ntrial_error: bool not null\ntrial_error_type: dictionary<values=string, indices=uint32, ordered=0>\nmethod: dictionary<values=string, indices=uint32, ordered=0> not null\nra: dictionary<values=string, indices=uint32, ordered=0>"}]},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"cba027a3-a202-4c55-a84f-91351fc10450","type":"message","text":"if I remember correctly the arrow spec requires signed indices for everything, so technically I don't think anything is supposed to support that","user":"U9VG1AYSG","ts":"1611700403.090000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"E9eo9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"if I remember correctly the arrow spec requires signed indices for everything, so technically I don't think anything is supposed to support that"}]}]}]},{"client_msg_id":"19bd6d27-a436-470e-8368-7f887f7d2515","type":"message","text":"I am using Arrow.jl v1.2.1","user":"UBGRZ7FSP","ts":"1611700545.090400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2m1nQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am using Arrow.jl v1.2.1"}]}]}]},{"client_msg_id":"d1eee205-90a0-43b2-a3b3-d578d583decf","type":"message","text":"In\n```julia&gt; using DBInterface, SQLite, Tables\n\njulia&gt; db = DBInterface.connect(SQLite.DB, \"db.sqlite\");\n\njulia&gt; DBInterface.execute(db, \"CREATE TABLE testtable (col1, col2);\");\n\njulia&gt; DBInterface.execute(db, \"SELECT * FROM testtable;\") |&gt; Tables.columntable\n(q = SQLite.Query[],)\n\njulia&gt; DBInterface.execute(db, \"INSERT INTO testtable VALUES (1, 2);\");\n\njulia&gt; DBInterface.execute(db, \"SELECT * FROM testtable;\") |&gt; Tables.columntable\n(col1 = [1], col2 = [2])```\nwhy do I not get `(col1 = [], col2 = [])` for the first select? The information seems to be there.","user":"U67SCG4HG","ts":"1611704667.092000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4yJp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> using DBInterface, SQLite, Tables\n\njulia> db = DBInterface.connect(SQLite.DB, \"db.sqlite\");\n\njulia> DBInterface.execute(db, \"CREATE TABLE testtable (col1, col2);\");\n\njulia> DBInterface.execute(db, \"SELECT * FROM testtable;\") |> Tables.columntable\n(q = SQLite.Query[],)\n\njulia> DBInterface.execute(db, \"INSERT INTO testtable VALUES (1, 2);\");\n\njulia> DBInterface.execute(db, \"SELECT * FROM testtable;\") |> Tables.columntable\n(col1 = [1], col2 = [2])"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"why do I not get "},{"type":"text","text":"(col1 = [], col2 = [])","style":{"code":true}},{"type":"text","text":" for the first select? The information seems to be there."}]}]}],"thread_ts":"1611704667.092000","reply_count":1,"reply_users_count":1,"latest_reply":"1611764231.099200","reply_users":["U681ELA87"],"subscribed":false},{"client_msg_id":"0af69e45-65e6-46fa-be21-01ad140e5c45","type":"message","text":"Anyone know what's going on here?","user":"U69BL50BF","ts":"1611707435.092200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GvPU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Anyone know what's going on here?"}]}]}]},{"client_msg_id":"83a125dc-d69e-4c0d-8662-c57857afa7dc","type":"message","text":"```ERROR: LoadError: ArgumentError: Package Indexing [313cdc1a-70c2-5d6a-ae34-0150d3930a38] is required but does not seem to be installed:\n - Run `Pkg.instantiate()` to install all recorded dependencies.\n\nStacktrace:\n [1] _require(::Base.PkgId) at .\\loading.jl:999\n [2] require(::Base.PkgId) at .\\loading.jl:928\n [3] require(::Module, ::Symbol) at .\\loading.jl:923\n [4] include(::Function, ::Module, ::String) at .\\Base.jl:380\n [5] include(::Module, ::String) at .\\Base.jl:368\n [6] top-level scope at none:2\n [7] eval at .\\boot.jl:331 [inlined]\n [8] eval(::Expr) at .\\client.jl:467\n [9] top-level scope at .\\none:3\nin expression starting at C:\\Users\\accou\\.julia\\packages\\SplitApplyCombine\\2fFYd\\src\\SplitApplyCombine.jl:4\nERROR: LoadError: Failed to precompile SplitApplyCombine [03a91e81-4c3e-53e1-a0a4-9c0c8f19dd66] to C:\\Users\\accou\\.julia\\compiled\\v1.5\\SplitApplyCombine\\vbX6o_e1Vc1.ji.\nStacktrace:\n [1] error(::String) at .\\error.jl:33\n [2] compilecache(::Base.PkgId, ::String) at .\\loading.jl:1305\n [3] _require(::Base.PkgId) at .\\loading.jl:1030\n [4] require(::Base.PkgId) at .\\loading.jl:928\n [5] require(::Module, ::Symbol) at .\\loading.jl:923\n [6] include(::Function, ::Module, ::String) at .\\Base.jl:380\n [7] include(::Module, ::String) at .\\Base.jl:368\n [8] top-level scope at none:2\n [9] eval at .\\boot.jl:331 [inlined]\n [10] eval(::Expr) at .\\client.jl:467\n [11] top-level scope at .\\none:3\nin expression starting at C:\\Users\\accou\\.julia\\packages\\TypedTables\\ZF2b3\\src\\TypedTables.jl:5\nERROR: LoadError: LoadError: Failed to precompile TypedTables [9d95f2ec-7b3d-5a63-8d20-e2491e220bb9] to C:\\Users\\accou\\.julia\\compiled\\v1.5\\TypedTables\\NU69s_e1Vc1.ji.\nStacktrace:\n [1] error(::String) at .\\error.jl:33\n [2] compilecache(::Base.PkgId, ::String) at .\\loading.jl:1305\n [3] _require(::Base.PkgId) at .\\loading.jl:1030\n [4] require(::Base.PkgId) at .\\loading.jl:928\n [5] require(::Module, ::Symbol) at .\\loading.jl:923\n [6] include(::Function, ::Module, ::String) at .\\Base.jl:380\n [7] include at .\\Base.jl:368 [inlined]\n [8] include(::String) at C:\\Users\\accou\\.julia\\packages\\Catlab\\yfjLt\\src\\Catlab.jl:1\n [9] top-level scope at C:\\Users\\accou\\.julia\\packages\\Catlab\\yfjLt\\src\\Catlab.jl:6\n [10] include(::Function, ::Module, ::String) at .\\Base.jl:380\n [11] include(::Module, ::String) at .\\Base.jl:368\n [12] top-level scope at none:2\n [13] eval at .\\boot.jl:331 [inlined]\n [14] eval(::Expr) at .\\client.jl:467\n [15] top-level scope at .\\none:3\nin expression starting at C:\\Users\\accou\\.julia\\packages\\Catlab\\yfjLt\\src\\categorical_algebra\\CSetDataStructures.jl:14\nin expression starting at C:\\Users\\accou\\.julia\\packages\\Catlab\\yfjLt\\src\\Catlab.jl:6\nERROR: LoadError: Failed to precompile Catlab [134e5e36-593f-5add-ad60-77f754baafbe] to C:\\Users\\accou\\.julia\\compiled\\v1.5\\Catlab\\fBQ1G_e1Vc1.ji.\nStacktrace:\n [1] error(::String) at .\\error.jl:33\n [2] compilecache(::Base.PkgId, ::String) at .\\loading.jl:1305\n [3] _require(::Base.PkgId) at .\\loading.jl:1030\n [4] require(::Base.PkgId) at .\\loading.jl:928\n [5] require(::Module, ::Symbol) at .\\loading.jl:923\n [6] include(::Function, ::Module, ::String) at .\\Base.jl:380\n [7] include(::Module, ::String) at .\\Base.jl:368\n [8] top-level scope at none:2\n [9] eval at .\\boot.jl:331 [inlined]\n [10] eval(::Expr) at .\\client.jl:467\n [11] top-level scope at .\\none:3\nin expression starting at C:\\Users\\accou\\.julia\\packages\\Catalyst\\cSPjT\\src\\Catalyst.jl:29```","user":"U69BL50BF","ts":"1611707436.092500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QIgZ","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: LoadError: ArgumentError: Package Indexing [313cdc1a-70c2-5d6a-ae34-0150d3930a38] is required but does not seem to be installed:\n - Run `Pkg.instantiate()` to install all recorded dependencies.\n\nStacktrace:\n [1] _require(::Base.PkgId) at .\\loading.jl:999\n [2] require(::Base.PkgId) at .\\loading.jl:928\n [3] require(::Module, ::Symbol) at .\\loading.jl:923\n [4] include(::Function, ::Module, ::String) at .\\Base.jl:380\n [5] include(::Module, ::String) at .\\Base.jl:368\n [6] top-level scope at none:2\n [7] eval at .\\boot.jl:331 [inlined]\n [8] eval(::Expr) at .\\client.jl:467\n [9] top-level scope at .\\none:3\nin expression starting at C:\\Users\\accou\\.julia\\packages\\SplitApplyCombine\\2fFYd\\src\\SplitApplyCombine.jl:4\nERROR: LoadError: Failed to precompile SplitApplyCombine [03a91e81-4c3e-53e1-a0a4-9c0c8f19dd66] to C:\\Users\\accou\\.julia\\compiled\\v1.5\\SplitApplyCombine\\vbX6o_e1Vc1.ji.\nStacktrace:\n [1] error(::String) at .\\error.jl:33\n [2] compilecache(::Base.PkgId, ::String) at .\\loading.jl:1305\n [3] _require(::Base.PkgId) at .\\loading.jl:1030\n [4] require(::Base.PkgId) at .\\loading.jl:928\n [5] require(::Module, ::Symbol) at .\\loading.jl:923\n [6] include(::Function, ::Module, ::String) at .\\Base.jl:380\n [7] include(::Module, ::String) at .\\Base.jl:368\n [8] top-level scope at none:2\n [9] eval at .\\boot.jl:331 [inlined]\n [10] eval(::Expr) at .\\client.jl:467\n [11] top-level scope at .\\none:3\nin expression starting at C:\\Users\\accou\\.julia\\packages\\TypedTables\\ZF2b3\\src\\TypedTables.jl:5\nERROR: LoadError: LoadError: Failed to precompile TypedTables [9d95f2ec-7b3d-5a63-8d20-e2491e220bb9] to C:\\Users\\accou\\.julia\\compiled\\v1.5\\TypedTables\\NU69s_e1Vc1.ji.\nStacktrace:\n [1] error(::String) at .\\error.jl:33\n [2] compilecache(::Base.PkgId, ::String) at .\\loading.jl:1305\n [3] _require(::Base.PkgId) at .\\loading.jl:1030\n [4] require(::Base.PkgId) at .\\loading.jl:928\n [5] require(::Module, ::Symbol) at .\\loading.jl:923\n [6] include(::Function, ::Module, ::String) at .\\Base.jl:380\n [7] include at .\\Base.jl:368 [inlined]\n [8] include(::String) at C:\\Users\\accou\\.julia\\packages\\Catlab\\yfjLt\\src\\Catlab.jl:1\n [9] top-level scope at C:\\Users\\accou\\.julia\\packages\\Catlab\\yfjLt\\src\\Catlab.jl:6\n [10] include(::Function, ::Module, ::String) at .\\Base.jl:380\n [11] include(::Module, ::String) at .\\Base.jl:368\n [12] top-level scope at none:2\n [13] eval at .\\boot.jl:331 [inlined]\n [14] eval(::Expr) at .\\client.jl:467\n [15] top-level scope at .\\none:3\nin expression starting at C:\\Users\\accou\\.julia\\packages\\Catlab\\yfjLt\\src\\categorical_algebra\\CSetDataStructures.jl:14\nin expression starting at C:\\Users\\accou\\.julia\\packages\\Catlab\\yfjLt\\src\\Catlab.jl:6\nERROR: LoadError: Failed to precompile Catlab [134e5e36-593f-5add-ad60-77f754baafbe] to C:\\Users\\accou\\.julia\\compiled\\v1.5\\Catlab\\fBQ1G_e1Vc1.ji.\nStacktrace:\n [1] error(::String) at .\\error.jl:33\n [2] compilecache(::Base.PkgId, ::String) at .\\loading.jl:1305\n [3] _require(::Base.PkgId) at .\\loading.jl:1030\n [4] require(::Base.PkgId) at .\\loading.jl:928\n [5] require(::Module, ::Symbol) at .\\loading.jl:923\n [6] include(::Function, ::Module, ::String) at .\\Base.jl:380\n [7] include(::Module, ::String) at .\\Base.jl:368\n [8] top-level scope at none:2\n [9] eval at .\\boot.jl:331 [inlined]\n [10] eval(::Expr) at .\\client.jl:467\n [11] top-level scope at .\\none:3\nin expression starting at C:\\Users\\accou\\.julia\\packages\\Catalyst\\cSPjT\\src\\Catalyst.jl:29"}]}]}],"thread_ts":"1611707436.092500","reply_count":2,"reply_users_count":2,"latest_reply":"1611755974.095600","reply_users":["U66QZ3QF3","U69BL50BF"],"subscribed":false},{"client_msg_id":"19c778cd-9a47-4f88-94f8-05b21a723227","type":"message","text":"it's happening even when I instantiate...","user":"U69BL50BF","ts":"1611707443.092800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/GW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it's happening even when I instantiate..."}]}]}]},{"client_msg_id":"c5cb405e-f561-41ec-aaa6-6f105f24d3b4","type":"message","text":"Hello JuliaData github org members - I was wondering if someone could make me an admin for <https://github.com/JuliaData/SplitApplyCombine.jl> ? Thanks :slightly_smiling_face:","user":"U66QZ3QF3","ts":"1611744715.094200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tK50","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello JuliaData github org members - I was wondering if someone could make me an admin for "},{"type":"link","url":"https://github.com/JuliaData/SplitApplyCombine.jl"},{"type":"text","text":" ? Thanks "},{"type":"emoji","name":"slightly_smiling_face"}]}]}],"thread_ts":"1611744715.094200","reply_count":3,"reply_users_count":2,"latest_reply":"1611744997.094900","reply_users":["U67431ELR","U66QZ3QF3"],"subscribed":false},{"client_msg_id":"00850188-741c-447b-9e56-7811370f998f","type":"message","text":"<@U681ELA87> I opened <https://github.com/JuliaData/Arrow.jl/issues/113> regarding unsigned indices in the Arrow.jl-generated DictEncoding types.  If you don't have time to look at this and can point me to the section of code where conversion to signed ints is expected to happen I can take a look.","user":"UBGRZ7FSP","ts":"1611764414.101200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bkph","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U681ELA87"},{"type":"text","text":" I opened "},{"type":"link","url":"https://github.com/JuliaData/Arrow.jl/issues/113"},{"type":"text","text":" regarding unsigned indices in the Arrow.jl-generated DictEncoding types.  If you don't have time to look at this and can point me to the section of code where conversion to signed ints is expected to happen I can take a look."}]}]}],"thread_ts":"1611764414.101200","reply_count":4,"reply_users_count":2,"latest_reply":"1611766767.103800","reply_users":["U681ELA87","UBGRZ7FSP"],"subscribed":false},{"client_msg_id":"cd02f951-1ddd-49fc-9d7f-7b909ce1c7cd","type":"message","text":"I'm trying to understand if/how the Arrow format and Parquet format can be used together.\nI see the Apache Arrow [FAQ](<https://arrow.apache.org/faq/>) says\n&gt;  Arrow and Parquet complement each other and are commonly used together in applications. Storing your data on disk using Parquet and reading it into memory in the Arrow format\nBut i'm not 100% sure what this means and i couldn't see how to do it with Parquet.jl and Arrow.jl\nIs e.g. reading Parquet files as an Arrow table possible?","user":"UDXST8ARK","ts":"1611766429.103500","team":"T68168MUP","edited":{"user":"UDXST8ARK","ts":"1611766804.000000"},"blocks":[{"type":"rich_text","block_id":"=ZUa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm trying to understand if/how the Arrow format and Parquet format can be used together.\nI see the Apache Arrow [FAQ]("},{"type":"link","url":"https://arrow.apache.org/faq/"},{"type":"text","text":") says\n"}]},{"type":"rich_text_quote","elements":[{"type":"text","text":" Arrow and Parquet complement each other and are commonly used together in applications. Storing your data on disk using Parquet and reading it into memory in the Arrow format"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"But i'm not 100% sure what this means and i couldn't see how to do it with Parquet.jl and Arrow.jl\nIs e.g. reading Parquet files as an Arrow table possible?"}]}]}],"thread_ts":"1611766429.103500","reply_count":3,"reply_users_count":2,"latest_reply":"1611769002.108400","reply_users":["UDXST8ARK","U681ELA87"],"subscribed":false},{"client_msg_id":"05cecde8-0ef2-46fb-99b7-284d3ffb607c","type":"message","text":"I'm betraying my ignorance, here. But let's say I'm doing a project that is one-off, i.e. nothing else depends on it. I've been using `includet` so far, but I think it's time to make my project a module. What's the workflow again? I shouldn't use `LOAD_PATH`. Should I put it on `git`? I don't want it to be in my global environment or any other environment for that matter","user":"UBF9YRB6H","ts":"1611767280.106100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"CjF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm betraying my ignorance, here. But let's say I'm doing a project that is one-off, i.e. nothing else depends on it. I've been using "},{"type":"text","text":"includet","style":{"code":true}},{"type":"text","text":" so far, but I think it's time to make my project a module. What's the workflow again? I shouldn't use "},{"type":"text","text":"LOAD_PATH","style":{"code":true}},{"type":"text","text":". Should I put it on "},{"type":"text","text":"git","style":{"code":true}},{"type":"text","text":"? I don't want it to be in my global environment or any other environment for that matter"}]}]}],"thread_ts":"1611767280.106100","reply_count":10,"reply_users_count":3,"latest_reply":"1611776930.108900","reply_users":["U6A936746","UBF9YRB6H","U01FKQQ7J0J"],"subscribed":false},{"client_msg_id":"549f486e-cb7c-45fd-96cc-e2a778f9b48d","type":"message","text":"How can I do `standardize(ZScoreTransform, arraywithmissings)`?","user":"U01ARRMLM7E","ts":"1611886680.116200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"l9aP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I do "},{"type":"text","text":"standardize(ZScoreTransform, arraywithmissings)","style":{"code":true}},{"type":"text","text":"?"}]}]}],"thread_ts":"1611886680.116200","reply_count":1,"reply_users_count":1,"latest_reply":"1611887259.116300","reply_users":["UB197FRCL"],"subscribed":false},{"client_msg_id":"4e6ebb17-b137-4fbe-96ce-ada706800ec0","type":"message","text":"`df.c = Vector{Union{Missing, Float64}}(undef, nrow(df))`\n`func(df[!,:a], 20, df[!,:c])`\nIs this the correct way to add a column (c) to a dataframe?\nThen add to it by performing a function using the input from another column (a)","user":"U019S5MR0EN","ts":"1611919803.119400","team":"T68168MUP","edited":{"user":"U019S5MR0EN","ts":"1611920023.000000"},"blocks":[{"type":"rich_text","block_id":"dM5AO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"df.c = Vector{Union{Missing, Float64}}(undef, nrow(df))","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"func(df[!,:a], 20, df[!,:c])","style":{"code":true}},{"type":"text","text":"\nIs this the correct way to add a column (c) to a dataframe?\nThen add to it by performing a function using the input from another column (a)"}]}]}],"thread_ts":"1611919803.119400","reply_count":3,"reply_users_count":2,"latest_reply":"1611923851.121100","reply_users":["U8JAMQGQY","U019S5MR0EN"],"subscribed":false},{"client_msg_id":"c577d242-39cc-47fc-a94d-b4e0c571b6b5","type":"message","text":"Hi folks, I am having a small issue with importing datasets. I have 2 files, both in CSV, and they are relatively large (2GB each), I want to import them and then merge them on the basis of unique ID they have. Right now, I am using `CSV.jl` and my computer is protesting weirdly when I try to import them in Julia.\n\nBefore, I was trying to do same thing in R but again I was not able to get them through merging process. In Julia, I am not able to even import the second file (only one is importing). Wondering if there are some nice libraries which could make it possible? I sure, Julia must be having something, that I am missing!!\n\nThanks in advance!","user":"U01A0S07875","ts":"1611930069.127700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5JD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi folks, I am having a small issue with importing datasets. I have 2 files, both in CSV, and they are relatively large (2GB each), I want to import them and then merge them on the basis of unique ID they have. Right now, I am using "},{"type":"text","text":"CSV.jl","style":{"code":true}},{"type":"text","text":" and my computer is protesting weirdly when I try to import them in Julia.\n\nBefore, I was trying to do same thing in R but again I was not able to get them through merging process. In Julia, I am not able to even import the second file (only one is importing). Wondering if there are some nice libraries which could make it possible? I sure, Julia must be having something, that I am missing!!\n\nThanks in advance!"}]}]}],"thread_ts":"1611930069.127700","reply_count":59,"reply_users_count":6,"latest_reply":"1612011897.172900","reply_users":["UBF9YRB6H","U01A0S07875","U01CQTKB86N","U67431ELR","U681ELA87","U8JAMQGQY"],"subscribed":false},{"client_msg_id":"bd9cf471-0bb9-4174-b476-fd33c10e99a5","type":"message","text":"Are the any `CategoicalArrays`  that support 0-based indexing etc.?","user":"UAZP7LJLU","ts":"1611932059.145900","team":"T68168MUP","edited":{"user":"UAZP7LJLU","ts":"1611932066.000000"},"blocks":[{"type":"rich_text","block_id":"YI17W","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Are the any "},{"type":"text","text":"CategoicalArrays","style":{"code":true}},{"type":"text","text":"  that support 0-based indexing etc.?"}]}]}],"thread_ts":"1611932059.145900","reply_count":2,"reply_users_count":2,"latest_reply":"1611932562.154000","reply_users":["U67431ELR","U681ELA87"],"subscribed":false},{"client_msg_id":"41d6e8b1-93f2-43a2-92a4-b7e4d4777dff","type":"message","text":"```a = [:a=&gt;1 :b=&gt;2; :a=&gt;3 :b=&gt;4]\n\n# Want equivalent of \nDataFrame(a = [1,3], b=[2,4])```","user":"U017JTQFNEQ","ts":"1611935154.157000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qGKZG","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"a = [:a=>1 :b=>2; :a=>3 :b=>4]\n\n# Want equivalent of \nDataFrame(a = [1,3], b=[2,4])"}]}]}],"thread_ts":"1611935154.157000","reply_count":6,"reply_users_count":2,"latest_reply":"1611935394.158100","reply_users":["U017JTQFNEQ","UBF9YRB6H"],"subscribed":false},{"client_msg_id":"e5e863c7-f3a0-4d7c-a25c-6ea942116cb6","type":"message","text":"Not exactly table-like, but <@U680THK2S> could you take a look at <https://github.com/JuliaCollections/OrderedCollections.jl/issues/71>? Seems like a pretty stealthy and dangerous bug so it would be nice to patch it promptly.","user":"U82LX4ACB","ts":"1611948991.164400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QuQda","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not exactly table-like, but "},{"type":"user","user_id":"U680THK2S"},{"type":"text","text":" could you take a look at "},{"type":"link","url":"https://github.com/JuliaCollections/OrderedCollections.jl/issues/71"},{"type":"text","text":"? Seems like a pretty stealthy and dangerous bug so it would be nice to patch it promptly."}]}]}]},{"client_msg_id":"fa6eb033-c71b-471a-9a21-8a11a64ae1ba","type":"message","text":"I don't know when I'll have time to look at that, and I'm not super familiar with the OrderedCollections internals","user":"U680THK2S","ts":"1611949358.165100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j7/r","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I don't know when I'll have time to look at that, and I'm not super familiar with the OrderedCollections internals"}]}]}]},{"client_msg_id":"5f941ff2-a9de-4f55-8349-557655756ddd","type":"message","text":"Is there a package or function to add a new column to a `DataFrame` with some numerical ranking based on the ordering of another column?","user":"UCAFZ51L3","ts":"1611958178.167800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"BH1lm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a package or function to add a new column to a "},{"type":"text","text":"DataFrame","style":{"code":true}},{"type":"text","text":" with some numerical ranking based on the ordering of another column?"}]}]}],"thread_ts":"1611958178.167800","reply_count":9,"reply_users_count":3,"latest_reply":"1611960959.170400","reply_users":["UCZ7VBGUD","U67431ELR","UCAFZ51L3"],"subscribed":false},{"client_msg_id":"4c4c4a44-27ee-4027-bc49-b14d6649f004","type":"message","text":"Why does this function (PercentageChange) allocate?\n```function PercentageChange(input,output)\n    N = length(input)\n    output[1] = missing\n    output[2:N] = @view(input[2:N]) ./ @view(input[1:N-1]) .- 1\nend\n\nfunction test()\n    N = 10^8\n    d = Normal()\n    df = DataFrame(:data =&gt; rand(d, N))\n    df.PctChange = Vector{Union{Missing, Float64}}(undef, nrow(df))\n    @btime PercentageChange($df[!,:data], $df[!,:PctChange])\nend```\n407.100 ms (2 allocations: 762.94 MiB)","user":"U019S5MR0EN","ts":"1612005320.171600","team":"T68168MUP","edited":{"user":"U019S5MR0EN","ts":"1612005727.000000"},"blocks":[{"type":"rich_text","block_id":"i3P","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Why does this function (PercentageChange) allocate?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function PercentageChange(input,output)\n    N = length(input)\n    output[1] = missing\n    output[2:N] = @view(input[2:N]) ./ @view(input[1:N-1]) .- 1\nend\n\nfunction test()\n    N = 10^8\n    d = Normal()\n    df = DataFrame(:data => rand(d, N))\n    df.PctChange = Vector{Union{Missing, Float64}}(undef, nrow(df))\n    @btime PercentageChange($df[!,:data], $df[!,:PctChange])\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"407.100 ms (2 allocations: 762.94 MiB)"}]}]}],"thread_ts":"1612005320.171600","reply_count":4,"reply_users_count":2,"latest_reply":"1612006593.172500","reply_users":["U8JAMQGQY","U019S5MR0EN"],"subscribed":false},{"client_msg_id":"e43986d3-6043-4fdc-aea9-63c97132d418","type":"message","text":"Can I use DataFrames with custom row index type, other than Integer value? Suppose I want to index rows by a set of unique values (like alphabet: `:a, :b, :c`) or by some increasing floating-point scale, or by timestamps?","user":"UB2QSHWPN","ts":"1612013479.175200","team":"T68168MUP","edited":{"user":"UB2QSHWPN","ts":"1612013532.000000"},"blocks":[{"type":"rich_text","block_id":"iO=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can I use DataFrames with custom row index type, other than Integer value? Suppose I want to index rows by a set of unique values (like alphabet: "},{"type":"text","text":":a, :b, :c","style":{"code":true}},{"type":"text","text":") or by some increasing floating-point scale, or by timestamps?"}]}]}],"thread_ts":"1612013479.175200","reply_count":7,"reply_users_count":3,"latest_reply":"1612014286.176600","reply_users":["U8JAMQGQY","UB2QSHWPN","U67431ELR"],"subscribed":false},{"client_msg_id":"686ab13a-6419-4d10-b930-92c68760670b","type":"message","text":"it seems that updating to PooledArrays 1.0.0 reverts back DataFrames and CSV versions to very old ones. Both packages are still depending on v0.5 of PooledArrays <@U681ELA87> <@U8JAMQGQY>","user":"U6SHSF4R0","ts":"1612020421.178800","team":"T68168MUP","edited":{"user":"U6SHSF4R0","ts":"1612020856.000000"},"blocks":[{"type":"rich_text","block_id":"dhsf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it seems that updating to PooledArrays 1.0.0 reverts back DataFrames and CSV versions to very old ones. Both packages are still depending on v0.5 of PooledArrays "},{"type":"user","user_id":"U681ELA87"},{"type":"text","text":" "},{"type":"user","user_id":"U8JAMQGQY"}]}]}],"thread_ts":"1612020421.178800","reply_count":6,"reply_users_count":3,"latest_reply":"1612025710.185000","reply_users":["U8JAMQGQY","U681ELA87","U6SHSF4R0"],"subscribed":false},{"client_msg_id":"d5d55a6f-1125-434b-b886-d0ddae403736","type":"message","text":"Why do the two functions have very different run times? Shouldn't they compile to the same code?\n```@inbounds function MovingAverage(period, input, output)\n    output[1:(period-1)] .= missing\n    sm = mean(@view input[1:period])\n    output[period] = sm\n    @simd for i in (period+1):lastindex(input)\n        sm += (input[i] - input[i-period]) / period\n        output[i] = sm\n    end\n    return output\nend```\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     536.300 μs (0.00% GC)\n  median time:      605.200 μs (0.00% GC)\n  mean time:        701.222 μs (0.00% GC)\n  maximum time:     3.247 ms (0.00% GC)\n  --------------\n  samples:          7107\n  evals/sample:     1\n\nVS\n```function MovingAverage(period, input, output)\n    @inbounds begin\n        output[1:(period-1)] .= missing\n        sm = mean(@view input[1:period])\n        output[period] = sm\n        @simd for i in (period+1):lastindex(input)\n            sm += (input[i] - input[i-period]) / period\n            output[i] = sm\n        end\n        return output\n    end\nend```\nBenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     308.600 μs (0.00% GC)\n  median time:      320.100 μs (0.00% GC)\n  mean time:        350.521 μs (0.00% GC)\n  maximum time:     2.049 ms (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     1","user":"U019S5MR0EN","ts":"1612020542.179300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"R1R","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Why do the two functions have very different run times? Shouldn't they compile to the same code?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@inbounds function MovingAverage(period, input, output)\n    output[1:(period-1)] .= missing\n    sm = mean(@view input[1:period])\n    output[period] = sm\n    @simd for i in (period+1):lastindex(input)\n        sm += (input[i] - input[i-period]) / period\n        output[i] = sm\n    end\n    return output\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"BenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     536.300 μs (0.00% GC)\n  median time:      605.200 μs (0.00% GC)\n  mean time:        701.222 μs (0.00% GC)\n  maximum time:     3.247 ms (0.00% GC)\n  --------------\n  samples:          7107\n  evals/sample:     1\n\nVS\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function MovingAverage(period, input, output)\n    @inbounds begin\n        output[1:(period-1)] .= missing\n        sm = mean(@view input[1:period])\n        output[period] = sm\n        @simd for i in (period+1):lastindex(input)\n            sm += (input[i] - input[i-period]) / period\n            output[i] = sm\n        end\n        return output\n    end\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"BenchmarkTools.Trial:\n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     308.600 μs (0.00% GC)\n  median time:      320.100 μs (0.00% GC)\n  mean time:        350.521 μs (0.00% GC)\n  maximum time:     2.049 ms (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     1"}]}]}],"thread_ts":"1612020542.179300","reply_count":6,"reply_users_count":3,"latest_reply":"1612023692.181800","reply_users":["U67SCG4HG","U019S5MR0EN","UH24GRBLL"],"subscribed":false},{"client_msg_id":"559ab9fe-9924-4254-a636-7588c4d744dc","type":"message","text":"Videos of the presentations at RStudio::Global 2021 held earlier this month are now available.  Neal Richardson of Ursa Labs spoke about the R package for Arrow.  The link is","user":"UBGRZ7FSP","ts":"1612024430.184200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6/btG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Videos of the presentations at RStudio::Global 2021 held earlier this month are now available.  Neal Richardson of Ursa Labs spoke about the R package for Arrow.  The link is"}]}]}]},{"client_msg_id":"5fa29913-a3bd-412c-b272-74688fa73fb5","type":"message","text":"<https://rstudio.com/resources/rstudioglobal-2021/bigger-data-with-ease-using-apache-arrow/>","user":"UBGRZ7FSP","ts":"1612024435.184400","team":"T68168MUP","attachments":[{"title":"Bigger Data With Ease Using Apache Arrow","title_link":"https://rstudio.com/resources/rstudioglobal-2021/bigger-data-with-ease-using-apache-arrow/","text":"R is unparalleled in its ability to transform raw data into a wide array of beautiful graphics, all within the same environment.","fallback":"Bigger Data With Ease Using Apache Arrow","image_url":"https://rstudio.com/resources/rstudioglobal-2021/bigger-data-with-ease-using-apache-arrow/thumbnail.jpg","ts":1611187200,"from_url":"https://rstudio.com/resources/rstudioglobal-2021/bigger-data-with-ease-using-apache-arrow/","image_width":444,"image_height":250,"image_bytes":332333,"service_icon":"https://rstudio.com/apple-touch-icon.png","service_name":"rstudio.com","id":1,"original_url":"https://rstudio.com/resources/rstudioglobal-2021/bigger-data-with-ease-using-apache-arrow/"}],"blocks":[{"type":"rich_text","block_id":"WEP","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://rstudio.com/resources/rstudioglobal-2021/bigger-data-with-ease-using-apache-arrow/"}]}]}],"thread_ts":"1612024435.184400","reply_count":1,"reply_users_count":1,"latest_reply":"1612139338.199800","reply_users":["U6GD6JN2K"],"subscribed":false},{"client_msg_id":"331c90ab-9ed2-4d13-8849-87a1d28f3ff7","type":"message","text":"This week I decided to write a post about `!` row selector in DataFrames.jl as I see it often used (and I feel it is overused) and recently there were some questions about the details and reasons of differences between `!` and `:` row selectors. I hope it helps (though the reading is probably a bit heavy this time): <https://bkamins.github.io/julialang/2021/01/30/bang.html>","user":"U8JAMQGQY","ts":"1612028921.186700","team":"T68168MUP","attachments":[{"service_name":"Blog by Bogumił Kamiński","title":"On the bang row selector in DataFrames.jl","title_link":"https://bkamins.github.io/julialang/2021/01/30/bang.html","text":"Introduction","fallback":"Blog by Bogumił Kamiński: On the bang row selector in DataFrames.jl","ts":1611995064,"from_url":"https://bkamins.github.io/julialang/2021/01/30/bang.html","service_icon":"https://bkamins.github.io/favicon.ico","id":1,"original_url":"https://bkamins.github.io/julialang/2021/01/30/bang.html"}],"blocks":[{"type":"rich_text","block_id":"=yML","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This week I decided to write a post about "},{"type":"text","text":"!","style":{"code":true}},{"type":"text","text":" row selector in DataFrames.jl as I see it often used (and I feel it is overused) and recently there were some questions about the details and reasons of differences between "},{"type":"text","text":"!","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":":","style":{"code":true}},{"type":"text","text":" row selectors. I hope it helps (though the reading is probably a bit heavy this time): "},{"type":"link","url":"https://bkamins.github.io/julialang/2021/01/30/bang.html"}]}]}],"thread_ts":"1612028921.186700","reply_count":37,"reply_users_count":6,"latest_reply":"1612130049.199400","reply_users":["U6A936746","UDXST8ARK","U6SHSF4R0","U8JAMQGQY","UBF9YRB6H","UH8A351DJ"],"subscribed":false,"reactions":[{"name":"heart","users":["U6A936746","U681ELA87","U6795JH6H","U01C15GH58B","U7PGB5DU3","UAZP7LJLU","UPH1M2MB2","U012J1E5SFR","U6GD6JN2K","U68RVK2D7","UC2H100V8","U68QW0PUZ"],"count":12}]},{"client_msg_id":"b9240a53-96b2-4d88-85e8-f95d26884c11","type":"message","text":"<@UB2QSHWPN> I just read your review on TimeSeries.jl\nI :heart: it.\nlet's overhaul the package.\nI will have more free time as chinese new year is comming :stuck_out_tongue:","user":"U68RVK2D7","ts":"1612166259.201600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7JG","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UB2QSHWPN"},{"type":"text","text":" I just read your review on TimeSeries.jl\nI "},{"type":"emoji","name":"heart"},{"type":"text","text":" it.\nlet's overhaul the package.\nI will have more free time as chinese new year is comming "},{"type":"emoji","name":"stuck_out_tongue"}]}]}],"thread_ts":"1612166259.201600","reply_count":1,"reply_users_count":1,"latest_reply":"1612193052.209000","reply_users":["UB2QSHWPN"],"subscribed":false,"reactions":[{"name":"+1","users":["U7JQGPGCQ","UB2QSHWPN","ULMSM9MAL","U01C15GH58B"],"count":4}]},{"client_msg_id":"b556aafd-d6cb-4f3f-a454-77b54f7f7362","type":"message","text":"What is the R function that can read Arrow.jl's Arrow.write()? I just tried feather::read_feather() but it said\n```Error in openFeather(path) : Invalid: Not a feather file\nCalls: read_feather -&gt; feather -&gt; openFeather```","user":"U01ARRMLM7E","ts":"1612171394.202800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"aWtSt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What is the R function that can read Arrow.jl's Arrow.write()? I just tried feather::read_feather() but it said\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Error in openFeather(path) : Invalid: Not a feather file\nCalls: read_feather -> feather -> openFeather"}]}]}],"thread_ts":"1612171394.202800","reply_count":4,"reply_users_count":2,"latest_reply":"1612220438.214900","reply_users":["U67431ELR","UBGRZ7FSP"],"subscribed":false},{"client_msg_id":"7feb4efa-e530-4ade-aea9-3dda2980b437","type":"message","text":"Do we have any up-to-date BigQuery client library? The only recent one I can see is GCP.jl, but it seems quite bare-bones, and doesn't seem to allow authentication with anything other than a service account token.","user":"U7EF5AWHW","ts":"1612180968.206100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+HJem","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do we have any up-to-date BigQuery client library? The only recent one I can see is GCP.jl, but it seems quite bare-bones, and doesn't seem to allow authentication with anything other than a service account token."}]}]}]},{"client_msg_id":"7da18ca9-6c60-4705-96d8-5d975239bc3a","type":"message","text":"Is there a ~straightforward~ generic way with a `Tables.jl` -compatible object to transpose? Eg I have a 1 row N col table, and I want that to become an N row x 2 col table (where one column contains the headers of the original table, and the other has the row values)","user":"U8JP5B9T2","ts":"1612190566.208600","team":"T68168MUP","edited":{"user":"U8JP5B9T2","ts":"1612190596.000000"},"blocks":[{"type":"rich_text","block_id":"8FfK2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a "},{"type":"text","text":"straightforward","style":{"strike":true}},{"type":"text","text":" generic way with a "},{"type":"text","text":"Tables.jl","style":{"code":true}},{"type":"text","text":" -compatible object to transpose? Eg I have a 1 row N col table, and I want that to become an N row x 2 col table (where one column contains the headers of the original table, and the other has the row values)"}]}]}],"thread_ts":"1612190566.208600","reply_count":7,"reply_users_count":4,"latest_reply":"1612342903.220600","reply_users":["U8JAMQGQY","U8JP5B9T2","U681ELA87","U68QW0PUZ"],"subscribed":false},{"client_msg_id":"cb25385a-485c-4b70-b8ac-178d5db33642","type":"message","text":"Trying to write a DF to a file, but coming across this,\n\n```julia&gt; CSV.write(\"file.txt\", df)\nERROR: MethodError: no method matching length(::Symbol)```\nIs there another simple way of doing this?","user":"ULDQSHD41","ts":"1612215754.213000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Xve","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Trying to write a DF to a file, but coming across this,\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> CSV.write(\"file.txt\", df)\nERROR: MethodError: no method matching length(::Symbol)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nIs there another simple way of doing this?"}]}]}],"thread_ts":"1612215754.213000","reply_count":5,"reply_users_count":3,"latest_reply":"1612223979.215400","reply_users":["U681ELA87","ULDQSHD41","U6A936746"],"subscribed":false},{"client_msg_id":"8b8b5c6f-1bf6-4b52-9c86-7945df10d94b","type":"message","text":"Is there a version of `DataFrames.combine` that will get me past `a single value or vector result is required`?","user":"U69CJGKEY","ts":"1612303550.217700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"t=Pvm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a version of "},{"type":"text","text":"DataFrames.combine","style":{"code":true}},{"type":"text","text":" that will get me past "},{"type":"text","text":"a single value or vector result is required","style":{"code":true}},{"type":"text","text":"?"}]}]}],"thread_ts":"1612303550.217700","reply_count":11,"reply_users_count":3,"latest_reply":"1612306254.219900","reply_users":["UBF9YRB6H","U69CJGKEY","U8JAMQGQY"],"subscribed":false},{"client_msg_id":"367c3e1b-a627-43da-88f7-ee6c1b0c67da","type":"message","text":"How do I allow `ByRow` functions to have `Vector` input?\n```f(x::AbstractVector) = sum(x)\nselect( df, All()  =&gt; ByRow(f) =&gt; \"Output\" )```\nproduces error since the column values are parsed as positional arguments to f\n```MethodError: no method matching f(::Number, ::Number, ::Number, ... )```","user":"UMAJCM1DF","ts":"1612349086.223200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yLa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How do I allow "},{"type":"text","text":"ByRow","style":{"code":true}},{"type":"text","text":" functions to have "},{"type":"text","text":"Vector","style":{"code":true}},{"type":"text","text":" input?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"f(x::AbstractVector) = sum(x)\nselect( df, All()  => ByRow(f) => \"Output\" )"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"produces error since the column values are parsed as positional arguments to f\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"MethodError: no method matching f(::Number, ::Number, ::Number, ... )"}]}]}],"thread_ts":"1612349086.223200","reply_count":6,"reply_users_count":2,"latest_reply":"1612353177.224500","reply_users":["UMAJCM1DF","U8JAMQGQY"],"subscribed":false},{"client_msg_id":"a5200465-0f62-47f1-b88b-c0d1d046648c","type":"message","text":"Has anything changed in DataFrame joins recently? That big join I posted about a few days ago on here seems to have gone down from ~1 min to ~3 sec? EDIT: spoke to soon, I actually changed the join condition without noticing :D","user":"U7JQGPGCQ","ts":"1612357156.225600","team":"T68168MUP","edited":{"user":"U7JQGPGCQ","ts":"1612357567.000000"},"blocks":[{"type":"rich_text","block_id":"PqcoN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Has anything changed in DataFrame joins recently? That big join I posted about a few days ago on here seems to have gone down from ~1 min to ~3 sec? EDIT: spoke to soon, I actually changed the join condition without noticing :D"}]}]}],"thread_ts":"1612357156.225600","reply_count":8,"reply_users_count":4,"latest_reply":"1612377085.235900","reply_users":["U8JAMQGQY","U7JQGPGCQ","U6740K1SP","UM4TSHKF1"],"subscribed":false,"reactions":[{"name":"fast_parrot","users":["UH24GRBLL","U66M57AN4"],"count":2},{"name":"sonic","users":["UH24GRBLL","U66M57AN4"],"count":2},{"name":"sweat_smile","users":["U01GXNFKY6R"],"count":1}]},{"client_msg_id":"6d862b0a-771a-40b8-a94f-99f70c13ffb3","type":"message","text":"What's the most efficient way to concatenate the first character of four string columns into a new column, i.e. `reduce(.*, eachcol(first.(string.(df[:, end-3:end]))))`","user":"U7JQGPGCQ","ts":"1612362886.228200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ysw1L","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What's the most efficient way to concatenate the first character of four string columns into a new column, i.e. "},{"type":"text","text":"reduce(.*, eachcol(first.(string.(df[:, end-3:end]))))","style":{"code":true}}]}]}],"thread_ts":"1612362886.228200","reply_count":8,"reply_users_count":3,"latest_reply":"1612383288.236300","reply_users":["UBF9YRB6H","U7JQGPGCQ","U8JAMQGQY"],"subscribed":false},{"client_msg_id":"0f2180d6-2b9d-435e-910e-3c89f124ac1e","type":"message","text":"is there seriously no way to query a fixed number of rows at random from a sql database?  look at [this](<https://stackoverflow.com/questions/8674718/best-way-to-select-random-rows-postgresql>) horror...","user":"U9VG1AYSG","ts":"1612366492.229900","team":"T68168MUP","attachments":[{"service_name":"Stack Overflow","title":"Best way to select random rows PostgreSQL","title_link":"https://stackoverflow.com/questions/8674718/best-way-to-select-random-rows-postgresql","text":"I want a random selection of rows in PostgreSQL, I tried this: select * from table where random() &lt; 0.01; But some other recommend this: select * from table order by random() limit 1000; I h...","fallback":"Stack Overflow: Best way to select random rows PostgreSQL","thumb_url":"https://cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon@2.png?v=73d79a89bded","from_url":"https://stackoverflow.com/questions/8674718/best-way-to-select-random-rows-postgresql","thumb_width":316,"thumb_height":316,"service_icon":"https://cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon.png?v=c78bd457575a","id":1,"original_url":"https://stackoverflow.com/questions/8674718/best-way-to-select-random-rows-postgresql"}],"blocks":[{"type":"rich_text","block_id":"bHdE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is there seriously no way to query a fixed number of rows at random from a sql database?  look at [this]("},{"type":"link","url":"https://stackoverflow.com/questions/8674718/best-way-to-select-random-rows-postgresql"},{"type":"text","text":") horror..."}]}]}],"thread_ts":"1612366492.229900","reply_count":10,"reply_users_count":5,"latest_reply":"1612370995.235100","reply_users":["U7JQGPGCQ","U9VG1AYSG","U681ELA87","UH24GRBLL","ULG5V164A"],"subscribed":false,"reactions":[{"name":"wat","users":["U6740K1SP"],"count":1}]},{"client_msg_id":"af0c6528-b501-42e2-b086-17da857a175a","type":"message","text":"Glad we are doing `subset` with a grouped data frame. Just performed a filtering operation of that sort in R.","user":"UBF9YRB6H","ts":"1612367716.232300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UQmtg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Glad we are doing "},{"type":"text","text":"subset","style":{"code":true}},{"type":"text","text":" with a grouped data frame. Just performed a filtering operation of that sort in R."}]}]}],"thread_ts":"1612367716.232300","reply_count":1,"reply_users_count":1,"latest_reply":"1612367906.232400","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"2ed62711-0273-400a-8d4c-a385018ad02f","type":"message","text":"is there a more idiomatic way of doing carry-forward of values in a dataframe than this answer? <https://stackoverflow.com/questions/41196748/julia-dataframe-fill-na-with-locf>","user":"UGKHXS9J6","ts":"1612368989.233500","team":"T68168MUP","attachments":[{"service_name":"Stack Overflow","title":"Julia DataFrame Fill NA with LOCF","title_link":"https://stackoverflow.com/questions/41196748/julia-dataframe-fill-na-with-locf","text":"Is there any fast way to convert a DataFrame's NA values to the last observed value? using DataFrames d = @data [1,NA,5,NA,NA] df = DataFrame(d=d) result = filled_with_locf(df) expected = [1,1,...","fallback":"Stack Overflow: Julia DataFrame Fill NA with LOCF","thumb_url":"https://cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon@2.png?v=73d79a89bded","from_url":"https://stackoverflow.com/questions/41196748/julia-dataframe-fill-na-with-locf","thumb_width":316,"thumb_height":316,"service_icon":"https://cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon.png?v=c78bd457575a","id":1,"original_url":"https://stackoverflow.com/questions/41196748/julia-dataframe-fill-na-with-locf"}],"blocks":[{"type":"rich_text","block_id":"Cz+Cm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is there a more idiomatic way of doing carry-forward of values in a dataframe than this answer? "},{"type":"link","url":"https://stackoverflow.com/questions/41196748/julia-dataframe-fill-na-with-locf"}]}]}]},{"client_msg_id":"b344e5e3-4a93-4a71-892c-84b34859f457","type":"message","text":"I'm sure that answer works fine, but figured I'd ask since the ecosystem has changed a lot since 2016","user":"UGKHXS9J6","ts":"1612369006.234100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"U20H1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm sure that answer works fine, but figured I'd ask since the ecosystem has changed a lot since 2016"}]}]}],"thread_ts":"1612369006.234100","reply_count":2,"reply_users_count":2,"latest_reply":"1612369085.234400","reply_users":["U8JAMQGQY","UGKHXS9J6"],"subscribed":false},{"client_msg_id":"b0484c99-4baf-4110-86b4-6b940a3a38bb","type":"message","text":"Is it sensible to make a type that contains a dataframe and the column names/types specified? This could enable analysis and plotting functions based on that type.","user":"U017JTQFNEQ","ts":"1612405077.237900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yua","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is it sensible to make a type that contains a dataframe and the column names/types specified? This could enable analysis and plotting functions based on that type."}]}]}],"thread_ts":"1612405077.237900","reply_count":10,"reply_users_count":4,"latest_reply":"1612441583.242400","reply_users":["UBF9YRB6H","U017JTQFNEQ","U681ELA87","U6A936746"],"subscribed":false},{"client_msg_id":"63c7fd61-7a8b-44c6-b0b8-7303ce918c01","type":"message","text":"is there a blessed way to add new columsn to a Table (without assuming any particular implementation)?  I didn't see anything in Tables.jl or TableOperations.jl...","user":"U66M57AN4","ts":"1612452580.243400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=tfE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is there a blessed way to add new columsn to a Table (without assuming any particular implementation)?  I didn't see anything in Tables.jl or TableOperations.jl..."}]}]}],"thread_ts":"1612452580.243400","reply_count":2,"reply_users_count":2,"latest_reply":"1612454769.243800","reply_users":["U66M57AN4","U681ELA87"],"subscribed":false},{"client_msg_id":"d9b13dfe-cf6e-4abc-9a69-f816d2ee9e8e","type":"message","text":"What are benefits of CategoricalArrays vs simple Array of `@enum` type?","user":"UB2QSHWPN","ts":"1612486599.246400","team":"T68168MUP","edited":{"user":"UB2QSHWPN","ts":"1612486716.000000"},"blocks":[{"type":"rich_text","block_id":"YLxx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What are benefits of CategoricalArrays vs simple Array of "},{"type":"text","text":"@enum","style":{"code":true}},{"type":"text","text":" type?"}]}]}],"reactions":[{"name":"eyes","users":["UB197FRCL"],"count":1}]},{"client_msg_id":"08b9d95e-80e1-4378-885b-a1d1f0cf2921","type":"message","text":"In general, how often do you create bespoke types for the columns in your data frames? If you had something like, say, a 6-digit customer number, would you encode that as a string, or would you create a `CustomerNumber` type? I can see some benefits in terms of inbuilt checking that each customer number is valid.","user":"U0198R8T7LY","ts":"1612570004.252800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xZmV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In general, how often do you create bespoke types for the columns in your data frames? If you had something like, say, a 6-digit customer number, would you encode that as a string, or would you create a "},{"type":"text","text":"CustomerNumber","style":{"code":true}},{"type":"text","text":" type? I can see some benefits in terms of inbuilt checking that each customer number is valid."}]}]}],"thread_ts":"1612570004.252800","reply_count":7,"reply_users_count":2,"latest_reply":"1612570461.254200","reply_users":["U9VG1AYSG","U0198R8T7LY"],"subscribed":false},{"client_msg_id":"94678dc9-e4bc-43bc-af18-6ade4939099b","type":"message","text":"Following the row selection post from last week, this week I have covered column selection rules: <https://bkamins.github.io/julialang/2021/02/06/colsel.html>","user":"U8JAMQGQY","ts":"1612622914.255600","team":"T68168MUP","attachments":[{"service_name":"Blog by Bogumił Kamiński","title":"Column selectors in DataFrames.jl","title_link":"https://bkamins.github.io/julialang/2021/02/06/colsel.html","text":"Introduction","fallback":"Blog by Bogumił Kamiński: Column selectors in DataFrames.jl","ts":1612595661,"from_url":"https://bkamins.github.io/julialang/2021/02/06/colsel.html","service_icon":"https://bkamins.github.io/favicon.ico","id":1,"original_url":"https://bkamins.github.io/julialang/2021/02/06/colsel.html"}],"blocks":[{"type":"rich_text","block_id":"kgXg5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Following the row selection post from last week, this week I have covered column selection rules: "},{"type":"link","url":"https://bkamins.github.io/julialang/2021/02/06/colsel.html"}]}]}]},{"client_msg_id":"7b0ebe3c-3f16-43cd-b6fb-db1c091f26f1","type":"message","text":"What is this \"hardcoded threshold length\" and how does reaching it change the growing behavior?\n\nWhat I can concede is that until today I find truly horrifying that Julia Arrays have a internal not-very-well-publicized hardcoded threshold length that, if an array grows larger than it, it stops doubling when reaching full capacity and instead have a smaller growth. In other words, the amortized `O(1)` insertion guarantee is thrown in the trash and nothing tells you about that.\nfrom a Discourse post &lt;<https://discourse.julialang.org/t/what-dont-you-like-about-julia-for-serious-work/54591/28>&gt;","user":"U68QW0PUZ","ts":"1612628592.257900","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"What don't you like about Julia for \"serious work\"?","title_link":"https://discourse.julialang.org/t/what-dont-you-like-about-julia-for-serious-work/54591/28","text":"What exactly do you mean by that? I fell outraged, XD. I must be out of the touch with C++ (although, I have programmed at least five of the last seven years in the language). If cutting my little finger would guarantee me I would never need to maintain a large C++ project and instead I could do it in Julia, then I would start reaching for my sharpest knife. Ok, now I understand a little better what you mean. Your problem is building a large project that needs reliable critical performance....","fallback":"JuliaLang: What don't you like about Julia for \"serious work\"?","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","ts":1612627305,"from_url":"https://discourse.julialang.org/t/what-dont-you-like-about-julia-for-serious-work/54591/28","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/what-dont-you-like-about-julia-for-serious-work/54591/28"}],"blocks":[{"type":"rich_text","block_id":"qZnIR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What is this \"hardcoded threshold length\" and how does reaching it change the growing behavior?\n\nWhat I can concede is that until today I find truly horrifying that Julia Arrays have a internal not-very-well-publicized hardcoded threshold length that, if an array grows larger than it, it stops doubling when reaching full capacity and instead have a smaller growth. In other words, the amortized "},{"type":"text","text":"O(1)","style":{"code":true}},{"type":"text","text":" insertion guarantee is thrown in the trash and nothing tells you about that.\nfrom a Discourse post <"},{"type":"link","url":"https://discourse.julialang.org/t/what-dont-you-like-about-julia-for-serious-work/54591/28"},{"type":"text","text":">"}]}]}],"thread_ts":"1612628592.257900","reply_count":10,"reply_users_count":2,"latest_reply":"1612630191.260100","reply_users":["UH24GRBLL","U68QW0PUZ"],"subscribed":false},{"client_msg_id":"426eb5f4-6a27-4908-8fd6-4946335c78e5","type":"message","text":"I have a project keeping track of a large number of files grouped in two levels of directories (author and title within author).  Each content directory has a JSON file with the metadata including a unique Int64 key (the 13 digit ISBN).  For ease of creating summaries of the entries I use an SQLite database so I can prepare tables sorted by Author and Year, etc.\n\nOccasionally I run a Julia script to update the database table.  This uses `walkdir` and parses the `metadata.json` files then checks if the entry needs to be added to the table.  The ISBN column is primary key for the SQLite table.\n\nI don't think I am doing the checking very well at this point in that I extract the existing column of ISBNs in increasing order and use Julia's `searchsorted` to determine if the entry being examined is already in the table.  I could probably use the SQL statement \"INSERT OR IGNORE INTO ...\" and have SQLite do the work of checking for duplicates.  I also feel that I should prepare the statement with `SQLite.Stmt` and use `SQLite.bind` with a `NamedTuple` but I am not exactly sure how to go about that.  The definition of the table is attached.  Could someone show me what  the SQL for the INSERT statement and what the call to `SQLite.bind` should be?","user":"UBGRZ7FSP","ts":"1612634445.274500","team":"T68168MUP","edited":{"user":"UBGRZ7FSP","ts":"1612634596.000000"},"blocks":[{"type":"rich_text","block_id":"cgo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a project keeping track of a large number of files grouped in two levels of directories (author and title within author).  Each content directory has a JSON file with the metadata including a unique Int64 key (the 13 digit ISBN).  For ease of creating summaries of the entries I use an SQLite database so I can prepare tables sorted by Author and Year, etc.\n\nOccasionally I run a Julia script to update the database table.  This uses "},{"type":"text","text":"walkdir","style":{"code":true}},{"type":"text","text":" and parses the "},{"type":"text","text":"metadata.json","style":{"code":true}},{"type":"text","text":" files then checks if the entry needs to be added to the table.  The ISBN column is primary key for the SQLite table.\n\nI don't think I am doing the checking very well at this point in that I extract the existing column of ISBNs in increasing order and use Julia's "},{"type":"text","text":"searchsorted","style":{"code":true}},{"type":"text","text":" to determine if the entry being examined is already in the table.  I could probably use the SQL statement \"INSERT OR IGNORE INTO ...\" and have SQLite do the work of checking for duplicates.  I also feel that I should prepare the statement with "},{"type":"text","text":"SQLite.Stmt","style":{"code":true}},{"type":"text","text":" and use "},{"type":"text","text":"SQLite.bind","style":{"code":true}},{"type":"text","text":" with a "},{"type":"text","text":"NamedTuple","style":{"code":true}},{"type":"text","text":" but I am not exactly sure how to go about that.  The definition of the table is attached.  Could someone show me what  the SQL for the INSERT statement and what the call to "},{"type":"text","text":"SQLite.bind","style":{"code":true}},{"type":"text","text":" should be?"}]}]}]},{"client_msg_id":"f6d3057f-3232-4149-8219-5f9540ec9056","type":"message","text":"```CREATE TABLE IF NOT EXISTS \"Audiobooks\" (\n    \"Author\" TEXT NOT NULL,\n    \"Title\" TEXT NOT NULL,\n    \"Year\" INT NOT NULL,\n    \"Goodreads\" REAL NOT NULL,\n    \"Series\" TEXT,\n    \"ISBN\" INT PRIMARY KEY NOT NULL\n);```\n","user":"UBGRZ7FSP","ts":"1612634473.274900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7Rof","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"CREATE TABLE IF NOT EXISTS \"Audiobooks\" (\n    \"Author\" TEXT NOT NULL,\n    \"Title\" TEXT NOT NULL,\n    \"Year\" INT NOT NULL,\n    \"Goodreads\" REAL NOT NULL,\n    \"Series\" TEXT,\n    \"ISBN\" INT PRIMARY KEY NOT NULL\n);"}]},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"676ae0e0-e54d-478c-9900-5db2772d0be1","type":"message","text":"It brings me great joy to think that this is how the legendary Doug Bates chooses to spend his Saturday: organizing audiobooks in a database and interacting with it using Julia.","user":"U680THK2S","ts":"1612641777.277800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xKf9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It brings me great joy to think that this is how the legendary Doug Bates chooses to spend his Saturday: organizing audiobooks in a database and interacting with it using Julia."}]}]}]},{"client_msg_id":"8512db2a-438c-4cf0-9a2b-b6bd21db6dcb","type":"message","text":"Just discovered an interesting problem that I don't see an immediate solution for: in Tables.jl, when a row-oriented input doesn't have a defined schema, but columns are requested, we have a fallback routine that \"builds columns\" by iterating rows and widening column vectors as necessary. There is a lot of code overlap with `collect`, but we're building up a whole set of columns instead of just one collection. The problem is <https://github.com/JuliaData/Tables.jl/blob/bdde6d343a9717cf78c7b442d142a9699688dbdc/src/fallbacks.jl#L146|here>: to check if we need to widen a vector, we check if the next element `isa T`, and if not, widen. This leads to an issue in the case I ran into locally where one element was a `Float64`, while a bunch of subsequent elements were `Int64`. The `Int64` were not `isa Float64`, so it attempted to widen, but `promote_type(Int64, Float64) === Float64`, so it's not really widening, we're just reallocating a new `Float64` vector for no reason.\n\nWhat's the best way to handle this? Ideally, our `val isa T` check would instead be something like `val is convertible to T` because `setindex!` will call `convert(T, val)` for us, but I'm not sure something like that exists? Just wondering if anyone has any ideas; cc: <@U67431ELR> <@U8JAMQGQY> <@U6740K1SP>","user":"U681ELA87","ts":"1612695925.285600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FjXck","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Just discovered an interesting problem that I don't see an immediate solution for: in Tables.jl, when a row-oriented input doesn't have a defined schema, but columns are requested, we have a fallback routine that \"builds columns\" by iterating rows and widening column vectors as necessary. There is a lot of code overlap with "},{"type":"text","text":"collect","style":{"code":true}},{"type":"text","text":", but we're building up a whole set of columns instead of just one collection. The problem is "},{"type":"link","url":"https://github.com/JuliaData/Tables.jl/blob/bdde6d343a9717cf78c7b442d142a9699688dbdc/src/fallbacks.jl#L146","text":"here"},{"type":"text","text":": to check if we need to widen a vector, we check if the next element "},{"type":"text","text":"isa T","style":{"code":true}},{"type":"text","text":", and if not, widen. This leads to an issue in the case I ran into locally where one element was a "},{"type":"text","text":"Float64","style":{"code":true}},{"type":"text","text":", while a bunch of subsequent elements were "},{"type":"text","text":"Int64","style":{"code":true}},{"type":"text","text":". The "},{"type":"text","text":"Int64","style":{"code":true}},{"type":"text","text":" were not "},{"type":"text","text":"isa Float64","style":{"code":true}},{"type":"text","text":", so it attempted to widen, but "},{"type":"text","text":"promote_type(Int64, Float64) === Float64","style":{"code":true}},{"type":"text","text":", so it's not really widening, we're just reallocating a new "},{"type":"text","text":"Float64","style":{"code":true}},{"type":"text","text":" vector for no reason.\n\nWhat's the best way to handle this? Ideally, our "},{"type":"text","text":"val isa T","style":{"code":true}},{"type":"text","text":" check would instead be something like "},{"type":"text","text":"val is convertible to T","style":{"code":true}},{"type":"text","text":" because "},{"type":"text","text":"setindex!","style":{"code":true}},{"type":"text","text":" will call "},{"type":"text","text":"convert(T, val)","style":{"code":true}},{"type":"text","text":" for us, but I'm not sure something like that exists? Just wondering if anyone has any ideas; cc: "},{"type":"user","user_id":"U67431ELR"},{"type":"text","text":" "},{"type":"user","user_id":"U8JAMQGQY"},{"type":"text","text":" "},{"type":"user","user_id":"U6740K1SP"}]}]}],"thread_ts":"1612695925.285600","reply_count":5,"reply_users_count":3,"latest_reply":"1612696412.286500","reply_users":["U681ELA87","UM30MT6RF","U67431ELR"],"subscribed":false},{"client_msg_id":"377b637a-1e7e-465e-8c40-220a339e1f69","type":"message","text":"Slightly related, I remember that there was a comment on how some \"recyclable machinery\" for this kind of collection lives in the Transducers universe: <https://github.com/JuliaArrays/StructArrays.jl/pull/97#issuecomment-631937452>. It may lose some performance compared to the current implementations, but it is a useful thing to keep in mind to reduce code duplication in the future","user":"U6BJ9E351","ts":"1612698094.289200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DoVS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Slightly related, I remember that there was a comment on how some \"recyclable machinery\" for this kind of collection lives in the Transducers universe: "},{"type":"link","url":"https://github.com/JuliaArrays/StructArrays.jl/pull/97#issuecomment-631937452"},{"type":"text","text":". It may lose some performance compared to the current implementations, but it is a useful thing to keep in mind to reduce code duplication in the future"}]}]}],"thread_ts":"1612698094.289200","reply_count":1,"reply_users_count":1,"latest_reply":"1612698277.290300","reply_users":["U6BJ9E351"],"subscribed":false,"reactions":[{"name":"heavy_check_mark","users":["U67431ELR"],"count":1}]},{"client_msg_id":"865c7aaf-785f-4b01-9560-8c9c1c7d0386","type":"message","text":"`innerjoin` rewrite in DataFrames.jl (see <https://github.com/JuliaData/DataFrames.jl/pull/2612#issuecomment-774743175>) is almost ready. Independent correctness and performance tests would be welcome (as the algorithm we use now is complex). Thank you!","user":"U8JAMQGQY","ts":"1612730780.292800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VCN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"innerjoin","style":{"code":true}},{"type":"text","text":" rewrite in DataFrames.jl (see "},{"type":"link","url":"https://github.com/JuliaData/DataFrames.jl/pull/2612#issuecomment-774743175"},{"type":"text","text":") is almost ready. Independent correctness and performance tests would be welcome (as the algorithm we use now is complex). Thank you!"}]}]}]},{"client_msg_id":"5fa5cd5d-1b7b-41ca-ae19-70a90cef0c09","type":"message","text":"The recommendation in\n```Warning: `categorical!(df)` is deprecated. Use `transform!(df, names(df, Union{Missing, AbstractString}) .=&gt; (x -&gt; categorical(x, compress=false)), renamecols=false)` instead.```\nis quite verbose. Is that really the new recommendation or is this one of the cases where the user should change workflow in a non-equivalent way?","user":"U680T6770","ts":"1612775519.295200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"q4LzV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The recommendation in\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Warning: `categorical!(df)` is deprecated. Use `transform!(df, names(df, Union{Missing, AbstractString}) .=> (x -> categorical(x, compress=false)), renamecols=false)` instead."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"is quite verbose. Is that really the new recommendation or is this one of the cases where the user should change workflow in a non-equivalent way?"}]}]}]},{"client_msg_id":"aa44c80d-707b-44d9-a2e7-0c65cf9b0441","type":"message","text":"Can anyone show me an example of `SQLite.bind` (or `DBInterface.execute` on a `SQLite.Stmt` ) where the parameters are specified by identifiers and the values are in a `NamedTuple`?  I keep getting error messages like","user":"UBGRZ7FSP","ts":"1612805045.300400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"izpAO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can anyone show me an example of "},{"type":"text","text":"SQLite.bind","style":{"code":true}},{"type":"text","text":" (or "},{"type":"text","text":"DBInterface.execute","style":{"code":true}},{"type":"text","text":" on a "},{"type":"text","text":"SQLite.Stmt","style":{"code":true}},{"type":"text","text":" ) where the parameters are specified by identifiers and the values are in a "},{"type":"text","text":"NamedTuple","style":{"code":true}},{"type":"text","text":"?  I keep getting error messages like"}]}]}]},{"client_msg_id":"13cbc343-0414-43b8-b7f5-4dd91b253a9b","type":"message","text":"`Tables.columntable(DBInterface.execute(db, \"SELECT * FROM Audiobooks WHERE Author = :ARTIST\", NamedTuple(meta)))`\nERROR: SQLiteException(\"values should be provided for all query placeholders\")","user":"UBGRZ7FSP","ts":"1612805066.300600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rng","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Tables.columntable(DBInterface.execute(db, \"SELECT * FROM Audiobooks WHERE Author = :ARTIST\", NamedTuple(meta)))","style":{"code":true}},{"type":"text","text":"\nERROR: SQLiteException(\"values should be provided for all query placeholders\")"}]}]}]},{"client_msg_id":"df35aa6a-613e-4aa7-a3c5-47dae72535fe","type":"message","text":"In this example there is a string named `:ARTIST` in the `NamedTuple` but there are several other elements.  I'm wondering if having too many elements in the `NamedTuple` is the problem.","user":"UBGRZ7FSP","ts":"1612805153.302000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"aaW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In this example there is a string named "},{"type":"text","text":":ARTIST","style":{"code":true}},{"type":"text","text":" in the "},{"type":"text","text":"NamedTuple","style":{"code":true}},{"type":"text","text":" but there are several other elements.  I'm wondering if having too many elements in the "},{"type":"text","text":"NamedTuple","style":{"code":true}},{"type":"text","text":" is the problem."}]}]}]},{"client_msg_id":"7c63f1e9-e947-47cb-afb4-6603ea523b47","type":"message","text":"I think it's expecting the NamedTuple to have a field called `ARTIST`","user":"UH24GRBLL","ts":"1612805388.302800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tmaw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think it's expecting the NamedTuple to have a field called "},{"type":"text","text":"ARTIST","style":{"code":true}}]}]}]},{"client_msg_id":"abea1233-c47f-4dc0-b073-2562fab1bd8c","type":"message","text":"like this:\n```julia&gt; a = (ARTIST=\"hello\",)\n(ARTIST = \"hello\",)\n\njulia&gt; a.ARTIST\n\"hello\"```","user":"UH24GRBLL","ts":"1612805441.303200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rKLHu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"like this:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> a = (ARTIST=\"hello\",)\n(ARTIST = \"hello\",)\n\njulia> a.ARTIST\n\"hello\""}]}]}]},{"client_msg_id":"f476a88c-6b5f-40a8-bdf4-8ef3eb9b2341","type":"message","text":"Thanks, I probably misspoke in that the name is the Symbol `:ARTIST` which should be the same as what you showed.\n`julia&gt; keys((ARTIST = \"John Doe\",))`\n`(:ARTIST,)`","user":"UBGRZ7FSP","ts":"1612805683.306000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VhCV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks, I probably misspoke in that the name is the Symbol "},{"type":"text","text":":ARTIST","style":{"code":true}},{"type":"text","text":" which should be the same as what you showed.\n"},{"type":"text","text":"julia> keys((ARTIST = \"John Doe\",))","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"(:ARTIST,)","style":{"code":true}}]}]}],"thread_ts":"1612805683.306000","reply_count":5,"reply_users_count":2,"latest_reply":"1612806740.307000","reply_users":["UBGRZ7FSP","UH24GRBLL"],"subscribed":false}]}