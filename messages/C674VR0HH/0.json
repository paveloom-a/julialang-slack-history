{"cursor": 2, "messages": [{"client_msg_id":"8512db2a-438c-4cf0-9a2b-b6bd21db6dcb","type":"message","text":"Just discovered an interesting problem that I don't see an immediate solution for: in Tables.jl, when a row-oriented input doesn't have a defined schema, but columns are requested, we have a fallback routine that \"builds columns\" by iterating rows and widening column vectors as necessary. There is a lot of code overlap with `collect`, but we're building up a whole set of columns instead of just one collection. The problem is <https://github.com/JuliaData/Tables.jl/blob/bdde6d343a9717cf78c7b442d142a9699688dbdc/src/fallbacks.jl#L146|here>: to check if we need to widen a vector, we check if the next element `isa T`, and if not, widen. This leads to an issue in the case I ran into locally where one element was a `Float64`, while a bunch of subsequent elements were `Int64`. The `Int64` were not `isa Float64`, so it attempted to widen, but `promote_type(Int64, Float64) === Float64`, so it's not really widening, we're just reallocating a new `Float64` vector for no reason.\n\nWhat's the best way to handle this? Ideally, our `val isa T` check would instead be something like `val is convertible to T` because `setindex!` will call `convert(T, val)` for us, but I'm not sure something like that exists? Just wondering if anyone has any ideas; cc: <@U67431ELR> <@U8JAMQGQY> <@U6740K1SP>","user":"U681ELA87","ts":"1612695925.285600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FjXck","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Just discovered an interesting problem that I don't see an immediate solution for: in Tables.jl, when a row-oriented input doesn't have a defined schema, but columns are requested, we have a fallback routine that \"builds columns\" by iterating rows and widening column vectors as necessary. There is a lot of code overlap with "},{"type":"text","text":"collect","style":{"code":true}},{"type":"text","text":", but we're building up a whole set of columns instead of just one collection. The problem is "},{"type":"link","url":"https://github.com/JuliaData/Tables.jl/blob/bdde6d343a9717cf78c7b442d142a9699688dbdc/src/fallbacks.jl#L146","text":"here"},{"type":"text","text":": to check if we need to widen a vector, we check if the next element "},{"type":"text","text":"isa T","style":{"code":true}},{"type":"text","text":", and if not, widen. This leads to an issue in the case I ran into locally where one element was a "},{"type":"text","text":"Float64","style":{"code":true}},{"type":"text","text":", while a bunch of subsequent elements were "},{"type":"text","text":"Int64","style":{"code":true}},{"type":"text","text":". The "},{"type":"text","text":"Int64","style":{"code":true}},{"type":"text","text":" were not "},{"type":"text","text":"isa Float64","style":{"code":true}},{"type":"text","text":", so it attempted to widen, but "},{"type":"text","text":"promote_type(Int64, Float64) === Float64","style":{"code":true}},{"type":"text","text":", so it's not really widening, we're just reallocating a new "},{"type":"text","text":"Float64","style":{"code":true}},{"type":"text","text":" vector for no reason.\n\nWhat's the best way to handle this? Ideally, our "},{"type":"text","text":"val isa T","style":{"code":true}},{"type":"text","text":" check would instead be something like "},{"type":"text","text":"val is convertible to T","style":{"code":true}},{"type":"text","text":" because "},{"type":"text","text":"setindex!","style":{"code":true}},{"type":"text","text":" will call "},{"type":"text","text":"convert(T, val)","style":{"code":true}},{"type":"text","text":" for us, but I'm not sure something like that exists? Just wondering if anyone has any ideas; cc: "},{"type":"user","user_id":"U67431ELR"},{"type":"text","text":" "},{"type":"user","user_id":"U8JAMQGQY"},{"type":"text","text":" "},{"type":"user","user_id":"U6740K1SP"}]}]}],"thread_ts":"1612695925.285600","reply_count":5,"reply_users_count":3,"latest_reply":"1612696412.286500","reply_users":["U681ELA87","UM30MT6RF","U67431ELR"],"subscribed":false},{"client_msg_id":"377b637a-1e7e-465e-8c40-220a339e1f69","type":"message","text":"Slightly related, I remember that there was a comment on how some \"recyclable machinery\" for this kind of collection lives in the Transducers universe: <https://github.com/JuliaArrays/StructArrays.jl/pull/97#issuecomment-631937452>. It may lose some performance compared to the current implementations, but it is a useful thing to keep in mind to reduce code duplication in the future","user":"U6BJ9E351","ts":"1612698094.289200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DoVS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Slightly related, I remember that there was a comment on how some \"recyclable machinery\" for this kind of collection lives in the Transducers universe: "},{"type":"link","url":"https://github.com/JuliaArrays/StructArrays.jl/pull/97#issuecomment-631937452"},{"type":"text","text":". It may lose some performance compared to the current implementations, but it is a useful thing to keep in mind to reduce code duplication in the future"}]}]}],"thread_ts":"1612698094.289200","reply_count":1,"reply_users_count":1,"latest_reply":"1612698277.290300","reply_users":["U6BJ9E351"],"subscribed":false,"reactions":[{"name":"heavy_check_mark","users":["U67431ELR"],"count":1}]},{"client_msg_id":"865c7aaf-785f-4b01-9560-8c9c1c7d0386","type":"message","text":"`innerjoin` rewrite in DataFrames.jl (see <https://github.com/JuliaData/DataFrames.jl/pull/2612#issuecomment-774743175>) is almost ready. Independent correctness and performance tests would be welcome (as the algorithm we use now is complex). Thank you!","user":"U8JAMQGQY","ts":"1612730780.292800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VCN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"innerjoin","style":{"code":true}},{"type":"text","text":" rewrite in DataFrames.jl (see "},{"type":"link","url":"https://github.com/JuliaData/DataFrames.jl/pull/2612#issuecomment-774743175"},{"type":"text","text":") is almost ready. Independent correctness and performance tests would be welcome (as the algorithm we use now is complex). Thank you!"}]}]}]},{"client_msg_id":"5fa5cd5d-1b7b-41ca-ae19-70a90cef0c09","type":"message","text":"The recommendation in\n```Warning: `categorical!(df)` is deprecated. Use `transform!(df, names(df, Union{Missing, AbstractString}) .=&gt; (x -&gt; categorical(x, compress=false)), renamecols=false)` instead.```\nis quite verbose. Is that really the new recommendation or is this one of the cases where the user should change workflow in a non-equivalent way?","user":"U680T6770","ts":"1612775519.295200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"q4LzV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The recommendation in\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Warning: `categorical!(df)` is deprecated. Use `transform!(df, names(df, Union{Missing, AbstractString}) .=> (x -> categorical(x, compress=false)), renamecols=false)` instead."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"is quite verbose. Is that really the new recommendation or is this one of the cases where the user should change workflow in a non-equivalent way?"}]}]}]},{"client_msg_id":"aa44c80d-707b-44d9-a2e7-0c65cf9b0441","type":"message","text":"Can anyone show me an example of `SQLite.bind` (or `DBInterface.execute` on a `SQLite.Stmt` ) where the parameters are specified by identifiers and the values are in a `NamedTuple`?  I keep getting error messages like","user":"UBGRZ7FSP","ts":"1612805045.300400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"izpAO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can anyone show me an example of "},{"type":"text","text":"SQLite.bind","style":{"code":true}},{"type":"text","text":" (or "},{"type":"text","text":"DBInterface.execute","style":{"code":true}},{"type":"text","text":" on a "},{"type":"text","text":"SQLite.Stmt","style":{"code":true}},{"type":"text","text":" ) where the parameters are specified by identifiers and the values are in a "},{"type":"text","text":"NamedTuple","style":{"code":true}},{"type":"text","text":"?  I keep getting error messages like"}]}]}]},{"client_msg_id":"13cbc343-0414-43b8-b7f5-4dd91b253a9b","type":"message","text":"`Tables.columntable(DBInterface.execute(db, \"SELECT * FROM Audiobooks WHERE Author = :ARTIST\", NamedTuple(meta)))`\nERROR: SQLiteException(\"values should be provided for all query placeholders\")","user":"UBGRZ7FSP","ts":"1612805066.300600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rng","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Tables.columntable(DBInterface.execute(db, \"SELECT * FROM Audiobooks WHERE Author = :ARTIST\", NamedTuple(meta)))","style":{"code":true}},{"type":"text","text":"\nERROR: SQLiteException(\"values should be provided for all query placeholders\")"}]}]}]},{"client_msg_id":"df35aa6a-613e-4aa7-a3c5-47dae72535fe","type":"message","text":"In this example there is a string named `:ARTIST` in the `NamedTuple` but there are several other elements.  I'm wondering if having too many elements in the `NamedTuple` is the problem.","user":"UBGRZ7FSP","ts":"1612805153.302000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"aaW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In this example there is a string named "},{"type":"text","text":":ARTIST","style":{"code":true}},{"type":"text","text":" in the "},{"type":"text","text":"NamedTuple","style":{"code":true}},{"type":"text","text":" but there are several other elements.  I'm wondering if having too many elements in the "},{"type":"text","text":"NamedTuple","style":{"code":true}},{"type":"text","text":" is the problem."}]}]}]},{"client_msg_id":"7c63f1e9-e947-47cb-afb4-6603ea523b47","type":"message","text":"I think it's expecting the NamedTuple to have a field called `ARTIST`","user":"UH24GRBLL","ts":"1612805388.302800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tmaw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think it's expecting the NamedTuple to have a field called "},{"type":"text","text":"ARTIST","style":{"code":true}}]}]}]},{"client_msg_id":"abea1233-c47f-4dc0-b073-2562fab1bd8c","type":"message","text":"like this:\n```julia&gt; a = (ARTIST=\"hello\",)\n(ARTIST = \"hello\",)\n\njulia&gt; a.ARTIST\n\"hello\"```","user":"UH24GRBLL","ts":"1612805441.303200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rKLHu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"like this:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> a = (ARTIST=\"hello\",)\n(ARTIST = \"hello\",)\n\njulia> a.ARTIST\n\"hello\""}]}]}]},{"client_msg_id":"f476a88c-6b5f-40a8-bdf4-8ef3eb9b2341","type":"message","text":"Thanks, I probably misspoke in that the name is the Symbol `:ARTIST` which should be the same as what you showed.\n`julia&gt; keys((ARTIST = \"John Doe\",))`\n`(:ARTIST,)`","user":"UBGRZ7FSP","ts":"1612805683.306000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VhCV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks, I probably misspoke in that the name is the Symbol "},{"type":"text","text":":ARTIST","style":{"code":true}},{"type":"text","text":" which should be the same as what you showed.\n"},{"type":"text","text":"julia> keys((ARTIST = \"John Doe\",))","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"(:ARTIST,)","style":{"code":true}}]}]}],"thread_ts":"1612805683.306000","reply_count":5,"reply_users_count":2,"latest_reply":"1612806740.307000","reply_users":["UBGRZ7FSP","UH24GRBLL"],"subscribed":false},{"client_msg_id":"db036e27-48b5-4376-9fbd-be41e24a56a6","type":"message","text":"Is it possible to make a nested dictionary a Tables source?\nSpecifically, I often have this kind of data structure:\n```\nDict{Any,Dict{Any,Baz}} # pkey -&gt; skey -&gt; value\n```\nwhere\n```\nstruct Baz\n\tid\n\tdata::NamedTuple\nend\n```\nAny thoughts appreciated. Thanks!","user":"UBL1R2BE1","ts":"1612824850.308400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6U=1o","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is it possible to make a nested dictionary a Tables source?\nSpecifically, I often have this kind of data structure:\n```\nDict{Any,Dict{Any,Baz}} # pkey -> skey -> value\n```\nwhere\n```\nstruct Baz\n\tid\n\tdata::NamedTuple\nend\n```\nAny thoughts appreciated. Thanks!"}]}]}]},{"client_msg_id":"b777d3cc-ccbc-4940-9af6-9e6d9d470a5e","type":"message","text":"Hello all, I just published a very small package that I'm quite happy to have around [Minio.jl](<https://gitlab.com/ExpandingMan/Minio.jl>).  For those who don't know [<http://min.io|min.io>](<https://min.io/>) is an open source alternative to the AWS's S3 data storage system, with a fully compatible rest API.  Thanks to some recent work in AWS.jl to make configurations more flexible, followed by my PR to bring this to AWSS3.jl, you can use the S3 API through AWSS3.jl and AWS.jl with <http://min.io|min.io> simply by declaring the config from Minio, e.g. `MinioConfig(\"<http://localhost:9000>\")`.\n\nSince I wanted to include unit tests for this simple config, I figured I'd make any tools I created to execute the unit tests into the package.  So, if you don't have minio installed already (since it's a go program it can be downloaded as a single binary) it will fetch it for you during its build step via the Julia Pkg artifacts system during the build step.  You can then run a server with e.g. `run(Minio.Server(directory), wait=false)`.  This gives you a super convenient way of turning any directory into an S3 bucket from Julia.  Since <http://min.io|min.io> and S3 are fully compatible, Minio can also be useful for unit tests of any S3 calls you need to make.","user":"U9VG1AYSG","ts":"1612831115.308600","team":"T68168MUP","attachments":[{"service_name":"GitLab","title":"Expanding Man / Minio.jl","title_link":"https://gitlab.com/ExpandingMan/Minio.jl","text":"unofficial Julia tools for working with <http://min.io|min.io>","fallback":"GitLab: Expanding Man / Minio.jl","thumb_url":"https://assets.gitlab-static.net/assets/gitlab_logo-7ae504fe4f68fdebb3c2034e36621930cd36ea87924c11ff65dbcb8ed50dca58.png","from_url":"https://gitlab.com/ExpandingMan/Minio.jl","thumb_width":128,"thumb_height":128,"service_icon":"https://assets.gitlab-static.net/assets/touch-icon-iphone-5a9cee0e8a51212e70b90c87c12f382c428870c0ff67d1eb034d884b78d2dae7.png","id":1,"original_url":"https://gitlab.com/ExpandingMan/Minio.jl"},{"service_name":"MinIO","title":"MinIO | High Performance, Kubernetes Native Object Storage","title_link":"https://min.io/","text":"MinIO's High Performance Object Storage is Open Source, Amazon S3 compatible, Kubernetes Native and is designed for cloud native workloads like AI.","fallback":"MinIO: MinIO | High Performance, Kubernetes Native Object Storage","image_url":"https://min.io/resources/img/minio_share.png","from_url":"https://min.io/","image_width":500,"image_height":250,"image_bytes":293776,"service_icon":"https://min.io/resources/favs/apple-icon-180x180.png","id":2,"original_url":"https://min.io/"}],"blocks":[{"type":"rich_text","block_id":"Rix","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello all, I just published a very small package that I'm quite happy to have around [Minio.jl]("},{"type":"link","url":"https://gitlab.com/ExpandingMan/Minio.jl"},{"type":"text","text":").  For those who don't know ["},{"type":"link","url":"http://min.io","text":"min.io"},{"type":"text","text":"]("},{"type":"link","url":"https://min.io/"},{"type":"text","text":") is an open source alternative to the AWS's S3 data storage system, with a fully compatible rest API.  Thanks to some recent work in AWS.jl to make configurations more flexible, followed by my PR to bring this to AWSS3.jl, you can use the S3 API through AWSS3.jl and AWS.jl with "},{"type":"link","url":"http://min.io","text":"min.io"},{"type":"text","text":" simply by declaring the config from Minio, e.g. "},{"type":"text","text":"MinioConfig(\"","style":{"code":true}},{"type":"link","url":"http://localhost:9000","style":{"code":true}},{"type":"text","text":"\")","style":{"code":true}},{"type":"text","text":".\n\nSince I wanted to include unit tests for this simple config, I figured I'd make any tools I created to execute the unit tests into the package.  So, if you don't have minio installed already (since it's a go program it can be downloaded as a single binary) it will fetch it for you during its build step via the Julia Pkg artifacts system during the build step.  You can then run a server with e.g. "},{"type":"text","text":"run(Minio.Server(directory), wait=false)","style":{"code":true}},{"type":"text","text":".  This gives you a super convenient way of turning any directory into an S3 bucket from Julia.  Since "},{"type":"link","url":"http://min.io","text":"min.io"},{"type":"text","text":" and S3 are fully compatible, Minio can also be useful for unit tests of any S3 calls you need to make."}]}]}],"reactions":[{"name":"thumbsup_all","users":["U01FAHWCMFF","UB197FRCL","U681ELA87","U6A936746","U01724Q3PGW"],"count":5},{"name":"+1","users":["U011V2YN59N"],"count":1}]},{"client_msg_id":"f41e1d2e-929a-41c7-a4c6-3eeb419baa0f","type":"message","text":"What's the most efficient way of filtering a DataFrame based on an existing BitVector? E.g. take the example from the docstring `filter!([:x, :y] =&gt; (x, y) -&gt; x == 1 || y == \"b\", df)`, but I've already computed the vector `(df.x .== 1) .| (df.y .== \"b\")` separately","user":"U7JQGPGCQ","ts":"1612884796.313400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OJGkN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What's the most efficient way of filtering a DataFrame based on an existing BitVector? E.g. take the example from the docstring "},{"type":"text","text":"filter!([:x, :y] => (x, y) -> x == 1 || y == \"b\", df)","style":{"code":true}},{"type":"text","text":", but I've already computed the vector "},{"type":"text","text":"(df.x .== 1) .| (df.y .== \"b\")","style":{"code":true}},{"type":"text","text":" separately"}]}]}],"thread_ts":"1612884796.313400","reply_count":13,"reply_users_count":3,"latest_reply":"1612886264.316300","reply_users":["UBF9YRB6H","U7JQGPGCQ","U8JAMQGQY"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"I'm trying to test out `innerjoin` PR on an existing project. I can do this via `]` right? Do you know how to `add` your branch to use in my environment?","user":"UBF9YRB6H","ts":"1612897126.321400","thread_ts":"1612730780.292800","root":{"client_msg_id":"865c7aaf-785f-4b01-9560-8c9c1c7d0386","type":"message","text":"`innerjoin` rewrite in DataFrames.jl (see <https://github.com/JuliaData/DataFrames.jl/pull/2612#issuecomment-774743175>) is almost ready. Independent correctness and performance tests would be welcome (as the algorithm we use now is complex). Thank you!","user":"U8JAMQGQY","ts":"1612730780.292800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VCN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"innerjoin","style":{"code":true}},{"type":"text","text":" rewrite in DataFrames.jl (see "},{"type":"link","url":"https://github.com/JuliaData/DataFrames.jl/pull/2612#issuecomment-774743175"},{"type":"text","text":") is almost ready. Independent correctness and performance tests would be welcome (as the algorithm we use now is complex). Thank you!"}]}]}],"thread_ts":"1612730780.292800","reply_count":2,"reply_users_count":2,"latest_reply":"1612897126.321400","reply_users":["U01A0S07875","UBF9YRB6H"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"EKUw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm trying to test out "},{"type":"text","text":"innerjoin","style":{"code":true}},{"type":"text","text":" PR on an existing project. I can do this via "},{"type":"text","text":"]","style":{"code":true}},{"type":"text","text":" right? Do you know how to "},{"type":"text","text":"add","style":{"code":true}},{"type":"text","text":" your branch to use in my environment?"}]}]}],"client_msg_id":"b6c2ece7-713c-43a2-b58b-5fdd836c875d"},{"client_msg_id":"d24a1604-0bff-4234-9caa-bafb3c584a7b","type":"message","text":"Can I run a test suite for a DataFrames.jl installation like `pd.test()` for Pandas?","user":"U01CQTKB86N","ts":"1612947227.326200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2ry","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can I run a test suite for a DataFrames.jl installation like "},{"type":"text","text":"pd.test()","style":{"code":true}},{"type":"text","text":" for Pandas?"}]}]}],"thread_ts":"1612947227.326200","reply_count":4,"reply_users_count":2,"latest_reply":"1612947362.326900","reply_users":["U67431ELR","U01CQTKB86N"],"subscribed":false},{"client_msg_id":"af91f0f4-fbfd-4073-a0af-420aef9fb34f","type":"message","text":"Would anyone care to explain to me the design decision and rationale behind the syntax `df[!, col]` as opposed to `df[col]`? I was out of the loop when this decision was made and was curious why the syntax was written this way. Not opposed or griping about it, just wondering why as I know a lot of thought goes into DataFrames (thanks in no small part to the legendary <@U8JAMQGQY>). Thank you!","user":"US64J0NPQ","ts":"1612966817.332700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IZG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Would anyone care to explain to me the design decision and rationale behind the syntax "},{"type":"text","text":"df[!, col]","style":{"code":true}},{"type":"text","text":" as opposed to "},{"type":"text","text":"df[col]","style":{"code":true}},{"type":"text","text":"? I was out of the loop when this decision was made and was curious why the syntax was written this way. Not opposed or griping about it, just wondering why as I know a lot of thought goes into DataFrames (thanks in no small part to the legendary "},{"type":"user","user_id":"U8JAMQGQY"},{"type":"text","text":"). Thank you!"}]}]}],"thread_ts":"1612966817.332700","reply_count":9,"reply_users_count":3,"latest_reply":"1612968552.339000","reply_users":["UBF9YRB6H","U67431ELR","U8JAMQGQY"],"subscribed":false},{"client_msg_id":"ab7ecf15-2895-452d-9220-c3d88d7ed55e","type":"message","text":"in dplyr I often use `groupby(...) %&gt;% summarise()` to get a table of the unique combinations of hte grouping variables.  is there something analogous with DataFrames?","user":"U66M57AN4","ts":"1612978883.339900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"D+abF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"in dplyr I often use "},{"type":"text","text":"groupby(...) %>% summarise()","style":{"code":true}},{"type":"text","text":" to get a table of the unique combinations of hte grouping variables.  is there something analogous with DataFrames?"}]}]}]},{"client_msg_id":"630de8c0-e046-4719-914d-66b8635c8e9a","type":"message","text":"if i use `combine(groupby(...))` I get an empty table","user":"U66M57AN4","ts":"1612978897.340200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"U1P","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"if i use "},{"type":"text","text":"combine(groupby(...))","style":{"code":true}},{"type":"text","text":" I get an empty table"}]}]}],"thread_ts":"1612978897.340200","reply_count":22,"reply_users_count":3,"latest_reply":"1612980075.344900","reply_users":["U67431ELR","U66M57AN4","UBF9YRB6H"],"subscribed":false},{"client_msg_id":"17dd4adb-d3bf-46e9-bea2-5ef40533cc86","type":"message","text":"<@UD0NS8PDF> in DataFrames 1.0 will we still need `Tables.columntable` to convert a AxisKeys `KeyedArray` into a a DataFrame?\n` df = DataFrame(DataFrames.Tables.columntable(array))`\n\nDid we deprecate  default `AbstractMatrix` constructor for DataFrames?","user":"U6A936746","ts":"1613054641.351900","team":"T68168MUP","edited":{"user":"U6A936746","ts":"1613054646.000000"},"blocks":[{"type":"rich_text","block_id":"OLzJs","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UD0NS8PDF"},{"type":"text","text":" in DataFrames 1.0 will we still need "},{"type":"text","text":"Tables.columntable","style":{"code":true}},{"type":"text","text":" to convert a AxisKeys "},{"type":"text","text":"KeyedArray","style":{"code":true}},{"type":"text","text":" into a a DataFrame?\n` df = DataFrame(DataFrames.Tables.columntable(array))`\n\nDid we deprecate  default "},{"type":"text","text":"AbstractMatrix","style":{"code":true}},{"type":"text","text":" constructor for DataFrames?"}]}]}],"thread_ts":"1613054641.351900","reply_count":4,"reply_users_count":2,"latest_reply":"1613055309.352700","reply_users":["UBF9YRB6H","U8JAMQGQY"],"subscribed":false},{"client_msg_id":"616a1469-2458-4837-80cb-27ccfc73d7c7","type":"message","text":"hmm, i used to do `rand(2,3) |&gt; DataFrame` but now because of :auto, you have to `x-&gt;DataFrame(x, :auto)`. Also, before, DataFrame(DataFrame(x)) works which should be expected because it should be a closed operation.…","user":"U6SHSF4R0","ts":"1613063676.356200","team":"T68168MUP","edited":{"user":"U6SHSF4R0","ts":"1613063717.000000"},"blocks":[{"type":"rich_text","block_id":"PxAQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hmm, i used to do "},{"type":"text","text":"rand(2,3) |> DataFrame","style":{"code":true}},{"type":"text","text":" but now because of :auto, you have to "},{"type":"text","text":"x->DataFrame(x, :auto)","style":{"code":true}},{"type":"text","text":". Also, before, DataFrame(DataFrame(x)) works which should be expected because it should be a closed operation.…"}]}]}],"thread_ts":"1613063676.356200","reply_count":1,"reply_users_count":1,"latest_reply":"1613063752.356400","reply_users":["U67431ELR"],"subscribed":false},{"client_msg_id":"869b18e9-0517-48dd-9f09-dfac4a0c59ed","type":"message","text":"DataFrame should be pipe friendly :wink:. now the :auto thingy is not saving keystrokes","user":"U6SHSF4R0","ts":"1613063791.357100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xhgB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"DataFrame should be pipe friendly "},{"type":"emoji","name":"wink"},{"type":"text","text":". now the :auto thingy is not saving keystrokes"}]}]}],"thread_ts":"1613063791.357100","reply_count":6,"reply_users_count":3,"latest_reply":"1613064139.358200","reply_users":["U67431ELR","U6SHSF4R0","UBF9YRB6H"],"subscribed":false},{"client_msg_id":"e21cd2fc-fa8d-47e7-862f-c821158aa11a","type":"message","text":"the DataFrame is breaking all packages in JuliaML such as MLDataPatterns, MLDataUtils, etc.","user":"U6SHSF4R0","ts":"1613072428.360300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fCq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"the DataFrame is breaking all packages in JuliaML such as MLDataPatterns, MLDataUtils, etc."}]}]}]},{"client_msg_id":"1e4f2c78-6283-469b-ba7c-ad749436503c","type":"message","text":"problem if you have so many small utils depending on dataframes and each will have to be updated….","user":"U6SHSF4R0","ts":"1613072467.360900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/yO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"problem if you have so many small utils depending on dataframes and each will have to be updated…."}]}]}],"thread_ts":"1613072467.360900","reply_count":7,"reply_users_count":4,"latest_reply":"1613074493.362300","reply_users":["UBF9YRB6H","U8JAMQGQY","U8JP5B9T2","U6SHSF4R0"],"subscribed":false},{"client_msg_id":"7d5a481a-530d-41fc-b51f-792a22cf9fee","type":"message","text":"I haven't looked into the packages you are referring to, but those package probably should not depend on DataFrames at all anyway.  MLJ ensures everything uses the Tables.jl interface and that aspect of it works great","user":"U9VG1AYSG","ts":"1613078439.363600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4ij8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I haven't looked into the packages you are referring to, but those package probably should not depend on DataFrames at all anyway.  MLJ ensures everything uses the Tables.jl interface and that aspect of it works great"}]}]}]},{"client_msg_id":"41c4a946-aacb-41b4-a5ab-cf7d7f4b4240","type":"message","text":"Is there a more sensible way to do this transform?\n```julia&gt; df = DataFrame(a = [\"test|one\", \"test|two\", \"test\"])\n3×1 DataFrame\n Row │ a\n     │ String\n─────┼──────────\n   1 │ test|one\n   2 │ test|two\n   3 │ test\n\njulia&gt; transform(df, :a =&gt; ByRow(s-&gt;begin\n           things = split(s, \"|\")\n           (p1, p2) = length(things) == 1 ? (first(things), nothing) : things\n           end) =&gt; [:p1, :p2])\n3×3 DataFrame\n Row │ a         p1         p2\n     │ String    SubStrin…  Union…\n─────┼─────────────────────────────\n   1 │ test|one  test       one\n   2 │ test|two  test       two\n   3 │ test      test```\n","user":"U8JP5B9T2","ts":"1613078515.364400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"heNM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a more sensible way to do this transform?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> df = DataFrame(a = [\"test|one\", \"test|two\", \"test\"])\n3×1 DataFrame\n Row │ a\n     │ String\n─────┼──────────\n   1 │ test|one\n   2 │ test|two\n   3 │ test\n\njulia> transform(df, :a => ByRow(s->begin\n           things = split(s, \"|\")\n           (p1, p2) = length(things) == 1 ? (first(things), nothing) : things\n           end) => [:p1, :p2])\n3×3 DataFrame\n Row │ a         p1         p2\n     │ String    SubStrin…  Union…\n─────┼─────────────────────────────\n   1 │ test|one  test       one\n   2 │ test|two  test       two\n   3 │ test      test"}]},{"type":"rich_text_section","elements":[]}]}],"thread_ts":"1613078515.364400","reply_count":10,"reply_users_count":3,"latest_reply":"1613079443.367100","reply_users":["U9VG1AYSG","U8JP5B9T2","U67431ELR"],"subscribed":false},{"client_msg_id":"7f16970f-a623-4a5e-83d9-7fd7688d9a98","type":"message","text":"separate question - can groupby take a function on a column? For example, after the transformation above, is there a way to get the equivalent of\n\n```julia&gt; df.hasp2 = map(!isnothing, df.p2)\n3-element Vector{Bool}:\n 1\n 1\n 0\n\njulia&gt; groupby(df, :hasp2)\nGroupedDataFrame with 2 groups based on key: hasp2\nFirst Group (2 rows): hasp2 = true\n Row │ a         p1         p2      hasp2\n     │ String    SubStrin…  Union…  Bool\n─────┼────────────────────────────────────\n   1 │ test|one  test       one      true\n   2 │ test|two  test       two      true```\nwithout making the intermediate column?","user":"U8JP5B9T2","ts":"1613079663.368700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6WRGh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"separate question - can groupby take a function on a column? For example, after the transformation above, is there a way to get the equivalent of\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> df.hasp2 = map(!isnothing, df.p2)\n3-element Vector{Bool}:\n 1\n 1\n 0\n\njulia> groupby(df, :hasp2)\nGroupedDataFrame with 2 groups based on key: hasp2\nFirst Group (2 rows): hasp2 = true\n Row │ a         p1         p2      hasp2\n     │ String    SubStrin…  Union…  Bool\n─────┼────────────────────────────────────\n   1 │ test|one  test       one      true\n   2 │ test|two  test       two      true"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"without making the intermediate column?"}]}]}],"thread_ts":"1613079663.368700","reply_count":7,"reply_users_count":2,"latest_reply":"1613080728.370000","reply_users":["U7JQGPGCQ","U8JP5B9T2"],"subscribed":false},{"client_msg_id":"f74453c1-221f-4229-b81b-8d5d58219525","type":"message","text":"<@UUJ6L8N8P> did you get anywhere with a successor to MLDataUtils.jl?","user":"U6A936746","ts":"1613082465.371200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KXxh","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UUJ6L8N8P"},{"type":"text","text":" did you get anywhere with a successor to MLDataUtils.jl?"}]}]}]},{"client_msg_id":"7002eea7-22c0-46d7-adab-70fdbd05605f","type":"message","text":"MLDataUtils just require bump of DataFrames version in its Project.toml. anyone with commit power to merge?","user":"U6SHSF4R0","ts":"1613090518.373100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"e/Nnc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"MLDataUtils just require bump of DataFrames version in its Project.toml. anyone with commit power to merge?"}]}]}],"thread_ts":"1613090518.373100","reply_count":2,"reply_users_count":2,"latest_reply":"1613091751.373500","reply_users":["U6A936746","U6SHSF4R0"],"subscribed":false},{"client_msg_id":"ebb760b9-5021-48a9-a6a3-2ef37e797672","type":"message","text":"Struggling a bit with elementary DataFrames manipulations. Trying to add the columns 2 and 3 (A and B below) like this\n```julia&gt; df = DataFrame(dataset=[\"old\", \"new\"], A=[5678,567], B=[876,987])\n2×3 DataFrame\n Row │ dataset  A      B\n     │ String   Int64  Int64\n─────┼───────────────────────\n   1 │ old       5678    876\n   2 │ new        567    987\n\njulia&gt; transform(df, [2,3] =&gt; sum)\nERROR: MethodError: objects of type Vector{Int64} are not callable```\nthrows as you can see. How should I do that?","user":"UB197FRCL","ts":"1613107362.375800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jVAkS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Struggling a bit with elementary DataFrames manipulations. Trying to add the columns 2 and 3 (A and B below) like this\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> df = DataFrame(dataset=[\"old\", \"new\"], A=[5678,567], B=[876,987])\n2×3 DataFrame\n Row │ dataset  A      B\n     │ String   Int64  Int64\n─────┼───────────────────────\n   1 │ old       5678    876\n   2 │ new        567    987\n\njulia> transform(df, [2,3] => sum)\nERROR: MethodError: objects of type Vector{Int64} are not callable"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"throws as you can see. How should I do that?"}]}]}],"thread_ts":"1613107362.375800","reply_count":2,"reply_users_count":1,"latest_reply":"1613107578.376100","reply_users":["UB197FRCL"],"subscribed":false},{"client_msg_id":"1e375059-8d78-4a34-94f6-c9eaf8b5877c","type":"message","text":"This week I have written a basic post on consequences of using of anonymous functions in top-level scope (which is quite common in workflows including DataFrames.jl): <https://bkamins.github.io/julialang/2021/02/12/fun-compile.html>. I also added hints what improvements you can expect in 1.0 in terms of compilation latency.","user":"U8JAMQGQY","ts":"1613121080.378800","team":"T68168MUP","attachments":[{"service_name":"Blog by Bogumił Kamiński","title":"Reducing compilation cost in DataFrames.jl","title_link":"https://bkamins.github.io/julialang/2021/02/12/fun-compile.html","text":"Introduction","fallback":"Blog by Bogumił Kamiński: Reducing compilation cost in DataFrames.jl","ts":1613108683,"from_url":"https://bkamins.github.io/julialang/2021/02/12/fun-compile.html","service_icon":"https://bkamins.github.io/favicon.ico","id":1,"original_url":"https://bkamins.github.io/julialang/2021/02/12/fun-compile.html"}],"blocks":[{"type":"rich_text","block_id":"yEju","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This week I have written a basic post on consequences of using of anonymous functions in top-level scope (which is quite common in workflows including DataFrames.jl): "},{"type":"link","url":"https://bkamins.github.io/julialang/2021/02/12/fun-compile.html"},{"type":"text","text":". I also added hints what improvements you can expect in 1.0 in terms of compilation latency."}]}]}]},{"client_msg_id":"d5f0cf93-1f4d-437b-baec-378c22f09ff9","type":"message","text":"Does the Arrow.Flatbuffer have approximately feature parity with Flatbuffers.jl? I used this pr to flatc <https://github.com/google/flatbuffers/pull/5088> to generate julia code from a flatbuffer schema, and it works with Flatbuffers.jl. If it is possible: what would I have to translate to get that code to work with the Arrow implementation?","user":"U01G3TX4F9A","ts":"1613123275.382300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lDvii","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does the Arrow.Flatbuffer have approximately feature parity with Flatbuffers.jl? I used this pr to flatc "},{"type":"link","url":"https://github.com/google/flatbuffers/pull/5088"},{"type":"text","text":" to generate julia code from a flatbuffer schema, and it works with Flatbuffers.jl. If it is possible: what would I have to translate to get that code to work with the Arrow implementation?"}]}]}]},{"client_msg_id":"c1dbbe67-5ad1-4892-b5ba-17423224e504","type":"message","text":"Is there some overview or benchmark between DataFrames and IndexedTables (used in JuliaDB), with both compile-time and run-time estimations?","user":"UB2QSHWPN","ts":"1613137307.385400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ktex","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there some overview or benchmark between DataFrames and IndexedTables (used in JuliaDB), with both compile-time and run-time estimations?"}]}]}],"thread_ts":"1613137307.385400","reply_count":1,"reply_users_count":1,"latest_reply":"1613139020.385600","reply_users":["U8JAMQGQY"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"It's worth noting that currently DataFramesMeta does not automatically realize that `:x = f(:y)` can be written as `:y =&gt; f =&gt; :y`\n\nI think the reason is that I wanted to wait for `1.6` so that I could deal with broadcasting syntax better.\n\nBut if anyone is an expert in this kind of thing I would love a PR.","user":"UBF9YRB6H","ts":"1613159896.388300","thread_ts":"1613121080.378800","root":{"client_msg_id":"1e375059-8d78-4a34-94f6-c9eaf8b5877c","type":"message","text":"This week I have written a basic post on consequences of using of anonymous functions in top-level scope (which is quite common in workflows including DataFrames.jl): <https://bkamins.github.io/julialang/2021/02/12/fun-compile.html>. I also added hints what improvements you can expect in 1.0 in terms of compilation latency.","user":"U8JAMQGQY","ts":"1613121080.378800","team":"T68168MUP","attachments":[{"service_name":"Blog by Bogumił Kamiński","title":"Reducing compilation cost in DataFrames.jl","title_link":"https://bkamins.github.io/julialang/2021/02/12/fun-compile.html","text":"Introduction","fallback":"Blog by Bogumił Kamiński: Reducing compilation cost in DataFrames.jl","ts":1613108683,"from_url":"https://bkamins.github.io/julialang/2021/02/12/fun-compile.html","service_icon":"https://bkamins.github.io/favicon.ico","id":1,"original_url":"https://bkamins.github.io/julialang/2021/02/12/fun-compile.html"}],"blocks":[{"type":"rich_text","block_id":"yEju","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This week I have written a basic post on consequences of using of anonymous functions in top-level scope (which is quite common in workflows including DataFrames.jl): "},{"type":"link","url":"https://bkamins.github.io/julialang/2021/02/12/fun-compile.html"},{"type":"text","text":". I also added hints what improvements you can expect in 1.0 in terms of compilation latency."}]}]}],"thread_ts":"1613121080.378800","reply_count":6,"reply_users_count":3,"latest_reply":"1613160236.388600","reply_users":["UCTJ1TGBV","U8JAMQGQY","UBF9YRB6H"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"iwv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It's worth noting that currently DataFramesMeta does not automatically realize that "},{"type":"text","text":":x = f(:y)","style":{"code":true}},{"type":"text","text":" can be written as "},{"type":"text","text":":y => f => :y","style":{"code":true}},{"type":"text","text":"\n\nI think the reason is that I wanted to wait for "},{"type":"text","text":"1.6","style":{"code":true}},{"type":"text","text":" so that I could deal with broadcasting syntax better.\n\nBut if anyone is an expert in this kind of thing I would love a PR."}]}]}],"client_msg_id":"4f7b4aa1-5fd8-4c40-86e9-9f580ea6bc53"},{"client_msg_id":"10097423-d108-4fbd-9df4-55c95443ed09","type":"message","text":"Does Pluto.jl not support the latest DataFrames.jl? I am new to Pluto and had to install DataFrames within the notebook session but it seems like it was 21.8 even after I did Pkg.update()","user":"U01EF0QVAB0","ts":"1613178233.391600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3OxN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does Pluto.jl not support the latest DataFrames.jl? I am new to Pluto and had to install DataFrames within the notebook session but it seems like it was 21.8 even after I did Pkg.update()"}]}]}]},{"client_msg_id":"252ee084-82d7-4f00-a57e-fde60141fa09","type":"message","text":"SQLite.jl seems to accept SQL strings that contain more than one SQL statement inside them. In a few quick experiments it appears to execute the first one and ignore the rest. I'm curious if I have understood the behavior correctly and, if so, if this is considered a bug. I ran into it in the course of work and had to spend a bit of time debugging before I figured out why statements that I thought were executing weren't executing.\n```julia&gt; using SQLite\n\njulia&gt; db = SQLite.DB()\nSQLite.DB(\":memory:\")\n\njulia&gt; collect(DBInterface.execute(db, \"select 1; select 2;\"))\n1-element Vector{SQLite.Row}:\n SQLite.Row: (1 = missing,)```","user":"U68M6ERG8","ts":"1613262629.396800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UKnx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"SQLite.jl seems to accept SQL strings that contain more than one SQL statement inside them. In a few quick experiments it appears to execute the first one and ignore the rest. I'm curious if I have understood the behavior correctly and, if so, if this is considered a bug. I ran into it in the course of work and had to spend a bit of time debugging before I figured out why statements that I thought were executing weren't executing.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> using SQLite\n\njulia> db = SQLite.DB()\nSQLite.DB(\":memory:\")\n\njulia> collect(DBInterface.execute(db, \"select 1; select 2;\"))\n1-element Vector{SQLite.Row}:\n SQLite.Row: (1 = missing,)"}]}]}]},{"client_msg_id":"5A425F95-4585-4F85-9F4D-71B4AC6DA494","type":"message","text":"Anyone have experience and subsequent recommendations in attaching Julia based transforms to airflow dags?","user":"UUYRZ3LU8","ts":"1613295496.400900","team":"T68168MUP","edited":{"user":"UUYRZ3LU8","ts":"1613295585.000000"},"blocks":[{"type":"rich_text","block_id":"tWAJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Anyone have experience and subsequent recommendations in attaching Julia based transforms to airflow dags?"}]}]}]},{"client_msg_id":"ff687587-244b-4ec7-812e-9a8d93a1e484","type":"message","text":"Hi. Beginner here. Is there a one-liner to convert in place a DataFrame column that has yyyymmdd (e.g. 19991231) as Int64 to Date type? Thank you.","user":"U014RV0ESB0","ts":"1613321514.405900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"scIe8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi. Beginner here. Is there a one-liner to convert in place a DataFrame column that has yyyymmdd (e.g. 19991231) as Int64 to Date type? Thank you."}]}]}],"thread_ts":"1613321514.405900","reply_count":2,"reply_users_count":2,"latest_reply":"1613323273.406200","reply_users":["UBF9YRB6H","U7JQGPGCQ"],"subscribed":false},{"client_msg_id":"0e91da3e-638f-495b-8291-f83fb85bca31","type":"message","text":"How to convert between an array and a nested struct? That is, `[1, 2, 3, 4, 5]` &lt;-&gt; `MyStruct(1, 2, AnotherStruct(3, 4), 5)`.","user":"UGTUKUHLN","ts":"1613329995.408200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SCD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How to convert between an array and a nested struct? That is, "},{"type":"text","text":"[1, 2, 3, 4, 5]","style":{"code":true}},{"type":"text","text":" <-> "},{"type":"text","text":"MyStruct(1, 2, AnotherStruct(3, 4), 5)","style":{"code":true}},{"type":"text","text":"."}]}]}],"thread_ts":"1613329995.408200","reply_count":1,"reply_users_count":1,"latest_reply":"1613330380.408300","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"feff71f7-50b5-48c1-8200-b0f81757fec2","type":"message","text":"Why does this happen? Why aren't row-wise operations fast in DataFrames.jl?\n\nEdit: To give some context I'm trying to do a DataFrames.jl vs Python Pandas benchmarking\n```julia&gt; @time map(sum, eachcol(df));\n  0.000585 seconds (248 allocations: 11.875 KiB)\n\njulia&gt; @time map(sum, eachrow(df));\n  0.309704 seconds (3.47 M allocations: 68.282 MiB, 4.02% gc time)```","user":"U01FAHWCMFF","ts":"1613341568.411500","team":"T68168MUP","edited":{"user":"U01FAHWCMFF","ts":"1613341687.000000"},"blocks":[{"type":"rich_text","block_id":"Ky7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Why does this happen? Why aren't row-wise operations fast in DataFrames.jl?\n\nEdit: To give some context I'm trying to do a DataFrames.jl vs Python Pandas benchmarking\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> @time map(sum, eachcol(df));\n  0.000585 seconds (248 allocations: 11.875 KiB)\n\njulia> @time map(sum, eachrow(df));\n  0.309704 seconds (3.47 M allocations: 68.282 MiB, 4.02% gc time)"}]}]}]},{"client_msg_id":"b26db966-9a05-4127-92ec-4362d7dd4501","type":"message","text":"Following up from the previous thread: Is there a discussion somewhere about pros/cons of `TypedTables` vs. `DataFrames` and when to use one or the other? It seems like for the case where one is reading data in from a CSV or SQL query and using that as a read-only data source, `TypedTables` will be more performant.","user":"U017JTQFNEQ","ts":"1613374737.415300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"t7k","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Following up from the previous thread: Is there a discussion somewhere about pros/cons of "},{"type":"text","text":"TypedTables","style":{"code":true}},{"type":"text","text":" vs. "},{"type":"text","text":"DataFrames","style":{"code":true}},{"type":"text","text":" and when to use one or the other? It seems like for the case where one is reading data in from a CSV or SQL query and using that as a read-only data source, "},{"type":"text","text":"TypedTables","style":{"code":true}},{"type":"text","text":" will be more performant."}]}]}],"thread_ts":"1613374737.415300","reply_count":4,"reply_users_count":2,"latest_reply":"1613376560.416300","reply_users":["U7PGB5DU3","U017JTQFNEQ"],"subscribed":false},{"client_msg_id":"508a0373-dc67-4ea6-89bc-c7bf8f74a015","type":"message","text":"I have been thinking of how the evaluation of some of the quantities in the iteratively reweighted least squares (IRLS) algorithm for fitting glm's in the GLM package can be accelerated.  One of the tasks in every iteration is, starting from the linear predictor vector, evaluate the mean vector, the derivative of the mean w.r.t the lin. pred., etc. until you have the working weights and the working residuals, which are used in a weighted least squares calculation.  The `GlmResp` struct contains several vectors of the same length that are used in these calculations.  All of this was formulated long before Tables.jl, fused broadcasting, etc. came on the scene and it seems reasonable that using some of the new technologies there could be gains in speed.\n\nSo, starting with something like a column table (where all the columns are of the same floating point type) we want to evaluate and store several quantities in each row from other values in the same row.  Is there anything to be gained by a `Tables.rows` iterator or are those values immutable?  I have the feeling that there should be some aspects of the Tables package that can be used here but I haven't quite worked it out.","user":"UBGRZ7FSP","ts":"1613428111.429100","team":"T68168MUP","edited":{"user":"UBGRZ7FSP","ts":"1613428162.000000"},"blocks":[{"type":"rich_text","block_id":"q6Sn6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have been thinking of how the evaluation of some of the quantities in the iteratively reweighted least squares (IRLS) algorithm for fitting glm's in the GLM package can be accelerated.  One of the tasks in every iteration is, starting from the linear predictor vector, evaluate the mean vector, the derivative of the mean w.r.t the lin. pred., etc. until you have the working weights and the working residuals, which are used in a weighted least squares calculation.  The "},{"type":"text","text":"GlmResp","style":{"code":true}},{"type":"text","text":" struct contains several vectors of the same length that are used in these calculations.  All of this was formulated long before Tables.jl, fused broadcasting, etc. came on the scene and it seems reasonable that using some of the new technologies there could be gains in speed.\n\nSo, starting with something like a column table (where all the columns are of the same floating point type) we want to evaluate and store several quantities in each row from other values in the same row.  Is there anything to be gained by a "},{"type":"text","text":"Tables.rows","style":{"code":true}},{"type":"text","text":" iterator or are those values immutable?  I have the feeling that there should be some aspects of the Tables package that can be used here but I haven't quite worked it out."}]}]}]},{"client_msg_id":"dfaab5e9-2247-4581-846b-f67ae9eeff1e","type":"message","text":"Anyone here an expert on weather data? I'm trying to find a dataset of historical weather forecasts in the US (say 5-day forecasts at the county level or something) and not finding much. Lots of historical actualized weather data, but not much in the way of forecasts.","user":"U01399SPFT5","ts":"1613432997.431000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vqVPE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Anyone here an expert on weather data? I'm trying to find a dataset of historical weather forecasts in the US (say 5-day forecasts at the county level or something) and not finding much. Lots of historical actualized weather data, but not much in the way of forecasts."}]}]}],"thread_ts":"1613432997.431000","reply_count":4,"reply_users_count":2,"latest_reply":"1613435571.431900","reply_users":["U6A936746","U01399SPFT5"],"subscribed":false},{"client_msg_id":"4e22010d-729e-4169-aaa7-ea02cb6d6794","type":"message","text":"I'm thinking of experimenting with something we could potentially do to make untyped tables/dataframes fast for operations like `map(f, eachrow(df))` - we can use reflection to look inside `f`, determine which fields are required, and make a temporary, narrow, named tuple iterator and `map` over that instead. For simple anonymous functions like `f = row -&gt; row.x + row.y` it should be pretty easy to determine from the IR we only need columns `:x`  and `:y`. Whenever it gets too hard to determine we could bail and just use whatever currently happens.\n\nIt turns out you can do all kinds of magic in generated functions and add custom backedges to make sure it stays in sync, like this:\n\n<https://github.com/oxinabox/Tricks.jl/blob/master/src/Tricks.jl>","user":"U66QZ3QF3","ts":"1613459721.438800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5BY/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm thinking of experimenting with something we could potentially do to make untyped tables/dataframes fast for operations like "},{"type":"text","text":"map(f, eachrow(df))","style":{"code":true}},{"type":"text","text":" - we can use reflection to look inside "},{"type":"text","text":"f","style":{"code":true}},{"type":"text","text":", determine which fields are required, and make a temporary, narrow, named tuple iterator and "},{"type":"text","text":"map","style":{"code":true}},{"type":"text","text":" over that instead. For simple anonymous functions like "},{"type":"text","text":"f = row -> row.x + row.y","style":{"code":true}},{"type":"text","text":" it should be pretty easy to determine from the IR we only need columns "},{"type":"text","text":":x","style":{"code":true}},{"type":"text","text":"  and "},{"type":"text","text":":y","style":{"code":true}},{"type":"text","text":". Whenever it gets too hard to determine we could bail and just use whatever currently happens.\n\nIt turns out you can do all kinds of magic in generated functions and add custom backedges to make sure it stays in sync, like this:\n\n"},{"type":"link","url":"https://github.com/oxinabox/Tricks.jl/blob/master/src/Tricks.jl"}]}]}]},{"type":"message","subtype":"thread_broadcast","text":"DataFramesMeta's `@eachrow` does basically exactly this!","user":"UBF9YRB6H","ts":"1613486592.442600","thread_ts":"1613459721.438800","root":{"client_msg_id":"4e22010d-729e-4169-aaa7-ea02cb6d6794","type":"message","text":"I'm thinking of experimenting with something we could potentially do to make untyped tables/dataframes fast for operations like `map(f, eachrow(df))` - we can use reflection to look inside `f`, determine which fields are required, and make a temporary, narrow, named tuple iterator and `map` over that instead. For simple anonymous functions like `f = row -&gt; row.x + row.y` it should be pretty easy to determine from the IR we only need columns `:x`  and `:y`. Whenever it gets too hard to determine we could bail and just use whatever currently happens.\n\nIt turns out you can do all kinds of magic in generated functions and add custom backedges to make sure it stays in sync, like this:\n\n<https://github.com/oxinabox/Tricks.jl/blob/master/src/Tricks.jl>","user":"U66QZ3QF3","ts":"1613459721.438800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5BY/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm thinking of experimenting with something we could potentially do to make untyped tables/dataframes fast for operations like "},{"type":"text","text":"map(f, eachrow(df))","style":{"code":true}},{"type":"text","text":" - we can use reflection to look inside "},{"type":"text","text":"f","style":{"code":true}},{"type":"text","text":", determine which fields are required, and make a temporary, narrow, named tuple iterator and "},{"type":"text","text":"map","style":{"code":true}},{"type":"text","text":" over that instead. For simple anonymous functions like "},{"type":"text","text":"f = row -> row.x + row.y","style":{"code":true}},{"type":"text","text":" it should be pretty easy to determine from the IR we only need columns "},{"type":"text","text":":x","style":{"code":true}},{"type":"text","text":"  and "},{"type":"text","text":":y","style":{"code":true}},{"type":"text","text":". Whenever it gets too hard to determine we could bail and just use whatever currently happens.\n\nIt turns out you can do all kinds of magic in generated functions and add custom backedges to make sure it stays in sync, like this:\n\n"},{"type":"link","url":"https://github.com/oxinabox/Tricks.jl/blob/master/src/Tricks.jl"}]}]}],"thread_ts":"1613459721.438800","reply_count":9,"reply_users_count":4,"latest_reply":"1613488468.443400","reply_users":["U6A936746","U66QZ3QF3","U01GXNFKY6R","UBF9YRB6H"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"ZfIr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"DataFramesMeta's "},{"type":"text","text":"@eachrow","style":{"code":true}},{"type":"text","text":" does basically exactly this!"}]}]}],"client_msg_id":"d5c1f60e-9f01-47f0-b5a3-3e3b572b2882"},{"client_msg_id":"967367ae-598c-4ed4-aa18-fa5ed7850985","type":"message","text":"What is the correct way to drop missing columns? (i.e. columns with entirely missing entries)? I’m trying to pass along a nice DataFrame example to a scientist friend with slightly messy data. CSV has lots of empty columns.\n\nPretty surprised by how difficult it is to just drop missing columns / find any documentation about this. Directly searching any variant for how to drop columns of missing data only describe the process for dropping rows with missing entries.","user":"U90JR0C80","ts":"1613495859.447500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"I6JmF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What is the correct way to drop missing columns? (i.e. columns with entirely missing entries)? I’m trying to pass along a nice DataFrame example to a scientist friend with slightly messy data. CSV has lots of empty columns.\n\nPretty surprised by how difficult it is to just drop missing columns / find any documentation about this. Directly searching any variant for how to drop columns of missing data only describe the process for dropping rows with missing entries."}]}]}],"thread_ts":"1613495859.447500","reply_count":1,"reply_users_count":1,"latest_reply":"1613496181.449500","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"6d885050-f809-45ce-b2ee-f82533ef2c92","type":"message","text":"`skipmissing(eachcol(df))` why should this not work?","user":"U90JR0C80","ts":"1613495939.447800","team":"T68168MUP","edited":{"user":"U90JR0C80","ts":"1613495950.000000"},"blocks":[{"type":"rich_text","block_id":"bv4D","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"skipmissing(eachcol(df))","style":{"code":true}},{"type":"text","text":" why should this not work?"}]}]}],"thread_ts":"1613495939.447800","reply_count":5,"reply_users_count":3,"latest_reply":"1613496546.452600","reply_users":["U67431ELR","U90JR0C80","UBF9YRB6H"],"subscribed":false},{"client_msg_id":"cc42987e-5b75-47ab-a22f-db890396ced1","type":"message","text":"If the data is coming from a CSV originally and you know the columns, you can avoid making the columns at all by doing something like `CSV.File(file; drop=[:missing_col1, :missing_col2])`.","user":"U681ELA87","ts":"1613496100.448900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dj6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If the data is coming from a CSV originally and you know the columns, you can avoid making the columns at all by doing something like "},{"type":"text","text":"CSV.File(file; drop=[:missing_col1, :missing_col2])","style":{"code":true}},{"type":"text","text":"."}]}]}]},{"client_msg_id":"75cac707-77ca-44aa-bef6-4ba41c2460e5","type":"message","text":"The issue is that I don’t know the missing columns","user":"U90JR0C80","ts":"1613496172.449200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jogO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The issue is that I don’t know the missing columns"}]}]}]},{"client_msg_id":"a773ba19-2fa2-42bb-abc6-679427a1e63f","type":"message","text":"the csv has a bunch of `,,,,,` at the end of each row.","user":"U90JR0C80","ts":"1613496189.450000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GVxj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"the csv has a bunch of "},{"type":"text","text":",,,,,","style":{"code":true}},{"type":"text","text":" at the end of each row."}]}]}]},{"client_msg_id":"e8ed359b-fd3c-4f02-b3c3-dd83d7facc36","type":"message","text":"So the resulting dataframe has a bunch of columns with type Missing","user":"U90JR0C80","ts":"1613496204.450600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0ZCCi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So the resulting dataframe has a bunch of columns with type Missing"}]}]}]},{"client_msg_id":"9822de05-9f38-48e2-8c96-f83eb0d57d26","type":"message","text":"Try `df[:, eltype.(eachcol(df)) .!= Missing]` .","user":"U67431ELR","ts":"1613496236.451200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ln0A","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Try "},{"type":"text","text":"df[:, eltype.(eachcol(df)) .!= Missing]","style":{"code":true}},{"type":"text","text":" ."}]}]}],"thread_ts":"1613496236.451200","reply_count":1,"reply_users_count":1,"latest_reply":"1613496424.452000","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"bacc40fc-0348-46fe-ba59-e0f1594c4ce7","type":"message","text":"It's pretty easy to make a Tables.jl column wrapper that drops columns w/ all missings, like:\n```struct DropMissingColumns{T}\n    schema::Tables.Schema\n    x::T\nend\n\nschema(x::DropMissingColumns) = getfield(x, :schema)\n\nfunction dropmissingcolumns(x)\n    cols = Tables.columns(x)\n    sch = Tables.schema(cols)\n    nonmissingcols = [T !== Missing for T in sch.types]\n    newsch = Tables.Schema(sch.names[nonmissingcols], sch.types[nonmissingcols])\n    return DropMissingColumns(newsch, cols)\nend\n\nTables.istable(::Type{&lt;:DropMissingColumns}) = true\nTables.columnaccess(::Type{&lt;:DropMissingColumns}) = true\nTables.columns(x::DropMissingColumns) = x\nTables.schema(nt::DropMissingColumns) = schema(nt)\n\nTables.columnnames(nt::DropMissingColumns) = schema(nt).names\n\nTables.getcolumn(nt::DropMissingColumns, nm::Symbol) = Tables.getcolumn(getfield(nt, :x), nm)\nTables.getcolumn(nt::DropMissingColumns, i::Int) = Tables.getcolumn(getfield(nt, :x), i)```\nThen you would call it like:\n```julia&gt; df = CSV.File(\"/Users/jacobquinn/dotfiles/randoms.csv\"; types=Dict(:id=&gt;Missing)) |&gt; dropmissingcolumns |&gt; DataFrame```\nNot sure if that's easier than DataFrames operations, but it's more generic at least?","user":"U681ELA87","ts":"1613496670.455300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"N2U2+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It's pretty easy to make a Tables.jl column wrapper that drops columns w/ all missings, like:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"struct DropMissingColumns{T}\n    schema::Tables.Schema\n    x::T\nend\n\nschema(x::DropMissingColumns) = getfield(x, :schema)\n\nfunction dropmissingcolumns(x)\n    cols = Tables.columns(x)\n    sch = Tables.schema(cols)\n    nonmissingcols = [T !== Missing for T in sch.types]\n    newsch = Tables.Schema(sch.names[nonmissingcols], sch.types[nonmissingcols])\n    return DropMissingColumns(newsch, cols)\nend\n\nTables.istable(::Type{<:DropMissingColumns}) = true\nTables.columnaccess(::Type{<:DropMissingColumns}) = true\nTables.columns(x::DropMissingColumns) = x\nTables.schema(nt::DropMissingColumns) = schema(nt)\n\nTables.columnnames(nt::DropMissingColumns) = schema(nt).names\n\nTables.getcolumn(nt::DropMissingColumns, nm::Symbol) = Tables.getcolumn(getfield(nt, :x), nm)\nTables.getcolumn(nt::DropMissingColumns, i::Int) = Tables.getcolumn(getfield(nt, :x), i)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Then you would call it like:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> df = CSV.File(\"/Users/jacobquinn/dotfiles/randoms.csv\"; types=Dict(:id=>Missing)) |> dropmissingcolumns |> DataFrame"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Not sure if that's easier than DataFrames operations, but it's more generic at least?"}]}]}]},{"client_msg_id":"92c33643-b2bd-445a-8301-bfa53d08ae9d","type":"message","text":"We could add that to TableOperations.jl if people felt like it'd be helpful generically","user":"U681ELA87","ts":"1613496712.456200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fXo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We could add that to TableOperations.jl if people felt like it'd be helpful generically"}]}]}]},{"client_msg_id":"c72678fa-b1e4-471b-94c1-05e3ae1e6e8c","type":"message","text":"<@UBF9YRB6H>’s suggestion to use `df[:, Not(names(df, Missing))]` is pretty close to the approach I was anticipating from reading  <https://dataframes.juliadata.org/v0.17.0/man/getting_started.html#Taking-a-Subset-1>","user":"U90JR0C80","ts":"1613496857.458800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qys","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UBF9YRB6H"},{"type":"text","text":"’s suggestion to use "},{"type":"text","text":"df[:, Not(names(df, Missing))]","style":{"code":true}},{"type":"text","text":" is pretty close to the approach I was anticipating from reading  "},{"type":"link","url":"https://dataframes.juliadata.org/v0.17.0/man/getting_started.html#Taking-a-Subset-1"}]}]}]},{"client_msg_id":"3b80e2c3-5934-4885-af33-6cf14b9e37e4","type":"message","text":"Is it possible to read a subset of columns into a DataFrame? I see `Arrow.Table(\"mytable.arrow\")[:col1]` in the docs, but this doesn't seem to extend to vectors of symbols. `DataFrame([Arrow.Table(\"mytable.arrow\")[c] for c in [:col1, :col2]])` works but is quite slow as I guess it materializes everything?","user":"U7JQGPGCQ","ts":"1613496884.459100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VOJK0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is it possible to read a subset of columns into a DataFrame? I see "},{"type":"text","text":"Arrow.Table(\"mytable.arrow\")[:col1]","style":{"code":true}},{"type":"text","text":" in the docs, but this doesn't seem to extend to vectors of symbols. "},{"type":"text","text":"DataFrame([Arrow.Table(\"mytable.arrow\")[c] for c in [:col1, :col2]])","style":{"code":true}},{"type":"text","text":" works but is quite slow as I guess it materializes everything?"}]}]}],"thread_ts":"1613496884.459100","reply_count":2,"reply_users_count":1,"latest_reply":"1613498061.476400","reply_users":["U681ELA87"],"subscribed":false},{"client_msg_id":"c1cf80a6-760a-4df2-bb6e-2a3f5dbf81fb","type":"message","text":"Fair enough. We should improve the docs to discuss `names`","user":"UBF9YRB6H","ts":"1613497009.460000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oggwa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Fair enough. We should improve the docs to discuss "},{"type":"text","text":"names","style":{"code":true}}]}]}]},{"client_msg_id":"49e5fb27-b362-45a2-8391-a78f8191fce5","type":"message","text":"<@UBF9YRB6H> your “ready-made function” approach isn’t really a function, it’s just pre-building the index array.\n\nIs there a filter-like approach where `nonmissing` is a function that returns `0`or `1` and can be called like `df[:,nonmissing]`?","user":"U90JR0C80","ts":"1613497132.461900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7zQ","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UBF9YRB6H"},{"type":"text","text":" your “ready-made function” approach isn’t really a function, it’s just pre-building the index array.\n\nIs there a filter-like approach where "},{"type":"text","text":"nonmissing","style":{"code":true}},{"type":"text","text":" is a function that returns "},{"type":"text","text":"0","style":{"code":true}},{"type":"text","text":"or "},{"type":"text","text":"1","style":{"code":true}},{"type":"text","text":" and can be called like "},{"type":"text","text":"df[:,nonmissing]","style":{"code":true}},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"6cbfd9d2-5cf7-4070-8666-e6dbec424a19","type":"message","text":"If you’re taking  a look at documentation can I also say that this subsection really should be promoted out of Getting Started?\n\nIt is not discoverable in the side bar. The whole Working with Data Frames section, imo, should be deleted and all those sub sub sections promoted to appear on the left navigation bar.\n\n<https://dataframes.juliadata.org/v0.17.0/man/getting_started.html#Taking-a-Subset-1>","user":"U90JR0C80","ts":"1613497251.463900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"aZd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If you’re taking  a look at documentation can I also say that this subsection really should be promoted out of Getting Started?\n\nIt is not discoverable in the side bar. The whole Working with Data Frames section, imo, should be deleted and all those sub sub sections promoted to appear on the left navigation bar.\n\n"},{"type":"link","url":"https://dataframes.juliadata.org/v0.17.0/man/getting_started.html#Taking-a-Subset-1"}]}]}],"thread_ts":"1613497251.463900","reply_count":2,"reply_users_count":2,"latest_reply":"1613497965.475500","reply_users":["U90JR0C80","U67431ELR"],"subscribed":false},{"client_msg_id":"9f0791ab-4e07-4346-b4c0-3c3cb015780f","type":"message","text":"No, it's not a ready made function. I'm just highlighting the fact that this isn't something I think anyone has asked for before, so it's undertandable that this exact operation isn't something that's highlighted on introductory docs pages.","user":"UBF9YRB6H","ts":"1613497343.465400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sCIz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"No, it's not a ready made function. I'm just highlighting the fact that this isn't something I think anyone has asked for before, so it's undertandable that this exact operation isn't something that's highlighted on introductory docs pages."}]}]}]},{"client_msg_id":"18bb95c5-1282-4487-8fe4-9fbb95da781a","type":"message","text":"Imo `select(df, Not(names(df, Missing))` is the closest you will get to a ready-made function. And it's pretty good imo. I don't know ow you would do this in Stata or R, for example. Its frustrating that our subsetting docs are not better, though.","user":"UBF9YRB6H","ts":"1613497418.466800","team":"T68168MUP","edited":{"user":"UBF9YRB6H","ts":"1613497430.000000"},"blocks":[{"type":"rich_text","block_id":"rcc1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Imo "},{"type":"text","text":"select(df, Not(names(df, Missing))","style":{"code":true}},{"type":"text","text":" is the closest you will get to a ready-made function. And it's pretty good imo. I don't know ow you would do this in Stata or R, for example. Its frustrating that our subsetting docs are not better, though."}]}]}],"reactions":[{"name":"+1","users":["U67431ELR"],"count":1}]},{"client_msg_id":"963e84b1-a84c-452b-aeb5-f29306df06a7","type":"message","text":"I actually like Milan's solution best - all base functionality, no surprises. I didn't even know about the 2-arg version of `names`, and find it slightly surprising to the reader (`Missing` is of course not the name of any column, so you have to know that if the second argument is a type it acts as a filter)","user":"U7JQGPGCQ","ts":"1613497528.469000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JU8D","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I actually like Milan's solution best - all base functionality, no surprises. I didn't even know about the 2-arg version of "},{"type":"text","text":"names","style":{"code":true}},{"type":"text","text":", and find it slightly surprising to the reader ("},{"type":"text","text":"Missing","style":{"code":true}},{"type":"text","text":" is of course not the name of any column, so you have to know that if the second argument is a type it acts as a filter)"}]}]}],"thread_ts":"1613497528.469000","reply_count":2,"reply_users_count":1,"latest_reply":"1613497610.470800","reply_users":["U7JQGPGCQ"],"subscribed":false,"reactions":[{"name":"+1","users":["UBF9YRB6H"],"count":1}]},{"client_msg_id":"216c8f63-15db-49a4-8527-4e516cd80f30","type":"message","text":"<@UBF9YRB6H> actually my issue with the “ready made function” was not that it isn’t “ready-made”, I agree with you it’s fine that this isn’t provided.\n\nMy issue is that it isn’t a function.\n\nIs there a syntax for subset selection that, e.g. wouldn’t require first building the index array? I.e.\n\n```idx = nonmissing.(eachcol(df))\ndf[:,idx]```\nvs\n```df[:,nonmissing]```\nSomething like a filter that applies to columns.","user":"U90JR0C80","ts":"1613497744.472600","team":"T68168MUP","edited":{"user":"U90JR0C80","ts":"1613497783.000000"},"blocks":[{"type":"rich_text","block_id":"o37","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UBF9YRB6H"},{"type":"text","text":" actually my issue with the “ready made function” was not that it isn’t “ready-made”, I agree with you it’s fine that this isn’t provided.\n\nMy issue is that it isn’t a function.\n\nIs there a syntax for subset selection that, e.g. wouldn’t require first building the index array? I.e.\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"idx = nonmissing.(eachcol(df))\ndf[:,idx]"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"vs\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"df[:,nonmissing]"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nSomething like a filter that applies to columns."}]}]}]},{"client_msg_id":"f8b1c638-4612-4fdb-b621-8853dc0d863c","type":"message","text":"Are you looking for a piping solution?","user":"UBF9YRB6H","ts":"1613497784.473500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7KR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Are you looking for a piping solution?"}]}]}]},{"client_msg_id":"ee324d3d-6fa8-4ea3-9757-31a7fce69d79","type":"message","text":"no, there is nothing like `filter` that applies to columns.","user":"UBF9YRB6H","ts":"1613497794.473800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+LbG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"no, there is nothing like "},{"type":"text","text":"filter","style":{"code":true}},{"type":"text","text":" that applies to columns."}]}]}]},{"client_msg_id":"88c1df52-1665-4765-aa53-f1ab43424726","type":"message","text":"If you are using, for example `Chain.jl`, then you can use underscores to make it more function-like. But there is no solution that doesn't involve building an array.","user":"UBF9YRB6H","ts":"1613497866.475000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qEcji","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If you are using, for example "},{"type":"text","text":"Chain.jl","style":{"code":true}},{"type":"text","text":", then you can use underscores to make it more function-like. But there is no solution that doesn't involve building an array."}]}]}],"reactions":[{"name":"+1","users":["U90JR0C80"],"count":1}]},{"client_msg_id":"5b206ad8-6c2c-4d31-919f-c69949ee43ea","type":"message","text":"```@chain df begin \n    select(_, Not(names(_, Missing))\nend```","user":"UBF9YRB6H","ts":"1613498029.476200","team":"T68168MUP","edited":{"user":"UBF9YRB6H","ts":"1613498045.000000"},"blocks":[{"type":"rich_text","block_id":"weHaf","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@chain df begin \n    select(_, Not(names(_, Missing))\nend"}]}]}]},{"client_msg_id":"f23860b5-1e9c-4565-8015-eb85e7d3433a","type":"message","text":"Ah, so I think my issue is that since I was not able to find the taking subset documentation initially I was looking at external guides that use things like `select` and `filter` and conflating that with the method that `getindex` is providing.","user":"U90JR0C80","ts":"1613498253.478000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uAG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah, so I think my issue is that since I was not able to find the taking subset documentation initially I was looking at external guides that use things like "},{"type":"text","text":"select","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"filter","style":{"code":true}},{"type":"text","text":" and conflating that with the method that "},{"type":"text","text":"getindex","style":{"code":true}},{"type":"text","text":" is providing."}]}]}]},{"client_msg_id":"e4ec1085-5ba0-44f7-a435-09ae56e3dfd8","type":"message","text":"*Call for proposals*\nIn cooperation with <@UAVMYR0F4> and <https://pumas.ai/|Pumas AI> I am glad to announce that DataFrames.jl that we are currently looking for a person who would be willing to work on improving the DataFrames.jl manual as this is an important area to improve the package before its 1.0 release.\nIf you would be interested to undertake this project please message <@UAVMYR0F4> + <@U67431ELR> + me, preferably including links to PRs with your previous work on documentation of packages (also previous experience with working with DataFrames.jl is of course welcome).\nWe expect to pick one or two developer for this project (depending on the proposed scope of cooperation) that would work in cooperation with <@U67431ELR> and me. As a part of this project, we also want to build a set of pharma field examples.\nThe work would be financially supported by <https://pumas.ai/|Pumas AI> (<@UAVMYR0F4> - thank you very much for generously offering to support this task).","user":"U8JAMQGQY","ts":"1613505914.484900","team":"T68168MUP","attachments":[{"title":"Pumas AI","title_link":"https://pumas.ai/","text":"Pharmaceutical Modeling and Simulation","fallback":"Pumas AI","image_url":"https://pumas.ai/images/common/Pumas_favicon_lg.png","image_width":250,"image_height":250,"from_url":"https://pumas.ai/","image_bytes":19801,"service_icon":"https://pumas.ai/icons/icon-48x48.png?v=ded15072736cdbfac62cb44bca471829","service_name":"pumas.ai","id":1,"original_url":"https://pumas.ai/"}],"blocks":[{"type":"rich_text","block_id":"9bO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Call for proposals","style":{"bold":true}},{"type":"text","text":"\nIn cooperation with "},{"type":"user","user_id":"UAVMYR0F4"},{"type":"text","text":" and "},{"type":"link","url":"https://pumas.ai/","text":"Pumas AI"},{"type":"text","text":" I am glad to announce that DataFrames.jl that we are currently looking for a person who would be willing to work on improving the DataFrames.jl manual as this is an important area to improve the package before its 1.0 release.\nIf you would be interested to undertake this project please message "},{"type":"user","user_id":"UAVMYR0F4"},{"type":"text","text":" + "},{"type":"user","user_id":"U67431ELR"},{"type":"text","text":" + me, preferably including links to PRs with your previous work on documentation of packages (also previous experience with working with DataFrames.jl is of course welcome).\nWe expect to pick one or two developer for this project (depending on the proposed scope of cooperation) that would work in cooperation with "},{"type":"user","user_id":"U67431ELR"},{"type":"text","text":" and me. As a part of this project, we also want to build a set of pharma field examples.\nThe work would be financially supported by "},{"type":"link","url":"https://pumas.ai/","text":"Pumas AI"},{"type":"text","text":" ("},{"type":"user","user_id":"UAVMYR0F4"},{"type":"text","text":" - thank you very much for generously offering to support this task)."}]}]}],"reactions":[{"name":"100","users":["U681ELA87","U69KQT9DL","U680THK2S"],"count":3},{"name":"tada","users":["U681ELA87","U680THK2S","UDXST8ARK"],"count":3},{"name":"money_mouth_face","users":["U680THK2S"],"count":1},{"name":"+1","users":["U7JQGPGCQ","UDXST8ARK","UK1BNFHFV"],"count":3}]},{"client_msg_id":"8f5f119e-61fa-46f5-bce4-3c4c44bea9de","type":"message","text":"Crossposting from <#C6A044SQH|helpdesk>: Is anyone here familiar with CSV.jl? Is there a way to iterate through each row element by element? Basically, I need to access the elements of each row in order but I don’t know before reading the file how many columns there will be.","user":"USBRJS6BU","ts":"1613542131.491600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Xadu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Crossposting from "},{"type":"channel","channel_id":"C6A044SQH"},{"type":"text","text":": Is anyone here familiar with CSV.jl? Is there a way to iterate through each row element by element? Basically, I need to access the elements of each row in order but I don’t know before reading the file how many columns there will be."}]}]}]},{"client_msg_id":"f31caebb-599f-4410-a6d6-e85ad541418c","type":"message","text":"Is anyone interested in code golf here? I’m trying to translate this python program into the smallest julia program possible:\n```import pandas as pd\ndf = pd.read_json(\"<gs://bucket/file.json.gz>\", lines=True)```\nThis is the best that I have:\n```;gsutil cp \"<gs://bucket/file.json.gz>\" .\nusing DataFrames, CodecZlib, JSON3\n\ndf = open(\"file.json.gz\") do file\n    DataFrame(JSON3.read(GzipDecompressorStream(file); jsonlines=true); copycols=false)\nend```\n","user":"U01GXNFKY6R","ts":"1613564098.497900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4KzuO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is anyone interested in code golf here? I’m trying to translate this python program into the smallest julia program possible:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"import pandas as pd\ndf = pd.read_json(\"gs://bucket/file.json.gz\", lines=True)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"This is the best that I have:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":";gsutil cp \"gs://bucket/file.json.gz\" .\nusing DataFrames, CodecZlib, JSON3\n\ndf = open(\"file.json.gz\") do file\n    DataFrame(JSON3.read(GzipDecompressorStream(file); jsonlines=true); copycols=false)\nend"}]},{"type":"rich_text_section","elements":[]}]}],"thread_ts":"1613564098.497900","reply_count":2,"reply_users_count":1,"latest_reply":"1613564547.498200","reply_users":["UH24GRBLL"],"subscribed":false},{"client_msg_id":"8abf1e72-c41f-403c-a489-8c07ba8cb00a","type":"message","text":"How can I best save a DataFrame, where columns sometimes contain arrays, and some of these arrays contain staticvectors/matrices? It's experimental data where each row is a trial, but there is some temporal data in each trial. The important data is the response and not the temporal data, so I think it makes no sense to flatten the vectors into a \"long\" format. I've just tried Arrow with high hopes, but the Vectors of SVectors are getting lost and read back in as Vectors of Vectors","user":"UK1BNFHFV","ts":"1613577305.001800","team":"T68168MUP","edited":{"user":"UK1BNFHFV","ts":"1613577343.000000"},"blocks":[{"type":"rich_text","block_id":"nqpA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I best save a DataFrame, where columns sometimes contain arrays, and some of these arrays contain staticvectors/matrices? It's experimental data where each row is a trial, but there is some temporal data in each trial. The important data is the response and not the temporal data, so I think it makes no sense to flatten the vectors into a \"long\" format. I've just tried Arrow with high hopes, but the Vectors of SVectors are getting lost and read back in as Vectors of Vectors"}]}]}],"thread_ts":"1613577305.001800","reply_count":16,"reply_users_count":2,"latest_reply":"1613578999.005800","reply_users":["U9VG1AYSG","UK1BNFHFV"],"subscribed":false},{"client_msg_id":"6315ac76-42ca-4849-9035-406e48cb7626","type":"message","text":"Question about Julia and Kaggle: I know it’s possible to run julia notebooks on kaggle, but is it actually possible to *participate in competitions* using julia?","user":"US8V7JSKB","ts":"1613598278.009600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4zSpV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Question about Julia and Kaggle: I know it’s possible to run julia notebooks on kaggle, but is it actually possible to "},{"type":"text","text":"participate in competitions","style":{"bold":true}},{"type":"text","text":" using julia?"}]}]}]},{"type":"message","subtype":"thread_broadcast","text":"<https://www.kaggle.com/product-feedback/83644|https://www.kaggle.com/product-feedback/83644> can you upvote and comment","user":"UDGT4PM41","ts":"1613602650.010100","thread_ts":"1613598278.009600","root":{"client_msg_id":"6315ac76-42ca-4849-9035-406e48cb7626","type":"message","text":"Question about Julia and Kaggle: I know it’s possible to run julia notebooks on kaggle, but is it actually possible to *participate in competitions* using julia?","user":"US8V7JSKB","ts":"1613598278.009600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4zSpV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Question about Julia and Kaggle: I know it’s possible to run julia notebooks on kaggle, but is it actually possible to "},{"type":"text","text":"participate in competitions","style":{"bold":true}},{"type":"text","text":" using julia?"}]}]}],"thread_ts":"1613598278.009600","reply_count":1,"reply_users_count":1,"latest_reply":"1613602650.010100","reply_users":["UDGT4PM41"],"subscribed":false},"attachments":[{"title":"Will kaggle include Julia? | Data Science and Machine Learning","title_link":"https://www.kaggle.com/product-feedback/83644","text":"Will kaggle include Julia?.","fallback":"Will kaggle include Julia? | Data Science and Machine Learning","from_url":"https://www.kaggle.com/product-feedback/83644","service_name":"kaggle.com","id":1,"original_url":"https://www.kaggle.com/product-feedback/83644"}],"blocks":[{"type":"rich_text","block_id":"mJSS3","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://www.kaggle.com/product-feedback/83644","text":"https://www.kaggle.com/product-feedback/83644"},{"type":"text","text":" can you upvote and comment"}]}]}],"client_msg_id":"76800f17-2d7a-42a9-aeff-563c3c0e109e"},{"client_msg_id":"c3c52c8e-5663-4cb3-873c-7ac3b3130e58","type":"message","text":"is there a good function to unpack vector-valued columns with DataFrames.jl? i.e. this transformation\n```julia&gt; df = DataFrame( (; a =  rand(3), b = [rand(2) for _ = 1:3]))\n3×2 DataFrame\n Row │ a         b                    \n     │ Float64   Array…               \n─────┼────────────────────────────────\n   1 │ 0.562557  [0.805228, 0.323733]\n   2 │ 0.438863  [0.509096, 0.656699]\n   3 │ 0.417421  [0.824068, 0.48768]\n\njulia&gt; expand(df, :b)\n6×2 DataFrame\n Row │ a         b        \n     │ Float64   Float64  \n─────┼────────────────────\n   1 │ 0.562557  0.805228\n   2 │ 0.562557  0.323733\n   3 │ 0.438863  0.509096\n   4 │ 0.438863  0.656699\n   5 │ 0.417421  0.824068\n   6 │ 0.417421  0.48768```","user":"UCZ7VBGUD","ts":"1613608529.011700","team":"T68168MUP","edited":{"user":"UCZ7VBGUD","ts":"1613608581.000000"},"blocks":[{"type":"rich_text","block_id":"=g44","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is there a good function to unpack vector-valued columns with DataFrames.jl? i.e. this transformation\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> df = DataFrame( (; a =  rand(3), b = [rand(2) for _ = 1:3]))\n3×2 DataFrame\n Row │ a         b                    \n     │ Float64   Array…               \n─────┼────────────────────────────────\n   1 │ 0.562557  [0.805228, 0.323733]\n   2 │ 0.438863  [0.509096, 0.656699]\n   3 │ 0.417421  [0.824068, 0.48768]\n\njulia> expand(df, :b)\n6×2 DataFrame\n Row │ a         b        \n     │ Float64   Float64  \n─────┼────────────────────\n   1 │ 0.562557  0.805228\n   2 │ 0.562557  0.323733\n   3 │ 0.438863  0.509096\n   4 │ 0.438863  0.656699\n   5 │ 0.417421  0.824068\n   6 │ 0.417421  0.48768"}]}]}],"thread_ts":"1613608529.011700","reply_count":1,"reply_users_count":1,"latest_reply":"1613608546.011800","reply_users":["UCZ7VBGUD"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"`flatten`. I think it existed before Juliacon... we really need to improve the data frames docs i guess","user":"UBF9YRB6H","ts":"1613610237.015400","thread_ts":"1613608529.011700","root":{"client_msg_id":"c3c52c8e-5663-4cb3-873c-7ac3b3130e58","type":"message","text":"is there a good function to unpack vector-valued columns with DataFrames.jl? i.e. this transformation\n```julia&gt; df = DataFrame( (; a =  rand(3), b = [rand(2) for _ = 1:3]))\n3×2 DataFrame\n Row │ a         b                    \n     │ Float64   Array…               \n─────┼────────────────────────────────\n   1 │ 0.562557  [0.805228, 0.323733]\n   2 │ 0.438863  [0.509096, 0.656699]\n   3 │ 0.417421  [0.824068, 0.48768]\n\njulia&gt; expand(df, :b)\n6×2 DataFrame\n Row │ a         b        \n     │ Float64   Float64  \n─────┼────────────────────\n   1 │ 0.562557  0.805228\n   2 │ 0.562557  0.323733\n   3 │ 0.438863  0.509096\n   4 │ 0.438863  0.656699\n   5 │ 0.417421  0.824068\n   6 │ 0.417421  0.48768```","user":"UCZ7VBGUD","ts":"1613608529.011700","team":"T68168MUP","edited":{"user":"UCZ7VBGUD","ts":"1613608581.000000"},"blocks":[{"type":"rich_text","block_id":"=g44","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is there a good function to unpack vector-valued columns with DataFrames.jl? i.e. this transformation\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> df = DataFrame( (; a =  rand(3), b = [rand(2) for _ = 1:3]))\n3×2 DataFrame\n Row │ a         b                    \n     │ Float64   Array…               \n─────┼────────────────────────────────\n   1 │ 0.562557  [0.805228, 0.323733]\n   2 │ 0.438863  [0.509096, 0.656699]\n   3 │ 0.417421  [0.824068, 0.48768]\n\njulia> expand(df, :b)\n6×2 DataFrame\n Row │ a         b        \n     │ Float64   Float64  \n─────┼────────────────────\n   1 │ 0.562557  0.805228\n   2 │ 0.562557  0.323733\n   3 │ 0.438863  0.509096\n   4 │ 0.438863  0.656699\n   5 │ 0.417421  0.824068\n   6 │ 0.417421  0.48768"}]}]}],"thread_ts":"1613608529.011700","reply_count":23,"reply_users_count":5,"latest_reply":"1613611369.017700","reply_users":["UCZ7VBGUD","U680THK2S","U6A936746","UBF9YRB6H","U681ELA87"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"b+q/u","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"flatten","style":{"code":true}},{"type":"text","text":". I think it existed before Juliacon... we really need to improve the data frames docs i guess"}]}]}],"client_msg_id":"724d2f37-0187-49dd-8f34-bce7a7236016"},{"client_msg_id":"86ab4aee-7860-431d-9078-bee4257a7d74","type":"message","text":"I have a table that looks something like this:\n\n```148824×2 DataFrame\n    Row │ Break_Out_Category  BreakOutCategoryID\n        │ String              String\n────────┼────────────────────────────────────────\n      1 │ Overall             CAT1\n      2 │ Overall             CAT1\n      3 │ Overall             CAT1\n      4 │ Overall             CAT1\n      5 │ Overall             CAT1\n   ⋮    │         ⋮                   ⋮\n 148821 │ Household Income    CAT6\n 148822 │ Household Income    CAT6\n 148823 │ Household Income    CAT6\n 148824 │ Household Income    CAT6```\nWhat I would like to do is get each unique row only once where I could have a subdataframe like this\n\n```2×2 DataFrame\n    Row │ Break_Out_Category  BreakOutCategoryID\n        │ String              String\n────────┼────────────────────────────────────────\n      1 │ Overall             CAT1\n      2 │ Household Income    CAT6```\nHow do I do something like this? I was thinking I should filter somehow, but am not sure how to best approach it. Any thoughts? Thank you!","user":"US64J0NPQ","ts":"1613680848.023900","team":"T68168MUP","edited":{"user":"US64J0NPQ","ts":"1613680892.000000"},"blocks":[{"type":"rich_text","block_id":"YhK=b","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a table that looks something like this:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"148824×2 DataFrame\n    Row │ Break_Out_Category  BreakOutCategoryID\n        │ String              String\n────────┼────────────────────────────────────────\n      1 │ Overall             CAT1\n      2 │ Overall             CAT1\n      3 │ Overall             CAT1\n      4 │ Overall             CAT1\n      5 │ Overall             CAT1\n   ⋮    │         ⋮                   ⋮\n 148821 │ Household Income    CAT6\n 148822 │ Household Income    CAT6\n 148823 │ Household Income    CAT6\n 148824 │ Household Income    CAT6"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nWhat I would like to do is get each unique row only once where I could have a subdataframe like this\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"2×2 DataFrame\n    Row │ Break_Out_Category  BreakOutCategoryID\n        │ String              String\n────────┼────────────────────────────────────────\n      1 │ Overall             CAT1\n      2 │ Household Income    CAT6"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nHow do I do something like this? I was thinking I should filter somehow, but am not sure how to best approach it. Any thoughts? Thank you!"}]}]}]},{"client_msg_id":"e4cbd00d-9c45-42d1-a7db-7e3b8d7ec155","type":"message","text":"```unique(df, [:Break_Out_Category, :BreakOutCategoryID])```\n","user":"UBF9YRB6H","ts":"1613680940.025300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"47Tu","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"unique(df, [:Break_Out_Category, :BreakOutCategoryID])"}]},{"type":"rich_text_section","elements":[]}]}],"thread_ts":"1613680940.025300","reply_count":1,"reply_users_count":1,"latest_reply":"1613681038.025400","reply_users":["US64J0NPQ"],"subscribed":false},{"client_msg_id":"8153e314-a283-4b81-b905-4d02177c9cc8","type":"message","text":"Hi everyone\ni am trying to run a for loop on a function but this does not update the values so say i have some df like so.\n```data = DataFrame(start= collect(Dates.Date(2015,12,31):Dates.Month(12):Dates.Date(2020,12,31)),\n                valuesOne = [2.0,1.0,3.0,2.0,4.0,2.5],\n                 valuesTwo =[4.0,6.0,5.50,5.50,5.50,6.0])```\nand now here is the for loop\n```\n    data2 = data\n    valuesd = data2[!,:2:end] \n    v = Array(last(valuesd))\n    xs = Array[v]\n     for i in 1:n\n        v = last(xs)\n        g = myFunOne(data2)\n        v =  (v .* g)                                                                           \n        push!(xs,v)\n        addDate = last(data2[!,:1]) + Dates.Year(1)\n        push!(data2,vcat(addDate,v))\n    end \n    ```\nso my challenge is at `g = myFunOne(data2)` which does not update after each iteration. my `myFunOne` takes in a df and returns an array something like(`myFunOne(data::DataFrame)`.\nthe intended output is after updating the data2 `myFunOne` should use the updated data2  to generate g for the next iteration.\nThank you for your help.","user":"UPH1M2MB2","ts":"1613728850.036000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hKgP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi everyone\ni am trying to run a for loop on a function but this does not update the values so say i have some df like so.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"data = DataFrame(start= collect(Dates.Date(2015,12,31):Dates.Month(12):Dates.Date(2020,12,31)),\n                valuesOne = [2.0,1.0,3.0,2.0,4.0,2.5],\n                 valuesTwo =[4.0,6.0,5.50,5.50,5.50,6.0])"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"and now here is the for loop\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"\n    data2 = data\n    valuesd = data2[!,:2:end] \n    v = Array(last(valuesd))\n    xs = Array[v]\n     for i in 1:n\n        v = last(xs)\n        g = myFunOne(data2)\n        v =  (v .* g)                                                                           \n        push!(xs,v)\n        addDate = last(data2[!,:1]) + Dates.Year(1)\n        push!(data2,vcat(addDate,v))\n    end \n    "}]},{"type":"rich_text_section","elements":[{"type":"text","text":"so my challenge is at "},{"type":"text","text":"g = myFunOne(data2)","style":{"code":true}},{"type":"text","text":" which does not update after each iteration. my "},{"type":"text","text":"myFunOne","style":{"code":true}},{"type":"text","text":" takes in a df and returns an array something like("},{"type":"text","text":"myFunOne(data::DataFrame)","style":{"code":true}},{"type":"text","text":".\nthe intended output is after updating the data2 "},{"type":"text","text":"myFunOne","style":{"code":true}},{"type":"text","text":" should use the updated data2  to generate g for the next iteration.\nThank you for your help."}]}]}]},{"client_msg_id":"3312b1a2-d4fd-440f-9401-b8b826650875","type":"message","text":"why does rename() switch the arguments in only one method?","user":"U01ARRMLM7E","ts":"1613770095.039000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"XDcEq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"why does rename() switch the arguments in only one method?"}]}]}],"thread_ts":"1613770095.039000","reply_count":3,"reply_users_count":3,"latest_reply":"1613770279.039800","reply_users":["U01ARRMLM7E","UBF9YRB6H","U8JAMQGQY"],"subscribed":false},{"client_msg_id":"db3c4997-f687-455e-8462-9c75e8bab516","type":"message","text":"Hi all. I'm sure this is possible, I'm just not familiar enough with DataFrames.jl to know which function to use in this case. My data looks like this:\n```22×3 DataFrame\n Row │ step   nutrients  replicate \n     │ Int64  Float64    Int64     \n─────┼─────────────────────────────\n   1 │     0    1.00504          1\n   2 │     1    1.03524          1\n   3 │     2    1.07272          1\n   4 │     3    1.07911          1\n   5 │     4    1.10866          1\n   6 │     5    1.04709          1\n   7 │     6    1.01967          1\n   8 │     7    1.0493           1\n   9 │     8    1.09155          1\n  10 │     9    1.08902          1\n  11 │    10    1.20758          1\n  12 │     0    1.00504          2\n  13 │     1    1.03524          2\n  14 │     2    1.07272          2\n  15 │     3    1.07911          2\n  16 │     4    1.10866          2\n  17 │     5    1.04709          2\n  18 │     6    1.01967          2\n  19 │     7    1.0493           2\n  20 │     8    1.09155          2\n  21 │     9    1.08902          2\n  22 │    10    1.20758          2```\nI'd like to reshape it so that I have `step`, `nutrients_1`, `nutrients_2` . So basically a 10x3 dataframe as the output.","user":"UUTEK2XTM","ts":"1613921329.004500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"l0aK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi all. I'm sure this is possible, I'm just not familiar enough with DataFrames.jl to know which function to use in this case. My data looks like this:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"22×3 DataFrame\n Row │ step   nutrients  replicate \n     │ Int64  Float64    Int64     \n─────┼─────────────────────────────\n   1 │     0    1.00504          1\n   2 │     1    1.03524          1\n   3 │     2    1.07272          1\n   4 │     3    1.07911          1\n   5 │     4    1.10866          1\n   6 │     5    1.04709          1\n   7 │     6    1.01967          1\n   8 │     7    1.0493           1\n   9 │     8    1.09155          1\n  10 │     9    1.08902          1\n  11 │    10    1.20758          1\n  12 │     0    1.00504          2\n  13 │     1    1.03524          2\n  14 │     2    1.07272          2\n  15 │     3    1.07911          2\n  16 │     4    1.10866          2\n  17 │     5    1.04709          2\n  18 │     6    1.01967          2\n  19 │     7    1.0493           2\n  20 │     8    1.09155          2\n  21 │     9    1.08902          2\n  22 │    10    1.20758          2"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I'd like to reshape it so that I have "},{"type":"text","text":"step","style":{"code":true}},{"type":"text","text":", "},{"type":"text","text":"nutrients_1","style":{"code":true}},{"type":"text","text":", "},{"type":"text","text":"nutrients_2","style":{"code":true}},{"type":"text","text":" . So basically a 10x3 dataframe as the output."}]}]}],"thread_ts":"1613921329.004500","reply_count":4,"reply_users_count":2,"latest_reply":"1613922586.005200","reply_users":["UGKHXS9J6","UUTEK2XTM"],"subscribed":false},{"client_msg_id":"5b5615b3-2a73-440e-9532-e4421a0c638a","type":"message","text":"I want to replace all Real columns in my DataFrame with Float64 versions. How can I do that?","user":"U01ARRMLM7E","ts":"1614034080.016900","team":"T68168MUP","edited":{"user":"U01ARRMLM7E","ts":"1614034093.000000"},"blocks":[{"type":"rich_text","block_id":"0D0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I want to replace all Real columns in my DataFrame with Float64 versions. How can I do that?"}]}]}]},{"client_msg_id":"bc2c7849-b201-4684-8f5a-dda239b03129","type":"message","text":"Are all the values actually Float64? You could do `TableOperations.narrowtypes!(df)`","user":"U681ELA87","ts":"1614034210.017600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nlkc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Are all the values actually Float64? You could do "},{"type":"text","text":"TableOperations.narrowtypes!(df)","style":{"code":true}}]}]}]},{"client_msg_id":"eb4fc13b-c54d-4ece-837e-133bbc6368e6","type":"message","text":"Nice, thanks","user":"U01ARRMLM7E","ts":"1614034292.017900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Aixq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Nice, thanks"}]}]}]},{"client_msg_id":"0a0f6268-6e87-4e28-a5e9-0501bcd3b2a3","type":"message","text":"is there any way to tell Parquet.jl to read a column as a `DateTime` ?","user":"UDXST8ARK","ts":"1614100796.020400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hTsg/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is there any way to tell Parquet.jl to read a column as a "},{"type":"text","text":"DateTime","style":{"code":true}},{"type":"text","text":" ?"}]}]}],"thread_ts":"1614100796.020400","reply_count":1,"reply_users_count":1,"latest_reply":"1614100835.020500","reply_users":["UDXST8ARK"],"subscribed":false},{"client_msg_id":"d9f1c0e5-c3a7-485b-aa86-44c8bd0ec9fc","type":"message","text":"<@U681ELA87> I'm running into something weird with `JSON3.jl` - I'm reading in a 258Mb JSON file, and `show`ing seems to freeze things:\n\n```julia&gt; mystr = read(\"test.json\", String);\n\njulia&gt; results = JSON3.read(mystr)\nJSON3.Object{Base.CodeUnits{UInt8, String}, Vector{UInt64}} with 1 entry:```\nI don't get a prompt back, and I can't execute anything. I can ctrl-C to get the prompt back, and the object was successfully created, and I can access fields, though that often freezes too. If I end the line with a `;` to suppress printing, it's no problem.\n\n```julia&gt; results[\"BlastOutput2\"]; # without semicolon, this freezes\n\njulia&gt; first(results[\"BlastOutput2\"]) # this works, but takes about 4 seconds to get the prompt back\nJSON3.Object{Base.CodeUnits{UInt8, String}, SubArray{UInt64, 1, Vector{UInt64}, Tuple{UnitRange{Int64}}, true}} with 1 entry:\n  :report =&gt; {…\n\njulia&gt;```","user":"U8JP5B9T2","ts":"1614107387.026500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xhp","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U681ELA87"},{"type":"text","text":" I'm running into something weird with "},{"type":"text","text":"JSON3.jl","style":{"code":true}},{"type":"text","text":" - I'm reading in a 258Mb JSON file, and `show`ing seems to freeze things:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> mystr = read(\"test.json\", String);\n\njulia> results = JSON3.read(mystr)\nJSON3.Object{Base.CodeUnits{UInt8, String}, Vector{UInt64}} with 1 entry:"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nI don't get a prompt back, and I can't execute anything. I can ctrl-C to get the prompt back, and the object was successfully created, and I can access fields, though that often freezes too. If I end the line with a "},{"type":"text","text":";","style":{"code":true}},{"type":"text","text":" to suppress printing, it's no problem.\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> results[\"BlastOutput2\"]; # without semicolon, this freezes\n\njulia> first(results[\"BlastOutput2\"]) # this works, but takes about 4 seconds to get the prompt back\nJSON3.Object{Base.CodeUnits{UInt8, String}, SubArray{UInt64, 1, Vector{UInt64}, Tuple{UnitRange{Int64}}, true}} with 1 entry:\n  :report => {…\n\njulia>"}]}]}],"thread_ts":"1614107387.026500","reply_count":3,"reply_users_count":2,"latest_reply":"1614107456.027500","reply_users":["U8JP5B9T2","U681ELA87"],"subscribed":false},{"client_msg_id":"48b50ea6-8334-47d4-a2b7-d6905a2201c9","type":"message","text":"Does CSV.jl allow for skipping of columns say I want to skip the first column  while reading in a file?","user":"UPH1M2MB2","ts":"1614173173.033900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"t4dmH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does CSV.jl allow for skipping of columns say I want to skip the first column  while reading in a file?"}]}]}],"thread_ts":"1614173173.033900","reply_count":6,"reply_users_count":3,"latest_reply":"1614175691.035600","reply_users":["U01C3624SGJ","UPH1M2MB2","U681ELA87"],"subscribed":false},{"client_msg_id":"c7ade1b9-52f9-4c02-ae49-68f21b881013","type":"message","text":"how do you programatically assign column names\n```cname = \"DOSE\"\nDataFrame(Symbol(cname) = 1)```\nI want the dataframe to have the column name that I stored in the variable cname.","user":"UAVMYR0F4","ts":"1614193842.037900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"I=XG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"how do you programatically assign column names\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"cname = \"DOSE\"\nDataFrame(Symbol(cname) = 1)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I want the dataframe to have the column name that I stored in the variable cname."}]}]}]},{"client_msg_id":"d466633d-2cdc-4303-8794-77028ededfb5","type":"message","text":"is there a function for in-place horizontal concatenation of DataFrames, or do I have to do it column by column?","user":"U9VG1AYSG","ts":"1614200795.040100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3XR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is there a function for in-place horizontal concatenation of DataFrames, or do I have to do it column by column?"}]}]}],"thread_ts":"1614200795.040100","reply_count":1,"reply_users_count":1,"latest_reply":"1614201025.040200","reply_users":["U8JAMQGQY"],"subscribed":false},{"client_msg_id":"b5ef132a-986a-45b9-86a5-8a248b64ce7b","type":"message","text":"Given the growing number of type-related issues w/ Arrow.jl, I’ve been doing some noodling on the best approach forward. The current system requires:\n• Registering your type(s) in order to serialize/deserialize (and _before_ you serialize/deserialize)\n• Not great support for anything outside a small set of “standard arrow” types, unless you custom register your own\n• Not very robust support for registering your own types even (like if physical structure changes between serial/deserial, parametric types, etc.)\n","user":"U681ELA87","ts":"1614227793.044900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"59y=I","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Given the growing number of type-related issues w/ Arrow.jl, I’ve been doing some noodling on the best approach forward. The current system requires:\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Registering your type(s) in order to serialize/deserialize (and "},{"type":"text","text":"before","style":{"italic":true}},{"type":"text","text":" you serialize/deserialize)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Not great support for anything outside a small set of “standard arrow” types, unless you custom register your own"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Not very robust support for registering your own types even (like if physical structure changes between serial/deserial, parametric types, etc.)"}]}],"style":"bullet","indent":0},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"af60689c-6284-425a-9387-64563eac78a5","type":"message","text":"My current thoughts are bouncing between two different ideas/options (that may or may not even actually work):\n1. Taking more of an approach like JSON3.jl, where serialization would rely directly on having a StructType/ArrowType defined, but deserializing would have two flavors: `Arrow.read(source)`, which would _only_ return the small set of standard arrow types, and `Arrow.read(source, T)`, where a custom type (or types) would be provided in order to “pull” the types out of the serialized arrow data. This seems to have provided a much more robust experience in JSON3; though users have to provide the types to be deserialized, it solves the “registering types” problem because the user is providing it directly. We can also more robustly deserialize, because again, it’s more in the user’s hands to provide the right types to expect when deserializing.\n2. The 2nd approach is a somewhat similar approach, but would push users towards the Strapping.jl package for custom types; I just finished some pretty decent upgrades to the package and I feel much more confident in its aim: mapping custom structs to 2D table columns. So in this scenario, Arrow.jl would continue more or less as-is; maybe clean up the ArrowTypes stuff and people can use if desired and works for them; otherwise, we’d ensure a convenient/robust integration w/ Strapping.","user":"U681ELA87","ts":"1614228176.051000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"N2yzd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"My current thoughts are bouncing between two different ideas/options (that may or may not even actually work):\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Taking more of an approach like JSON3.jl, where serialization would rely directly on having a StructType/ArrowType defined, but deserializing would have two flavors: "},{"type":"text","text":"Arrow.read(source)","style":{"code":true}},{"type":"text","text":", which would "},{"type":"text","text":"only","style":{"italic":true}},{"type":"text","text":" return the small set of standard arrow types, and "},{"type":"text","text":"Arrow.read(source, T)","style":{"code":true}},{"type":"text","text":", where a custom type (or types) would be provided in order to “pull” the types out of the serialized arrow data. This seems to have provided a much more robust experience in JSON3; though users have to provide the types to be deserialized, it solves the “registering types” problem because the user is providing it directly. We can also more robustly deserialize, because again, it’s more in the user’s hands to provide the right types to expect when deserializing."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"The 2nd approach is a somewhat similar approach, but would push users towards the Strapping.jl package for custom types; I just finished some pretty decent upgrades to the package and I feel much more confident in its aim: mapping custom structs to 2D table columns. So in this scenario, Arrow.jl would continue more or less as-is; maybe clean up the ArrowTypes stuff and people can use if desired and works for them; otherwise, we’d ensure a convenient/robust integration w/ Strapping."}]}],"style":"ordered","indent":0}]}]},{"client_msg_id":"be76c705-f30e-4921-97fd-a5d24a36bd0f","type":"message","text":"But I need to admit that this is all still kind of fuzzy in my head; it might be most productive to hear some of the use-cases people have where they’re running into the issues mentioned above and have a solution grow out of the best way forward given the issues. In particular, it’s not clear to me whether people are trying to save a `Vector{CustomStruct}` in arrow format, or if they have DataFrames that happen to have `Tuple{Int, String}` columns that are running into issues. If anyone has thoughts/ideas, I’d love to hear them. I’ll try to post more thoughts as I have them.","user":"U681ELA87","ts":"1614228333.053300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IFuJK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But I need to admit that this is all still kind of fuzzy in my head; it might be most productive to hear some of the use-cases people have where they’re running into the issues mentioned above and have a solution grow out of the best way forward given the issues. In particular, it’s not clear to me whether people are trying to save a "},{"type":"text","text":"Vector{CustomStruct}","style":{"code":true}},{"type":"text","text":" in arrow format, or if they have DataFrames that happen to have "},{"type":"text","text":"Tuple{Int, String}","style":{"code":true}},{"type":"text","text":" columns that are running into issues. If anyone has thoughts/ideas, I’d love to hear them. I’ll try to post more thoughts as I have them."}]}]}]},{"client_msg_id":"9b4cf79d-d2a8-4d9d-8ce3-842a3b976975","type":"message","text":"cc: <@U674T0Y9Z> <@U679T6QF7> <@U695B1S2X> <@UCZ7VBGUD> <@UM30MT6RF> <@U66M57AN4> ^^","user":"U681ELA87","ts":"1614228384.054000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kBZD6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"cc: "},{"type":"user","user_id":"U674T0Y9Z"},{"type":"text","text":" "},{"type":"user","user_id":"U679T6QF7"},{"type":"text","text":" "},{"type":"user","user_id":"U695B1S2X"},{"type":"text","text":" "},{"type":"user","user_id":"UCZ7VBGUD"},{"type":"text","text":" "},{"type":"user","user_id":"UM30MT6RF"},{"type":"text","text":" "},{"type":"user","user_id":"U66M57AN4"},{"type":"text","text":" ^^"}]}]}]},{"client_msg_id":"d9b442d3-9cef-4fcf-b3c9-6436e07df13a","type":"message","text":"<@U681ELA87> et al - semi-relatedly, I’ve been working on a type provider for JSON3 that might alleviate some of the need to provide your own types.  It’s still a bit rough and I’m not sure what the best integration points are with the workflow.  My current thought is a user could provide a sample JSON, get type definitions and either directly eval it (a bit risky with namespace clashes and maybe bringing in unneeded fields), or write the result to file then use that in their work.  Is this a useful flow?  Where would this best fit into a current JSON3 reading workflow?  <https://github.com/mcmcgrath13/JSONTypeProvider.jl>","user":"UC53031QU","ts":"1614261182.064000","team":"T68168MUP","edited":{"user":"UC53031QU","ts":"1614261427.000000"},"blocks":[{"type":"rich_text","block_id":"GYWu2","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U681ELA87"},{"type":"text","text":" et al - semi-relatedly, I’ve been working on a type provider for JSON3 that might alleviate some of the need to provide your own types.  It’s still a bit rough and I’m not sure what the best integration points are with the workflow.  My current thought is a user could provide a sample JSON, get type definitions and either directly eval it (a bit risky with namespace clashes and maybe bringing in unneeded fields), or write the result to file then use that in their work.  Is this a useful flow?  Where would this best fit into a current JSON3 reading workflow?  "},{"type":"link","url":"https://github.com/mcmcgrath13/JSONTypeProvider.jl"}]}]}]},{"client_msg_id":"2588e29a-6d6e-435e-a1ee-e606a373a5b9","type":"message","text":"Hey, I am working with DataFrames.jl and was wondering - is there a better/more efficient way of filtering DataFrame objects? Right now, whenever I need a sub-dataframe, I use `filter` with generally more than one condition in it but was wondering if this isn't the best way to do things. Could anyone give me some pointers/guidance on best practices with filtering dataframes in performant/effective ways? Thank you!","user":"US64J0NPQ","ts":"1614262767.066400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"h76/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey, I am working with DataFrames.jl and was wondering - is there a better/more efficient way of filtering DataFrame objects? Right now, whenever I need a sub-dataframe, I use "},{"type":"text","text":"filter","style":{"code":true}},{"type":"text","text":" with generally more than one condition in it but was wondering if this isn't the best way to do things. Could anyone give me some pointers/guidance on best practices with filtering dataframes in performant/effective ways? Thank you!"}]}]}],"thread_ts":"1614262767.066400","reply_count":8,"reply_users_count":2,"latest_reply":"1614265534.068200","reply_users":["UBF9YRB6H","US64J0NPQ"],"subscribed":false},{"client_msg_id":"e19373d0-b878-4f98-9c9d-283d173feefa","type":"message","text":"I have a file where I want to read in all the bytes, but after every 3rd byte I want to insert a `0` byte.\n\nThe bigger picture is that the three bytes contain two `UInt12`s which I want to unpack into `UInt16`s\n\nWhat's the fastest way to do this?","user":"USU9FRPEU","ts":"1614279772.076900","team":"T68168MUP","edited":{"user":"USU9FRPEU","ts":"1614279799.000000"},"blocks":[{"type":"rich_text","block_id":"DX2O+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a file where I want to read in all the bytes, but after every 3rd byte I want to insert a "},{"type":"text","text":"0","style":{"code":true}},{"type":"text","text":" byte.\n\nThe bigger picture is that the three bytes contain two `UInt12`s which I want to unpack into `UInt16`s\n\nWhat's the fastest way to do this?"}]}]}]},{"client_msg_id":"7ae01a07-5b8b-43a9-bb8a-de3764acb6e3","type":"message","text":"in DataFrames, why don’t `select` and `transform` “see” previously created columns within the same command? e.g.\n```julia&gt; using DataFrames\n\njulia&gt; df = DataFrame((;a = rand(10), b = rand(10)))\n10×2 DataFrame\n Row │ a           b\n     │ Float64     Float64\n─────┼────────────────────────\n   1 │ 0.73957     0.747759\n   2 │ 0.888763    0.317682\n   3 │ 0.138173    0.92976\n   4 │ 0.015185    0.752267\n   5 │ 0.434004    0.767345\n   6 │ 0.699355    0.566346\n   7 │ 0.0603922   0.264627\n   8 │ 0.702297    0.00609892\n   9 │ 0.854265    0.762492\n  10 │ 0.00221184  0.826804\n\njulia&gt; transform(df, [:a, :b] =&gt; ByRow(+) =&gt; :c, :c =&gt; ByRow(x -&gt; x^2) =&gt; :d)\nERROR: ArgumentError: column name :c not found in the data frame; existing most similar names are: :a and :b\nStacktrace:\n  [1] lookupname\n    @ ~/.julia/packages/DataFrames/oQ5c7/src/other/index.jl:291 [inlined]\n  [2] getindex\n    @ ~/.julia/packages/DataFrames/oQ5c7/src/other/index.jl:297 [inlined]\n  [3] normalize_selection(idx::DataFrames.Index, sel::Pair{Symbol, Pair{ByRow{var\"#1#2\"}, Symbol}}, renamecols::Bool)\n    @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:214\n  [4] (::DataFrames.var\"#395#396\"{Bool, DataFrame})(c::Pair{Symbol, Pair{ByRow{var\"#1#2\"}, Symbol}})\n    @ DataFrames ./none:0\n  [5] iterate\n    @ ./generator.jl:47 [inlined]\n  [6] collect_to!(dest::Vector{Any}, itr::Base.Generator{Vector{Any}, DataFrames.var\"#395#396\"{Bool, DataFrame}}, offs::Int64, st::Int64)\n    @ Base ./array.jl:724\n  [7] collect_to!(dest::Vector{Base.OneTo{Int64}}, itr::Base.Generator{Vector{Any}, DataFrames.var\"#395#396\"{Bool, DataFrame}}, offs::Int64, st::Int64)\n    @ Base ./array.jl:732\n  [8] collect_to_with_first!(dest::Vector{Base.OneTo{Int64}}, v1::Base.OneTo{Int64}, itr::Base.Generator{Vector{Any}, DataFrames.var\"#395#396\"{Bool, DataFrame}}, st::Int64)\n    @ Base ./array.jl:702\n  [9] collect(itr::Base.Generator{Vector{Any}, DataFrames.var\"#395#396\"{Bool, DataFrame}})\n    @ Base ./array.jl:683\n [10] manipulate(::DataFrame, ::Any, ::Vararg{Any, N} where N; copycols::Bool, keeprows::Bool, renamecols::Bool)\n    @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:1209\n [11] #select#384\n    @ ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:847 [inlined]\n [12] #transform#386\n    @ ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:913 [inlined]\n [13] transform(::DataFrame, ::Any, ::Any)\n    @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:913\n [14] top-level scope\n    @ REPL[14]:1```\nI looked around but didn’t see an issue. I figure there must be a reason, but I don’t see what it is.","user":"UCZ7VBGUD","ts":"1614353838.080200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"n0M","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"in DataFrames, why don’t "},{"type":"text","text":"select","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"transform","style":{"code":true}},{"type":"text","text":" “see” previously created columns within the same command? e.g.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> using DataFrames\n\njulia> df = DataFrame((;a = rand(10), b = rand(10)))\n10×2 DataFrame\n Row │ a           b\n     │ Float64     Float64\n─────┼────────────────────────\n   1 │ 0.73957     0.747759\n   2 │ 0.888763    0.317682\n   3 │ 0.138173    0.92976\n   4 │ 0.015185    0.752267\n   5 │ 0.434004    0.767345\n   6 │ 0.699355    0.566346\n   7 │ 0.0603922   0.264627\n   8 │ 0.702297    0.00609892\n   9 │ 0.854265    0.762492\n  10 │ 0.00221184  0.826804\n\njulia> transform(df, [:a, :b] => ByRow(+) => :c, :c => ByRow(x -> x^2) => :d)\nERROR: ArgumentError: column name :c not found in the data frame; existing most similar names are: :a and :b\nStacktrace:\n  [1] lookupname\n    @ ~/.julia/packages/DataFrames/oQ5c7/src/other/index.jl:291 [inlined]\n  [2] getindex\n    @ ~/.julia/packages/DataFrames/oQ5c7/src/other/index.jl:297 [inlined]\n  [3] normalize_selection(idx::DataFrames.Index, sel::Pair{Symbol, Pair{ByRow{var\"#1#2\"}, Symbol}}, renamecols::Bool)\n    @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:214\n  [4] (::DataFrames.var\"#395#396\"{Bool, DataFrame})(c::Pair{Symbol, Pair{ByRow{var\"#1#2\"}, Symbol}})\n    @ DataFrames ./none:0\n  [5] iterate\n    @ ./generator.jl:47 [inlined]\n  [6] collect_to!(dest::Vector{Any}, itr::Base.Generator{Vector{Any}, DataFrames.var\"#395#396\"{Bool, DataFrame}}, offs::Int64, st::Int64)\n    @ Base ./array.jl:724\n  [7] collect_to!(dest::Vector{Base.OneTo{Int64}}, itr::Base.Generator{Vector{Any}, DataFrames.var\"#395#396\"{Bool, DataFrame}}, offs::Int64, st::Int64)\n    @ Base ./array.jl:732\n  [8] collect_to_with_first!(dest::Vector{Base.OneTo{Int64}}, v1::Base.OneTo{Int64}, itr::Base.Generator{Vector{Any}, DataFrames.var\"#395#396\"{Bool, DataFrame}}, st::Int64)\n    @ Base ./array.jl:702\n  [9] collect(itr::Base.Generator{Vector{Any}, DataFrames.var\"#395#396\"{Bool, DataFrame}})\n    @ Base ./array.jl:683\n [10] manipulate(::DataFrame, ::Any, ::Vararg{Any, N} where N; copycols::Bool, keeprows::Bool, renamecols::Bool)\n    @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:1209\n [11] #select#384\n    @ ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:847 [inlined]\n [12] #transform#386\n    @ ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:913 [inlined]\n [13] transform(::DataFrame, ::Any, ::Any)\n    @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:913\n [14] top-level scope\n    @ REPL[14]:1"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I looked around but didn’t see an issue. I figure there must be a reason, but I don’t see what it is."}]}]}],"thread_ts":"1614353838.080200","reply_count":5,"reply_users_count":2,"latest_reply":"1614355352.082700","reply_users":["UBF9YRB6H","UCZ7VBGUD"],"subscribed":false},{"client_msg_id":"33d65f5b-1a84-4230-b75a-1485fcddc8fa","type":"message","text":"Hi everyone\nHow can i go about converting a string time into a time format ,the format is this way  `11:54AM`\ntried this but does not work `convert.(Time,myCol)` but definitely not correct.","user":"UPH1M2MB2","ts":"1614427948.096100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Is/VS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi everyone\nHow can i go about converting a string time into a time format ,the format is this way  "},{"type":"text","text":"11:54AM","style":{"code":true}},{"type":"text","text":"\ntried this but does not work "},{"type":"text","text":"convert.(Time,myCol)","style":{"code":true}},{"type":"text","text":" but definitely not correct."}]}]}]},{"client_msg_id":"525eba86-648b-4d7c-8f31-d2ccfaeb00c1","type":"message","text":"Hi everyone, can someone explain the error in this code? I want to have multiple output columns as opposed to a tuple:\n```transform(df, [:xstart, :xend, :ystart, :yend] =&gt; ByRow(magnitude_angle) =&gt; AsTable)\n\nArgumentError: Unrecognized column selector: [:xstart, :xend, :ystart, :yend] =&gt; (ByRow{typeof(magnitude_angle)}(magnitude_angle) =&gt; AsTable)```","user":"U01C2E6TYEM","ts":"1614436521.099500","team":"T68168MUP","edited":{"user":"U01C2E6TYEM","ts":"1614436692.000000"},"blocks":[{"type":"rich_text","block_id":"B7R5s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi everyone, can someone explain the error in this code? I want to have multiple output columns as opposed to a tuple:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"transform(df, [:xstart, :xend, :ystart, :yend] => ByRow(magnitude_angle) => AsTable)\n\nArgumentError: Unrecognized column selector: [:xstart, :xend, :ystart, :yend] => (ByRow{typeof(magnitude_angle)}(magnitude_angle) => AsTable)"}]}]}]},{"type":"message","subtype":"thread_broadcast","text":"I have decided to write a post about checking versions as this is a most common issue people report. You can find it here <https://bkamins.github.io/julialang/2021/02/27/pkg_version.html>","user":"U8JAMQGQY","ts":"1614453057.101100","thread_ts":"1614436521.099500","root":{"client_msg_id":"525eba86-648b-4d7c-8f31-d2ccfaeb00c1","type":"message","text":"Hi everyone, can someone explain the error in this code? I want to have multiple output columns as opposed to a tuple:\n```transform(df, [:xstart, :xend, :ystart, :yend] =&gt; ByRow(magnitude_angle) =&gt; AsTable)\n\nArgumentError: Unrecognized column selector: [:xstart, :xend, :ystart, :yend] =&gt; (ByRow{typeof(magnitude_angle)}(magnitude_angle) =&gt; AsTable)```","user":"U01C2E6TYEM","ts":"1614436521.099500","team":"T68168MUP","edited":{"user":"U01C2E6TYEM","ts":"1614436692.000000"},"blocks":[{"type":"rich_text","block_id":"B7R5s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi everyone, can someone explain the error in this code? I want to have multiple output columns as opposed to a tuple:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"transform(df, [:xstart, :xend, :ystart, :yend] => ByRow(magnitude_angle) => AsTable)\n\nArgumentError: Unrecognized column selector: [:xstart, :xend, :ystart, :yend] => (ByRow{typeof(magnitude_angle)}(magnitude_angle) => AsTable)"}]}]}],"thread_ts":"1614436521.099500","reply_count":4,"reply_users_count":2,"latest_reply":"1614453057.101100","reply_users":["U8JAMQGQY","U01C2E6TYEM"],"subscribed":false},"attachments":[{"service_name":"Blog by Bogumił Kamiński","title":"How to check the version of a package?","title_link":"https://bkamins.github.io/julialang/2021/02/27/pkg_version.html","text":"Introduction","fallback":"Blog by Bogumił Kamiński: How to check the version of a package?","ts":1614431662,"from_url":"https://bkamins.github.io/julialang/2021/02/27/pkg_version.html","service_icon":"https://bkamins.github.io/favicon.ico","id":1,"original_url":"https://bkamins.github.io/julialang/2021/02/27/pkg_version.html"}],"blocks":[{"type":"rich_text","block_id":"uXF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have decided to write a post about checking versions as this is a most common issue people report. You can find it here "},{"type":"link","url":"https://bkamins.github.io/julialang/2021/02/27/pkg_version.html"}]}]}],"client_msg_id":"3dce87b7-562d-468b-9e81-d76402a3d276"},{"client_msg_id":"f7191144-2abd-49a9-941e-37181d1a7627","type":"message","text":"Hey, does anyone have any idea how to measure interactions between regression variables? I can't seem to find the functionality in e.g. GLM.jl, and I'm willing to write some code to calculate it, but I can't find any formula or anything","user":"U01PJ63E11N","ts":"1614484388.103500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PUhw4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey, does anyone have any idea how to measure interactions between regression variables? I can't seem to find the functionality in e.g. GLM.jl, and I'm willing to write some code to calculate it, but I can't find any formula or anything"}]}]}],"thread_ts":"1614484388.103500","reply_count":1,"reply_users_count":1,"latest_reply":"1614484590.103600","reply_users":["U01PJ63E11N"],"subscribed":false},{"client_msg_id":"579280a8-f860-41cf-beec-deca9a066441","type":"message","text":"when making a DataFrame from a vector of JSON objects, is it possible to NOT include a column? (current approach is drop! it after the fact","user":"UH8A351DJ","ts":"1614498908.105000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kWv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"when making a DataFrame from a vector of JSON objects, is it possible to NOT include a column? (current approach is drop! it after the fact"}]}]}]},{"client_msg_id":"4454cc48-3118-4fc2-b527-f1b3f9841df4","type":"message","text":"Hi guys,\n\nAny insight on this error? Is this due to weird thread issue (I'm using Julia 1.3.1).\n```[20:15:28] ERROR: LoadError: TaskFailedException:\n[20:15:28] IOError: close: i/o error (EIO)\n[20:15:28] Stacktrace:\n[20:15:28]  [1] uv_error at ./libuv.jl:97 [inlined]\n[20:15:28]  [2] close at ./filesystem.jl:107 [inlined]\n[20:15:28]  [3] sendfile(::String, ::String) at ./file.jl:814\n[20:15:28]  [4] #cp#12(::Bool, ::Bool, ::typeof(cp), ::String, ::String) at ./file.jl:344\n[20:15:28]  [5] (::Base.Filesystem.var\"#kw##cp\")(::NamedTuple{(:force,),Tuple{Bool}}, ::typeof(cp), ::String, ::String) at ./none:0\n[20:15:28]  [6] macro expansion at /home/darren/project/MRVerify/src/MRVerify.jl:856 [inlined]\n[20:15:28]  [7] (::Main.MRVerify.var\"#232#threadsfor_fun#44\"{NamedTuple{(:ses_dir, :config, :out_path, :sub, :ses, :ses_report_path, :labnotes_path, :issue_path, :anon_rda, :overwrite),Tuple{String,Dict{String,Any},String,String,String,Nothing,Nothing,Nothing,Bool,Bool}},Array{Tuple{Int64,Pair{Any,Any}},1}})(::Bool) at ./threadingconstructs.jl:61\n[20:15:28]  [8] (::Main.MRVerify.var\"#232#threadsfor_fun#44\"{NamedTuple{(:ses_dir, :config, :out_path, :sub, :ses, :ses_report_path, :labnotes_path, :issue_path, :anon_rda, :overwrite),Tuple{String,Dict{String,Any},String,String,String,Nothing,Nothing,Nothing,Bool,Bool}},Array{Tuple{Int64,Pair{Any,Any}},1}})() at ./threadingconstructs.jl:28\n[20:15:29] Stacktrace:\n[20:15:29]  [1] wait(::Task) at ./task.jl:251\n[20:15:29]  [2] macro expansion at ./threadingconstructs.jl:69 [inlined]\n[20:15:29]  [3] #process_session_data#15(::String, ::String, ::String, ::Nothing, ::Nothing, ::Nothing, ::Bool, ::Bool, ::typeof(Main.MRVerify.process_session_data), ::String, ::Dict{String,Any}) at /home/darren/project/MRVerify/src/MRVerify.jl:852\n[20:15:29]  [4] (::Main.MRVerify.var\"#kw##process_session_data\")(::NamedTuple{(:out_path, :sub, :ses, :issue_path, :ses_report_path, :labnotes_path, :anon_rda, :overwrite),Tuple{String,String,String,Nothing,Nothing,Nothing,Bool,Bool}}, ::typeof(Main.MRVerify.process_session_data), ::String, ::Dict{String,Any}) at ./none:0\n[20:15:29]  [5] top-level scope at /home/darren/project/MRVerify/src/MRVerify.jl:1329\n[20:15:29]  [6] include at ./boot.jl:328 [inlined]\n[20:15:29]  [7] include_relative(::Module, ::String) at ./loading.jl:1105\n[20:15:29]  [8] include(::Module, ::String) at ./Base.jl:31\n[20:15:29]  [9] exec_options(::Base.JLOptions) at ./client.jl:287\n[20:15:29]  [10] _start() at ./client.jl:460\n[20:15:29] in expression starting at /home/darren/project/MRVerify/src/MRVerify.jl:1326```\nMy code in Line 852 is the `Threads.@threads` , i.e.\n```Threads.@threads for (idx, (k, v)) in collect(enumerate(twix_result))```\nAny help would be appreciated!","user":"UUT4VGTE2","ts":"1614524183.106600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"q+2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi guys,\n\nAny insight on this error? Is this due to weird thread issue (I'm using Julia 1.3.1).\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"[20:15:28] ERROR: LoadError: TaskFailedException:\n[20:15:28] IOError: close: i/o error (EIO)\n[20:15:28] Stacktrace:\n[20:15:28]  [1] uv_error at ./libuv.jl:97 [inlined]\n[20:15:28]  [2] close at ./filesystem.jl:107 [inlined]\n[20:15:28]  [3] sendfile(::String, ::String) at ./file.jl:814\n[20:15:28]  [4] #cp#12(::Bool, ::Bool, ::typeof(cp), ::String, ::String) at ./file.jl:344\n[20:15:28]  [5] (::Base.Filesystem.var\"#kw##cp\")(::NamedTuple{(:force,),Tuple{Bool}}, ::typeof(cp), ::String, ::String) at ./none:0\n[20:15:28]  [6] macro expansion at /home/darren/project/MRVerify/src/MRVerify.jl:856 [inlined]\n[20:15:28]  [7] (::Main.MRVerify.var\"#232#threadsfor_fun#44\"{NamedTuple{(:ses_dir, :config, :out_path, :sub, :ses, :ses_report_path, :labnotes_path, :issue_path, :anon_rda, :overwrite),Tuple{String,Dict{String,Any},String,String,String,Nothing,Nothing,Nothing,Bool,Bool}},Array{Tuple{Int64,Pair{Any,Any}},1}})(::Bool) at ./threadingconstructs.jl:61\n[20:15:28]  [8] (::Main.MRVerify.var\"#232#threadsfor_fun#44\"{NamedTuple{(:ses_dir, :config, :out_path, :sub, :ses, :ses_report_path, :labnotes_path, :issue_path, :anon_rda, :overwrite),Tuple{String,Dict{String,Any},String,String,String,Nothing,Nothing,Nothing,Bool,Bool}},Array{Tuple{Int64,Pair{Any,Any}},1}})() at ./threadingconstructs.jl:28\n[20:15:29] Stacktrace:\n[20:15:29]  [1] wait(::Task) at ./task.jl:251\n[20:15:29]  [2] macro expansion at ./threadingconstructs.jl:69 [inlined]\n[20:15:29]  [3] #process_session_data#15(::String, ::String, ::String, ::Nothing, ::Nothing, ::Nothing, ::Bool, ::Bool, ::typeof(Main.MRVerify.process_session_data), ::String, ::Dict{String,Any}) at /home/darren/project/MRVerify/src/MRVerify.jl:852\n[20:15:29]  [4] (::Main.MRVerify.var\"#kw##process_session_data\")(::NamedTuple{(:out_path, :sub, :ses, :issue_path, :ses_report_path, :labnotes_path, :anon_rda, :overwrite),Tuple{String,String,String,Nothing,Nothing,Nothing,Bool,Bool}}, ::typeof(Main.MRVerify.process_session_data), ::String, ::Dict{String,Any}) at ./none:0\n[20:15:29]  [5] top-level scope at /home/darren/project/MRVerify/src/MRVerify.jl:1329\n[20:15:29]  [6] include at ./boot.jl:328 [inlined]\n[20:15:29]  [7] include_relative(::Module, ::String) at ./loading.jl:1105\n[20:15:29]  [8] include(::Module, ::String) at ./Base.jl:31\n[20:15:29]  [9] exec_options(::Base.JLOptions) at ./client.jl:287\n[20:15:29]  [10] _start() at ./client.jl:460\n[20:15:29] in expression starting at /home/darren/project/MRVerify/src/MRVerify.jl:1326"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"My code in Line 852 is the "},{"type":"text","text":"Threads.@threads","style":{"code":true}},{"type":"text","text":" , i.e.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Threads.@threads for (idx, (k, v)) in collect(enumerate(twix_result))"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Any help would be appreciated!"}]}]}]},{"client_msg_id":"d36a934f-b4a3-49c3-b474-f6f1136c651a","type":"message","text":"can I select columns by eltype?","user":"U01ARRMLM7E","ts":"1614544183.109600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"S3U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"can I select columns by eltype?"}]}]}],"thread_ts":"1614544183.109600","reply_count":1,"reply_users_count":1,"latest_reply":"1614545542.112300","reply_users":["U011QC7QLPL"],"subscribed":false},{"client_msg_id":"bd1ca99a-71e9-4d97-8e45-854e07bc1d9e","type":"message","text":"I've that idealistic thought in my mind, about streamlining ProtoBuf.jl and StructTypes.jl such that the former is generating code from proto files that matches the API of the latter. But for that I rn am not entirely sure I understood it correctly. StructTypes is a valid API for both, binary as well as textual de/serialization, isn't it?","user":"UMWFZF5DW","ts":"1614544751.112000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"e0K66","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I've that idealistic thought in my mind, about streamlining ProtoBuf.jl and StructTypes.jl such that the former is generating code from proto files that matches the API of the latter. But for that I rn am not entirely sure I understood it correctly. StructTypes is a valid API for both, binary as well as textual de/serialization, isn't it?"}]}]}]},{"client_msg_id":"ccd048aa-a288-4948-bea2-8cc061df944d","type":"message","text":"We could probably have `describe` on tables be it's own package that acts on tables and returns a named tuple of vectors. and then we override it for `DataFrame`s to return a DataFrame. It's a self-contained function for the most part.","user":"UBF9YRB6H","ts":"1614548440.115100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sXhi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We could probably have "},{"type":"text","text":"describe","style":{"code":true}},{"type":"text","text":" on tables be it's own package that acts on tables and returns a named tuple of vectors. and then we override it for `DataFrame`s to return a DataFrame. It's a self-contained function for the most part."}]}]}]},{"client_msg_id":"6c86d8b1-c334-4cc9-ade6-00ed68f4603f","type":"message","text":"what is the function to call on a `DataFrame` to say how big the `CSV.write` will be?","user":"UM8JUNJG7","ts":"1614549529.115900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rz4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"what is the function to call on a "},{"type":"text","text":"DataFrame","style":{"code":true}},{"type":"text","text":" to say how big the "},{"type":"text","text":"CSV.write","style":{"code":true}},{"type":"text","text":" will be?"}]}]}],"thread_ts":"1614549529.115900","reply_count":7,"reply_users_count":3,"latest_reply":"1614550189.117400","reply_users":["UBF9YRB6H","UM8JUNJG7","U01ARRMLM7E"],"subscribed":false},{"client_msg_id":"ee62cb5a-9fbc-4646-ae6e-f82f8eaa4fe0","type":"message","text":"is there a version of `lag` from ShiftedArrays that doesn't include the `missing`s? Just shortens the array","user":"UBF9YRB6H","ts":"1614552197.119400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Lgv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is there a version of "},{"type":"text","text":"lag","style":{"code":true}},{"type":"text","text":" from ShiftedArrays that doesn't include the `missing`s? Just shortens the array"}]}]}]},{"client_msg_id":"efb4739c-95b9-48ee-b779-0ad087e0727a","type":"message","text":"Does anyone know the situation with JSONTables. It worked when I used it about a month ago, but running the same program after an update fails. in expression starting at /Users/impero/.julia/packages/JSONTables/g5bSA/src/JSONTables.jl:3 it loads JSON3, but then: WARNING: could not import StructTypes.OrderedStruct into JSON3\nERROR: LoadError: LoadError: UndefVarError: OrderedStruct not defined","user":"U01GFAJRZ44","ts":"1614617954.140200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JkOfQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does anyone know the situation with JSONTables. It worked when I used it about a month ago, but running the same program after an update fails. in expression starting at /Users/impero/.julia/packages/JSONTables/g5bSA/src/JSONTables.jl:3 it loads JSON3, but then: WARNING: could not import StructTypes.OrderedStruct into JSON3\nERROR: LoadError: LoadError: UndefVarError: OrderedStruct not defined"}]}]}]},{"client_msg_id":"b08aef7f-1a17-462d-aec1-7f408d6fc2bc","type":"message","text":"I have got around this for now by removing JSON3 and loading JSON3@1.5.1. 1.7.1 seems to fail","user":"U01GFAJRZ44","ts":"1614618262.140800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"53vZ2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have got around this for now by removing JSON3 and loading JSON3@1.5.1. 1.7.1 seems to fail"}]}]}]},{"client_msg_id":"c4074f43-4358-4e53-824c-d78545ebe73b","type":"message","text":"Sounds like your versions are getting mismatched? JSON3 v1.7+ requires StructTypes v1.4+. If you somehow updated JSON3 without updating StructTypes (which your error makes it sound like), then things would get out of sync","user":"U681ELA87","ts":"1614618530.141900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"m5U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Sounds like your versions are getting mismatched? JSON3 v1.7+ requires StructTypes v1.4+. If you somehow updated JSON3 without updating StructTypes (which your error makes it sound like), then things would get out of sync"}]}]}]},{"client_msg_id":"393e9fbe-b581-45a1-b3d8-54d1c9f24b68","type":"message","text":"JSONTables.jl just depends on JSON3 “1” and StructTypes “1\", so it’s not imposing any hard requirements","user":"U681ELA87","ts":"1614618577.142400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jbA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"JSONTables.jl just depends on JSON3 “1” and StructTypes “1\", so it’s not imposing any hard requirements"}]}]}]},{"client_msg_id":"7e3a2579-b423-4a02-82af-90cbb2e79867","type":"message","text":"StructTypes.= 1.4.0. I just did an ‘update’ in the package manager, and got JSON3 1.7.1 (was 1.5.1) and StructTypes 1.4.0.","user":"U01GFAJRZ44","ts":"1614618648.143200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+Dmia","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"StructTypes.= 1.4.0. I just did an ‘update’ in the package manager, and got JSON3 1.7.1 (was 1.5.1) and StructTypes 1.4.0."}]}]}]},{"client_msg_id":"755faa4c-6faa-46fb-b7f3-84a89463f07c","type":"message","text":"so if you restart Julia and do `using JSON3`, what happens?","user":"U681ELA87","ts":"1614618989.143600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pvDtp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"so if you restart Julia and do "},{"type":"text","text":"using JSON3","style":{"code":true}},{"type":"text","text":", what happens?"}]}]}]},{"client_msg_id":"08975616-7976-46e2-bef5-2874dd8c1ea3","type":"message","text":"(and are you using a plain terminal REPL or some other Juno/Atom/VSCode/Jupyter/IJulia setup?)","user":"U681ELA87","ts":"1614619019.144200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JndDe","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(and are you using a plain terminal REPL or some other Juno/Atom/VSCode/Jupyter/IJulia setup?)"}]}]}]},{"client_msg_id":"50ca2c18-b93b-4543-983c-12eae6e32a83","type":"message","text":"Can I initialize a `CategoricalArray{String}` with levels from `Dict{String,Int}` and an array of encoded values `Vector{Int}`, without creating array of strings?","user":"UB2QSHWPN","ts":"1614620425.146100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TcHd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can I initialize a "},{"type":"text","text":"CategoricalArray{String}","style":{"code":true}},{"type":"text","text":" with levels from "},{"type":"text","text":"Dict{String,Int}","style":{"code":true}},{"type":"text","text":" and an array of encoded values "},{"type":"text","text":"Vector{Int}","style":{"code":true}},{"type":"text","text":", without creating array of strings?"}]}]}],"thread_ts":"1614620425.146100","reply_count":1,"reply_users_count":1,"latest_reply":"1614620528.146200","reply_users":["U67431ELR"],"subscribed":false},{"client_msg_id":"9c15a255-685e-4603-8b21-506757ddab2b","type":"message","text":"I'm confused; is the DataFrames syntax `df.a = v` deprecated or not?  becuase I could have sworn it was but I don't seem to be getting deprecation warnings anymore","user":"U9VG1AYSG","ts":"1614623444.147100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yHAx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm confused; is the DataFrames syntax "},{"type":"text","text":"df.a = v","style":{"code":true}},{"type":"text","text":" deprecated or not?  becuase I could have sworn it was but I don't seem to be getting deprecation warnings anymore"}]}]}],"thread_ts":"1614623444.147100","reply_count":4,"reply_users_count":3,"latest_reply":"1614624697.148000","reply_users":["U8JAMQGQY","U6A936746","U9VG1AYSG"],"subscribed":false},{"client_msg_id":"86e6fdea-468d-45eb-b816-f910049f34d7","type":"message","text":"I have a 3x76 Array{Any,2}. Row1 is string, rest are float64. How can I sort it according to ascending order of row2?","user":"U01NX6V5A2D","ts":"1614674539.155300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GpOT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a 3x76 Array{Any,2}. Row1 is string, rest are float64. How can I sort it according to ascending order of row2?"}]}]}],"thread_ts":"1614674539.155300","reply_count":3,"reply_users_count":2,"latest_reply":"1614675907.155800","reply_users":["U01NX6V5A2D","U6A936746"],"subscribed":false},{"client_msg_id":"0575e023-7aed-46b2-969c-67b502fbaba9","type":"message","text":"what's the best way to keep *all* columns in the result of a `combine`?  I'm surprised there seems to be no keyword argument for this","user":"U9VG1AYSG","ts":"1614699376.161300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"A7U1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"what's the best way to keep "},{"type":"text","text":"all","style":{"bold":true}},{"type":"text","text":" columns in the result of a "},{"type":"text","text":"combine","style":{"code":true}},{"type":"text","text":"?  I'm surprised there seems to be no keyword argument for this"}]}]}],"thread_ts":"1614699376.161300","reply_count":5,"reply_users_count":3,"latest_reply":"1614699731.162200","reply_users":["U67431ELR","U8JAMQGQY","U9VG1AYSG"],"subscribed":false},{"client_msg_id":"0e37cc79-a383-452c-bb34-1bfb2ba9dec7","type":"message","text":"thank you <@U8JAMQGQY> and <@U67431ELR> for putting me right on using `DataFrames: transform, transform!`; somehow I had all but forgotten about their existence.  One thing that I sometimes find myself needing to do that these functions make very easy (and seemingly even very efficient?) is adding a column to a dataframe while it's split into groups.  Giant pain in the ass without `transform!`","user":"U9VG1AYSG","ts":"1614711247.164300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"utP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"thank you "},{"type":"user","user_id":"U8JAMQGQY"},{"type":"text","text":" and "},{"type":"user","user_id":"U67431ELR"},{"type":"text","text":" for putting me right on using "},{"type":"text","text":"DataFrames: transform, transform!","style":{"code":true}},{"type":"text","text":"; somehow I had all but forgotten about their existence.  One thing that I sometimes find myself needing to do that these functions make very easy (and seemingly even very efficient?) is adding a column to a dataframe while it's split into groups.  Giant pain in the ass without "},{"type":"text","text":"transform!","style":{"code":true}}]}]}]},{"client_msg_id":"272b8303-8e0d-4120-9124-868ea94d856a","type":"message","text":"overall the DataFrames API is so wonderful, you guys did a really amazing job carefully thinking it through","user":"U9VG1AYSG","ts":"1614711309.164800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=gN8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"overall the DataFrames API is so wonderful, you guys did a really amazing job carefully thinking it through"}]}]}],"reactions":[{"name":"heart","users":["U681ELA87","U8JAMQGQY"],"count":2}]},{"client_msg_id":"e9ae3e13-3df4-49be-bafe-f3acd0275895","type":"message","text":"really, look at this:\n```transform!(groupby(df, :A), :A=&gt;(x -&gt; (D=x .+ 1, E=x .+ 2))=&gt;[:D, :E])```\nit's just so cool that that's trivial to do","user":"U9VG1AYSG","ts":"1614711546.165300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8z=Eb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"really, look at this:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"transform!(groupby(df, :A), :A=>(x -> (D=x .+ 1, E=x .+ 2))=>[:D, :E])"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"it's just so cool that that's trivial to do"}]}]}]},{"client_msg_id":"91b71397-80eb-4859-b765-babbde3a3304","type":"message","text":"(btw, is there a way to do that without returning a `NamedTuple` from the intermediate function? it seems redundant because it already tells it what the column names are)","user":"U9VG1AYSG","ts":"1614711575.165900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pXOfS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(btw, is there a way to do that without returning a "},{"type":"text","text":"NamedTuple","style":{"code":true}},{"type":"text","text":" from the intermediate function? it seems redundant because it already tells it what the column names are)"}]}]}]},{"client_msg_id":"85c5e84b-5a68-4bb8-ae11-433fa7e1bad4","type":"message","text":"```transform!(groupby(df, :A), :A=&gt; ByRow(x -&gt; x .+ (1,2)) =&gt; [:D, :E])```\nshould work and be fast (I am writing it from my head so please check :smile:)","user":"U8JAMQGQY","ts":"1614711947.167200","team":"T68168MUP","edited":{"user":"U8JAMQGQY","ts":"1614712020.000000"},"blocks":[{"type":"rich_text","block_id":"Lbxn","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"transform!(groupby(df, :A), :A=> ByRow(x -> x .+ (1,2)) => [:D, :E])"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"should work and be fast (I am writing it from my head so please check "},{"type":"emoji","name":"smile"},{"type":"text","text":")"}]}]}]},{"client_msg_id":"9cb79740-b3eb-43e5-a46f-cdf8e411fd12","type":"message","text":"wait what about `=&gt; AsTable`?","user":"UBF9YRB6H","ts":"1614712001.167500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hVp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"wait what about "},{"type":"text","text":"=> AsTable","style":{"code":true}},{"type":"text","text":"?"}]}]}],"reactions":[{"name":"point_up","users":["U9VG1AYSG"],"count":1}]},{"client_msg_id":"82ba6a7c-f068-4bcd-ac07-87152947d7da","type":"message","text":"<@U9VG1AYSG> did not want to return a `NamedTuple`","user":"U8JAMQGQY","ts":"1614712032.168200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"z7n","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U9VG1AYSG"},{"type":"text","text":" did not want to return a "},{"type":"text","text":"NamedTuple","style":{"code":true}}]}]}]},{"client_msg_id":"d613be86-21de-4bc7-a362-48f114ec2c3c","type":"message","text":"thanks, I think the `AsTable` is what I was looking for...","user":"U9VG1AYSG","ts":"1614712043.168500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9q/DV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"thanks, I think the "},{"type":"text","text":"AsTable","style":{"code":true}},{"type":"text","text":" is what I was looking for..."}]}]}]},{"client_msg_id":"a88b38e9-9edf-4961-bcaa-8d35dd74f2d9","type":"message","text":"well, what I relaly wanted was to elminate the redundancy of specifying table columns in two separate places, that's all","user":"U9VG1AYSG","ts":"1614712061.169000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"cfb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"well, what I relaly wanted was to elminate the redundancy of specifying table columns in two separate places, that's all"}]}]}]},{"client_msg_id":"53ea8e1b-f950-4a65-a54a-1b0337cf5688","type":"message","text":"so I think dumping to `AsTable` is the easiest way to do that","user":"U9VG1AYSG","ts":"1614712074.169600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FtI9L","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"so I think dumping to "},{"type":"text","text":"AsTable","style":{"code":true}},{"type":"text","text":" is the easiest way to do that"}]}]}]},{"client_msg_id":"c254830a-326a-46b8-8e1f-17b625c6b312","type":"message","text":"So as you can see - you can do it in two ways :slightly_smiling_face:.","user":"U8JAMQGQY","ts":"1614712078.169700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0CDyu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So as you can see - you can do it in two ways "},{"type":"emoji","name":"slightly_smiling_face"},{"type":"text","text":"."}]}]}],"reactions":[{"name":"+1","users":["U9VG1AYSG"],"count":1}]},{"client_msg_id":"35aa4ec7-9d80-49e8-b50d-8f31e15e51b3","type":"message","text":"it might be good if we add a section to the docs including lots of examples on the really \"fancy\" stuff one can do with the column pair syntax.  It wasn't at all obvious to me that you can do `cols=&gt;f=&gt;AsTable`","user":"U9VG1AYSG","ts":"1614712182.170900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"45dTK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it might be good if we add a section to the docs including lots of examples on the really \"fancy\" stuff one can do with the column pair syntax.  It wasn't at all obvious to me that you can do "},{"type":"text","text":"cols=>f=>AsTable","style":{"code":true}}]}]}]},{"client_msg_id":"9cbf5403-39c2-4431-92df-6e064f0007a2","type":"message","text":"in particular, it might be really nice to have a section on the underlying magic that makes the pairs syntax work.  Right now it seems very magical to me, but I assume it's all built around a few core, primitive functions.  Understanding what those are I think would be very enlightening","user":"U9VG1AYSG","ts":"1614712356.172000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/Ur","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"in particular, it might be really nice to have a section on the underlying magic that makes the pairs syntax work.  Right now it seems very magical to me, but I assume it's all built around a few core, primitive functions.  Understanding what those are I think would be very enlightening"}]}]}]},{"client_msg_id":"e19e3b7a-fb06-41ce-91f3-c591cbacbbb1","type":"message","text":"Those would be 2 different piece of documentation.\nUnderstanding based Explination for the later.\nand either learning based tutorials, or goal based guides for the former.\nI agree though. both would be valuable\n(4 kinds of docs <https://documentation.divio.com/introduction/>)","user":"U6A936746","ts":"1614712506.173600","team":"T68168MUP","edited":{"user":"U6A936746","ts":"1614712514.000000"},"blocks":[{"type":"rich_text","block_id":"S9ZC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Those would be 2 different piece of documentation.\nUnderstanding based Explination for the later.\nand either learning based tutorials, or goal based guides for the former.\nI agree though. both would be valuable\n(4 kinds of docs "},{"type":"link","url":"https://documentation.divio.com/introduction/"},{"type":"text","text":")"}]}]}]},{"client_msg_id":"69f67d7e-7ebc-4e8a-bf67-60c884ead749","type":"message","text":"<@U9VG1AYSG> there's this blog-post that has some of that: <https://bkamins.github.io/julialang/2020/12/24/minilanguage.html>","user":"U8JP5B9T2","ts":"1614713663.174400","team":"T68168MUP","attachments":[{"service_name":"Blog by Bogumił Kamiński","title":"DataFrames.jl minilanguage explained","title_link":"https://bkamins.github.io/julialang/2020/12/24/minilanguage.html","text":"Introduction","fallback":"Blog by Bogumił Kamiński: DataFrames.jl minilanguage explained","ts":1608785495,"from_url":"https://bkamins.github.io/julialang/2020/12/24/minilanguage.html","service_icon":"https://bkamins.github.io/favicon.ico","id":1,"original_url":"https://bkamins.github.io/julialang/2020/12/24/minilanguage.html"}],"blocks":[{"type":"rich_text","block_id":"IoLg","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U9VG1AYSG"},{"type":"text","text":" there's this blog-post that has some of that: "},{"type":"link","url":"https://bkamins.github.io/julialang/2020/12/24/minilanguage.html"}]}]}]},{"client_msg_id":"bc66637f-17d9-4263-9f37-c3b42d1d0df8","type":"message","text":"<@UL1475QDN> will soon work on a new version of DataFrames.jl documentation. However, I think we need to start with learning based tutorial part (as it is very hard to do all four kinds of docs in one shot).","user":"U8JAMQGQY","ts":"1614714018.175800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vcjc","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UL1475QDN"},{"type":"text","text":" will soon work on a new version of DataFrames.jl documentation. However, I think we need to start with learning based tutorial part (as it is very hard to do all four kinds of docs in one shot)."}]}]}]},{"client_msg_id":"4d2794e7-e24a-4a49-9c06-5b0d5fcc584b","type":"message","text":"came up on twitter that R `read_csv` can introduce missing values if extreme values of a numeric column are introduced deep in the file.  this repo has a reproducible example (I haven't checked myself but I trust the referrer...) <https://github.com/ebergelson/read_csv_issue_quickeg>  might be useful to add to our CSV tests/benchmarking","user":"U66M57AN4","ts":"1614738858.177500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0Zf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"came up on twitter that R "},{"type":"text","text":"read_csv","style":{"code":true}},{"type":"text","text":" can introduce missing values if extreme values of a numeric column are introduced deep in the file.  this repo has a reproducible example (I haven't checked myself but I trust the referrer...) "},{"type":"link","url":"https://github.com/ebergelson/read_csv_issue_quickeg"},{"type":"text","text":"  might be useful to add to our CSV tests/benchmarking"}]}]}]},{"client_msg_id":"54ac8e98-76a2-4fee-aaf2-dcacbe96f003","type":"message","text":"<https://twitter.com/bergelsonlab/status/1366866851884589056>","user":"U66M57AN4","ts":"1614738893.177700","team":"T68168MUP","attachments":[{"fallback":"<https://twitter.com/bergelsonlab|@bergelsonlab>: uhoh i love the #tidyverse &amp; all the <https://twitter.com/hadleywickham|@hadleywickham> tools but danger alert, read_csv() can make a column look full of NAs when it's not if you don't use guess_max() wisely (i.e. if u let it default when u have a rare thing 10k rows down) !!! ( read.csv() doesn't do this) #rstats","ts":1614721427,"author_name":"Bergelson Lab","author_link":"https://twitter.com/bergelsonlab/status/1366866851884589056","author_icon":"https://pbs.twimg.com/profile_images/981987616148066304/CP77wY4m_normal.jpg","author_subname":"@bergelsonlab","text":"uhoh i love the #tidyverse &amp; all the <https://twitter.com/hadleywickham|@hadleywickham> tools but danger alert, read_csv() can make a column look full of NAs when it's not if you don't use guess_max() wisely (i.e. if u let it default when u have a rare thing 10k rows down) !!! ( read.csv() doesn't do this) #rstats","service_name":"twitter","service_url":"https://twitter.com/","from_url":"https://twitter.com/bergelsonlab/status/1366866851884589056","id":1,"original_url":"https://twitter.com/bergelsonlab/status/1366866851884589056","footer":"Twitter","footer_icon":"https://a.slack-edge.com/80588/img/services/twitter_pixel_snapped_32.png"}],"blocks":[{"type":"rich_text","block_id":"9XCu","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://twitter.com/bergelsonlab/status/1366866851884589056"}]}]}]}]}