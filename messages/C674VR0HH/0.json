{"cursor": 0, "messages": [{"type":"message","text":"","files":[{"id":"F01GQEELVMK","created":1607910041,"timestamp":1607910041,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"UH8A351DJ","editable":false,"size":48519,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01GQEELVMK/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01GQEELVMK/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01GQEELVMK-c9e75caa28/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01GQEELVMK-c9e75caa28/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01GQEELVMK-c9e75caa28/image_360.png","thumb_360_w":360,"thumb_360_h":222,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01GQEELVMK-c9e75caa28/image_480.png","thumb_480_w":480,"thumb_480_h":296,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01GQEELVMK-c9e75caa28/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01GQEELVMK-c9e75caa28/image_720.png","thumb_720_w":720,"thumb_720_h":443,"original_w":742,"original_h":457,"thumb_tiny":"AwAdADCpx6UDHoaAcdjTs+xoAbx6Gk/CnbgD0o3DNADfwpKViD0pKAHYJPFGGpQcUbjQAmDRgmlBxQDigBrdaSlbrSUAf//Z","permalink":"https://julialang.slack.com/files/UH8A351DJ/F01GQEELVMK/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01GQEELVMK-dbbf2404e3","is_starred":false,"has_rich_preview":false}],"upload":false,"user":"UH8A351DJ","display_as_bot":false,"ts":"1607910043.121300","thread_ts":"1607910043.121300","reply_count":1,"reply_users_count":1,"latest_reply":"1607935054.125500","reply_users":["U67431ELR"],"subscribed":false},{"client_msg_id":"50fe1f18-cd10-41b5-8eb5-b382d83b278d","type":"message","text":"any idea to make the `Float32[]` less spammy?","user":"UH8A351DJ","ts":"1607910055.121800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"25c6E","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"any idea to make the "},{"type":"text","text":"Float32[]","style":{"code":true}},{"type":"text","text":" less spammy?"}]}]}]},{"client_msg_id":"d8df33b3-1dde-4573-80ba-372d06d2b39f","type":"message","text":"is there an advised way in LibPQ to convert psql types to julia types?\n\nright now I have x = LibPQ.PQ_SYSTEM_TYPES[sym] and then LibPQ.__DEFAULT_TYPE_MAP[x]_\n\nbut I have an issue when I have a type \"timestamp with time zone\", which I believe maps to :timestamptz but I'm not 100% sure and I can't seem to find the piece of code where this occurs","user":"U6CF3AA5Q","ts":"1607911699.125000","team":"T68168MUP","edited":{"user":"U6CF3AA5Q","ts":"1607911753.000000"},"blocks":[{"type":"rich_text","block_id":"S9NZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is there an advised way in LibPQ to convert psql types to julia types?\n\nright now I have x = LibPQ.PQ_SYSTEM_TYPES[sym] and then LibPQ._"},{"type":"text","text":"DEFAULT_TYPE_MAP[x]","style":{"italic":true}},{"type":"text","text":"\n\nbut I have an issue when I have a type \"timestamp with time zone\", which I believe maps to :timestamptz but I'm not 100% sure and I can't seem to find the piece of code where this occurs"}]}]}],"thread_ts":"1607911699.125000","reply_count":6,"reply_users_count":3,"latest_reply":"1607980868.133800","reply_users":["UDXST8ARK","U69J94HT9","U6CF3AA5Q"],"subscribed":false},{"client_msg_id":"7bd78e23-ecb6-4e17-b618-a02f5ed3d8ad","type":"message","text":"Is there a recommended format for incorporating metadata in an Arrow file?  I know that `Arrow.setmetadata!` takes a `Dict{String,String}` but I was wondering about what keys to use.  Coming from an R background I would use keys like `title` , `description` and `reference` for a data set and perhaps `description` and, optionally, `units` to individual columns.  Is there prior art or alternative suggestions?","user":"UBGRZ7FSP","ts":"1607967082.131900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"CKDfV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a recommended format for incorporating metadata in an Arrow file?  I know that "},{"type":"text","text":"Arrow.setmetadata!","style":{"code":true}},{"type":"text","text":" takes a "},{"type":"text","text":"Dict{String,String}","style":{"code":true}},{"type":"text","text":" but I was wondering about what keys to use.  Coming from an R background I would use keys like "},{"type":"text","text":"title","style":{"code":true}},{"type":"text","text":" , "},{"type":"text","text":"description","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"reference","style":{"code":true}},{"type":"text","text":" for a data set and perhaps "},{"type":"text","text":"description","style":{"code":true}},{"type":"text","text":" and, optionally, "},{"type":"text","text":"units","style":{"code":true}},{"type":"text","text":" to individual columns.  Is there prior art or alternative suggestions?"}]}]}],"thread_ts":"1607967082.131900","reply_count":3,"reply_users_count":2,"latest_reply":"1607967596.132500","reply_users":["U67431ELR","U681ELA87"],"subscribed":false,"reactions":[{"name":"+1","users":["U82LX4ACB"],"count":1}]},{"client_msg_id":"b824d895-f3e4-448a-b97e-32e6d5274ba8","type":"message","text":"When I `using Turing` I am no longer able to view DataFrame objects in the repl.\n","user":"U01ARRMLM7E","ts":"1608005446.134800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"occ9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"When I "},{"type":"text","text":"using Turing","style":{"code":true}},{"type":"text","text":" I am no longer able to view DataFrame objects in the repl.\n"}]}]}],"reactions":[{"name":"bug","users":["UBF9YRB6H","UEN48T0BT"],"count":2}]},{"client_msg_id":"b824d895-f3e4-448a-b97e-32e6d5274ba8","type":"message","text":"```julia&gt; DataFrame()\nError showing value of type DataFrame:\nERROR: TypeError: in keyword argument maximum_columns_width, expected Union{Int64, AbstractArray{Int64,1}}, got a value of type Array{Any,1}\nStacktrace:\n [1] _pt(::IOContext{REPL.Terminals.TTYTerminal}, ::PrettyTables.ColumnTable, ::Array{Any,2}; alignment::Array{Symbol,1}, backend::Nothing, cell_alignment::Nothing, cell_first_line_only::Bool, compact_printing::Bool, filters_row::Nothing, filters_col::Nothing, formatters::Tuple{typeof(DataFrames._pretty_tables_general_formatter),DataFrames.var\"#ft_float#558\"{Bool,Array{Int64,1},Array{Int64,1}}}, header_alignment::Symbol, header_cell_alignment::Nothing, renderer::Symbol, row_names::Nothing, row_name_alignment::Symbol, row_name_column_title::String, row_number_column_title::String, show_row_number::Bool, title::String, title_alignment::Symbol, kwargs::Base.Iterators.Pairs{Symbol,Any,NTuple{12,Symbol},NamedTuple{(:crop, :crop_num_lines_at_beginning, :ellipsis_line_skip, :hlines, :highlighters, :maximum_columns_width, :newline_at_end, :nosubheader, :row_name_crayon, :row_number_alignment, :vcrop_mode, :vlines),Tuple{Symbol,Int64,Int64,Array{Symbol,1},Tuple{PrettyTables.Highlighter},Array{Any,1},Bool,Bool,Crayons.Crayon,Symbol,Symbol,Array{Int64,1}}}}) at julia/packages/PrettyTables/W16qB/src/private.jl:422\n [2] _pretty_table(::IOContext{REPL.Terminals.TTYTerminal}, ::DataFrame, ::Array{Any,2}; kwargs::Base.Iterators.Pairs{Symbol,Any,NTuple{22,Symbol},NamedTuple{(:alignment, :compact_printing, :crop, :crop_num_lines_at_beginning, :ellipsis_line_skip, :formatters, :header_alignment, :hlines, :highlighters, :maximum_columns_width, :newline_at_end, :nosubheader, :row_name_alignment, :row_name_crayon, :row_name_column_title, :row_names, :row_number_alignment, :row_number_column_title, :show_row_number, :title, :vcrop_mode, :vlines),Tuple{Array{Symbol,1},Bool,Symbol,Int64,Int64,Tuple{typeof(DataFrames._pretty_tables_general_formatter),DataFrames.var\"#ft_float#558\"{Bool,Array{Int64,1},Array{Int64,1}}},Symbol,Array{Symbol,1},Tuple{PrettyTables.Highlighter},Array{Any,1},Bool,Bool,Symbol,Crayons.Crayon,String,Nothing,Symbol,String,Bool,String,Symbol,Array{Int64,1}}}}) at julia/packages/PrettyTables/W16qB/src/private.jl:356\n [3] pretty_table(::IOContext{REPL.Terminals.TTYTerminal}, ::DataFrame, ::Array{Any,2}; kwargs::Base.Iterators.Pairs{Symbol,Any,NTuple{22,Symbol},NamedTuple{(:alignment, :compact_printing, :crop, :crop_num_lines_at_beginning, :ellipsis_line_skip, :formatters, :header_alignment, :hlines, :highlighters, :maximum_columns_width, :newline_at_end, :nosubheader, :row_name_alignment, :row_name_crayon, :row_name_column_title, :row_names, :row_number_alignment, :row_number_column_title, :show_row_number, :title, :vcrop_mode, :vlines),Tuple{Array{Symbol,1},Bool,Symbol,Int64,Int64,Tuple{typeof(DataFrames._pretty_tables_general_formatter),DataFrames.var\"#ft_float#558\"{Bool,Array{Int64,1},Array{Int64,1}}},Symbol,Array{Symbol,1},Tuple{PrettyTables.Highlighter},Array{Any,1},Bool,Bool,Symbol,Crayons.Crayon,String,Nothing,Symbol,String,Bool,String,Symbol,Array{Int64,1}}}}) at julia/packages/PrettyTables/W16qB/src/print.jl:693\n [4] _show(::IOContext{REPL.Terminals.TTYTerminal}, ::DataFrame; allrows::Bool, allcols::Bool, rowlabel::Symbol, summary::Bool, eltypes::Bool, rowid::Nothing, truncate::Int64, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at julia/packages/DataFrames/Y3fUF/src/abstractdataframe/show.jl:388\n [5] #show#560 at julia/packages/DataFrames/Y3fUF/src/abstractdataframe/show.jl:480 [inlined]\n [6] show(::IOContext{REPL.Terminals.TTYTerminal}, ::DataFrame) at julia/packages/DataFrames/Y3fUF/src/abstractdataframe/show.jl:480\n [7] #show#575 at julia/packages/DataFrames/Y3fUF/src/abstractdataframe/io.jl:50 [inlined]\n [8] show(::IOContext{REPL.Terminals.TTYTerminal}, ::MIME{Symbol(\"text/plain\")}, ::DataFrame) at julia/packages/DataFrames/Y3fUF/src/abstractdataframe/io.jl:50\n [9] display(::REPL.REPLDisplay, ::MIME{Symbol(\"text/plain\")}, ::Any) at /build/source/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:214\n [10] display(::REPL.REPLDisplay, ::Any) at /build/source/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:218\n [11] display(::Any) at ./multimedia.jl:328```","user":"U01ARRMLM7E","ts":"1608005446.134900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"occ9-HG2A","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> DataFrame()\nError showing value of type DataFrame:\nERROR: TypeError: in keyword argument maximum_columns_width, expected Union{Int64, AbstractArray{Int64,1}}, got a value of type Array{Any,1}\nStacktrace:\n [1] _pt(::IOContext{REPL.Terminals.TTYTerminal}, ::PrettyTables.ColumnTable, ::Array{Any,2}; alignment::Array{Symbol,1}, backend::Nothing, cell_alignment::Nothing, cell_first_line_only::Bool, compact_printing::Bool, filters_row::Nothing, filters_col::Nothing, formatters::Tuple{typeof(DataFrames._pretty_tables_general_formatter),DataFrames.var\"#ft_float#558\"{Bool,Array{Int64,1},Array{Int64,1}}}, header_alignment::Symbol, header_cell_alignment::Nothing, renderer::Symbol, row_names::Nothing, row_name_alignment::Symbol, row_name_column_title::String, row_number_column_title::String, show_row_number::Bool, title::String, title_alignment::Symbol, kwargs::Base.Iterators.Pairs{Symbol,Any,NTuple{12,Symbol},NamedTuple{(:crop, :crop_num_lines_at_beginning, :ellipsis_line_skip, :hlines, :highlighters, :maximum_columns_width, :newline_at_end, :nosubheader, :row_name_crayon, :row_number_alignment, :vcrop_mode, :vlines),Tuple{Symbol,Int64,Int64,Array{Symbol,1},Tuple{PrettyTables.Highlighter},Array{Any,1},Bool,Bool,Crayons.Crayon,Symbol,Symbol,Array{Int64,1}}}}) at julia/packages/PrettyTables/W16qB/src/private.jl:422\n [2] _pretty_table(::IOContext{REPL.Terminals.TTYTerminal}, ::DataFrame, ::Array{Any,2}; kwargs::Base.Iterators.Pairs{Symbol,Any,NTuple{22,Symbol},NamedTuple{(:alignment, :compact_printing, :crop, :crop_num_lines_at_beginning, :ellipsis_line_skip, :formatters, :header_alignment, :hlines, :highlighters, :maximum_columns_width, :newline_at_end, :nosubheader, :row_name_alignment, :row_name_crayon, :row_name_column_title, :row_names, :row_number_alignment, :row_number_column_title, :show_row_number, :title, :vcrop_mode, :vlines),Tuple{Array{Symbol,1},Bool,Symbol,Int64,Int64,Tuple{typeof(DataFrames._pretty_tables_general_formatter),DataFrames.var\"#ft_float#558\"{Bool,Array{Int64,1},Array{Int64,1}}},Symbol,Array{Symbol,1},Tuple{PrettyTables.Highlighter},Array{Any,1},Bool,Bool,Symbol,Crayons.Crayon,String,Nothing,Symbol,String,Bool,String,Symbol,Array{Int64,1}}}}) at julia/packages/PrettyTables/W16qB/src/private.jl:356\n [3] pretty_table(::IOContext{REPL.Terminals.TTYTerminal}, ::DataFrame, ::Array{Any,2}; kwargs::Base.Iterators.Pairs{Symbol,Any,NTuple{22,Symbol},NamedTuple{(:alignment, :compact_printing, :crop, :crop_num_lines_at_beginning, :ellipsis_line_skip, :formatters, :header_alignment, :hlines, :highlighters, :maximum_columns_width, :newline_at_end, :nosubheader, :row_name_alignment, :row_name_crayon, :row_name_column_title, :row_names, :row_number_alignment, :row_number_column_title, :show_row_number, :title, :vcrop_mode, :vlines),Tuple{Array{Symbol,1},Bool,Symbol,Int64,Int64,Tuple{typeof(DataFrames._pretty_tables_general_formatter),DataFrames.var\"#ft_float#558\"{Bool,Array{Int64,1},Array{Int64,1}}},Symbol,Array{Symbol,1},Tuple{PrettyTables.Highlighter},Array{Any,1},Bool,Bool,Symbol,Crayons.Crayon,String,Nothing,Symbol,String,Bool,String,Symbol,Array{Int64,1}}}}) at julia/packages/PrettyTables/W16qB/src/print.jl:693\n [4] _show(::IOContext{REPL.Terminals.TTYTerminal}, ::DataFrame; allrows::Bool, allcols::Bool, rowlabel::Symbol, summary::Bool, eltypes::Bool, rowid::Nothing, truncate::Int64, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at julia/packages/DataFrames/Y3fUF/src/abstractdataframe/show.jl:388\n [5] #show#560 at julia/packages/DataFrames/Y3fUF/src/abstractdataframe/show.jl:480 [inlined]\n [6] show(::IOContext{REPL.Terminals.TTYTerminal}, ::DataFrame) at julia/packages/DataFrames/Y3fUF/src/abstractdataframe/show.jl:480\n [7] #show#575 at julia/packages/DataFrames/Y3fUF/src/abstractdataframe/io.jl:50 [inlined]\n [8] show(::IOContext{REPL.Terminals.TTYTerminal}, ::MIME{Symbol(\"text/plain\")}, ::DataFrame) at julia/packages/DataFrames/Y3fUF/src/abstractdataframe/io.jl:50\n [9] display(::REPL.REPLDisplay, ::MIME{Symbol(\"text/plain\")}, ::Any) at /build/source/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:214\n [10] display(::REPL.REPLDisplay, ::Any) at /build/source/usr/share/julia/stdlib/v1.5/REPL/src/REPL.jl:218\n [11] display(::Any) at ./multimedia.jl:328"}]}]}]},{"client_msg_id":"172c01f9-118c-4e3a-b6f4-b357d79d0d30","type":"message","text":"How can I avoid this problem?","user":"U01ARRMLM7E","ts":"1608005673.135300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DjS6p","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I avoid this problem?"}]}]}],"thread_ts":"1608005673.135300","reply_count":1,"reply_users_count":1,"latest_reply":"1608014725.147600","reply_users":["U8JAMQGQY"],"subscribed":false},{"client_msg_id":"d4f05541-2bef-46b2-88cc-43224f608c88","type":"message","text":"`map(f, eachrow(df))` is much slower (and a lot more allocation) than `map(f, Table(df))`, Table from TypedTable.jl","user":"UH8A351DJ","ts":"1608008494.137800","team":"T68168MUP","edited":{"user":"UH8A351DJ","ts":"1608008503.000000"},"blocks":[{"type":"rich_text","block_id":"RZ9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"map(f, eachrow(df))","style":{"code":true}},{"type":"text","text":" is much slower (and a lot more allocation) than "},{"type":"text","text":"map(f, Table(df))","style":{"code":true}},{"type":"text","text":", Table from TypedTable.jl"}]}]}],"thread_ts":"1608008494.137800","reply_count":4,"reply_users_count":3,"latest_reply":"1608024258.148900","reply_users":["UH8A351DJ","U8JAMQGQY","U01ARRMLM7E"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"```julia&gt; using DataFrames, TypedTables\n\njulia&gt; loop(row) = row.x1 + row.x2 + sin(row.x3)\nloop (generic function with 1 method)\n\njulia&gt; df = DataFrame(rand(30000,3));\n\njulia&gt; @btime map(loop, Table(df));\n  217.713 μs (17 allocations: 235.06 KiB)\n\njulia&gt; @btime map(loop, eachrow(df));\n  5.170 ms (358479 allocations: 5.70 MiB)\n\njulia&gt; map(loop, eachrow(df)) == map(loop, Table(df))\ntrue```","user":"UH8A351DJ","ts":"1608009011.138000","thread_ts":"1608008494.137800","root":{"client_msg_id":"d4f05541-2bef-46b2-88cc-43224f608c88","type":"message","text":"`map(f, eachrow(df))` is much slower (and a lot more allocation) than `map(f, Table(df))`, Table from TypedTable.jl","user":"UH8A351DJ","ts":"1608008494.137800","team":"T68168MUP","edited":{"user":"UH8A351DJ","ts":"1608008503.000000"},"blocks":[{"type":"rich_text","block_id":"RZ9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"map(f, eachrow(df))","style":{"code":true}},{"type":"text","text":" is much slower (and a lot more allocation) than "},{"type":"text","text":"map(f, Table(df))","style":{"code":true}},{"type":"text","text":", Table from TypedTable.jl"}]}]}],"thread_ts":"1608008494.137800","reply_count":4,"reply_users_count":3,"latest_reply":"1608024258.148900","reply_users":["UH8A351DJ","U8JAMQGQY","U01ARRMLM7E"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"nKcoH","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> using DataFrames, TypedTables\n\njulia> loop(row) = row.x1 + row.x2 + sin(row.x3)\nloop (generic function with 1 method)\n\njulia> df = DataFrame(rand(30000,3));\n\njulia> @btime map(loop, Table(df));\n  217.713 μs (17 allocations: 235.06 KiB)\n\njulia> @btime map(loop, eachrow(df));\n  5.170 ms (358479 allocations: 5.70 MiB)\n\njulia> map(loop, eachrow(df)) == map(loop, Table(df))\ntrue"}]}]}],"client_msg_id":"d1f830db-246c-4cdc-96a1-03b98fd6b2e2"},{"client_msg_id":"1A6F2FA7-FE55-4B96-9EF5-90B22ED8EBBF","type":"message","text":"This is expected. TypedTables is, as described, well-typed and will have that code advantage for specific operations. DataFrames eachrow is untyped, so relies on dynamic dispatch for each iteration. But try mapping over a df with a thousand columns and you’ll see TypedTables bring the compiler to its knees. There are trade offs, and TT (or `Tables.namedtupleiterator(df)`) should def be used in performance-critical situations.","user":"U681ELA87","ts":"1608010215.143300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5dU6v","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This is expected. TypedTables is, as described, well-typed and will have that code advantage for specific operations. DataFrames eachrow is untyped, so relies on dynamic dispatch for each iteration. But try mapping over a df with a thousand columns and you’ll see TypedTables bring the compiler to its knees. There are trade offs, and TT (or "},{"type":"text","text":"Tables.namedtupleiterator(df)","style":{"code":true}},{"type":"text","text":") should def be used in performance-critical situations."}]}]}],"thread_ts":"1608010215.143300","reply_count":5,"reply_users_count":4,"latest_reply":"1608015842.148100","reply_users":["UH8A351DJ","U681ELA87","U8JAMQGQY","UBF9YRB6H"],"subscribed":false,"reactions":[{"name":"heart","users":["UH8A351DJ","U6QGE7S86","UQE0NBSS1"],"count":3},{"name":"+1","users":["UH8A351DJ","UBF9YRB6H","U6A936746","U82LX4ACB","U6QGE7S86","UQE0NBSS1"],"count":6}]},{"client_msg_id":"f5f97666-309d-47b6-9314-a01397451149","type":"message","text":"Can someone with a windows machine check out this PR and run tests locally? We have a windows build failure but PR author says they pass locally, want to make sure that's not a fluke","user":"U8JP5B9T2","ts":"1608042915.150800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iwJFJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can someone with a windows machine check out this PR and run tests locally? We have a windows build failure but PR author says they pass locally, want to make sure that's not a fluke"}]}]}],"thread_ts":"1608042915.150800","reply_count":4,"reply_users_count":2,"latest_reply":"1608050094.152100","reply_users":["UEN48T0BT","U8JP5B9T2"],"subscribed":false},{"client_msg_id":"1ef96e12-99bc-464c-a6c9-0fd63e069c2b","type":"message","text":"<https://github.com/JuliaData/YAML.jl/pull/66>","user":"U8JP5B9T2","ts":"1608043008.151000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qSm","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/JuliaData/YAML.jl/pull/66"}]}]}]},{"client_msg_id":"87360e70-84ed-40d7-b064-490d772f67f4","type":"message","text":"How can I get back a `Date` from a `Dates.value(my_date)`? `Date(737774)` will of course just produce the first of January in the year 737774","user":"U7JQGPGCQ","ts":"1608052313.153300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5ZfrR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I get back a "},{"type":"text","text":"Date","style":{"code":true}},{"type":"text","text":" from a "},{"type":"text","text":"Dates.value(my_date)","style":{"code":true}},{"type":"text","text":"? "},{"type":"text","text":"Date(737774)","style":{"code":true}},{"type":"text","text":" will of course just produce the first of January in the year 737774"}]}]}],"thread_ts":"1608052313.153300","reply_count":4,"reply_users_count":2,"latest_reply":"1608053470.154000","reply_users":["U7JQGPGCQ","U681ELA87"],"subscribed":false},{"client_msg_id":"a29cbaad-09ab-4571-b70e-062e59119a0f","type":"message","text":"If I define a type, which is a generator of tables (`NamedTuple` of vectors), do I have to do / can I do anything other than define `Tables.partitions(x::MyGenerator) = x` ?","user":"U01ECBX4MB7","ts":"1608056601.156200","team":"T68168MUP","edited":{"user":"U01ECBX4MB7","ts":"1608056927.000000"},"blocks":[{"type":"rich_text","block_id":"uT31Z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If I define a type, which is a generator of tables ("},{"type":"text","text":"NamedTuple","style":{"code":true}},{"type":"text","text":" of vectors), do I have to do / can I do anything other than define "},{"type":"text","text":"Tables.partitions(x::MyGenerator) = x","style":{"code":true}},{"type":"text","text":" ?"}]}]}],"thread_ts":"1608056601.156200","reply_count":8,"reply_users_count":2,"latest_reply":"1608058871.159400","reply_users":["U681ELA87","U01ECBX4MB7"],"subscribed":false},{"client_msg_id":"78083daf-22ce-48c9-b85a-782771d877b9","type":"message","text":"What does one call the row-ness or column-ness of a table? Orientation? does someone have a better word?","user":"U01ECBX4MB7","ts":"1608057454.158400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5s3/Q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What does one call the row-ness or column-ness of a table? Orientation? does someone have a better word?"}]}]}],"thread_ts":"1608057454.158400","reply_count":1,"reply_users_count":1,"latest_reply":"1608057801.159200","reply_users":["U681ELA87"],"subscribed":false},{"client_msg_id":"c7002a13-d159-49e1-89a7-6372b86be5d6","type":"message","text":"<@U681ELA87> you described how to write arrow files from a bunch of CSV files just a couple of weeks ago, but it's already lost to the slack hole. I remember it had to do with `Tables.partitioner`, but I'm having issues. I currently have\n```Arrow.write(\"path/file.arrow\", Tables.partitioner(csvs) do file\n    df = CSV.File(file) |&gt; DataFrame\n    # some processing\n    return df\nend)```\nBut I'm having some issues. It's possible that the error is within my dataframe, but it looks like it's coming from Arrow - did I miss something? Should the call to `Arrow.write` come inside the partitioner loop?","user":"U8JP5B9T2","ts":"1608084336.164900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Uz2Iu","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U681ELA87"},{"type":"text","text":" you described how to write arrow files from a bunch of CSV files just a couple of weeks ago, but it's already lost to the slack hole. I remember it had to do with "},{"type":"text","text":"Tables.partitioner","style":{"code":true}},{"type":"text","text":", but I'm having issues. I currently have\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Arrow.write(\"path/file.arrow\", Tables.partitioner(csvs) do file\n    df = CSV.File(file) |> DataFrame\n    # some processing\n    return df\nend)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"But I'm having some issues. It's possible that the error is within my dataframe, but it looks like it's coming from Arrow - did I miss something? Should the call to "},{"type":"text","text":"Arrow.write","style":{"code":true}},{"type":"text","text":" come inside the partitioner loop?"}]}]}],"thread_ts":"1608084336.164900","reply_count":10,"reply_users_count":2,"latest_reply":"1608129295.170700","reply_users":["U681ELA87","U8JP5B9T2"],"subscribed":false},{"client_msg_id":"b1c36da9-6cc2-42ac-a733-1fa1578848a9","type":"message","text":"I have a .csv that has a (seemingly on purpose) emtpy last column with no missing marker. I.e., each row just ends on `;\\n`. Is there a way that I can tell CSV to ignore the last row, preferably without `drop`? As it is, I get an annoying warning that \"Thread 1 only found 6/7 columns\" in the last row","user":"UH24GRBLL","ts":"1608133613.172500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"cQX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a .csv that has a (seemingly on purpose) emtpy last column with no missing marker. I.e., each row just ends on "},{"type":"text","text":";\\n","style":{"code":true}},{"type":"text","text":". Is there a way that I can tell CSV to ignore the last row, preferably without "},{"type":"text","text":"drop","style":{"code":true}},{"type":"text","text":"? As it is, I get an annoying warning that \"Thread 1 only found 6/7 columns\" in the last row"}]}]}],"thread_ts":"1608133613.172500","reply_count":16,"reply_users_count":2,"latest_reply":"1608144633.177500","reply_users":["U681ELA87","UH24GRBLL"],"subscribed":false},{"client_msg_id":"6afe1744-12b0-47b1-a34a-c24ae6cb2e68","type":"message","text":"Did there used to be a `by()`  function in DataFrames.jl?","user":"U01ARRMLM7E","ts":"1608156180.178600","team":"T68168MUP","edited":{"user":"U01ARRMLM7E","ts":"1608156200.000000"},"blocks":[{"type":"rich_text","block_id":"IBhWJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Did there used to be a "},{"type":"text","text":"by()","style":{"code":true}},{"type":"text","text":"  function in DataFrames.jl?"}]}]}],"thread_ts":"1608156180.178600","reply_count":11,"reply_users_count":5,"latest_reply":"1608161694.181300","reply_users":["U67431ELR","U01ARRMLM7E","U6740K1SP","UBF9YRB6H","USTUBS9ED"],"subscribed":false},{"client_msg_id":"a43f3a41-19b3-4ce8-a826-c5cac4aa82aa","type":"message","text":"<https://discourse.julialang.org/t/techempower-frameworkbenchmarks-r20-deadline-12-28-first-julia-json-serialization-results/51936>","user":"UDGT4PM41","ts":"1608165958.182100","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"TechEmpower/FrameworkBenchmarks R20 deadline 12/28 + first julia \"JSON serialization\" results","title_link":"https://discourse.julialang.org/t/techempower-frameworkbenchmarks-r20-deadline-12-28-first-julia-json-serialization-results/51936","text":"TLDR: Techempower Framework benchmarks is a collaborative, open web framework benchmarking project The next release: Round 20 \"Last day for PR review is 12/28. \" First Julia results ( “http.jl” ) … current “rank: 149/166” not the best… :slightly_smiling_face: So if anybody has an experience in performance tuning ; then please check the current code. ( http.jl ) and/or please add a new/better/alternative Julia code Details: The good news, somebody ( not me ) has been added the first Ju...","fallback":"JuliaLang: TechEmpower/FrameworkBenchmarks R20 deadline 12/28 + first julia \"JSON serialization\" results","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/3X/0/3/03bea743f30b3b17cc3fb6b34a12068d13cd5ffb.png","fields":[{"title":"Reading time","value":"1 mins :clock2:","short":true},{"title":"Likes","value":"13 :heart:","short":true}],"ts":1608131860,"from_url":"https://discourse.julialang.org/t/techempower-frameworkbenchmarks-r20-deadline-12-28-first-julia-json-serialization-results/51936","thumb_width":486,"thumb_height":32,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/techempower-frameworkbenchmarks-r20-deadline-12-28-first-julia-json-serialization-results/51936"}],"blocks":[{"type":"rich_text","block_id":"Gto/","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://discourse.julialang.org/t/techempower-frameworkbenchmarks-r20-deadline-12-28-first-julia-json-serialization-results/51936"}]}]}]},{"client_msg_id":"1b85085d-84d6-4c43-972f-95ddce5d4cb8","type":"message","text":"is `mapreduce(DataFrame∘CSV.File, vcat, all_csvs);` the best practice of reading multiple CSV into a single DataFrame?","user":"UH8A351DJ","ts":"1608227930.183700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fWbV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is "},{"type":"text","text":"mapreduce(DataFrame∘CSV.File, vcat, all_csvs);","style":{"code":true}},{"type":"text","text":" the best practice of reading multiple CSV into a single DataFrame?"}]}]}],"thread_ts":"1608227930.183700","reply_count":22,"reply_users_count":6,"latest_reply":"1608243979.194000","reply_users":["UBF9YRB6H","U8JAMQGQY","UH8A351DJ","U681ELA87","UDXST8ARK","U82LX4ACB"],"subscribed":false},{"client_msg_id":"2c78d12a-a98e-4eb2-beed-657b93629ae2","type":"message","text":"Recently I have received a private message on how to reproduce `case_when` from dplyr in Julia. Here are my conclusions: <https://bkamins.github.io/julialang/2020/12/18/casewhen.html>.","user":"U8JAMQGQY","ts":"1608279573.196900","team":"T68168MUP","attachments":[{"service_name":"Blog by Bogumił Kamiński","title":"Is case_when needed in DataFrames.jl?","title_link":"https://bkamins.github.io/julialang/2020/12/18/casewhen.html","text":"Introduction","fallback":"Blog by Bogumił Kamiński: Is case_when needed in DataFrames.jl?","ts":1608273193,"from_url":"https://bkamins.github.io/julialang/2020/12/18/casewhen.html","service_icon":"https://bkamins.github.io/favicon.ico","id":1,"original_url":"https://bkamins.github.io/julialang/2020/12/18/casewhen.html"}],"blocks":[{"type":"rich_text","block_id":"E3ir","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Recently I have received a private message on how to reproduce "},{"type":"text","text":"case_when","style":{"code":true}},{"type":"text","text":" from dplyr in Julia. Here are my conclusions: "},{"type":"link","url":"https://bkamins.github.io/julialang/2020/12/18/casewhen.html"},{"type":"text","text":"."}]}]}],"thread_ts":"1608279573.196900","reply_count":14,"reply_users_count":3,"latest_reply":"1608313116.203300","reply_users":["U67431ELR","U8JAMQGQY","UBF9YRB6H"],"subscribed":false,"reactions":[{"name":"fire","users":["UM4TSHKF1"],"count":1}]},{"client_msg_id":"8a511119-62c0-489d-9ed6-4216105495ac","type":"message","text":"A quick H2O data wrangling benchmark update. It now introduced handling of missing values in the tests. The test suite uncovered several bugs in different ecosystems that are much more mature than DataFrames.jl and interestingly no bugs in Julia :smile: (see <https://github.com/h2oai/db-benchmark/issues/40#issuecomment-747919633> for details).","user":"U8JAMQGQY","ts":"1608280043.198600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hV8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"A quick H2O data wrangling benchmark update. It now introduced handling of missing values in the tests. The test suite uncovered several bugs in different ecosystems that are much more mature than DataFrames.jl and interestingly no bugs in Julia "},{"type":"emoji","name":"smile"},{"type":"text","text":" (see "},{"type":"link","url":"https://github.com/h2oai/db-benchmark/issues/40#issuecomment-747919633"},{"type":"text","text":" for details)."}]}]}],"thread_ts":"1608280043.198600","reply_count":2,"reply_users_count":2,"latest_reply":"1608315543.203900","reply_users":["UBF9YRB6H","U8JAMQGQY"],"subscribed":false,"reactions":[{"name":"+1","users":["U67431ELR","U6A936746","U681ELA87","U01C15GH58B","U66M57AN4","UKG4WF8PJ","US8V7JSKB","UFWQ6DP0S"],"count":8},{"name":"gunther","users":["USFR23ZHQ","U681ELA87","U66M57AN4","UKG4WF8PJ"],"count":4},{"name":"mask-parrot","users":["U01GXNFKY6R","U66M57AN4","UKG4WF8PJ"],"count":3}]},{"type":"message","text":"I appreciate this error. :smile: It's a lot more useful than a `MethodError` from deep within a libary-internal call stack","files":[{"id":"F01HP2VSZ8R","created":1608395988,"timestamp":1608395988,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U68M6ERG8","editable":false,"size":22572,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01HP2VSZ8R/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01HP2VSZ8R/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01HP2VSZ8R-7706960f54/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01HP2VSZ8R-7706960f54/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01HP2VSZ8R-7706960f54/image_360.png","thumb_360_w":360,"thumb_360_h":38,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01HP2VSZ8R-7706960f54/image_480.png","thumb_480_w":480,"thumb_480_h":51,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01HP2VSZ8R-7706960f54/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01HP2VSZ8R-7706960f54/image_720.png","thumb_720_w":720,"thumb_720_h":76,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01HP2VSZ8R-7706960f54/image_800.png","thumb_800_w":800,"thumb_800_h":85,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01HP2VSZ8R-7706960f54/image_960.png","thumb_960_w":960,"thumb_960_h":102,"thumb_1024":"https://files.slack.com/files-tmb/T68168MUP-F01HP2VSZ8R-7706960f54/image_1024.png","thumb_1024_w":1024,"thumb_1024_h":108,"original_w":1378,"original_h":146,"thumb_tiny":"AwAFADDR2n+9Rg/3qdRQAmPc0Y9zS0UAJj3NGPc0tFAH/9k=","permalink":"https://julialang.slack.com/files/U68M6ERG8/F01HP2VSZ8R/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01HP2VSZ8R-0420ffacfb","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"JIi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I appreciate this error. "},{"type":"emoji","name":"smile"},{"type":"text","text":" It's a lot more useful than a "},{"type":"text","text":"MethodError","style":{"code":true}},{"type":"text","text":" from deep within a libary-internal call stack"}]}]}],"user":"U68M6ERG8","display_as_bot":false,"ts":"1608396081.205800","reactions":[{"name":"+1","users":["UB197FRCL","U6795JH6H","U969CNQU9"],"count":3}]},{"client_msg_id":"2b8ccdf4-0018-42e0-8a5c-3871835393a5","type":"message","text":"```df = DataFrame(a=[1,2,3])\ntransform(df, [] =&gt; (()-&gt;[1,2,3]) =&gt; :b)```\nIs there a nicer way to do this (without macros, without in-place operations)?","user":"U01ARRMLM7E","ts":"1608408702.207500","team":"T68168MUP","edited":{"user":"U01ARRMLM7E","ts":"1608408943.000000"},"blocks":[{"type":"rich_text","block_id":"I+y9","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"df = DataFrame(a=[1,2,3])\ntransform(df, [] => (()->[1,2,3]) => :b)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a nicer way to do this (without macros, without in-place operations)?"}]}]}]},{"client_msg_id":"1dd28caf-77ee-4a6d-8d27-4531ae5bc22a","type":"message","text":"maybe\n```insertcols!(copy(df), :b =&gt; [1,2,3])```","user":"UK1BNFHFV","ts":"1608412286.208000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"CDI+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"maybe\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"insertcols!(copy(df), :b => [1,2,3])"}]}]}]},{"client_msg_id":"9323c54d-ba4f-4671-9bad-7b42b9991b77","type":"message","text":"yes, or `setindex!(df, [1,2,3], :, :b)` or `setindex!(df, [1,2,3], !, :b)` the difference between these three options is when `:b` would be present in `df` (`insertcols!` will error, `:` is in-place, and `!` replaces)","user":"U8JAMQGQY","ts":"1608412394.209400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Lfg2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yes, or "},{"type":"text","text":"setindex!(df, [1,2,3], :, :b)","style":{"code":true}},{"type":"text","text":" or "},{"type":"text","text":"setindex!(df, [1,2,3], !, :b)","style":{"code":true}},{"type":"text","text":" the difference between these three options is when "},{"type":"text","text":":b","style":{"code":true}},{"type":"text","text":" would be present in "},{"type":"text","text":"df","style":{"code":true}},{"type":"text","text":" ("},{"type":"text","text":"insertcols!","style":{"code":true}},{"type":"text","text":" will error, "},{"type":"text","text":":","style":{"code":true}},{"type":"text","text":" is in-place, and "},{"type":"text","text":"!","style":{"code":true}},{"type":"text","text":" replaces)"}]}]}]},{"client_msg_id":"ddb7ccc1-c8bf-44cc-ad8d-a08f8589934f","type":"message","text":"I’m trying to collapse the time dimension in a longitudinal dataset. For some columns, I just want the first value but some of the columns begin with a missing so there I’d like the first non-missing value. After reading the docs, <https://dataframes.juliadata.org/stable/man/split_apply_combine/>, I thought I could do something like this\n```datetime_df = combine(\n    groupby(df, :id),\n    AsTable([\"datetime\", \"Gender\"]) =&gt; (t -&gt; (datetime=first(t.datetime), Gender=first(t.Gender))) =&gt; AsTable)```\nbut it gives me\n```ERROR: ArgumentError: Unrecognized column selector: AsTable([\"datetime\", \"Gender\"]) =&gt; (var\"#283#284\"() =&gt; AsTable)```\nAm I doing something obviously wrong here?","user":"U680T6770","ts":"1608413892.211400","team":"T68168MUP","edited":{"user":"U680T6770","ts":"1608413920.000000"},"blocks":[{"type":"rich_text","block_id":"y=a1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’m trying to collapse the time dimension in a longitudinal dataset. For some columns, I just want the first value but some of the columns begin with a missing so there I’d like the first non-missing value. After reading the docs, "},{"type":"link","url":"https://dataframes.juliadata.org/stable/man/split_apply_combine/"},{"type":"text","text":", I thought I could do something like this\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"datetime_df = combine(\n    groupby(df, :id),\n    AsTable([\"datetime\", \"Gender\"]) => (t -> (datetime=first(t.datetime), Gender=first(t.Gender))) => AsTable)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"but it gives me\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: ArgumentError: Unrecognized column selector: AsTable([\"datetime\", \"Gender\"]) => (var\"#283#284\"() => AsTable)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Am I doing something obviously wrong here?"}]}]}],"thread_ts":"1608413892.211400","reply_count":15,"reply_users_count":2,"latest_reply":"1608418100.217500","reply_users":["U8JAMQGQY","U680T6770"],"subscribed":false},{"client_msg_id":"116c853c-9a30-42b7-b844-8c312c61ad7c","type":"message","text":"In `transform()` the name of the output goes at the end. Why is the `insertcols!` syntax `:b =&gt; [1,2,3]` instead of `[1,2,3] =&gt; :b` ?","user":"U01ARRMLM7E","ts":"1608414983.212400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"c7AeD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In "},{"type":"text","text":"transform()","style":{"code":true}},{"type":"text","text":" the name of the output goes at the end. Why is the "},{"type":"text","text":"insertcols!","style":{"code":true}},{"type":"text","text":" syntax "},{"type":"text","text":":b => [1,2,3]","style":{"code":true}},{"type":"text","text":" instead of "},{"type":"text","text":"[1,2,3] => :b","style":{"code":true}},{"type":"text","text":" ?"}]}]}],"thread_ts":"1608414983.212400","reply_count":1,"reply_users_count":1,"latest_reply":"1608415396.212900","reply_users":["U8JAMQGQY"],"subscribed":false},{"client_msg_id":"661c4992-34b5-4230-8b0d-bfbe3e694f01","type":"message","text":"Anyone that uses YAML.jl able to review this PR? It looks purely like added functionality, but I'm not super familiar with the internals (despite being the package steward somehow) <https://github.com/JuliaData/YAML.jl/pull/104>","user":"U8JP5B9T2","ts":"1608567196.220000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mJg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Anyone that uses YAML.jl able to review this PR? It looks purely like added functionality, but I'm not super familiar with the internals (despite being the package steward somehow) "},{"type":"link","url":"https://github.com/JuliaData/YAML.jl/pull/104"}]}]}]},{"client_msg_id":"893726ea-1965-42e2-b9d8-371f65709e52","type":"message","text":"This <https://bkamins.github.io/julialang/2020/12/24/minilanguage.html> blog post was supposed to be published on 2020-12-24, but it seems my blog management software messed something up. But maybe it is better it went out early, as it is much longer than the average. I hope to answer with it, by-example, most of the questions people have regarding DataFrames.jl minilanguage.","user":"U8JAMQGQY","ts":"1608572665.222000","team":"T68168MUP","attachments":[{"service_name":"Blog by Bogumił Kamiński","title":"DataFrames.jl minilanguage explained","title_link":"https://bkamins.github.io/julialang/2020/12/24/minilanguage.html","text":"Introduction","fallback":"Blog by Bogumił Kamiński: DataFrames.jl minilanguage explained","ts":1608785495,"from_url":"https://bkamins.github.io/julialang/2020/12/24/minilanguage.html","service_icon":"https://bkamins.github.io/favicon.ico","id":1,"original_url":"https://bkamins.github.io/julialang/2020/12/24/minilanguage.html"}],"blocks":[{"type":"rich_text","block_id":"RXa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This "},{"type":"link","url":"https://bkamins.github.io/julialang/2020/12/24/minilanguage.html"},{"type":"text","text":" blog post was supposed to be published on 2020-12-24, but it seems my blog management software messed something up. But maybe it is better it went out early, as it is much longer than the average. I hope to answer with it, by-example, most of the questions people have regarding DataFrames.jl minilanguage."}]}]}],"reactions":[{"name":"100","users":["UBF9YRB6H","UAVMYR0F4","U68P09RFZ","U01CQTKB86N","UCAFZ51L3","U8JP5B9T2","US8V7JSKB","UKG4WF8PJ","U01EF0QVAB0","UK1BNFHFV"],"count":10},{"name":"+1::skin-tone-2","users":["U8T0YV7QC"],"count":1},{"name":"clap","users":["U01FR784NSW","U7EF5AWHW"],"count":2}]},{"client_msg_id":"458eedc8-387f-4b85-ab26-7bcb0f554401","type":"message","text":"Hi, does anyone know how to load JDFs with FileTrees? (The issue is that JDFs get seen as directories rather than files.) I have asked about this before, but I can’t remember now how to do it, and the old response has disappeared into the slackhole. <@U68907M46> <@U6GD6JN2K>","user":"US8V7JSKB","ts":"1608675758.226300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"N8/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, does anyone know how to load JDFs with FileTrees? (The issue is that JDFs get seen as directories rather than files.) I have asked about this before, but I can’t remember now how to do it, and the old response has disappeared into the slackhole. "},{"type":"user","user_id":"U68907M46"},{"type":"text","text":" "},{"type":"user","user_id":"U6GD6JN2K"}]}]}]},{"client_msg_id":"c31a734f-3b9a-46c1-93af-46febb72b840","type":"message","text":"Can Arrow.jl not deal with heterogenously typed tuples?\n```julia&gt; Arrow.write(\"foo.arrow\", DataFrame(a = [(1, 2.)]))\nERROR: MethodError: no method matching String(::Int64)\nClosest candidates are:\n  String(::String) at boot.jl:321\n  String(::Array{UInt8,1}) at strings/string.jl:39\n  String(::Base.CodeUnits{UInt8,String}) at strings/string.jl:77\n  ...\nStacktrace:\n [1] fieldoffset(::Arrow.FlatBuffers.Builder, ::Int64, ::Arrow.Primitive{Int64,Arrow.ToStruct{Int64,1,Array{Tuple{Int64,Float64},1}}}) at /local/scratch/ssd/sschaub/.julia/packages/Arrow/CyJ4L/src/write.jl:313\n [2] (::Arrow.var\"#74#75\"{Arrow.FlatBuffers.Builder,Arrow.Struct{Tuple{Int64,Float64},Tuple{Arrow.Primitive{Int64,Arrow.ToStruct{Int64,1,Array{Tuple{Int64,Float64},1}}},Arrow.Primitive{Float64,Arrow.ToStruct{Float64,2,Array{Tuple{Int64,Float64},1}}}}},Tuple{Int64,Int64}})(::Int64) at ./tuple.jl:0\n[...]```","user":"UM30MT6RF","ts":"1608718638.227600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"13Mv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can Arrow.jl not deal with heterogenously typed tuples?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> Arrow.write(\"foo.arrow\", DataFrame(a = [(1, 2.)]))\nERROR: MethodError: no method matching String(::Int64)\nClosest candidates are:\n  String(::String) at boot.jl:321\n  String(::Array{UInt8,1}) at strings/string.jl:39\n  String(::Base.CodeUnits{UInt8,String}) at strings/string.jl:77\n  ...\nStacktrace:\n [1] fieldoffset(::Arrow.FlatBuffers.Builder, ::Int64, ::Arrow.Primitive{Int64,Arrow.ToStruct{Int64,1,Array{Tuple{Int64,Float64},1}}}) at /local/scratch/ssd/sschaub/.julia/packages/Arrow/CyJ4L/src/write.jl:313\n [2] (::Arrow.var\"#74#75\"{Arrow.FlatBuffers.Builder,Arrow.Struct{Tuple{Int64,Float64},Tuple{Arrow.Primitive{Int64,Arrow.ToStruct{Int64,1,Array{Tuple{Int64,Float64},1}}},Arrow.Primitive{Float64,Arrow.ToStruct{Float64,2,Array{Tuple{Int64,Float64},1}}}}},Tuple{Int64,Int64}})(::Int64) at ./tuple.jl:0\n[...]"}]}]}]},{"client_msg_id":"f23372e2-cb38-44e9-8e33-23ba04d19ed8","type":"message","text":"I am trying to use the `buckets` in the `GridFS` of the `Mongoc` package. Can anyone confirm if that works with any data type or do I need to convert to Json (String) before uploading?","user":"UKNN7CJ86","ts":"1608772798.232100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wkzRO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am trying to use the "},{"type":"text","text":"buckets","style":{"code":true}},{"type":"text","text":" in the "},{"type":"text","text":"GridFS","style":{"code":true}},{"type":"text","text":" of the "},{"type":"text","text":"Mongoc","style":{"code":true}},{"type":"text","text":" package. Can anyone confirm if that works with any data type or do I need to convert to Json (String) before uploading?"}]}]}]},{"client_msg_id":"01dd2a66-815d-4772-b93a-f1e61fea96a2","type":"message","text":"here an example: it works, however I would prefer not having to use JSON as intermediary\n```bucket = Mongoc.Bucket(database)\nio = Mongoc.open_upload_stream(bucket, \"xxxxx\")\nwrite(io, JSON.json([1,2]))\nclose(io)\n\nMongoc.open_download_stream(bucket, \"xxxxx\") do io\n    tmp_str = read(io, String)\n    value = JSON.parse(tmp_str)\nend```","user":"UKNN7CJ86","ts":"1608773143.232900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/Tr4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"here an example: it works, however I would prefer not having to use JSON as intermediary\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"bucket = Mongoc.Bucket(database)\nio = Mongoc.open_upload_stream(bucket, \"xxxxx\")\nwrite(io, JSON.json([1,2]))\nclose(io)\n\nMongoc.open_download_stream(bucket, \"xxxxx\") do io\n    tmp_str = read(io, String)\n    value = JSON.parse(tmp_str)\nend"}]}]}]},{"client_msg_id":"44bd9f8d-c436-4349-84fe-6dda8ca7bb90","type":"message","text":"Also, the type is lost and the resulting value is an `Array{Any}`","user":"UKNN7CJ86","ts":"1608773192.233700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JEZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Also, the type is lost and the resulting value is an "},{"type":"text","text":"Array{Any}","style":{"code":true}}]}]}]},{"client_msg_id":"adaf16af-db5d-4dab-b61d-13fd05f0a5ca","type":"message","text":"Can someone confirm `df[shuffle(1:nrow(df)), :]` is the proper way to shuffle the rows of a dataframe?","user":"U01ARRMLM7E","ts":"1608842586.235200","team":"T68168MUP","edited":{"user":"U01ARRMLM7E","ts":"1608842659.000000"},"blocks":[{"type":"rich_text","block_id":"gAaE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can someone confirm "},{"type":"text","text":"df[shuffle(1:nrow(df)), :]","style":{"code":true}},{"type":"text","text":" is the proper way to shuffle the rows of a dataframe?"}]}]}]},{"client_msg_id":"40bb0ef9-f2b7-4982-b4c6-287c0181bb20","type":"message","text":"Is it possible to use Apache Arrow to create a DataFrame that is larger than the available system memory?\nI don’t really need this ATM, but for peace of mind, I would like the capability to create a DataFrame of, say, 200 GB on my MacBook Pro that has 16 GB memory.","user":"U01CQTKB86N","ts":"1608974945.239300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MqCmj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is it possible to use Apache Arrow to create a DataFrame that is larger than the available system memory?\nI don’t really need this ATM, but for peace of mind, I would like the capability to create a DataFrame of, say, 200 GB on my MacBook Pro that has 16 GB memory."}]}]}],"thread_ts":"1608974945.239300","reply_count":2,"reply_users_count":1,"latest_reply":"1608976653.239600","reply_users":["U01CQTKB86N"],"subscribed":false},{"client_msg_id":"9c05f336-e18f-4e66-8aa4-2dfba04dba95","type":"message","text":"Did something break with the new DataFrames and JSONTables? I haven't been able to create a DF using the example on the Readme for jtables, it returns a DF with one column and a dictionary on each row. Seems like I'm missing some extra step that wasn't required before, but I cannot figure it out.","user":"UCAFZ51L3","ts":"1609014572.247500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JfT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Did something break with the new DataFrames and JSONTables? I haven't been able to create a DF using the example on the Readme for jtables, it returns a DF with one column and a dictionary on each row. Seems like I'm missing some extra step that wasn't required before, but I cannot figure it out."}]}]}]},{"client_msg_id":"c0aec440-686b-4a38-ae00-815f41c18bc2","type":"message","text":"Is there a way to non-copy get an Arrow array from Python (via PyCall)? Asking because need rugged array provided by a python package","user":"UH8A351DJ","ts":"1609053755.250900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GBedn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to non-copy get an Arrow array from Python (via PyCall)? Asking because need rugged array provided by a python package"}]}]}]},{"client_msg_id":"766082a8-acb7-4c47-adba-f833fe2cbb8a","type":"message","text":"Is there any way to convert a 1D vector into a DataFrame in the latest DataFrames.jl? I get the error RROR: ArgumentError: 'Array{Float64,1}' iterates 'Float64' values, which doesn't satisfy the Tables.jl `AbstractRow` interface. And also when subsetting dataframes, how do I get the old behavior of it staying a DataFrame rather than a DataFrameRow? The DataFrameRow is not compatible with packages like MLJ.jl where I get the error \"Function 'matrix' only supports AbstractMatrix or containers implementing the Tables.jl interface\"","user":"U01EF0QVAB0","ts":"1609109966.253200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KvOj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there any way to convert a 1D vector into a DataFrame in the latest DataFrames.jl? I get the error RROR: ArgumentError: 'Array{Float64,1}' iterates 'Float64' values, which doesn't satisfy the Tables.jl `AbstractRow` interface. And also when subsetting dataframes, how do I get the old behavior of it staying a DataFrame rather than a DataFrameRow? The DataFrameRow is not compatible with packages like MLJ.jl where I get the error \"Function 'matrix' only supports AbstractMatrix or containers implementing the Tables.jl interface\""}]}]}]},{"client_msg_id":"df3d2b9a-6b64-424f-bea3-bdf953360adb","type":"message","text":"Right now I have to do a hacky approach which is create a matrix with something like df=zeros(1,100) and then do df=DataFrame(df) and then df[1,:] = ... . In cases where for example someone wants to test 1 thing in an MLJ.jl deployed model it can be an issue","user":"U01EF0QVAB0","ts":"1609110158.254000","team":"T68168MUP","edited":{"user":"U01EF0QVAB0","ts":"1609110249.000000"},"blocks":[{"type":"rich_text","block_id":"lnlo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Right now I have to do a hacky approach which is create a matrix with something like df=zeros(1,100) and then do df=DataFrame(df) and then df[1,:] = ... . In cases where for example someone wants to test 1 thing in an MLJ.jl deployed model it can be an issue"}]}]}]},{"client_msg_id":"743d9a60-541b-4f4b-ad21-f5420f9a20f7","type":"message","text":"Is it possible to read a bunch of Parquet files and write an Arrow file, or does it need to be Parquet -&gt; DataFrame -&gt; Arrow?","user":"U01CQTKB86N","ts":"1609144076.258000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Qd0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is it possible to read a bunch of Parquet files and write an Arrow file, or does it need to be Parquet -> DataFrame -> Arrow?"}]}]}],"thread_ts":"1609144076.258000","reply_count":3,"reply_users_count":1,"latest_reply":"1609144912.258500","reply_users":["U01CQTKB86N"],"subscribed":false},{"client_msg_id":"1ba7b96a-9486-47d0-a944-b40dd6cfd916","type":"message","text":"<@U681ELA87> would it be reasonable to &lt;<https://github.com/JuliaData/Arrow.jl/pull/89%7Cdefine> a default UUID &lt;-&gt; UInt128 type extension in Arrow.jl?&gt; That PR seems to work well for my downstream use cases, I can add tests if you think it's sensible :slightly_smiling_face:","user":"U674T0Y9Z","ts":"1609170631.262200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SyXwa","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U681ELA87"},{"type":"text","text":" would it be reasonable to "},{"type":"link","url":"https://github.com/JuliaData/Arrow.jl/pull/89","text":"define a default UUID <-> UInt128 type extension in Arrow.jl?"},{"type":"text","text":" That PR seems to work well for my downstream use cases, I can add tests if you think it's sensible "},{"type":"emoji","name":"slightly_smiling_face"}]}]}]},{"client_msg_id":"085a18ff-b49e-40dd-87a7-1794056397bb","type":"message","text":"So I noticed with the MLJ.jl package, it seems that when you use PCA through it, it can't take in Matrix and the object must be a DataFrame? But then when you use MLJ.transform(PC, X) then X can be either a DataFrame or Matrix but using a DataFrame appears to be extremely inefficient. As in taking 450 seconds as opposed to 11 s. If you put in a DataFrame, it seems to return a DataFrame but if you put in a Matrix it returns something else but that can be coerced to a DF then Matrix and give the right result and this was 11s . Seems strange - any ideas why its inconsistent like this where you have to fit the model on a DF but then its better to use transform on a matrix then coerce it? I was wondering how matrix multiplication (which is what PCA is after its been fit already) could be so slow. My Input to the PCA is 640 x 327.5K and output is 640 x 100","user":"U01EF0QVAB0","ts":"1609216849.271800","team":"T68168MUP","edited":{"user":"U01EF0QVAB0","ts":"1609217057.000000"},"blocks":[{"type":"rich_text","block_id":"gBr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So I noticed with the MLJ.jl package, it seems that when you use PCA through it, it can't take in Matrix and the object must be a DataFrame? But then when you use MLJ.transform(PC, X) then X can be either a DataFrame or Matrix but using a DataFrame appears to be extremely inefficient. As in taking 450 seconds as opposed to 11 s. If you put in a DataFrame, it seems to return a DataFrame but if you put in a Matrix it returns something else but that can be coerced to a DF then Matrix and give the right result and this was 11s . Seems strange - any ideas why its inconsistent like this where you have to fit the model on a DF but then its better to use transform on a matrix then coerce it? I was wondering how matrix multiplication (which is what PCA is after its been fit already) could be so slow. My Input to the PCA is 640 x 327.5K and output is 640 x 100"}]}]}]},{"client_msg_id":"a2193933-96c3-48a2-b32f-0a3b8af7a2ab","type":"message","text":"a question that has confused me for a while: What is SentinelArrays when we read in a data via CSV or Arrow, and should we convert it to regular Vector?","user":"USTUBS9ED","ts":"1609216919.273100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"H0vDa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"a question that has confused me for a while: What is SentinelArrays when we read in a data via CSV or Arrow, and should we convert it to regular Vector?"}]}]}]},{"client_msg_id":"5ca3681d-ad91-4847-a828-e2b5bff9963a","type":"message","text":"Hello,\nI'm trying to serialize a DataFrame with multiple columns types, including ZonedDateTime, with `Arrow.jl`. It appears ZonedDateTime has a (too) huge impact on performances as compared to simple DateTime, when serializing with Arrow.jl.\n\nAny idea why? Any remedy trick?\n\n```using DataFrames, Dates, TimeZones, Arrow, BenchmarkTools\n\n# Create dataframe\ndf_length = Int(1e3)\n\ndf = DataFrame(\n    int = rand(0:1000, long_test_df_length),\n    int_missing = rand(vcat(collect(0:10), [missing]), long_test_df_length),\n    cont = randn(long_test_df_length),\n    cont_missing = rand(vcat(randn(10), [missing]), long_test_df_length),\n    str = rand([\"a\", \"b\", \"c\", \"d\"], long_test_df_length),\n    str_missing = rand([\"a\", \"b\", \"c\", \"d\", missing], long_test_df_length),\n)\n\n# DF to Arrow in buffer\nserialize(df::DataFrames.AbstractDataFrame) = Arrow.write(IOBuffer(), df)\n\n# Add regular DateTime\ndf_with_dt = copy(df)\ninsertcols!(df_with_dt, \"date\" =&gt; collect(DateTime(2020, 1, 1 , 0, 0, 0):Second(1):(DateTime(2020, 1, 1, 0, 0, 0) + Second(df_length- 1))))\n\n# Add ZonedDateTime\ndf_with_zdt = copy(df)\ninsertcols!(df_with_zdt, \"date\" =&gt; collect(ZonedDateTime(2020, 1, 1 , 0, 0, 0, tz\"Europe/Paris\"):Second(1):(ZonedDateTime(2020, 1, 1, 0, 0, 0, tz\"Europe/Paris\") + Second(df_length- 1))))\n\n# Regular DateTime\n@btime serialize(df_with_dt)    # =&gt; 311.645 μs\n\n# ZonedDateTime\n@btime serialize(df_with_zdt)    # ==&gt; 1.480 s```","user":"UC2H100V8","ts":"1609239911.277400","team":"T68168MUP","edited":{"user":"UC2H100V8","ts":"1609240052.000000"},"blocks":[{"type":"rich_text","block_id":"6fzp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello,\nI'm trying to serialize a DataFrame with multiple columns types, including ZonedDateTime, with "},{"type":"text","text":"Arrow.jl","style":{"code":true}},{"type":"text","text":". It appears ZonedDateTime has a (too) huge impact on performances as compared to simple DateTime, when serializing with Arrow.jl.\n\nAny idea why? Any remedy trick?\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using DataFrames, Dates, TimeZones, Arrow, BenchmarkTools\n\n# Create dataframe\ndf_length = Int(1e3)\n\ndf = DataFrame(\n    int = rand(0:1000, long_test_df_length),\n    int_missing = rand(vcat(collect(0:10), [missing]), long_test_df_length),\n    cont = randn(long_test_df_length),\n    cont_missing = rand(vcat(randn(10), [missing]), long_test_df_length),\n    str = rand([\"a\", \"b\", \"c\", \"d\"], long_test_df_length),\n    str_missing = rand([\"a\", \"b\", \"c\", \"d\", missing], long_test_df_length),\n)\n\n# DF to Arrow in buffer\nserialize(df::DataFrames.AbstractDataFrame) = Arrow.write(IOBuffer(), df)\n\n# Add regular DateTime\ndf_with_dt = copy(df)\ninsertcols!(df_with_dt, \"date\" => collect(DateTime(2020, 1, 1 , 0, 0, 0):Second(1):(DateTime(2020, 1, 1, 0, 0, 0) + Second(df_length- 1))))\n\n# Add ZonedDateTime\ndf_with_zdt = copy(df)\ninsertcols!(df_with_zdt, \"date\" => collect(ZonedDateTime(2020, 1, 1 , 0, 0, 0, tz\"Europe/Paris\"):Second(1):(ZonedDateTime(2020, 1, 1, 0, 0, 0, tz\"Europe/Paris\") + Second(df_length- 1))))\n\n# Regular DateTime\n@btime serialize(df_with_dt)    # => 311.645 μs\n\n# ZonedDateTime\n@btime serialize(df_with_zdt)    # ==> 1.480 s"}]}]}]},{"client_msg_id":"237a8abb-e39c-4691-b12f-82f6571f02e9","type":"message","text":"Is anyone familiar with c++/CxxWrap here? I am trying to find a way to reuse c++ memory in Arrow.jl and I’m running into some roadblocks. Posting details in this thread","user":"U01GXNFKY6R","ts":"1609256006.279700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qbuvi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is anyone familiar with c++/CxxWrap here? I am trying to find a way to reuse c++ memory in Arrow.jl and I’m running into some roadblocks. Posting details in this thread"}]}]}],"thread_ts":"1609256006.279700","reply_count":7,"reply_users_count":2,"latest_reply":"1609258640.283800","reply_users":["U01GXNFKY6R","U681ELA87"],"subscribed":false},{"client_msg_id":"0a6b3eab-c31a-425a-ab52-e8d3f6ede59f","type":"message","text":"I think this blog post may be interesting to folks in this channel: <https://www.algebraicjulia.org/blog/post/2020/12/cset-conjunctive-queries/>\n\nIt’s about relational DB capabilities in pure Julia and builds on the Tables.jl ecosystem, particularly TypedTables.jl","user":"UQE0NBSS1","ts":"1609269892.287400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"x9T","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think this blog post may be interesting to folks in this channel: "},{"type":"link","url":"https://www.algebraicjulia.org/blog/post/2020/12/cset-conjunctive-queries/"},{"type":"text","text":"\n\nIt’s about relational DB capabilities in pure Julia and builds on the Tables.jl ecosystem, particularly TypedTables.jl"}]}]}],"reactions":[{"name":"+1","users":["U681ELA87","U01FAHWCMFF","UDGT4PM41","UPH1M2MB2"],"count":4}]},{"client_msg_id":"5d0b92bf-59ff-4475-b9ab-d367e709b63f","type":"message","text":"I have written another longer blog-post for the new year (not using DataFrames.jl much, but still I found it useful): <https://bkamins.github.io/julialang/2020/12/31/randu.html>","user":"U8JAMQGQY","ts":"1609407463.292200","team":"T68168MUP","attachments":[{"service_name":"Blog by Bogumił Kamiński","title":"Dissecting RANDU","title_link":"https://bkamins.github.io/julialang/2020/12/31/randu.html","text":"Introduction","fallback":"Blog by Bogumił Kamiński: Dissecting RANDU","ts":1609391622,"from_url":"https://bkamins.github.io/julialang/2020/12/31/randu.html","service_icon":"https://bkamins.github.io/favicon.ico","id":1,"original_url":"https://bkamins.github.io/julialang/2020/12/31/randu.html"}],"blocks":[{"type":"rich_text","block_id":"Zih23","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have written another longer blog-post for the new year (not using DataFrames.jl much, but still I found it useful): "},{"type":"link","url":"https://bkamins.github.io/julialang/2020/12/31/randu.html"}]}]}]},{"client_msg_id":"d52bdbdd-1bc6-4091-85dc-bd2004209ada","type":"message","text":"Happy new year, everyone.","user":"U012UUNBFM0","ts":"1609520816.296400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+VXL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Happy new year, everyone."}]}]}]},{"client_msg_id":"0ae910a9-504d-43d0-8da0-e7e1d66ff1b3","type":"message","text":"Simple DifferentialEquations.jl example, anyone know why the Tables.jl traits don't work on DataFrames.jl anymore?","user":"U69BL50BF","ts":"1609544261.297500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"A7h","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Simple DifferentialEquations.jl example, anyone know why the Tables.jl traits don't work on DataFrames.jl anymore?"}]}]}]},{"client_msg_id":"21fe07ae-0318-49ca-a78c-456d14b5d091","type":"message","text":"```using DifferentialEquations\nusing DataFrames\n\ntest = (du,u,p,t) -&gt; du .= 1.01u\nu0  =   [7.9717478717769090e+000,\n        1.5105962705116731e+001]\n#prob = ODEProblem(test, rand(2,2), (-6.,100.))\nprob = ODEProblem(test, u0, (-6.,100.))\nsol = solve(prob)\ndf = DataFrame(sol)```","user":"U69BL50BF","ts":"1609544262.297700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"To3a","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using DifferentialEquations\nusing DataFrames\n\ntest = (du,u,p,t) -> du .= 1.01u\nu0  =   [7.9717478717769090e+000,\n        1.5105962705116731e+001]\n#prob = ODEProblem(test, rand(2,2), (-6.,100.))\nprob = ODEProblem(test, u0, (-6.,100.))\nsol = solve(prob)\ndf = DataFrame(sol)"}]}]}]},{"client_msg_id":"a19f73c8-b865-456e-b561-eed2be855fab","type":"message","text":"Using matrix initial conditions works though:","user":"U69BL50BF","ts":"1609544639.297900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WCmy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Using matrix initial conditions works though:"}]}]}]},{"client_msg_id":"9c557a80-9055-49c2-b341-adc0de1fc963","type":"message","text":"```prob = ODEProblem(test, rand(2,2), (-6.,100.))\nsol = solve(prob)\ndf = DataFrame(sol)```","user":"U69BL50BF","ts":"1609544641.298200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"AZO5","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"prob = ODEProblem(test, rand(2,2), (-6.,100.))\nsol = solve(prob)\ndf = DataFrame(sol)"}]}]}]},{"client_msg_id":"f3e51d0b-878b-4e2d-ad74-efa6491657ac","type":"message","text":"So I guess the question is, why would changing tensor dimension change whether or not the overloads get called?","user":"U69BL50BF","ts":"1609544718.298600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3WydV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So I guess the question is, why would changing tensor dimension change whether or not the overloads get called?"}]}]}]},{"client_msg_id":"3a64138b-9378-46de-bbcb-956ce82680e2","type":"message","text":"there's no dispatching on the tensor dimensions at all in the table trait code I did","user":"U69BL50BF","ts":"1609544743.299000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lbAfT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"there's no dispatching on the tensor dimensions at all in the table trait code I did"}]}]}]},{"client_msg_id":"16e731b2-081d-4aa3-b8ce-aba811cbf574","type":"message","text":"<https://github.com/SciML/DiffEqBase.jl/blob/master/src/tabletraits.jl>","user":"U69BL50BF","ts":"1609544770.299200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"s8k+","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/SciML/DiffEqBase.jl/blob/master/src/tabletraits.jl"}]}]}]},{"client_msg_id":"95a687b0-e3b6-4d8f-833d-bb0f6bfb671f","type":"message","text":"Maybe it’s just missing the required function `Tables.istable`? <https://tables.juliadata.org/stable/#Implementing-the-Interface-(i.e.-becoming-a-Tables.jl-source)-1>","user":"U8T0YV7QC","ts":"1609544977.299900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gI3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Maybe it’s just missing the required function "},{"type":"text","text":"Tables.istable","style":{"code":true}},{"type":"text","text":"? "},{"type":"link","url":"https://tables.juliadata.org/stable/#Implementing-the-Interface-(i.e.-becoming-a-Tables.jl-source)-1"}]}]}]},{"client_msg_id":"6ac67185-0e8b-4e3a-8f8c-6a35acb9a84e","type":"message","text":"Nvm…. it seems to be automatic according to this <https://tables.juliadata.org/stable/#Tables.isrowtable>","user":"U8T0YV7QC","ts":"1609545099.300300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"39eJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Nvm…. it seems to be automatic according to this "},{"type":"link","url":"https://tables.juliadata.org/stable/#Tables.isrowtable"}]}]}]},{"client_msg_id":"a24cb3f4-a06e-41de-bbeb-37aa4664ddc0","type":"message","text":"I am assuming you are on DataFrames 0.22 and Tables 1.2.2. Start Julia with `depwarn=yes`. The reason is that temporarily - in DataFrames.jl 0.22 - we keep old code for `AbstractMatrix` handling, and `sol` is an `AbstractMatrix` (in 1.0 which, will be released soon, this deprecation will be removed - if something is a Tables.jl table it will be treated as such even if it is an `AbstractMatrix`).\n\nAdditionally - your code has a problem - not related with DataFrames.jl, but Tables.jl integration. In the case that `sol` has three dimensions then e.g. `Tables.columns(sol)` works correctly. Wihle when `sol` is two dimensional you get an error than `sol` is not a table.\n\nIn conclusion:\n1. till DataFrames.jl is released the `AbstractMatrix` case is special cased, and you have to write `DataFrames(Tables.columns(sol))` to have things working under DataFrames.jl 0.22 (under DataFrames.jl 1.0 just what you have will work);\n2. In order for things to work you should make your `sol` compliant with Tables.jl API. The problem is that `Tables.columns` does not allow `AbstractMatrix` as an argument by default (see line 5 in matrix.jl in Tables.jl implementation). This needs to be overriden for your type","user":"U8JAMQGQY","ts":"1609546142.306700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wwdqn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am assuming you are on DataFrames 0.22 and Tables 1.2.2. Start Julia with "},{"type":"text","text":"depwarn=yes","style":{"code":true}},{"type":"text","text":". The reason is that temporarily - in DataFrames.jl 0.22 - we keep old code for "},{"type":"text","text":"AbstractMatrix","style":{"code":true}},{"type":"text","text":" handling, and "},{"type":"text","text":"sol","style":{"code":true}},{"type":"text","text":" is an "},{"type":"text","text":"AbstractMatrix","style":{"code":true}},{"type":"text","text":" (in 1.0 which, will be released soon, this deprecation will be removed - if something is a Tables.jl table it will be treated as such even if it is an "},{"type":"text","text":"AbstractMatrix","style":{"code":true}},{"type":"text","text":").\n\nAdditionally - your code has a problem - not related with DataFrames.jl, but Tables.jl integration. In the case that "},{"type":"text","text":"sol","style":{"code":true}},{"type":"text","text":" has three dimensions then e.g. "},{"type":"text","text":"Tables.columns(sol)","style":{"code":true}},{"type":"text","text":" works correctly. Wihle when "},{"type":"text","text":"sol","style":{"code":true}},{"type":"text","text":" is two dimensional you get an error than "},{"type":"text","text":"sol","style":{"code":true}},{"type":"text","text":" is not a table.\n\nIn conclusion:\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"till DataFrames.jl is released the "},{"type":"text","text":"AbstractMatrix","style":{"code":true}},{"type":"text","text":" case is special cased, and you have to write "},{"type":"text","text":"DataFrames(Tables.columns(sol))","style":{"code":true}},{"type":"text","text":" to have things working under DataFrames.jl 0.22 (under DataFrames.jl 1.0 just what you have will work);"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"In order for things to work you should make your "},{"type":"text","text":"sol","style":{"code":true}},{"type":"text","text":" compliant with Tables.jl API. The problem is that "},{"type":"text","text":"Tables.columns","style":{"code":true}},{"type":"text","text":" does not allow "},{"type":"text","text":"AbstractMatrix","style":{"code":true}},{"type":"text","text":" as an argument by default (see line 5 in matrix.jl in Tables.jl implementation). This needs to be overriden for your type"}]}],"style":"ordered","indent":0}]}],"thread_ts":"1609546142.306700","reply_count":7,"reply_users_count":2,"latest_reply":"1609548126.308000","reply_users":["U8JAMQGQY","U69BL50BF"],"subscribed":false},{"client_msg_id":"3dabac93-f1af-4ee1-ab96-f69e91b0b83b","type":"message","text":"It seems to me that Parquet.jl doesn’t work as documented. It says in the readme that:\n```# `tbl` is a Tables.jl compatible table\ntbl = read_parquet(path_to_parquet_file)```\nHowever, it doesn’t satisfy the Tables.jl interface:\n```using Parquet, DataFrames\ndf = DataFrame(ints = rand(Int, 100))\nwrite_parquet(\"tmp.parquet\", df)\ntbl = read_parquet(\"tmp.parquet\")\nTables.istable(tbl)```\nreturns `false`.","user":"U01CQTKB86N","ts":"1609582265.310000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HJsRf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It seems to me that Parquet.jl doesn’t work as documented. It says in the readme that:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"# `tbl` is a Tables.jl compatible table\ntbl = read_parquet(path_to_parquet_file)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"However, it doesn’t satisfy the Tables.jl interface:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Parquet, DataFrames\ndf = DataFrame(ints = rand(Int, 100))\nwrite_parquet(\"tmp.parquet\", df)\ntbl = read_parquet(\"tmp.parquet\")\nTables.istable(tbl)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"returns "},{"type":"text","text":"false","style":{"code":true}},{"type":"text","text":"."}]}]}]},{"client_msg_id":"716d6c6a-f134-40cd-a875-9c68e0934ef1","type":"message","text":"Arrow.jl does support Tables.jl interface, but it is missing from the list of integrations: <https://github.com/JuliaData/Tables.jl/blob/main/INTEGRATIONS.md>","user":"U01CQTKB86N","ts":"1609582838.310600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1ik","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Arrow.jl does support Tables.jl interface, but it is missing from the list of integrations: "},{"type":"link","url":"https://github.com/JuliaData/Tables.jl/blob/main/INTEGRATIONS.md"}]}]}]},{"client_msg_id":"affb3832-d195-419e-a8f5-7b7ac2aca98b","type":"message","text":"is there a reason why JSONTables doesn't integrate into JSON3/StructTypes for DataFrame parsing?","user":"UMWFZF5DW","ts":"1609724278.324500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fbij","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is there a reason why JSONTables doesn't integrate into JSON3/StructTypes for DataFrame parsing?"}]}]}],"thread_ts":"1609724278.324500","reply_count":5,"reply_users_count":2,"latest_reply":"1609725936.327200","reply_users":["U681ELA87","UMWFZF5DW"],"subscribed":false},{"client_msg_id":"7d0366c8-be3b-4565-b7e9-a632641e68cc","type":"message","text":"<https://github.com/JuliaData/Arrow.jl/pull/94|Arrow.jl: bump Project.toml to v1.1.0> bumping the bump :grin: if any Arrow.jl maintainers are around","user":"U674T0Y9Z","ts":"1609809872.330400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"N2zQL","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/JuliaData/Arrow.jl/pull/94","text":"Arrow.jl: bump Project.toml to v1.1.0"},{"type":"text","text":" bumping the bump "},{"type":"emoji","name":"grin"},{"type":"text","text":" if any Arrow.jl maintainers are around"}]}]}],"thread_ts":"1609809872.330400","reply_count":1,"reply_users_count":1,"latest_reply":"1609812360.330700","reply_users":["U681ELA87"],"subscribed":false},{"client_msg_id":"e40c3f99-f73f-4db4-a19f-6704b12f6880","type":"message","text":"how do i get the `Pr(|z|)`  from a glm fit with GLM.jl\n```Coefficients:\n────────────────────────────────────────────────────────────────────────────────\n                       Coef.  Std. Error     z  Pr(&gt;|z|)   Lower 95%   Upper 95%\n────────────────────────────────────────────────────────────────────────────────\n(Intercept)      1.88912      0.243939    7.74    &lt;1e-14   1.41101    2.36723\nsame_rule_score  0.000589754  0.00112245  0.53    0.5993  -0.0016102  0.00278971\n────────────────────────────────────────────────────────────────────────────────```","user":"U017JTQFNEQ","ts":"1609838555.331500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"klH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"how do i get the "},{"type":"text","text":"Pr(|z|)","style":{"code":true}},{"type":"text","text":"  from a glm fit with GLM.jl\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Coefficients:\n────────────────────────────────────────────────────────────────────────────────\n                       Coef.  Std. Error     z  Pr(>|z|)   Lower 95%   Upper 95%\n────────────────────────────────────────────────────────────────────────────────\n(Intercept)      1.88912      0.243939    7.74    <1e-14   1.41101    2.36723\nsame_rule_score  0.000589754  0.00112245  0.53    0.5993  -0.0016102  0.00278971\n────────────────────────────────────────────────────────────────────────────────"}]}]}],"thread_ts":"1609838555.331500","reply_count":8,"reply_users_count":2,"latest_reply":"1609838899.333000","reply_users":["U017JTQFNEQ","U67431ELR"],"subscribed":false},{"client_msg_id":"a732e621-a6f4-4e05-9766-53e5a3741123","type":"message","text":"I'm trying to document some nice DataFrames-based patterns for manipulating <https://github.com/beacon-biosignals/Onda.jl|Onda> tabular data and am looking for a concise/performant way to express the following operation:\n\n```\"\"\"\n    f(df)::DataFrame\n\nReturns the rows for which `y - x` is largest in `groupby(df, :id)`\n\"\"\"\nfunction f(df)\n    groups = groupby(transform(df, [:x, :y] =&gt; ((x, y) -&gt; y .- x) =&gt; :n), :id)\n    return DataFrame(map(group -&gt; first(sort(group, :n)), groups))\nend\n\nid = rand(1:10, 100)\nx = rand(0:100, 100)\ny = x .+ rand(0:100, 100)\nz = rand('a':'z', 100)\ndf = DataFrame(; id, x, y, z)\nf(df)```\nI feel like my `f` here is pretty ugly/slow compared to the \"right\" solution :grin:\n\n(my specific use case is grabbing the longest signal in a recording, but I've abstracted that away here for the sake of example)","user":"U674T0Y9Z","ts":"1609877723.335100","team":"T68168MUP","edited":{"user":"U674T0Y9Z","ts":"1609877741.000000"},"blocks":[{"type":"rich_text","block_id":"g2RLL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm trying to document some nice DataFrames-based patterns for manipulating "},{"type":"link","url":"https://github.com/beacon-biosignals/Onda.jl","text":"Onda"},{"type":"text","text":" tabular data and am looking for a concise/performant way to express the following operation:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"\"\"\"\n    f(df)::DataFrame\n\nReturns the rows for which `y - x` is largest in `groupby(df, :id)`\n\"\"\"\nfunction f(df)\n    groups = groupby(transform(df, [:x, :y] => ((x, y) -> y .- x) => :n), :id)\n    return DataFrame(map(group -> first(sort(group, :n)), groups))\nend\n\nid = rand(1:10, 100)\nx = rand(0:100, 100)\ny = x .+ rand(0:100, 100)\nz = rand('a':'z', 100)\ndf = DataFrame(; id, x, y, z)\nf(df)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nI feel like my `f` here is pretty ugly/slow compared to the \"right\" solution "},{"type":"emoji","name":"grin"},{"type":"text","text":"\n\n(my specific use case is grabbing the longest signal in a recording, but I've abstracted that away here for the sake of example)"}]}]}]},{"client_msg_id":"00990172-af03-465e-9845-4db7f6464b4b","type":"message","text":"I’m trying to import a file into a DataFrame while specifying the types for each column. I’ve attempted to do this with a type dictionary using the code below but it isn’t working (instead the types are being automatically assigned). How can I fix this?\n```using CSV, DataFrames\n\ntype123 = Dict(\n    :Date =&gt; String,\n    :Month =&gt; Int64,\n    :Year =&gt; Int64,\n    );\n\ndf = DataFrame(CSV.File(\"file.txt\"; header=1, delim=',', types=type123))```\nThe following method works, when specifying the types explicitly in the DataFrame command as opposed to using a type dictionary (but I’d prefer to use a type dictionary).\n```using CSV, DataFrames\n\ndf = DataFrame(CSV.File(\"file.txt\"; header=1, delim=',', types=[String, Int, Int]))```","user":"ULDJ1UC93","ts":"1609941033.338700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Hkpa=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’m trying to import a file into a DataFrame while specifying the types for each column. I’ve attempted to do this with a type dictionary using the code below but it isn’t working (instead the types are being automatically assigned). How can I fix this?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using CSV, DataFrames\n\ntype123 = Dict(\n    :Date => String,\n    :Month => Int64,\n    :Year => Int64,\n    );\n\ndf = DataFrame(CSV.File(\"file.txt\"; header=1, delim=',', types=type123))"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"The following method works, when specifying the types explicitly in the DataFrame command as opposed to using a type dictionary (but I’d prefer to use a type dictionary).\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using CSV, DataFrames\n\ndf = DataFrame(CSV.File(\"file.txt\"; header=1, delim=',', types=[String, Int, Int]))"}]}]}],"thread_ts":"1609941033.338700","reply_count":2,"reply_users_count":2,"latest_reply":"1609941273.339000","reply_users":["U67431ELR","ULDJ1UC93"],"subscribed":false},{"client_msg_id":"0ba981de-3d3b-4eb0-88ba-76113a04de05","type":"message","text":"Is there a nice way to do the equivalent of\n```combine(pairs(groupby(df, x))) do key, subdf```\n?","user":"U680THK2S","ts":"1609960969.340000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"BLJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a nice way to do the equivalent of\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"combine(pairs(groupby(df, x))) do key, subdf"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"?"}]}]}]},{"client_msg_id":"b49f723b-803c-4468-ad5c-2898bb7b2d63","type":"message","text":"Since `combine` doesn't currently allow that","user":"U680THK2S","ts":"1609960981.340200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7DD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Since "},{"type":"text","text":"combine","style":{"code":true}},{"type":"text","text":" doesn't currently allow that"}]}]}]},{"client_msg_id":"8bf8d364-461d-4cfa-8a2d-85d996cb0616","type":"message","text":"Does anyone have any interesting workflows/packages for for \"timeseries\" data?  I've been largely using a regular DataFrame with a sorted `:date` column that I treat as an index, and just Base functions from `Dates`.  This seems pretty functional - there's a bit of cognitive book-keeping, remembering the column name, keeping it sorted, etc.  I was curious if anyone had found a more specialized structure like `TimeSeries.jl`  worth it in practice.","user":"U011QC7QLPL","ts":"1609970140.345100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dsK0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does anyone have any interesting workflows/packages for for \"timeseries\" data?  I've been largely using a regular DataFrame with a sorted "},{"type":"text","text":":date","style":{"code":true}},{"type":"text","text":" column that I treat as an index, and just Base functions from "},{"type":"text","text":"Dates","style":{"code":true}},{"type":"text","text":".  This seems pretty functional - there's a bit of cognitive book-keeping, remembering the column name, keeping it sorted, etc.  I was curious if anyone had found a more specialized structure like "},{"type":"text","text":"TimeSeries.jl","style":{"code":true}},{"type":"text","text":"  worth it in practice."}]}]}]},{"client_msg_id":"33af9d7d-29ce-4b89-bda6-904e98aedaec","type":"message","text":"Hello, I think I might be using the bson format in Julia incorrectly but I am not sure how to do it properly:  I save my data structured as `Dict(:data=&gt;data::Array{MyStruct, 1})`  into `.bson`  file. However, when I load back the file, the type of data changes to `Array{Any, 1}` . For now I resolved this by repackaging the array with `data = [data...]`  when loading from file, which gives the data the expected type `Array{MyStruct, 1}` . What's the proper way to address this?","user":"UTMTRHLHY","ts":"1610033720.352100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mT4zj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello, I think I might be using the bson format in Julia incorrectly but I am not sure how to do it properly:  I save my data structured as "},{"type":"text","text":"Dict(:data=>data::Array{MyStruct, 1})","style":{"code":true}},{"type":"text","text":"  into "},{"type":"text","text":".bson","style":{"code":true}},{"type":"text","text":"  file. However, when I load back the file, the type of data changes to "},{"type":"text","text":"Array{Any, 1}","style":{"code":true}},{"type":"text","text":" . For now I resolved this by repackaging the array with "},{"type":"text","text":"data = [data...]","style":{"code":true}},{"type":"text","text":"  when loading from file, which gives the data the expected type "},{"type":"text","text":"Array{MyStruct, 1}","style":{"code":true}},{"type":"text","text":" . What's the proper way to address this?"}]}]}]},{"client_msg_id":"1afc8831-3185-41ca-9040-11e26f820283","type":"message","text":"I have some messy data, and I’m not quite sure what to do with it. It has many column names that are long questions.\nI guess the column names should become shorter acronyms and keys in a dict, and the questions values in the dict?","user":"U01CQTKB86N","ts":"1610048189.355300","team":"T68168MUP","edited":{"user":"U01CQTKB86N","ts":"1610048343.000000"},"blocks":[{"type":"rich_text","block_id":"x5sn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have some messy data, and I’m not quite sure what to do with it. It has many column names that are long questions.\nI guess the column names should become shorter acronyms and keys in a dict, and the questions values in the dict?"}]}]}],"thread_ts":"1610048189.355300","reply_count":1,"reply_users_count":1,"latest_reply":"1610048660.355600","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"7b0c6faf-6c6b-4549-a7a5-8a5424e923a1","type":"message","text":"Is it possible to read a csv.gz file in a \"streaming\" fashion? That is, without loading the whole uncompressed file as a bytes array. As I understand, the example from CSV.jl docs `CSV.File(transcode(GzipDecompressor, Mmap.mmap(\"a.csv.gz\"))) |&gt; DataFrame` materializes complete uncompressed file in memory.","user":"UGTUKUHLN","ts":"1610052147.358400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lWDY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is it possible to read a csv.gz file in a \"streaming\" fashion? That is, without loading the whole uncompressed file as a bytes array. As I understand, the example from CSV.jl docs "},{"type":"text","text":"CSV.File(transcode(GzipDecompressor, Mmap.mmap(\"a.csv.gz\"))) |> DataFrame","style":{"code":true}},{"type":"text","text":" materializes complete uncompressed file in memory."}]}]}],"thread_ts":"1610052147.358400","reply_count":4,"reply_users_count":2,"latest_reply":"1610052814.359200","reply_users":["U681ELA87","UGTUKUHLN"],"subscribed":false},{"client_msg_id":"cf1d3b51-94d5-4c4c-ad59-79a17891a0a0","type":"message","text":"I asked this over in <#C6A044SQH|helpdesk> but didn’t get any responses so thought I’d try my luck in here…\nI’m trying to import a file into a DataFrame using CSV.File. I have the following txt file:\n```Date, Time, Colour\n2019/7/21, 14:40:15, red\n2019/7/22, 14:40:15.001, yellow\n2019/7/23, 14:40:15.002, green```\nI can’t import this file using the below Julia code. However, I can if I change the type for column 2 to ‘string’ instead of ‘time’. It seems that I can’t set the type for column 2 to ‘time’ because I’m using a custom Dates.DateFormat for column 1.\nWhen I don’t specify any types for each column Julia automatically correctly applies the ‘time’ type to column 2 (and ‘string’ to column 1 which I don’t want).\nHow can I fix this? Is it possible to specify two custom Dates.DateFormat?\n```using CSV, Dates, DataFrames\n\nfiletypes = Dict(\n    1 =&gt; Date,\n    2 =&gt; Time,\n    3 =&gt; String,\n    );\n\nfiledate = Dates.DateFormat(\"y/m/d\");\n\ndf = DataFrame(CSV.File(\"file.txt\"; header=1, delim=',', dateformat=filedate, types=filetypes))```","user":"ULDJ1UC93","ts":"1610065250.360100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pdPX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I asked this over in "},{"type":"channel","channel_id":"C6A044SQH"},{"type":"text","text":" but didn’t get any responses so thought I’d try my luck in here…\nI’m trying to import a file into a DataFrame using CSV.File. I have the following txt file:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Date, Time, Colour\n2019/7/21, 14:40:15, red\n2019/7/22, 14:40:15.001, yellow\n2019/7/23, 14:40:15.002, green"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I can’t import this file using the below Julia code. However, I can if I change the type for column 2 to ‘string’ instead of ‘time’. It seems that I can’t set the type for column 2 to ‘time’ because I’m using a custom Dates.DateFormat for column 1.\nWhen I don’t specify any types for each column Julia automatically correctly applies the ‘time’ type to column 2 (and ‘string’ to column 1 which I don’t want).\nHow can I fix this? Is it possible to specify two custom Dates.DateFormat?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using CSV, Dates, DataFrames\n\nfiletypes = Dict(\n    1 => Date,\n    2 => Time,\n    3 => String,\n    );\n\nfiledate = Dates.DateFormat(\"y/m/d\");\n\ndf = DataFrame(CSV.File(\"file.txt\"; header=1, delim=',', dateformat=filedate, types=filetypes))"}]}]}],"thread_ts":"1610065250.360100","reply_count":1,"reply_users_count":1,"latest_reply":"1610066661.360200","reply_users":["U680THK2S"],"subscribed":false},{"client_msg_id":"937201fd-1ccb-4ee2-bd0d-f7ed34ed495c","type":"message","text":"This is probably simple, but I'm still struggling with it. I have a DataFrame, and one of the columns is 'policy' which contains strings that act as categories for the other columns. The trouble is the data is inconsistent. I have a list of policies that I want, such as 'Suicide', 'Self Harm', 'Bullying' and so on (this is about child protection). The problem is that the data I get allows users to make up their own synonyms. eg 'Eating Disorder', 'Eating Disorder V5.0', 'Eating Disorder (m)', etc. I want to go through and rename the variants into the standard policy name, in this case 'Eating Disorder'.  In python/pandas I do the following\n\n    df.loc[(df['policy'].str.contains(\"Drug\")), \"policy\"] = 'Drugs &amp; Substance Misuse'\n    df.loc[(df['policy'].str.contains(\"Groom\")), \"policy\"] = 'Grooming'\n    df.loc[(df['policy'].str.contains(\"Eating\")), \"policy\"] = 'Eating Disorders'\n\nand so on. But how do I do this in DataFrames?","user":"U01GFAJRZ44","ts":"1610116528.366200","team":"T68168MUP","edited":{"user":"U01GFAJRZ44","ts":"1610116582.000000"},"blocks":[{"type":"rich_text","block_id":"I9K3h","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This is probably simple, but I'm still struggling with it. I have a DataFrame, and one of the columns is 'policy' which contains strings that act as categories for the other columns. The trouble is the data is inconsistent. I have a list of policies that I want, such as 'Suicide', 'Self Harm', 'Bullying' and so on (this is about child protection). The problem is that the data I get allows users to make up their own synonyms. eg 'Eating Disorder', 'Eating Disorder V5.0', 'Eating Disorder (m)', etc. I want to go through and rename the variants into the standard policy name, in this case 'Eating Disorder'.  In python/pandas I do the following\n\n    df.loc[(df['policy'].str.contains(\"Drug\")), \"policy\"] = 'Drugs & Substance Misuse'\n    df.loc[(df['policy'].str.contains(\"Groom\")), \"policy\"] = 'Grooming'\n    df.loc[(df['policy'].str.contains(\"Eating\")), \"policy\"] = 'Eating Disorders'\n\nand so on. But how do I do this in DataFrames?"}]}]}]},{"client_msg_id":"4406005d-f0b5-46dd-8c49-0b0bd929da78","type":"message","text":"```df.policy[contains.(df.policy, \"Drug\")] .= \"Drugs &amp; Substance Misuse\"```\n","user":"U6740K1SP","ts":"1610116856.367800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"EQ3f","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"df.policy[contains.(df.policy, \"Drug\")] .= \"Drugs & Substance Misuse\""}]},{"type":"rich_text_section","elements":[]}]}],"thread_ts":"1610116856.367800","reply_count":5,"reply_users_count":2,"latest_reply":"1610119126.369100","reply_users":["U01GFAJRZ44","U6740K1SP"],"subscribed":false},{"client_msg_id":"a8180445-50e5-47a9-adbe-d59d389ff05f","type":"message","text":"as simple as that? Excellent, thanks!","user":"U01GFAJRZ44","ts":"1610116992.368100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"X8t","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"as simple as that? Excellent, thanks!"}]}]}],"reactions":[{"name":"tada","users":["U68M6ERG8"],"count":1}]},{"client_msg_id":"ba81510e-1cd2-4611-a382-adfa3ebfb799","type":"message","text":"This week I thought of posting about type-stability in DataFrames.jl. This is a pretty standard subject, but questions regarding it are repeating quite often, so I thought of summarizing the major considerations involved in the design. <https://bkamins.github.io/julialang/2021/01/08/typestable.html>","user":"U8JAMQGQY","ts":"1610122687.372500","team":"T68168MUP","attachments":[{"service_name":"Blog by Bogumił Kamiński","title":"Why DataFrame is not type stable and when it matters","title_link":"https://bkamins.github.io/julialang/2021/01/08/typestable.html","text":"Introduction","fallback":"Blog by Bogumił Kamiński: Why DataFrame is not type stable and when it matters","ts":1610115095,"from_url":"https://bkamins.github.io/julialang/2021/01/08/typestable.html","service_icon":"https://bkamins.github.io/favicon.ico","id":1,"original_url":"https://bkamins.github.io/julialang/2021/01/08/typestable.html"}],"blocks":[{"type":"rich_text","block_id":"yGg02","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This week I thought of posting about type-stability in DataFrames.jl. This is a pretty standard subject, but questions regarding it are repeating quite often, so I thought of summarizing the major considerations involved in the design. "},{"type":"link","url":"https://bkamins.github.io/julialang/2021/01/08/typestable.html"}]}]}],"reactions":[{"name":"heart","users":["U681ELA87","UB0G4DFEK","U66M57AN4","UBF9YRB6H","UAVMYR0F4","U9VG1AYSG"],"count":6}]},{"client_msg_id":"bb2f1bb1-6aaa-4c71-9014-28ab56ef89de","type":"message","text":"that's a useful article thanks, I suggest that you put a link to it in the DataFrames docs","user":"U9VG1AYSG","ts":"1610124070.374600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QBTkO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"that's a useful article thanks, I suggest that you put a link to it in the DataFrames docs"}]}]}]},{"client_msg_id":"23fa5461-75ae-48de-bef5-d5755c63778f","type":"message","text":"The blog post about DataFrames.jl minilanguage is for me much better than the official docs, i.e. excellent, so link to that too maybe? :slightly_smiling_face:","user":"U01CQTKB86N","ts":"1610127682.376100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yvD==","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The blog post about DataFrames.jl minilanguage is for me much better than the official docs, i.e. excellent, so link to that too maybe? "},{"type":"emoji","name":"slightly_smiling_face"}]}]}]},{"client_msg_id":"cd5a4bd4-6f8e-46e0-bf03-5504747a6463","type":"message","text":"At this point we could just redirect <https://juliadata.github.io/DataFrames.jl/> to <https://bkamins.github.io/julialang/>. :smile:","user":"U67431ELR","ts":"1610127734.376700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"cjrO1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"At this point we could just redirect "},{"type":"link","url":"https://juliadata.github.io/DataFrames.jl/"},{"type":"text","text":" to "},{"type":"link","url":"https://bkamins.github.io/julialang/"},{"type":"text","text":". "},{"type":"emoji","name":"smile"}]}]}],"reactions":[{"name":"smile","users":["U680THK2S"],"count":1}]},{"client_msg_id":"b0f34738-d04e-46da-9c51-1df5c1d7ecaf","type":"message","text":"More seriously, I'd rather add missing parts to the manual than add links to blog posts.","user":"U67431ELR","ts":"1610127765.377300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UbiJM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"More seriously, I'd rather add missing parts to the manual than add links to blog posts."}]}]}],"reactions":[{"name":"point_up","users":["U681ELA87"],"count":1}]},{"client_msg_id":"941eaee2-f7b0-4e48-8a34-b260ceaf79b8","type":"message","text":"That first link gives me 404","user":"U01CQTKB86N","ts":"1610128272.377700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TEa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That first link gives me 404"}]}]}],"thread_ts":"1610128272.377700","reply_count":2,"reply_users_count":2,"latest_reply":"1610128500.378000","reply_users":["U67431ELR","U01CQTKB86N"],"subscribed":false},{"client_msg_id":"1725fba4-d976-4289-b78b-00cc8c199bc6","type":"message","text":"I tried to find a way to apply select with a transformation to a dataframe’s columns 7:32 except column 8. I failed. I even found a package, InvertedIndices, but I don’t think it would have helped either. I got it to work by first re-ordering.\n(Actually I would have liked to apply it to all columns with a column name that’s a symbol starting with the letter ‘Q’ and the column data type would be String.)\nAnyway this worked after the re-ordering:\n```df2 = select(df, \n\t1:6, \n\tnames(df, 7:32) .=&gt; ByRow(string_with_na_to_float),\n\trenamecols=false)```","user":"U01CQTKB86N","ts":"1610129409.382100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=uRrM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I tried to find a way to apply select with a transformation to a dataframe’s columns 7:32 except column 8. I failed. I even found a package, InvertedIndices, but I don’t think it would have helped either. I got it to work by first re-ordering.\n(Actually I would have liked to apply it to all columns with a column name that’s a symbol starting with the letter ‘Q’ and the column data type would be String.)\nAnyway this worked after the re-ordering:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"df2 = select(df, \n\t1:6, \n\tnames(df, 7:32) .=> ByRow(string_with_na_to_float),\n\trenamecols=false)"}]}]}],"thread_ts":"1610129409.382100","reply_count":9,"reply_users_count":2,"latest_reply":"1610130519.384000","reply_users":["U8JAMQGQY","U01CQTKB86N"],"subscribed":false},{"client_msg_id":"546e0e80-3ada-4b9a-b88d-a8b7a318e021","type":"message","text":"in DataFrames.jl package how can we look for unique values in our Dataset?\nLike if I talk about Pandas then we have a function to print the unique values\n```#Looking for unique values\nprint(dataset.nunique)```\n","user":"U0140JW97SP","ts":"1610134349.385300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ityk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"in DataFrames.jl package how can we look for unique values in our Dataset?\nLike if I talk about Pandas then we have a function to print the unique values\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"#Looking for unique values\nprint(dataset.nunique)"}]},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"6ce39581-1b7a-434e-9ac2-401ddb3d261c","type":"message","text":"`unique(df)` or `unique(df, [cols...])`","user":"U8JP5B9T2","ts":"1610134499.386500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qqc7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"unique(df)","style":{"code":true}},{"type":"text","text":" or "},{"type":"text","text":"unique(df, [cols...])","style":{"code":true}}]}]}]},{"client_msg_id":"acf604ff-c48a-4f98-b24a-249c12a1e8a6","type":"message","text":"include `nrow` if you just want numbers","user":"U8JP5B9T2","ts":"1610134513.386800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"60Q09","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"include "},{"type":"text","text":"nrow","style":{"code":true}},{"type":"text","text":" if you just want numbers"}]}]}],"thread_ts":"1610134513.386800","reply_count":1,"reply_users_count":1,"latest_reply":"1610134533.386900","reply_users":["U0140JW97SP"],"subscribed":false},{"client_msg_id":"9baa0216-1288-431c-9dc9-7817a47cb704","type":"message","text":"Will `io = IOBuffer(); serialize(io, nothing); io.data` always be\n```32-element Array{UInt8,1}:\n 0x37\n 0x4a\n 0x4c\n 0x0a\n 0x04\n 0x00\n 0x00\n 0x00\n 0x4e\n 0x00\n ...\n 0x00```\n?","user":"U01J4FNA52A","ts":"1610149050.389900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2aj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Will "},{"type":"text","text":"io = IOBuffer(); serialize(io, nothing); io.data","style":{"code":true}},{"type":"text","text":" always be\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"32-element Array{UInt8,1}:\n 0x37\n 0x4a\n 0x4c\n 0x0a\n 0x04\n 0x00\n 0x00\n 0x00\n 0x4e\n 0x00\n ...\n 0x00"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"?"}]}]}]},{"client_msg_id":"671ab477-1341-48fc-b90a-51acbf453f8f","type":"message","text":"Looking at <https://github.com/JuliaLang/julia/blob/master/stdlib/Serialization/src/Serialization.jl> it looks like the first 8 bytes (which is the header) might vary depending on the machine it's run on, but will the 9th byte 0x4e always represent nothing?","user":"U01J4FNA52A","ts":"1610149189.391100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"L6e80","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Looking at "},{"type":"link","url":"https://github.com/JuliaLang/julia/blob/master/stdlib/Serialization/src/Serialization.jl"},{"type":"text","text":" it looks like the first 8 bytes (which is the header) might vary depending on the machine it's run on, but will the 9th byte 0x4e always represent nothing?"}]}]}]},{"client_msg_id":"d3773874-515a-443a-88bc-1bec0d049bbe","type":"message","text":"I don’t think it’s guaranteed across julia versions; like, someone may make an update to the Serialization.jl code and that wouldnt’ hold any more.","user":"U681ELA87","ts":"1610149666.392800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fim","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I don’t think it’s guaranteed across julia versions; like, someone may make an update to the Serialization.jl code and that wouldnt’ hold any more."}]}]}]},{"client_msg_id":"0a120d4b-2007-4076-b3a1-65f2e9d635e6","type":"message","text":"I figured out the \"4e\" comes from \"nothing\" being element #78 in the tags array: <https://github.com/JuliaLang/julia/blob/master/stdlib/Serialization/src/Serialization.jl#L36>","user":"U01J4FNA52A","ts":"1610149719.393400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UmR5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I figured out the \"4e\" comes from \"nothing\" being element #78 in the tags array: "},{"type":"link","url":"https://github.com/JuliaLang/julia/blob/master/stdlib/Serialization/src/Serialization.jl#L36"}]}]}]},{"client_msg_id":"a3ec293a-62b3-4e0d-84ec-365e457f0f16","type":"message","text":"I assume it will stay that way, no reason to needlessly break binary serialization between versions","user":"U01J4FNA52A","ts":"1610149784.394500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RIzY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I assume it will stay that way, no reason to needlessly break binary serialization between versions"}]}]}]},{"type":"message","subtype":"thread_broadcast","text":"This is not directly comparable as ggplot2 is a graphing library and an R package, but please note\n1. It’s very easy and fast to see something you’d possibly like to do.\n2. Hovering on top of a box gives some high-level information at once.\n3. Clicking a box gives example code and output you can copy-paste-edit.\n<https://www.r-graph-gallery.com/ggplot2-package.html>","user":"U01CQTKB86N","ts":"1610194242.396300","thread_ts":"1610127765.377300","root":{"client_msg_id":"b0f34738-d04e-46da-9c51-1df5c1d7ecaf","type":"message","text":"More seriously, I'd rather add missing parts to the manual than add links to blog posts.","user":"U67431ELR","ts":"1610127765.377300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UbiJM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"More seriously, I'd rather add missing parts to the manual than add links to blog posts."}]}]}],"thread_ts":"1610127765.377300","reply_count":6,"reply_users_count":4,"latest_reply":"1610194242.396300","reply_users":["U8JAMQGQY","U01CQTKB86N","U8T0YV7QC","U67431ELR"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"fu1cM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This is not directly comparable as ggplot2 is a graphing library and an R package, but please note\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It’s very easy and fast to see something you’d possibly like to do."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Hovering on top of a box gives some high-level information at once."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Clicking a box gives example code and output you can copy-paste-edit."}]}],"style":"ordered","indent":0},{"type":"rich_text_section","elements":[{"type":"link","url":"https://www.r-graph-gallery.com/ggplot2-package.html"}]}]}],"client_msg_id":"2a08ec20-ed90-4fa4-9f37-5ca49fe667ea"},{"client_msg_id":"33404a24-4dc6-40c0-9178-3763b55a8a41","type":"message","text":"Hey all! Question about DataFrames.jl. What’s the best way to parallelize a `map` call? This is my non-parallelized code:\n```df[\"target_num\"] = map(x -&gt; ifelse(x == \"item\", 1, 0), df[\"target\"])```\n","user":"U01GXNFKY6R","ts":"1610195927.398800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Y=t=E","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey all! Question about DataFrames.jl. What’s the best way to parallelize a "},{"type":"text","text":"map","style":{"code":true}},{"type":"text","text":" call? This is my non-parallelized code:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"df[\"target_num\"] = map(x -> ifelse(x == \"item\", 1, 0), df[\"target\"])"}]},{"type":"rich_text_section","elements":[]}]}],"thread_ts":"1610195927.398800","reply_count":3,"reply_users_count":3,"latest_reply":"1610196651.401600","reply_users":["U8JAMQGQY","U01GXNFKY6R","UH24GRBLL"],"subscribed":false},{"client_msg_id":"208b9db1-1e54-4b1e-9ee8-0d6495b23c2b","type":"message","text":"is that not fast enough?","user":"UH24GRBLL","ts":"1610196016.399000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zNbU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is that not fast enough?"}]}]}],"thread_ts":"1610196016.399000","reply_count":10,"reply_users_count":3,"latest_reply":"1610197364.402500","reply_users":["U01GXNFKY6R","UH24GRBLL","U67431ELR"],"subscribed":false},{"client_msg_id":"459a1e67-3268-41b2-9546-17a583b80a1b","type":"message","text":"On an unrelated note, here is how I convert Pandas dataframes to julia. Maybe someone else will find it useful.  Pandas helps with reading/writing from gcs/s3 and uses the blazing fast cpp parquet implementation. This method seems to be faster than using Pandas.jl\n```import Pandas\nusing PyCall\nusing DataFrames\nusing Arrow\npd = pyimport(\"pandas\")\npa = pyimport(\"pyarrow\")\n\nfunction convert_pandas_df_to_julia_df(df_pd::PyObject)\n    df_arrow = pa.Table.from_pandas(df_pd)\n    sink = pa.BufferOutputStream()\n    writer = pa.ipc.new_stream(sink, df_arrow.schema)\n    writer.write_table(df_arrow)\n    pa_buf = sink.getvalue()\n    byte_vector = Vector(reinterpret(UInt8, pa_buf))\n    tbl = Arrow.Table(byte_vector)\n    df = DataFrame(tbl; copycols=false)\n    return df\nend\n\ndf = pd.read_parquet(\"gs://...\") |&gt; \n    convert_pandas_df_to_julia_df ```","user":"U01GXNFKY6R","ts":"1610197950.405300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vy7M","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"On an unrelated note, here is how I convert Pandas dataframes to julia. Maybe someone else will find it useful.  Pandas helps with reading/writing from gcs/s3 and uses the blazing fast cpp parquet implementation. This method seems to be faster than using Pandas.jl\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"import Pandas\nusing PyCall\nusing DataFrames\nusing Arrow\npd = pyimport(\"pandas\")\npa = pyimport(\"pyarrow\")\n\nfunction convert_pandas_df_to_julia_df(df_pd::PyObject)\n    df_arrow = pa.Table.from_pandas(df_pd)\n    sink = pa.BufferOutputStream()\n    writer = pa.ipc.new_stream(sink, df_arrow.schema)\n    writer.write_table(df_arrow)\n    pa_buf = sink.getvalue()\n    byte_vector = Vector(reinterpret(UInt8, pa_buf))\n    tbl = Arrow.Table(byte_vector)\n    df = DataFrame(tbl; copycols=false)\n    return df\nend\n\ndf = pd.read_parquet(\"gs://...\") |> \n    convert_pandas_df_to_julia_df "}]}]}],"thread_ts":"1610197950.405300","reply_count":1,"reply_users_count":1,"latest_reply":"1610198384.405400","reply_users":["UH24GRBLL"],"subscribed":false},{"client_msg_id":"880416d4-d850-4479-a355-dcb361c3c4a8","type":"message","text":"I'm reading a CSV that has some columns with values like `[0.13936621 0.16367522 0.10260016]`. The function `CSV.File()`  assigns the String type to those columns by default and I end up with values like `\"[0.13936621 0.16367522 0.10260016]\"` . Is there a way to parse those values as Julia Arrays with CSV.File? I tried passing to the keyword argument `types`  a vector of types where some of them were `Array{Float64,2}`  but it didn't work.","user":"U013E7K55QT","ts":"1610404385.414200","team":"T68168MUP","edited":{"user":"U013E7K55QT","ts":"1610404470.000000"},"blocks":[{"type":"rich_text","block_id":"W4PG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm reading a CSV that has some columns with values like "},{"type":"text","text":"[0.13936621 0.16367522 0.10260016]","style":{"code":true}},{"type":"text","text":". The function "},{"type":"text","text":"CSV.File()","style":{"code":true}},{"type":"text","text":"  assigns the String type to those columns by default and I end up with values like "},{"type":"text","text":"\"[0.13936621 0.16367522 0.10260016]\"","style":{"code":true}},{"type":"text","text":" . Is there a way to parse those values as Julia Arrays with CSV.File? I tried passing to the keyword argument "},{"type":"text","text":"types","style":{"code":true}},{"type":"text","text":"  a vector of types where some of them were "},{"type":"text","text":"Array{Float64,2}","style":{"code":true}},{"type":"text","text":"  but it didn't work."}]}]}],"thread_ts":"1610404385.414200","reply_count":2,"reply_users_count":2,"latest_reply":"1610405376.414800","reply_users":["U681ELA87","U013E7K55QT"],"subscribed":false},{"client_msg_id":"9b76813d-b52b-4623-8bf3-2dcd552917d5","type":"message","text":"Currently working with a MySQL Cursor that is Selecting everything from a MySQL Table. I am trying to convert a the TextCursor object to Arrow.Table but it continues to fail - I expected it to use the Tables interface similar to DataFrames where I could simple wrap the Cursor around a DataFrame Object. How do I properly convert a MySQL Table to an Arrow.Table? Thank you!","user":"US64J0NPQ","ts":"1610474204.419600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LvC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Currently working with a MySQL Cursor that is Selecting everything from a MySQL Table. I am trying to convert a the TextCursor object to Arrow.Table but it continues to fail - I expected it to use the Tables interface similar to DataFrames where I could simple wrap the Cursor around a DataFrame Object. How do I properly convert a MySQL Table to an Arrow.Table? Thank you!"}]}]}]},{"client_msg_id":"d744d4a1-b7b8-4c81-9dd0-2c7e671f03d3","type":"message","text":"I exported a MATLAB table as a parquet file and I can read it in julia, but get an error when trying to convert it into a `DataFrame`\n```UndefRefError: access to undefined reference\n\ngetproperty(::ParquetFiles.RCType350, ::Symbol)@Base.jl:33\nmacro expansion@ParquetFiles.jl:48[inlined]\niterate(::ParquetFiles.ParquetNamedTupleIterator{NamedTuple{(:animalid, :subjid, :species, :strain, :gender, :arrivedage, :iacucprotocol, :rfid, :status, :dateordered, :datearrived, :datedead, :cageid, :health, :owner, :water, :expgroupid, :expgroup),Tuple{Float64,Float64,String,String,String,Float64,String,Float64,String,String,String,String,Float64,String,String,String,Float64,String}},ParquetFiles.RCType350}, ::Int64)@ParquetFiles.jl:39\niterate@tofromdatavalues.jl:53[inlined]\niterate@iterators.jl:139[inlined]\nbuildcolumns(::Tables.Schema{(:animalid, :subjid, :species, :strain, :gender, :arrivedage, :iacucprotocol, :rfid, :status, :dateordered, :datearrived, :datedead, :cageid, :health, :owner, :water, :expgroupid, :expgroup),Tuple{Float64,Float64,String,String,String,Float64,String,Float64,String,String,String,String,Float64,String,String,String,Float64,String}}, ::Tables.IteratorWrapper{ParquetFiles.ParquetNamedTupleIterator{NamedTuple{(:animalid, :subjid, :species, :strain, :gender, :arrivedage, :iacucprotocol, :rfid, :status, :dateordered, :datearrived, :datedead, :cageid, :health, :owner, :water, :expgroupid, :expgroup),Tuple{Float64,Float64,String,String,String,Float64,String,Float64,String,String,String,String,Float64,String,String,String,Float64,String}},ParquetFiles.RCType350}})@fallbacks.jl:135\ncolumns@fallbacks.jl:262[inlined]\n#DataFrame#572(::Bool, ::Type{DataFrames.DataFrame}, ::ParquetFiles.ParquetFile)@tables.jl:43\nDataFrames.DataFrame(::ParquetFiles.ParquetFile)@tables.jl:34\ntop-level scope@Local: 1[inlined]```","user":"U017JTQFNEQ","ts":"1610531039.422400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"d8/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I exported a MATLAB table as a parquet file and I can read it in julia, but get an error when trying to convert it into a "},{"type":"text","text":"DataFrame","style":{"code":true}},{"type":"text","text":"\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"UndefRefError: access to undefined reference\n\ngetproperty(::ParquetFiles.RCType350, ::Symbol)@Base.jl:33\nmacro expansion@ParquetFiles.jl:48[inlined]\niterate(::ParquetFiles.ParquetNamedTupleIterator{NamedTuple{(:animalid, :subjid, :species, :strain, :gender, :arrivedage, :iacucprotocol, :rfid, :status, :dateordered, :datearrived, :datedead, :cageid, :health, :owner, :water, :expgroupid, :expgroup),Tuple{Float64,Float64,String,String,String,Float64,String,Float64,String,String,String,String,Float64,String,String,String,Float64,String}},ParquetFiles.RCType350}, ::Int64)@ParquetFiles.jl:39\niterate@tofromdatavalues.jl:53[inlined]\niterate@iterators.jl:139[inlined]\nbuildcolumns(::Tables.Schema{(:animalid, :subjid, :species, :strain, :gender, :arrivedage, :iacucprotocol, :rfid, :status, :dateordered, :datearrived, :datedead, :cageid, :health, :owner, :water, :expgroupid, :expgroup),Tuple{Float64,Float64,String,String,String,Float64,String,Float64,String,String,String,String,Float64,String,String,String,Float64,String}}, ::Tables.IteratorWrapper{ParquetFiles.ParquetNamedTupleIterator{NamedTuple{(:animalid, :subjid, :species, :strain, :gender, :arrivedage, :iacucprotocol, :rfid, :status, :dateordered, :datearrived, :datedead, :cageid, :health, :owner, :water, :expgroupid, :expgroup),Tuple{Float64,Float64,String,String,String,Float64,String,Float64,String,String,String,String,Float64,String,String,String,Float64,String}},ParquetFiles.RCType350}})@fallbacks.jl:135\ncolumns@fallbacks.jl:262[inlined]\n#DataFrame#572(::Bool, ::Type{DataFrames.DataFrame}, ::ParquetFiles.ParquetFile)@tables.jl:43\nDataFrames.DataFrame(::ParquetFiles.ParquetFile)@tables.jl:34\ntop-level scope@Local: 1[inlined]"}]}]}]}]}