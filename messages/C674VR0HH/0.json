{"cursor": 4, "messages": [{"client_msg_id":"82af0166-9839-4172-aec4-06fb6051421d","type":"message","text":"Is there any chance to read a tsv file with multiple `delim` s using CSV.jl?\n\nThis file <https://ec.europa.eu/eurostat/estat-navtree-portlet-prod/BulkDownloadListing?file=data/nama_10r_2hhinc.tsv.gz> from Eurostat uses `\\t` ,   `\\t` and `,`  in one file.","user":"U836PQXSN","ts":"1615562563.361000","team":"T68168MUP","edited":{"user":"U836PQXSN","ts":"1615562577.000000"},"blocks":[{"type":"rich_text","block_id":"jLQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there any chance to read a tsv file with multiple "},{"type":"text","text":"delim","style":{"code":true}},{"type":"text","text":" s using CSV.jl?\n\nThis file "},{"type":"link","url":"https://ec.europa.eu/eurostat/estat-navtree-portlet-prod/BulkDownloadListing?file=data/nama_10r_2hhinc.tsv.gz"},{"type":"text","text":" from Eurostat uses "},{"type":"text","text":"\\t","style":{"code":true}},{"type":"text","text":" ,  "},{"type":"text","text":" \\t","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":",","style":{"code":true}},{"type":"text","text":"  in one file."}]}]}],"thread_ts":"1615562563.361000","reply_count":1,"reply_users_count":1,"latest_reply":"1615562719.361200","reply_users":["U836PQXSN"],"subscribed":false},{"client_msg_id":"eb703cc1-1ec0-4d6f-93a2-2d6566c5ad18","type":"message","text":"Could interested people have a look at <https://github.com/JuliaData/DataFrames.jl/pull/2649> so that we do not regret the design after we implement it? (we are now getting more restrictive with breaking things so experimenting is risky :smile:) Thank you!","user":"U8JAMQGQY","ts":"1615563217.362600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TcH0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Could interested people have a look at "},{"type":"link","url":"https://github.com/JuliaData/DataFrames.jl/pull/2649"},{"type":"text","text":" so that we do not regret the design after we implement it? (we are now getting more restrictive with breaking things so experimenting is risky "},{"type":"emoji","name":"smile"},{"type":"text","text":") Thank you!"}]}]}]},{"client_msg_id":"2563e67e-c56e-401e-88c5-d2c327e6ac31","type":"message","text":"In Julia do changes on a copy of a variable get applied to the original automatically?\nIn python I would generally assign data to a new variable in case I was going to experiment so as to not affect the original.\n\nI just tried doing that in Julia and I now see that my original is showing up just as my copy.\nFor reference, I was trying to remove the \"T\" from the DateTime Format and my column's data changed from datetime to Any.","user":"U01QJ915TFD","ts":"1615574484.366100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"AE6mV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In Julia do changes on a copy of a variable get applied to the original automatically?\nIn python I would generally assign data to a new variable in case I was going to experiment so as to not affect the original.\n\nI just tried doing that in Julia and I now see that my original is showing up just as my copy.\nFor reference, I was trying to remove the \"T\" from the DateTime Format and my column's data changed from datetime to Any."}]}]}],"thread_ts":"1615574484.366100","reply_count":37,"reply_users_count":4,"latest_reply":"1615577198.374000","reply_users":["UBF9YRB6H","U01QJ915TFD","USU9FRPEU","UH24GRBLL"],"subscribed":false},{"type":"message","text":"How can I remove the T from the middle of my DateTime rows?","files":[{"id":"F01R4JRBAAF","created":1615580327,"timestamp":1615580327,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01QJ915TFD","editable":false,"size":50464,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01R4JRBAAF/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01R4JRBAAF/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01R4JRBAAF-b877d7adbb/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01R4JRBAAF-b877d7adbb/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01R4JRBAAF-b877d7adbb/image_360.png","thumb_360_w":360,"thumb_360_h":254,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01R4JRBAAF-b877d7adbb/image_480.png","thumb_480_w":480,"thumb_480_h":339,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01R4JRBAAF-b877d7adbb/image_160.png","original_w":658,"original_h":465,"thumb_tiny":"AwAhADCruI70u7nPP502lFMALE96Snd6P896AG0HHalNKelADaUdqSlFAC9//wBdL/nvSZGaMj/IoAG6fjQen/66QkUEigBKKKKACiiigAooooA//9k=","permalink":"https://julialang.slack.com/files/U01QJ915TFD/F01R4JRBAAF/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01R4JRBAAF-df04c37dda","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"2M6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I remove the T from the middle of my DateTime rows?"}]}]}],"user":"U01QJ915TFD","display_as_bot":false,"ts":"1615580400.376200"},{"client_msg_id":"02c1b767-fcab-4040-a616-31a0d607e734","type":"message","text":"That’s the default of how `DateTime` objects are printed. If you’d rather transform it to a custom-formatted string column, you could do something like:\n```df.dt = Dates.format.(dt.dt, \"yyyy-mm-dd HH:MM:SS.s\")```","user":"U681ELA87","ts":"1615580706.378300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"B8X/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That’s the default of how "},{"type":"text","text":"DateTime","style":{"code":true}},{"type":"text","text":" objects are printed. If you’d rather transform it to a custom-formatted string column, you could do something like:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"df.dt = Dates.format.(dt.dt, \"yyyy-mm-dd HH:MM:SS.s\")"}]}]}],"thread_ts":"1615580706.378300","reply_count":2,"reply_users_count":2,"latest_reply":"1615583508.385800","reply_users":["U01QJ915TFD","UAGUENL2Y"],"subscribed":false,"reactions":[{"name":"slightly_smiling_face","users":["U01QJ915TFD"],"count":1}]},{"client_msg_id":"1864bc9e-8a69-46d3-8dad-20585cfa7482","type":"message","text":"the `T` is part of ISO 8601, the objectively best datetime format standard","user":"UH24GRBLL","ts":"1615580736.378800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FbdJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"the "},{"type":"text","text":"T","style":{"code":true}},{"type":"text","text":" is part of ISO 8601, the objectively best datetime format standard"}]}]}],"thread_ts":"1615580736.378800","reply_count":5,"reply_users_count":2,"latest_reply":"1615582668.381300","reply_users":["U01QJ915TFD","UH24GRBLL"],"subscribed":false,"reactions":[{"name":"slightly_smiling_face","users":["U01QJ915TFD","U82LX4ACB"],"count":2},{"name":"+1","users":["U01FKQQ7J0J"],"count":1}]},{"client_msg_id":"35f4ff51-5238-409f-9d03-481b1988837f","type":"message","text":"it's also just a representation thing","user":"UH24GRBLL","ts":"1615580784.379100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"V+B","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it's also just a representation thing"}]}]}]},{"type":"message","text":"a final conversion from dataframe to Array is usually ugly because I want to be in column-major for numerical reasons.\n\nIs there an elegant way to convert to an Array and have it be column-major?","user":"U90JR0C80","ts":"1615581633.379800","team":"T68168MUP","attachments":[{"fallback":"[March 12th, 2021 3:21 PM] jessebett: saving and loading csv data with DataFrames keeps things row-major.\nafter manipulating the data, e.g. for selection, I am prepared to do numerical work where I want my data as an Array stored column-major.\n\nIs there an elegant way to do this transformation? Currently converting to matrix, transposing, and collecting from the LinearAlgebra.Adjoint type.","ts":"1615580474.019200","author_id":"U90JR0C80","author_subname":"Jesse Bettencourt","channel_id":"C6A044SQH","channel_name":"helpdesk","is_msg_unfurl":true,"is_thread_root_unfurl":true,"text":"saving and loading csv data with DataFrames keeps things row-major.\nafter manipulating the data, e.g. for selection, I am prepared to do numerical work where I want my data as an Array stored column-major.\n\nIs there an elegant way to do this transformation? Currently converting to matrix, transposing, and collecting from the LinearAlgebra.Adjoint type.","author_name":"Jesse Bettencourt","author_link":"https://julialang.slack.com/team/U90JR0C80","author_icon":"https://avatars.slack-edge.com/2018-01-30/307291696386_d856e2350ce251cee88a_48.jpg","mrkdwn_in":["text"],"color":"D0D0D0","from_url":"https://julialang.slack.com/archives/C6A044SQH/p1615580474019200?thread_ts=1615580474019200&cid=C6A044SQH","is_share":true,"footer":"Thread in #helpdesk"}],"blocks":[{"type":"rich_text","block_id":"enyK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"a final conversion from dataframe to Array is usually ugly because I want to be in column-major for numerical reasons.\n\nIs there an elegant way to convert to an Array and have it be column-major?"}]}]}],"thread_ts":"1615581633.379800","reply_count":2,"reply_users_count":1,"latest_reply":"1615581969.380100","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"f2e29bfd-3c26-4f76-8638-baf7e5c4eea6","type":"message","text":"How can I set my DateTime rows to only include rows between between two times? The file I'm working with has data from 4:00AM to 8:00PM(20:00) but I only want to have data from 9:30AM to 4:00PM on my dataframe.","user":"U01QJ915TFD","ts":"1615590197.395100","team":"T68168MUP","edited":{"user":"U01QJ915TFD","ts":"1615590246.000000"},"blocks":[{"type":"rich_text","block_id":"gTV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I set my DateTime rows to only include rows between between two times? The file I'm working with has data from 4:00AM to 8:00PM(20:00) but I only want to have data from 9:30AM to 4:00PM on my dataframe."}]}]}],"thread_ts":"1615590197.395100","reply_count":1,"reply_users_count":1,"latest_reply":"1615590364.395300","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"0a205e6b-8499-41c3-a719-57251ca7c83f","type":"message","text":"<@U681ELA87> was typing this up in slack here before I realized it might be better to preserve in GitHub <https://github.com/JuliaData/Arrow.jl/issues/88#issuecomment-798806038>\n\na Slack-amenable tangent to that, though - I wonder if there's a better way to support/replace the `Foo`/`_FooArrow` pattern I'm demo'ing in that comment. Instead of:\n\n```struct _FooArrow ... end\n\nFoo(::_FooArrow) = ...\n\nArrow.ArrowTypes.registertype!(Foo, _FooArrow)\n\nArrow.ArrowTypes.arrowconvert(::Type{_FooArrow}, f::Foo) = ...```\nIt'd be nice if I could just define\n\n```toarrow(::Foo)::NamedTuple = ...\nfromarrow(Type{&lt;:Foo}, ::NamedTuple) = ...```\nwithout needing to define a `_FooArrow` type at all or dynamically register stuff\n\nhaven't thought too hard about it though","user":"U674T0Y9Z","ts":"1615681523.427700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"n5vVH","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U681ELA87"},{"type":"text","text":" was typing this up in slack here before I realized it might be better to preserve in GitHub "},{"type":"link","url":"https://github.com/JuliaData/Arrow.jl/issues/88#issuecomment-798806038"},{"type":"text","text":"\n\na Slack-amenable tangent to that, though - I wonder if there's a better way to support/replace the "},{"type":"text","text":"Foo","style":{"code":true}},{"type":"text","text":"/"},{"type":"text","text":"_FooArrow","style":{"code":true}},{"type":"text","text":" pattern I'm demo'ing in that comment. Instead of:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"struct _FooArrow ... end\n\nFoo(::_FooArrow) = ...\n\nArrow.ArrowTypes.registertype!(Foo, _FooArrow)\n\nArrow.ArrowTypes.arrowconvert(::Type{_FooArrow}, f::Foo) = ..."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nIt'd be nice if I could just define\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"toarrow(::Foo)::NamedTuple = ...\nfromarrow(Type{<:Foo}, ::NamedTuple) = ..."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nwithout needing to define a "},{"type":"text","text":"_FooArrow","style":{"code":true}},{"type":"text","text":" type at all or dynamically register stuff\n\nhaven't thought too hard about it though"}]}]}]},{"client_msg_id":"cd2a8008-c348-4ca1-904d-c2cbe7ef2d85","type":"message","text":"Yes, I've been slowly forming my thoughts around this over the last week. I would add a `ArrowTypes.lower` function (like `JSON.lower`) that would allow a hook into the serialization process to say how your custom struct should be serialized. You would overload `ArrowTypes.lower(::MyCustomStruct)` and it would need to return standard arrow supported structs.","user":"U681ELA87","ts":"1615681710.429600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wapF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes, I've been slowly forming my thoughts around this over the last week. I would add a "},{"type":"text","text":"ArrowTypes.lower","style":{"code":true}},{"type":"text","text":" function (like "},{"type":"text","text":"JSON.lower","style":{"code":true}},{"type":"text","text":") that would allow a hook into the serialization process to say how your custom struct should be serialized. You would overload "},{"type":"text","text":"ArrowTypes.lower(::MyCustomStruct)","style":{"code":true}},{"type":"text","text":" and it would need to return standard arrow supported structs."}]}]}],"thread_ts":"1615681710.429600","reply_count":1,"reply_users_count":1,"latest_reply":"1615681830.430600","reply_users":["U674T0Y9Z"],"subscribed":false},{"client_msg_id":"8d32632d-cf05-4c50-a996-bbc3446bee2a","type":"message","text":"What I haven't quite worked out the details on is still serializing the metadata of the pre-lowered struct so you can still get it back out automatically..","user":"U681ELA87","ts":"1615681758.430500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"M4V8B","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What I haven't quite worked out the details on is still serializing the metadata of the pre-lowered struct so you can still get it back out automatically.."}]}]}],"thread_ts":"1615681758.430500","reply_count":1,"reply_users_count":1,"latest_reply":"1615682229.430800","reply_users":["U674T0Y9Z"],"subscribed":false},{"client_msg_id":"892e951a-5673-4378-97e0-34d2d030910c","type":"message","text":"<https://h2oai.github.io/db-benchmark/|https://h2oai.github.io/db-benchmark/>","user":"UKG4WF8PJ","ts":"1615689460.433200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5qfr2","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://h2oai.github.io/db-benchmark/","text":"https://h2oai.github.io/db-benchmark/"}]}]}]},{"client_msg_id":"aa72022e-cc0c-400b-a690-e9a6d5ca564a","type":"message","text":"<https://www.ritchievink.com/blog/2021/02/28/i-wrote-one-of-the-fastest-dataframe-libraries/>","user":"UDGT4PM41","ts":"1615699697.434400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FGe","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://www.ritchievink.com/blog/2021/02/28/i-wrote-one-of-the-fastest-dataframe-libraries/"}]}]}],"thread_ts":"1615699697.434400","reply_count":1,"reply_users_count":1,"latest_reply":"1615700380.434500","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"fd4560a3-453c-4ed0-867a-2bfd99fdcc72","type":"message","text":"What’s the best way to read Parquet files from Google Cloud Storage?","user":"U01GXNFKY6R","ts":"1615711455.437000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eGHd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What’s the best way to read Parquet files from Google Cloud Storage?"}]}]}]},{"client_msg_id":"93ef6318-1ffc-49ac-b4a3-6301f17d1cf0","type":"message","text":"I have a DataFrame with a column of tuples, each containing three elements. How can I split that up into three columns where each one has the corresponding tuple elements?","user":"U011V2YN59N","ts":"1615749713.444300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"44J","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a DataFrame with a column of tuples, each containing three elements. How can I split that up into three columns where each one has the corresponding tuple elements?"}]}]}]},{"client_msg_id":"37e15e1c-4265-4863-9c6e-ee39f124fb3d","type":"message","text":"I can't seem to find a nice way to do it with the dataframes api","user":"U011V2YN59N","ts":"1615749755.445100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"NLH1+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I can't seem to find a nice way to do it with the dataframes api"}]}]}]},{"client_msg_id":"c893e776-7a59-499e-a655-7d670234d91c","type":"message","text":"```julia&gt; using DataFrames\n\njulia&gt; df = DataFrame(a = [(1, 2, 3), (4, 5, 6)])\n2×1 DataFrame\n Row │ a         \n     │ Tuple…    \n─────┼───────────\n   1 │ (1, 2, 3)\n   2 │ (4, 5, 6)\n\njulia&gt; transform(df, :a =&gt; identity =&gt; AsTable)\n2×4 DataFrame\n Row │ a          x1     x2     x3    \n     │ Tuple…     Int64  Int64  Int64 \n─────┼────────────────────────────────\n   1 │ (1, 2, 3)      1      2      3\n   2 │ (4, 5, 6)      4      5      6```\nTo give them informative names, change `identity` to be a function returning a named tuple or a DataFrame","user":"UBF9YRB6H","ts":"1615749845.446000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/mJ+0","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> using DataFrames\n\njulia> df = DataFrame(a = [(1, 2, 3), (4, 5, 6)])\n2×1 DataFrame\n Row │ a         \n     │ Tuple…    \n─────┼───────────\n   1 │ (1, 2, 3)\n   2 │ (4, 5, 6)\n\njulia> transform(df, :a => identity => AsTable)\n2×4 DataFrame\n Row │ a          x1     x2     x3    \n     │ Tuple…     Int64  Int64  Int64 \n─────┼────────────────────────────────\n   1 │ (1, 2, 3)      1      2      3\n   2 │ (4, 5, 6)      4      5      6"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nTo give them informative names, change "},{"type":"text","text":"identity","style":{"code":true}},{"type":"text","text":" to be a function returning a named tuple or a DataFrame"}]}]}]},{"client_msg_id":"4c881f55-8ef9-45b3-b00d-f77ffa96b91e","type":"message","text":"ah wow that is so much nicer than my mess of `map`  and `zip`","user":"U011V2YN59N","ts":"1615749880.446500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Q+r","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ah wow that is so much nicer than my mess of "},{"type":"text","text":"map","style":{"code":true}},{"type":"text","text":"  and "},{"type":"text","text":"zip","style":{"code":true}}]}]}],"reactions":[{"name":"+1","users":["UBF9YRB6H"],"count":1}]},{"client_msg_id":"1397af91-79bb-4096-83e5-b4ec0f921017","type":"message","text":"thanks!","user":"U011V2YN59N","ts":"1615749890.446800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"c+s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"thanks!"}]}]}]},{"client_msg_id":"6b784b2e-2280-4ecf-a053-c811949a1d00","type":"message","text":"hmm I am getting this error with that code","user":"U011V2YN59N","ts":"1615750127.447100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6sqT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hmm I am getting this error with that code"}]}]}]},{"client_msg_id":"2d1ee8a4-5727-473f-bf15-18dadc62d96a","type":"message","text":"```ERROR: ArgumentError: keys of the returned elements must be identical\nStacktrace:\n [1] _expand_to_table(res::Vector{String})\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:380\n [2] select_transform!(nc::Union{Function, Pair{var\"#s267\", var\"#s266\"} where {var\"#s267\"&lt;:Union{Int64, AsTable, AbstractVector{Int64}}, var\"#s266\"&lt;:(Pair{var\"#s164\", var\"#s163\"} where {var\"#s164\"&lt;:Union{Function, Type}, var\"#s163\"&lt;:Union{DataType, Symbol, AbstractVector{Symbol}}})}, Type}, df::DataFrame, newdf::DataFrame, transformed_cols::Set{Symbol}, copycols::Bool, allow_resizing_newdf::Base.RefValue{Bool})\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:522\n [3] _manipulate(df::DataFrame, normalized_cs::Any, copycols::Bool, keeprows::Bool)\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:1279\n [4] manipulate(::DataFrame, ::Any, ::Vararg{Any, N} where N; copycols::Bool, keeprows::Bool, renamecols::Bool)\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:1209\n [5] #select#384\n   @ ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:847 [inlined]\n [6] #transform#386\n   @ ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:913 [inlined]```","user":"U011V2YN59N","ts":"1615750146.447800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"H6l","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: ArgumentError: keys of the returned elements must be identical\nStacktrace:\n [1] _expand_to_table(res::Vector{String})\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:380\n [2] select_transform!(nc::Union{Function, Pair{var\"#s267\", var\"#s266\"} where {var\"#s267\"<:Union{Int64, AsTable, AbstractVector{Int64}}, var\"#s266\"<:(Pair{var\"#s164\", var\"#s163\"} where {var\"#s164\"<:Union{Function, Type}, var\"#s163\"<:Union{DataType, Symbol, AbstractVector{Symbol}}})}, Type}, df::DataFrame, newdf::DataFrame, transformed_cols::Set{Symbol}, copycols::Bool, allow_resizing_newdf::Base.RefValue{Bool})\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:522\n [3] _manipulate(df::DataFrame, normalized_cs::Any, copycols::Bool, keeprows::Bool)\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:1279\n [4] manipulate(::DataFrame, ::Any, ::Vararg{Any, N} where N; copycols::Bool, keeprows::Bool, renamecols::Bool)\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:1209\n [5] #select#384\n   @ ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:847 [inlined]\n [6] #transform#386\n   @ ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:913 [inlined]"}]}]}]},{"client_msg_id":"d201c99b-5b62-4078-85f6-ed665ff5583a","type":"message","text":"Do you have a mix of tuples and named tuples? Or some tuples that have 4 elements?","user":"UBF9YRB6H","ts":"1615750564.448400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RLU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do you have a mix of tuples and named tuples? Or some tuples that have 4 elements?"}]}]}]},{"client_msg_id":"33cc3e7f-5886-4b2b-b929-ae2116c852f3","type":"message","text":"ah I see what was happening, my tuples weren't be parsed from the CSV properly.","user":"U011V2YN59N","ts":"1615750777.448800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yy/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ah I see what was happening, my tuples weren't be parsed from the CSV properly."}]}]}]},{"client_msg_id":"2c023030-8b94-48ac-a652-0b63cf64acff","type":"message","text":"I am really trying to get a Pandas dataframe where one column is full of tuples","user":"U011V2YN59N","ts":"1615750794.449300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iYd5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am really trying to get a Pandas dataframe where one column is full of tuples"}]}]}]},{"client_msg_id":"ccd93f57-56fa-48e9-883f-0c0f9c4d91aa","type":"message","text":"from python to julia","user":"U011V2YN59N","ts":"1615750799.449500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uzLMm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"from python to julia"}]}]}]},{"client_msg_id":"f49a5572-fdc0-433a-805f-7b7de4cc6a74","type":"message","text":"Pandas.read_csv does not parse tuples though","user":"U011V2YN59N","ts":"1615750811.449800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"EeGo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Pandas.read_csv does not parse tuples though"}]}]}]},{"client_msg_id":"899013a1-90e5-451a-92a8-21281f661f8b","type":"message","text":"(Pandas being `Pandas.jl`)","user":"U011V2YN59N","ts":"1615750837.450200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qTPd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(Pandas being "},{"type":"text","text":"Pandas.jl","style":{"code":true}},{"type":"text","text":")"}]}]}]},{"client_msg_id":"bf8d2b46-bcbb-4872-853c-4f674690b29c","type":"message","text":"thanks!","user":"U011V2YN59N","ts":"1615750940.450600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eGebC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"thanks!"}]}]}]},{"client_msg_id":"33acb2e6-d817-44dc-ab76-b6e8e7546b86","type":"message","text":"dataframes qn: Why is it that\n```checkEntry(col) = map(cell-&gt;(cell == \"&lt;5\" ? rand([1,2,3,4]) : cell), col)\nmapcols!(checkEntry, df)```\nworks, but\n```for col in eachcol(df)\n  col = map(checkEntry, col)\nend```\ndoesn’t?\n(The first is obviously much nicer, but I’d expected the latter to work too.)","user":"US8V7JSKB","ts":"1615792371.454700","team":"T68168MUP","edited":{"user":"US8V7JSKB","ts":"1615792422.000000"},"blocks":[{"type":"rich_text","block_id":"/hS+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"dataframes qn: Why is it that\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"checkEntry(col) = map(cell->(cell == \"<5\" ? rand([1,2,3,4]) : cell), col)\nmapcols!(checkEntry, df)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"works, but\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"for col in eachcol(df)\n  col = map(checkEntry, col)\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"doesn’t?\n(The first is obviously much nicer, but I’d expected the latter to work too.)"}]}]}]},{"client_msg_id":"237c5f8f-4177-46fc-9204-0d892cbdf536","type":"message","text":"Trying to get the rows with the top N items from some column. Can anyone think of a better way than this?\n\n```julia&gt; df = DataFrame(x = 1:10, y = rand(10));\n\njulia&gt; function findmaxn(v, n)\n           srt = invperm(sortperm(v,rev=true))\n           findall(&lt;=(n), srt)\n       end\nfindmaxn (generic function with 1 method)\n\njulia&gt; df[findmaxn(df.y, 3), :]\n3×2 DataFrame\n Row │ x      y        \n     │ Int64  Float64  \n─────┼─────────────────\n   1 │     3  0.555818\n   2 │     7  0.818728\n   3 │     8  0.792478```","user":"U8JP5B9T2","ts":"1615822342.456800","team":"T68168MUP","edited":{"user":"U8JP5B9T2","ts":"1615822368.000000"},"blocks":[{"type":"rich_text","block_id":"=ty","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Trying to get the rows with the top N items from some column. Can anyone think of a better way than this?\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> df = DataFrame(x = 1:10, y = rand(10));\n\njulia> function findmaxn(v, n)\n           srt = invperm(sortperm(v,rev=true))\n           findall(<=(n), srt)\n       end\nfindmaxn (generic function with 1 method)\n\njulia> df[findmaxn(df.y, 3), :]\n3×2 DataFrame\n Row │ x      y        \n     │ Int64  Float64  \n─────┼─────────────────\n   1 │     3  0.555818\n   2 │     7  0.818728\n   3 │     8  0.792478"}]}]}],"thread_ts":"1615822342.456800","reply_count":4,"reply_users_count":2,"latest_reply":"1615822660.458100","reply_users":["U67431ELR","U8JP5B9T2"],"subscribed":false},{"client_msg_id":"258894bb-23cb-4523-b26f-e2047fcd9ae6","type":"message","text":"in particular, it seems like there should be something like `findmaxn` somewhere","user":"U8JP5B9T2","ts":"1615822388.457400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"aOSj0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"in particular, it seems like there should be something like "},{"type":"text","text":"findmaxn","style":{"code":true}},{"type":"text","text":" somewhere"}]}]}]},{"client_msg_id":"02e14eb2-d3ae-4c7d-8b3a-20ff931123c6","type":"message","text":"Is there a way to check if a high-resolution timestamp is contained within a low-resolution one?\n```using Dates\nt_month = floor(Dates.now(), Dates.Month)\nt_sec = floor(Dates.now(), Dates.Second)\nt_sec in t_month == true # method error```","user":"UB2QSHWPN","ts":"1615822928.459900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tlwq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to check if a high-resolution timestamp is contained within a low-resolution one?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Dates\nt_month = floor(Dates.now(), Dates.Month)\nt_sec = floor(Dates.now(), Dates.Second)\nt_sec in t_month == true # method error"}]}]}],"thread_ts":"1615822928.459900","reply_count":1,"reply_users_count":1,"latest_reply":"1615823034.460000","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"de8e5604-a952-4497-8070-8166c472a03d","type":"message","text":"hi guys! is there an equivalent of <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html> ? I’m trying to put something similar together than quantopian’s alphalens, and would need to bin the data effectively. I wrote my own function to do it (as I couldn’t find anything similar), but would love to compare it to some kind of official implementation","user":"U01N351DMT9","ts":"1615903158.002300","team":"T68168MUP","edited":{"user":"U01N351DMT9","ts":"1615903162.000000"},"blocks":[{"type":"rich_text","block_id":"yjw2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hi guys! is there an equivalent of "},{"type":"link","url":"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html"},{"type":"text","text":" ? I’m trying to put something similar together than quantopian’s alphalens, and would need to bin the data effectively. I wrote my own function to do it (as I couldn’t find anything similar), but would love to compare it to some kind of official implementation"}]}]}],"thread_ts":"1615903158.002300","reply_count":9,"reply_users_count":2,"latest_reply":"1615904212.005600","reply_users":["U01N351DMT9","U67431ELR"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"if there’s no such function, I’d love to collaborate with someone who is more experienced with Julia to make this happen as part of any package!","user":"U01N351DMT9","ts":"1615903350.004000","thread_ts":"1615903158.002300","root":{"client_msg_id":"de8e5604-a952-4497-8070-8166c472a03d","type":"message","text":"hi guys! is there an equivalent of <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html> ? I’m trying to put something similar together than quantopian’s alphalens, and would need to bin the data effectively. I wrote my own function to do it (as I couldn’t find anything similar), but would love to compare it to some kind of official implementation","user":"U01N351DMT9","ts":"1615903158.002300","team":"T68168MUP","edited":{"user":"U01N351DMT9","ts":"1615903162.000000"},"blocks":[{"type":"rich_text","block_id":"yjw2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hi guys! is there an equivalent of "},{"type":"link","url":"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html"},{"type":"text","text":" ? I’m trying to put something similar together than quantopian’s alphalens, and would need to bin the data effectively. I wrote my own function to do it (as I couldn’t find anything similar), but would love to compare it to some kind of official implementation"}]}]}],"thread_ts":"1615903158.002300","reply_count":9,"reply_users_count":2,"latest_reply":"1615904212.005600","reply_users":["U01N351DMT9","U67431ELR"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"8IDTe","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"if there’s no such function, I’d love to collaborate with someone who is more experienced with Julia to make this happen as part of any package!"}]}]}],"client_msg_id":"00db99f9-c6d7-4284-9ef0-f12f9553dd34"},{"type":"message","subtype":"thread_broadcast","text":"nothing! my own crappy implementation also uses it. but I couldn’t find a code snippet to easily use it with dataframes’ groupby / combine methods. do you have a direction to point me towards?","user":"U01N351DMT9","ts":"1615903618.004500","thread_ts":"1615903158.002300","root":{"client_msg_id":"de8e5604-a952-4497-8070-8166c472a03d","type":"message","text":"hi guys! is there an equivalent of <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html> ? I’m trying to put something similar together than quantopian’s alphalens, and would need to bin the data effectively. I wrote my own function to do it (as I couldn’t find anything similar), but would love to compare it to some kind of official implementation","user":"U01N351DMT9","ts":"1615903158.002300","team":"T68168MUP","edited":{"user":"U01N351DMT9","ts":"1615903162.000000"},"blocks":[{"type":"rich_text","block_id":"yjw2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hi guys! is there an equivalent of "},{"type":"link","url":"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html"},{"type":"text","text":" ? I’m trying to put something similar together than quantopian’s alphalens, and would need to bin the data effectively. I wrote my own function to do it (as I couldn’t find anything similar), but would love to compare it to some kind of official implementation"}]}]}],"thread_ts":"1615903158.002300","reply_count":9,"reply_users_count":2,"latest_reply":"1615904212.005600","reply_users":["U01N351DMT9","U67431ELR"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"+7/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"nothing! my own crappy implementation also uses it. but I couldn’t find a code snippet to easily use it with dataframes’ groupby / combine methods. do you have a direction to point me towards?"}]}]}],"client_msg_id":"7924922b-d1b9-415a-bbeb-2eb77f1b7cd2"},{"client_msg_id":"3034bcb7-8669-4da2-a73e-e3d69e6fb222","type":"message","text":"How would you merge 2 dataframes in the following situation? :\nDataFrame A has 2 columns, DataFrame B has 3 columns but both DataFrame A and B share some rows in their first column.\n\nAfter merging, I'd like to have a DataFrame with a total of 4 columns which only keeps the rows that DataFrame A and B have in common in their first column.","user":"U01QJ915TFD","ts":"1615916916.011400","team":"T68168MUP","edited":{"user":"U01QJ915TFD","ts":"1615917014.000000"},"blocks":[{"type":"rich_text","block_id":"Tuu2G","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How would you merge 2 dataframes in the following situation? :\nDataFrame A has 2 columns, DataFrame B has 3 columns but both DataFrame A and B share some rows in their first column.\n\nAfter merging, I'd like to have a DataFrame with a total of 4 columns which only keeps the rows that DataFrame A and B have in common in their first column."}]}]}],"thread_ts":"1615916916.011400","reply_count":2,"reply_users_count":2,"latest_reply":"1615917490.012300","reply_users":["U8JAMQGQY","U01QJ915TFD"],"subscribed":false},{"client_msg_id":"c1c57145-31dd-48d9-b475-af50b054bae3","type":"message","text":"Hello hello data peeps","user":"U6QGE7S86","ts":"1615944507.000300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YIvMt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello hello data peeps"}]}]}]},{"client_msg_id":"d5b861fe-e0ee-4a26-9b14-6903d759dc61","type":"message","text":"Are there more recent benchmarks vs other tabular readers than <https://h2oai.github.io/db-benchmark/>?","user":"U6QGE7S86","ts":"1615944526.000700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gcC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Are there more recent benchmarks vs other tabular readers than "},{"type":"link","url":"https://h2oai.github.io/db-benchmark/"},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"1e0685f3-8260-45ca-9d77-5ab1d4122e52","type":"message","text":"They're using 1.5.3 Julia so I'm guessing there will be some awesome speedups for their new round of benchmarking.","user":"U6QGE7S86","ts":"1615944547.001200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sCsp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"They're using 1.5.3 Julia so I'm guessing there will be some awesome speedups for their new round of benchmarking."}]}]}]},{"client_msg_id":"62c081a1-ffe8-4bb8-a54c-98ec3418e332","type":"message","text":"The speedups probably won't come from 1.6, but rather through multithreading in grouping and combining","user":"UBF9YRB6H","ts":"1615945131.001800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mI4p","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The speedups probably won't come from 1.6, but rather through multithreading in grouping and combining"}]}]}],"reactions":[{"name":"+1::skin-tone-5","users":["U6QGE7S86"],"count":1}]},{"client_msg_id":"d7c0ac02-fff0-4745-9304-ec4e1d481d7c","type":"message","text":"Ah, also found <https://bkamins.github.io/julialang/2021/03/05/pooledarrays.html>.","user":"U6QGE7S86","ts":"1615945175.002100","team":"T68168MUP","attachments":[{"service_name":"Blog by Bogumił Kamiński","title":"What is new in PooledArrays.jl?","title_link":"https://bkamins.github.io/julialang/2021/03/05/pooledarrays.html","text":"Introduction","fallback":"Blog by Bogumił Kamiński: What is new in PooledArrays.jl?","ts":1614941097,"from_url":"https://bkamins.github.io/julialang/2021/03/05/pooledarrays.html","service_icon":"https://bkamins.github.io/favicon.ico","id":1,"original_url":"https://bkamins.github.io/julialang/2021/03/05/pooledarrays.html"}],"blocks":[{"type":"rich_text","block_id":"SBd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah, also found "},{"type":"link","url":"https://bkamins.github.io/julialang/2021/03/05/pooledarrays.html"},{"type":"text","text":"."}]}]}]},{"client_msg_id":"34c2b1f1-3f25-4fd4-934e-a2d5886a4ecb","type":"message","text":"could we add a little utility to Arrow.jl that is the equivalent of\n\n```function f(data)\n           io = IOBuffer()\n           Arrow.write(io, data)\n           seekstart(io)\n           return Arrow.Table(io)\n       end```\neven if unexported (name bikeshed? :grin:). I feel like this isn't useful in \"real\" code but I copy-paste it in to my REPL all the time for interactive testing","user":"U674T0Y9Z","ts":"1615946006.005700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LWXx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"could we add a little utility to Arrow.jl that is the equivalent of\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function f(data)\n           io = IOBuffer()\n           Arrow.write(io, data)\n           seekstart(io)\n           return Arrow.Table(io)\n       end"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\neven if unexported (name bikeshed? "},{"type":"emoji","name":"grin"},{"type":"text","text":"). I feel like this isn't useful in \"real\" code but I copy-paste it in to my REPL all the time for interactive testing"}]}]}],"thread_ts":"1615946006.005700","reply_count":3,"reply_users_count":2,"latest_reply":"1615947403.007900","reply_users":["U681ELA87","U674T0Y9Z"],"subscribed":false},{"client_msg_id":"3825e63d-4c0c-4c60-8d94-99e26b8d22db","type":"message","text":"Is there any precedent for using Arrow.jl with image data? I'm realizing that several of the tasks I'm working on could be rephrased into pure data handling issues. In particular the LZ4 or ZStd support could be particular useful.","user":"USU9FRPEU","ts":"1616000902.013900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SBK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there any precedent for using Arrow.jl with image data? I'm realizing that several of the tasks I'm working on could be rephrased into pure data handling issues. In particular the LZ4 or ZStd support could be particular useful."}]}]}],"thread_ts":"1616000902.013900","reply_count":20,"reply_users_count":3,"latest_reply":"1616001950.020500","reply_users":["U9VG1AYSG","USU9FRPEU","U681ELA87"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"One thing I'm missing at the moment is a byte or bit shuffle such as implemented by Blosc:\n<https://github.com/Blosc/c-blosc/blob/5352508a420bec865c57cda116a5e5303df898d2/blosc/shuffle-generic.h#L32-L52>\n\nThat seems to have an impact on how well LZ4 compression works.","user":"USU9FRPEU","ts":"1616001188.015500","thread_ts":"1616000902.013900","root":{"client_msg_id":"3825e63d-4c0c-4c60-8d94-99e26b8d22db","type":"message","text":"Is there any precedent for using Arrow.jl with image data? I'm realizing that several of the tasks I'm working on could be rephrased into pure data handling issues. In particular the LZ4 or ZStd support could be particular useful.","user":"USU9FRPEU","ts":"1616000902.013900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SBK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there any precedent for using Arrow.jl with image data? I'm realizing that several of the tasks I'm working on could be rephrased into pure data handling issues. In particular the LZ4 or ZStd support could be particular useful."}]}]}],"thread_ts":"1616000902.013900","reply_count":20,"reply_users_count":3,"latest_reply":"1616001950.020500","reply_users":["U9VG1AYSG","USU9FRPEU","U681ELA87"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"Kbni8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"One thing I'm missing at the moment is a byte or bit shuffle such as implemented by Blosc:\n"},{"type":"link","url":"https://github.com/Blosc/c-blosc/blob/5352508a420bec865c57cda116a5e5303df898d2/blosc/shuffle-generic.h#L32-L52"},{"type":"text","text":"\n\nThat seems to have an impact on how well LZ4 compression works."}]}]}],"client_msg_id":"7b4003a7-d865-4974-a9df-999e5e52068c"},{"type":"message","subtype":"thread_broadcast","text":"Perhaps what I'm really asking about is reusing some of the components that Arrow uses to do other things. For example, the compression codecs interface has come to my attention.","user":"USU9FRPEU","ts":"1616001614.018000","thread_ts":"1616000902.013900","root":{"client_msg_id":"3825e63d-4c0c-4c60-8d94-99e26b8d22db","type":"message","text":"Is there any precedent for using Arrow.jl with image data? I'm realizing that several of the tasks I'm working on could be rephrased into pure data handling issues. In particular the LZ4 or ZStd support could be particular useful.","user":"USU9FRPEU","ts":"1616000902.013900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SBK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there any precedent for using Arrow.jl with image data? I'm realizing that several of the tasks I'm working on could be rephrased into pure data handling issues. In particular the LZ4 or ZStd support could be particular useful."}]}]}],"thread_ts":"1616000902.013900","reply_count":20,"reply_users_count":3,"latest_reply":"1616001950.020500","reply_users":["U9VG1AYSG","USU9FRPEU","U681ELA87"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"PJG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Perhaps what I'm really asking about is reusing some of the components that Arrow uses to do other things. For example, the compression codecs interface has come to my attention."}]}]}],"client_msg_id":"91ed8538-b964-4f55-bc78-68bb460d05d5"},{"client_msg_id":"428016db-09df-45ca-9bbf-69c46e8c6a67","type":"message","text":"I'm also looking around for any 12-bit integer support, especially in a packed format where two integers are packed into three bytes. I have this mostly figured out for my purposes. Before I go about creating a package for this, I wanted to check if there was any existing work.\n\nI've built some utilities on top of <https://github.com/rfourquet/BitIntegers.jl> and SIMD.jl to either simulate 12-bit integer arrays or unpack them quickly into `UInt16` .","user":"USU9FRPEU","ts":"1616001943.020400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ng6A1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm also looking around for any 12-bit integer support, especially in a packed format where two integers are packed into three bytes. I have this mostly figured out for my purposes. Before I go about creating a package for this, I wanted to check if there was any existing work.\n\nI've built some utilities on top of "},{"type":"link","url":"https://github.com/rfourquet/BitIntegers.jl"},{"type":"text","text":" and SIMD.jl to either simulate 12-bit integer arrays or unpack them quickly into "},{"type":"text","text":"UInt16","style":{"code":true}},{"type":"text","text":" ."}]}]}]},{"client_msg_id":"4f9ea39f-845d-4619-bfec-41620b02b268","type":"message","text":"Is there a way to add missing rows? I understand there are functions to backfill or even forward fill missing values but how can I add rows that are missing altogether ?\nHow would I add rows for the missing seconds in the example DateTime column below?\n\nHere's an example:\n\nDATETIME\n9:30:01\n9:30:05","user":"U01QJ915TFD","ts":"1616101028.026500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=2U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to add missing rows? I understand there are functions to backfill or even forward fill missing values but how can I add rows that are missing altogether ?\nHow would I add rows for the missing seconds in the example DateTime column below?\n\nHere's an example:\n\nDATETIME\n9:30:01\n9:30:05"}]}]}]},{"client_msg_id":"8da9c75a-b4cc-4f99-8789-6a5088907e80","type":"message","text":"I'm not entirely sure if what I'm wanting to do above is the best approach. Another approach I just thought of is instead joining two dataframes, one with no missing DateTime rows being one of the dataframes. In doing so, would this automatically maintain all the rows from the complete dataframe and thus now have missing values instead of just missing rows for the other incomplete dataframe?","user":"U01QJ915TFD","ts":"1616101944.030700","team":"T68168MUP","edited":{"user":"U01QJ915TFD","ts":"1616102035.000000"},"blocks":[{"type":"rich_text","block_id":"KFAx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm not entirely sure if what I'm wanting to do above is the best approach. Another approach I just thought of is instead joining two dataframes, one with no missing DateTime rows being one of the dataframes. In doing so, would this automatically maintain all the rows from the complete dataframe and thus now have missing values instead of just missing rows for the other incomplete dataframe?"}]}]}]},{"client_msg_id":"3749f949-3fd5-44ac-8cf5-45ce4ba65731","type":"message","text":"How can I roll back the time 5 hours in my DateTime column?\nFor example:\nfrom this *2020-01-12T14:30:00.016*  to  *2020-01-12T09:30:00.016*\n\nBased on the docs, DateTime is unaware or naive when it comes to time zones. It points to using TimeZones.jl. But I'm having trouble doing this.","user":"U01QJ915TFD","ts":"1616108146.034300","team":"T68168MUP","edited":{"user":"U01QJ915TFD","ts":"1616108175.000000"},"blocks":[{"type":"rich_text","block_id":"BXs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I roll back the time 5 hours in my DateTime column?\nFor example:\nfrom this "},{"type":"text","text":"2020-01-12T14:30:00.016","style":{"bold":true}},{"type":"text","text":"  to  "},{"type":"text","text":"2020-01-12T09:30:00.016","style":{"bold":true}},{"type":"text","text":"\n\nBased on the docs, DateTime is unaware or naive when it comes to time zones. It points to using TimeZones.jl. But I'm having trouble doing this."}]}]}],"thread_ts":"1616108146.034300","reply_count":1,"reply_users_count":1,"latest_reply":"1616109245.034600","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"e3bb2169-fd13-4c67-b025-0d7e66f95e52","type":"message","text":"Any idea on why the below code would produce an error?\n\n\n`df_comb = leftjoin(df_master, df_msft, df_fb, df_pypl, df_ba, df_cvx, df_goog, df_jpm, df_nvda, df_TSLA, on = :DateTime, makeunique=true)`\n\n\nHere's the error:\n\n`ERROR: MethodError: no method matching leftjoin(::DataFrame, ::DataFrame, ::DataFrame, ::DataFrame, ::DataFrame,`\n`::DataFrame, ::DataFrame, ::DataFrame, ::DataFrame, ::DataFrame; on=:DateTime, makeunique=true)`","user":"U01QJ915TFD","ts":"1616126515.040500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sC=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Any idea on why the below code would produce an error?\n\n\n"},{"type":"text","text":"df_comb = leftjoin(df_master, df_msft, df_fb, df_pypl, df_ba, df_cvx, df_goog, df_jpm, df_nvda, df_TSLA, on = :DateTime, makeunique=true)","style":{"code":true}},{"type":"text","text":"\n\n\nHere's the error:\n\n"},{"type":"text","text":"ERROR: MethodError: no method matching leftjoin(::DataFrame, ::DataFrame, ::DataFrame, ::DataFrame, ::DataFrame,","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"::DataFrame, ::DataFrame, ::DataFrame, ::DataFrame, ::DataFrame; on=:DateTime, makeunique=true)","style":{"code":true}}]}]}]},{"client_msg_id":"f7db74fd-71b3-4af5-b873-a4d91513688c","type":"message","text":"Not getting the error with `innerjoin` though so I wonder how am I implementing `leftjoin` wrong?","user":"U01QJ915TFD","ts":"1616127193.041700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Kg4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not getting the error with "},{"type":"text","text":"innerjoin","style":{"code":true}},{"type":"text","text":" though so I wonder how am I implementing "},{"type":"text","text":"leftjoin","style":{"code":true}},{"type":"text","text":" wrong?"}]}]}]},{"client_msg_id":"1383fa15-e254-4608-9e02-6da23c0a956a","type":"message","text":"Letfjoin doesn't allow for more than 2 arguments. Remember to check the docs with `? leftjoin`","user":"UBF9YRB6H","ts":"1616131288.042600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WoLm8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Letfjoin doesn't allow for more than 2 arguments. Remember to check the docs with "},{"type":"text","text":"? leftjoin","style":{"code":true}}]}]}]},{"client_msg_id":"4F1F4582-52E2-4D2D-9AFF-A08C14837A55","type":"message","text":"<@UBF9YRB6H> Thanks. ","user":"U01QJ915TFD","ts":"1616145444.044000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fPEYI","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UBF9YRB6H"},{"type":"text","text":" Thanks. "}]}]}]},{"client_msg_id":"1145550f-8363-4c29-856a-79db95bf67b9","type":"message","text":"Is there a simple way to select columns by a property in a `DataFrame`? For example, I would like to select all columns with no missing entries","user":"U6BJ9E351","ts":"1616146653.044900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9Iz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a simple way to select columns by a property in a "},{"type":"text","text":"DataFrame","style":{"code":true}},{"type":"text","text":"? For example, I would like to select all columns with no missing entries"}]}]}],"thread_ts":"1616146653.044900","reply_count":4,"reply_users_count":2,"latest_reply":"1616146885.046100","reply_users":["U67431ELR","U6BJ9E351"],"subscribed":false},{"client_msg_id":"f524b58b-c30c-44fc-82fc-69d316decfef","type":"message","text":"I have (lots of) csv files with a `Vector{Float64}` column a la `1, 2, \"[1.0, 2.0, 3.0]\"` . Is there a good way to parse this into a `DataFrame`? Since this is a one-shot script i’ve tried the hacky\n```Base.parse(::Type{Vector{Float64}}, s::String) = eval(Meta.parse(s))\ndf = CSV.read(f, DataFrame, types=Dict(:correlation=&gt;Vector{Float64}))```\nwhich works but it is very slow…","user":"U01BYANF42K","ts":"1616159455.052500","team":"T68168MUP","edited":{"user":"U01BYANF42K","ts":"1616159618.000000"},"blocks":[{"type":"rich_text","block_id":"39QBH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have (lots of) csv files with a "},{"type":"text","text":"Vector{Float64}","style":{"code":true}},{"type":"text","text":" column a la "},{"type":"text","text":"1, 2, \"[1.0, 2.0, 3.0]\"","style":{"code":true}},{"type":"text","text":" . Is there a good way to parse this into a "},{"type":"text","text":"DataFrame","style":{"code":true}},{"type":"text","text":"? Since this is a one-shot script i’ve tried the hacky\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Base.parse(::Type{Vector{Float64}}, s::String) = eval(Meta.parse(s))\ndf = CSV.read(f, DataFrame, types=Dict(:correlation=>Vector{Float64}))"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"which works but it is very slow…"}]}]}]},{"client_msg_id":"F12BA2FD-D0EA-48C5-B469-00E23BB6BE16","type":"message","text":"How can you use leftjoin with multiple data frames? For working with just a few, leftjoining two at a time would work but  if I want to leftjoin 100 data frames, how can I join them faster?","user":"U01QJ915TFD","ts":"1616159976.057600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KqGp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can you use leftjoin with multiple data frames? For working with just a few, leftjoining two at a time would work but  if I want to leftjoin 100 data frames, how can I join them faster?"}]}]}]},{"client_msg_id":"18c20856-a045-4896-8280-eacbab566be4","type":"message","text":"I know there used to be some talk about adding Pandas-style row indices to `DataFrames.jl`, but it seems that has now been abandoned. Are there any other packages out there that provide this functionality?","user":"UENHZ1M08","ts":"1616172266.066000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Xtx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I know there used to be some talk about adding Pandas-style row indices to "},{"type":"text","text":"DataFrames.jl","style":{"code":true}},{"type":"text","text":", but it seems that has now been abandoned. Are there any other packages out there that provide this functionality?"}]}]}],"thread_ts":"1616172266.066000","reply_count":9,"reply_users_count":3,"latest_reply":"1616174925.067900","reply_users":["UBF9YRB6H","U681ELA87","UENHZ1M08"],"subscribed":false},{"client_msg_id":"cc49a0fd-329d-49a5-8870-6a1e2d4c7db0","type":"message","text":"How do you view all columns of a dataframe? It seems that some solutions pertaining to using show and showall are no longer working.","user":"U01QJ915TFD","ts":"1616176816.069500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PFWl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How do you view all columns of a dataframe? It seems that some solutions pertaining to using show and showall are no longer working."}]}]}],"thread_ts":"1616176816.069500","reply_count":3,"reply_users_count":2,"latest_reply":"1616177749.070400","reply_users":["UBF9YRB6H","U01QJ915TFD"],"subscribed":false},{"client_msg_id":"26dcb0eb-aa25-47ef-915a-20fc2d53df23","type":"message","text":"I can't believe it! Finally, my first dataset made in Julia is ready for its intended purpose :slightly_smiling_face: Thank you to everyone here that has made that possible. Without your help, I may have given up. The Julia community is the best! Special thanks to <@UBF9YRB6H> <@U7JQGPGCQ>","user":"U01QJ915TFD","ts":"1616178946.073500","team":"T68168MUP","edited":{"user":"U01QJ915TFD","ts":"1616179046.000000"},"blocks":[{"type":"rich_text","block_id":"iLOS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I can't believe it! Finally, my first dataset made in Julia is ready for its intended purpose "},{"type":"emoji","name":"slightly_smiling_face"},{"type":"text","text":" Thank you to everyone here that has made that possible. Without your help, I may have given up. The Julia community is the best! Special thanks to "},{"type":"user","user_id":"UBF9YRB6H"},{"type":"text","text":" "},{"type":"user","user_id":"U7JQGPGCQ"}]}]}],"reactions":[{"name":"heart","users":["U8JAMQGQY","U01724Q3PGW","U01QRM4E8HL","U681ELA87"],"count":4}]},{"client_msg_id":"E1E8602F-734B-4A97-9FC0-AACDA803BC92","type":"message","text":"I’ve been learning all about collections in Julia today.\nI now have a decent grasp on the difference between a vector and a matrix. \nThe part that is now a bit more complex for me is that that there are also arrays of vectors and vectors of vectors. I can’t seem to differentiate them. How do these two differ? Are both evenly used in Julia?","user":"U01QJ915TFD","ts":"1616203062.082400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jtkL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’ve been learning all about collections in Julia today.\nI now have a decent grasp on the difference between a vector and a matrix. \nThe part that is now a bit more complex for me is that that there are also arrays of vectors and vectors of vectors. I can’t seem to differentiate them. How do these two differ? Are both evenly used in Julia?"}]}]}]},{"client_msg_id":"BB479A01-F040-4C36-BD68-11AEF29AFC6D","type":"message","text":"I have a feeling that it may not be all that essential to differentiate these two but at the same time I have the desire to understand in case one use case is necessary or preferred over the other.  ","user":"U01QJ915TFD","ts":"1616203557.084100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1CQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a feeling that it may not be all that essential to differentiate these two but at the same time I have the desire to understand in case one use case is necessary or preferred over the other.  "}]}]}]},{"client_msg_id":"DD893D46-D8E2-4BEF-9E77-517ACE1D3BAE","type":"message","text":"Wait a second.. are these the same thing? If  a vector is a 1 dimensional array, should a vector of vectors not be the same as an array of vectors?","user":"U01QJ915TFD","ts":"1616203865.086100","team":"T68168MUP","edited":{"user":"U01QJ915TFD","ts":"1616203937.000000"},"blocks":[{"type":"rich_text","block_id":"Eyud","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Wait a second.. are these the same thing? If  a vector is a 1 dimensional array, should a vector of vectors not be the same as an array of vectors?"}]}]}]},{"client_msg_id":"0ec7d527-11df-4cf4-987a-ab0409cb611c","type":"message","text":"A since `Vector &lt;: Array`, you could say that the category of “vectors of vectors” is a subset of the category of “arrays of vectors”","user":"UGU761DU2","ts":"1616204118.087500","team":"T68168MUP","edited":{"user":"UGU761DU2","ts":"1616204159.000000"},"blocks":[{"type":"rich_text","block_id":"N6J3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"A since "},{"type":"text","text":"Vector <: Array","style":{"code":true}},{"type":"text","text":", you could say that the category of “vectors of vectors” is a subset of the category of “arrays of vectors”"}]}]}],"reactions":[{"name":"+1","users":["U01QJ915TFD"],"count":1}]},{"client_msg_id":"11734b25-865e-440a-9559-d1450ea05f9b","type":"message","text":"Vector isn’t a subtype technically. It’s a type alias. `Array{T, N}` is a concrete type, where `T` is the element type and `N` is the number of dimensions. and `const Vector{T} = Array{T, 1}`","user":"U681ELA87","ts":"1616204219.088900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Pro","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Vector isn’t a subtype technically. It’s a type alias. "},{"type":"text","text":"Array{T, N}","style":{"code":true}},{"type":"text","text":" is a concrete type, where "},{"type":"text","text":"T","style":{"code":true}},{"type":"text","text":" is the element type and "},{"type":"text","text":"N","style":{"code":true}},{"type":"text","text":" is the number of dimensions. and "},{"type":"text","text":"const Vector{T} = Array{T, 1}","style":{"code":true}}]}]}],"thread_ts":"1616204219.088900","reply_count":1,"reply_users_count":1,"latest_reply":"1616204242.089100","reply_users":["UGU761DU2"],"subscribed":false,"reactions":[{"name":"+1","users":["U01QJ915TFD"],"count":1}]},{"client_msg_id":"d57bb696-a739-49cc-913e-d71ea5713d46","type":"message","text":"But “vector of vectors” `!=` “2d-array” — those are very different concepts, in contrast to how people sometimes _implement_ 2d arrays in languages like C that don’t really properly support them","user":"UGU761DU2","ts":"1616204220.089000","team":"T68168MUP","edited":{"user":"UGU761DU2","ts":"1616204312.000000"},"blocks":[{"type":"rich_text","block_id":"s+XF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But “vector of vectors” "},{"type":"text","text":"!=","style":{"code":true}},{"type":"text","text":" “2d-array” — those are very different concepts, in contrast to how people sometimes "},{"type":"text","text":"implement","style":{"italic":true}},{"type":"text","text":" 2d arrays in languages like C that don’t really properly support them"}]}]}],"thread_ts":"1616204220.089000","reply_count":17,"reply_users_count":4,"latest_reply":"1616204892.100100","reply_users":["U01QJ915TFD","UGU761DU2","U0138UTB7A4","U8JP5B9T2"],"subscribed":false,"reactions":[{"name":"+1","users":["U01QJ915TFD"],"count":1}]},{"client_msg_id":"b097635c-3e75-4abc-a2e3-2ad2340e46df","type":"message","text":"&lt;<https://github.com/JuliaData/Arrow.jl/pull/150%7Cdefine> caller-overloadable arrownameof(T) to allow callers to specify preferred Julia &lt;-&gt; Arrow type matching behavior&gt;\n\nan easily replaceable stopgap to enable some custom (de)serialization behaviors until something like <@U681ELA87>'s plan <https://julialang.slack.com/archives/C674VR0HH/p1615846377461400?thread_ts=1615681758.430500&amp;cid=C674VR0HH|here> lands","user":"U674T0Y9Z","ts":"1616262076.113300","team":"T68168MUP","attachments":[{"from_url":"https://julialang.slack.com/archives/C674VR0HH/p1615846377461400?thread_ts=1615681758.430500&amp;cid=C674VR0HH","fallback":"[March 15th, 2021 3:12 PM] quinnj: <@U674T0Y9Z>, for deserialization, what you think about ditching the *_MAPPING Dicts and instead parsing a metadata type name of the form `JuliaLang.UUID` as `(Val(:JuliaLang), Val(:UUID))`, or maybe just `Val(:JuliaLang.UUID)`. so you’d overload something like:\n```function Arrow.ArrowTypes.fromarrow(::Val{:JuliaLang.UUID}, x...)\n\nend```\nwhere `x…` would be the deserialized fields. There’d be generic definitions something like `fromarrow(V, x…) = nothing` and if there’s no overload, then we’d just return a NamedTuple..\n\nOn the serialization side, we’d have:\n• `Arrow.ArrowTypes.toarrow(x) = x`, and `Arrow.ArrowTypes.arrowname(::Type{T})` which would be overloadable for custom types where you’d return the arrow object to serialize, and `arrowname` would be the field metadata type name. By default `arrowname(T) = Symbol()`, so we wouldn’t name it, so you’d only get a NamedTuple out unless you overload. \nMy biggest question then is if this would really work for parametric types. You could serialize using `nameof` in `arrowname`, but then in `fromarrow`, you’d have to be able to build your type. I think it gets awkward if you have non-field-based type parameters that you don’t necessarily want to serialize in `arrowname`, but maybe that’s just a pill you have to swallow. If you can’t infer the type parameters from the data itself, you’ll need to serialize it in the `arrowname` and pull it back out somehow.","ts":"1615846377.461400","author_id":"U681ELA87","author_subname":"Jacob Quinn","channel_id":"C674VR0HH","channel_name":"data","is_msg_unfurl":true,"is_reply_unfurl":true,"text":"<@U674T0Y9Z>, for deserialization, what you think about ditching the *_MAPPING Dicts and instead parsing a metadata type name of the form `JuliaLang.UUID` as `(Val(:JuliaLang), Val(:UUID))`, or maybe just `Val(:JuliaLang.UUID)`. so you’d overload something like:\n```function Arrow.ArrowTypes.fromarrow(::Val{:JuliaLang.UUID}, x...)\n\nend```\nwhere `x…` would be the deserialized fields. There’d be generic definitions something like `fromarrow(V, x…) = nothing` and if there’s no overload, then we’d just return a NamedTuple..\n\nOn the serialization side, we’d have:\n• `Arrow.ArrowTypes.toarrow(x) = x`, and `Arrow.ArrowTypes.arrowname(::Type{T})` which would be overloadable for custom types where you’d return the arrow object to serialize, and `arrowname` would be the field metadata type name. By default `arrowname(T) = Symbol()`, so we wouldn’t name it, so you’d only get a NamedTuple out unless you overload. \nMy biggest question then is if this would really work for parametric types. You could serialize using `nameof` in `arrowname`, but then in `fromarrow`, you’d have to be able to build your type. I think it gets awkward if you have non-field-based type parameters that you don’t necessarily want to serialize in `arrowname`, but maybe that’s just a pill you have to swallow. If you can’t infer the type parameters from the data itself, you’ll need to serialize it in the `arrowname` and pull it back out somehow.","author_name":"Jacob Quinn","author_link":"https://julialang.slack.com/team/U681ELA87","author_icon":"https://secure.gravatar.com/avatar/d788bf7fd037ebef5798d8881c5faa2f.jpg?s=48&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0024-48.png","mrkdwn_in":["text"],"id":1,"original_url":"https://julialang.slack.com/archives/C674VR0HH/p1615846377461400?thread_ts=1615681758.430500&amp;cid=C674VR0HH","footer":"From a thread in #data"}],"blocks":[{"type":"rich_text","block_id":"Qld+/","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/JuliaData/Arrow.jl/pull/150","text":"define caller-overloadable arrownameof(T) to allow callers to specify preferred Julia <-> Arrow type matching behavior"},{"type":"text","text":"\n\nan easily replaceable stopgap to enable some custom (de)serialization behaviors until something like "},{"type":"user","user_id":"U681ELA87"},{"type":"text","text":"'s plan "},{"type":"link","url":"https://julialang.slack.com/archives/C674VR0HH/p1615846377461400?thread_ts=1615681758.430500&cid=C674VR0HH","text":"here"},{"type":"text","text":" lands"}]}]}]},{"client_msg_id":"42618c2d-fb35-4609-931c-4c9081839af2","type":"message","text":"<@UBF9YRB6H> some time ago you had this...\n`@where(flights, (:month .== 1) | (:month .== 2))` as an example of an `or` clause. Is this syntax still supported?","user":"UUYRZ3LU8","ts":"1616273235.114700","team":"T68168MUP","edited":{"user":"UUYRZ3LU8","ts":"1616273245.000000"},"blocks":[{"type":"rich_text","block_id":"opY6l","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UBF9YRB6H"},{"type":"text","text":" some time ago you had this...\n"},{"type":"text","text":"@where(flights, (:month .== 1) | (:month .== 2))","style":{"code":true}},{"type":"text","text":" as an example of an "},{"type":"text","text":"or","style":{"code":true}},{"type":"text","text":" clause. Is this syntax still supported?"}]}]}],"thread_ts":"1616273235.114700","reply_count":1,"reply_users_count":1,"latest_reply":"1616274463.114900","reply_users":["UBF9YRB6H"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"I don't think there is a vectorized `or` that would work for that use-case, actually. it's a shortcoming that we should fix.\n\nhere is something that will have the behavior you want\n\n```julia&gt; function ⩔(x, y) # \\Or\n       if x === missing || y === missing \n           return false\n       else\n           x || y\n       end\n       end\n⩔ (generic function with 1 method)\n\njulia&gt; [true] .⩔ [false]\n1-element BitArray{1}:\n 1```\nThis seems like an embarrassing absence in the ecosystem. Maybe I'm missing an obvious solution? In particular, you should be able to do `x .|| y`, but you can't. Should I file an issue in base?","user":"UBF9YRB6H","ts":"1616274463.114900","thread_ts":"1616273235.114700","root":{"client_msg_id":"42618c2d-fb35-4609-931c-4c9081839af2","type":"message","text":"<@UBF9YRB6H> some time ago you had this...\n`@where(flights, (:month .== 1) | (:month .== 2))` as an example of an `or` clause. Is this syntax still supported?","user":"UUYRZ3LU8","ts":"1616273235.114700","team":"T68168MUP","edited":{"user":"UUYRZ3LU8","ts":"1616273245.000000"},"blocks":[{"type":"rich_text","block_id":"opY6l","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UBF9YRB6H"},{"type":"text","text":" some time ago you had this...\n"},{"type":"text","text":"@where(flights, (:month .== 1) | (:month .== 2))","style":{"code":true}},{"type":"text","text":" as an example of an "},{"type":"text","text":"or","style":{"code":true}},{"type":"text","text":" clause. Is this syntax still supported?"}]}]}],"thread_ts":"1616273235.114700","reply_count":1,"reply_users_count":1,"latest_reply":"1616274463.114900","reply_users":["UBF9YRB6H"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"tK2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I don't think there is a vectorized "},{"type":"text","text":"or","style":{"code":true}},{"type":"text","text":" that would work for that use-case, actually. it's a shortcoming that we should fix.\n\nhere is something that will have the behavior you want\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> function ⩔(x, y) # \\Or\n       if x === missing || y === missing \n           return false\n       else\n           x || y\n       end\n       end\n⩔ (generic function with 1 method)\n\njulia> [true] .⩔ [false]\n1-element BitArray{1}:\n 1"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nThis seems like an embarrassing absence in the ecosystem. Maybe I'm missing an obvious solution? In particular, you should be able to do "},{"type":"text","text":"x .|| y","style":{"code":true}},{"type":"text","text":", but you can't. Should I file an issue in base?"}]}]}],"client_msg_id":"974c5b72-280a-4851-9025-fcdcf91a0366"},{"client_msg_id":"5815192f-1b1c-40db-93a0-eb49c8f7acf9","type":"message","text":"<@U7THT3TM3>, interested in your thoughts on <https://github.com/JuliaData/StructTypes.jl/pull/42> if you have a minute to review. I think it’s a common enough pattern that a few of us have just done in our own code (i.e. just calling `obj = JSON3.read(source); x = StructTypes.constructfrom(T, obj)`), that it would be nice to just support natively. Now you lower your own type when serializing (which is also familiar for regular JSON.jl users), and follow the `constructfrom` pattern when deserializing automatically.","user":"U681ELA87","ts":"1616298202.122400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QvK","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U7THT3TM3"},{"type":"text","text":", interested in your thoughts on "},{"type":"link","url":"https://github.com/JuliaData/StructTypes.jl/pull/42"},{"type":"text","text":" if you have a minute to review. I think it’s a common enough pattern that a few of us have just done in our own code (i.e. just calling "},{"type":"text","text":"obj = JSON3.read(source); x = StructTypes.constructfrom(T, obj)","style":{"code":true}},{"type":"text","text":"), that it would be nice to just support natively. Now you lower your own type when serializing (which is also familiar for regular JSON.jl users), and follow the "},{"type":"text","text":"constructfrom","style":{"code":true}},{"type":"text","text":" pattern when deserializing automatically."}]}]}],"thread_ts":"1616298202.122400","reply_count":1,"reply_users_count":1,"latest_reply":"1616298928.122700","reply_users":["U681ELA87"],"subscribed":false},{"type":"message","subtype":"channel_join","ts":"1616298216.122600","user":"U7THT3TM3","text":"<@U7THT3TM3> has joined the channel","inviter":"U681ELA87"},{"client_msg_id":"803AAC2A-4ED3-4852-B661-AAE064E2F4AC","type":"message","text":"In general, what do people recommend as the \"best\" approach to serializing/deserializing tabular data (e.g. DataFrames) to/from disk?\n\nArrow.jl?","user":"U7THT3TM3","ts":"1616374261.151200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dKxq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In general, what do people recommend as the \"best\" approach to serializing/deserializing tabular data (e.g. DataFrames) to/from disk?\n"},{"type":"text","text":"\nArrow.jl?"}]}]}]},{"client_msg_id":"EC6DC2C2-8C43-469D-815F-FBCABEFD6835","type":"message","text":"There are so many file formats out there, that I have a hard time keeping track of which one is the state of the art.","user":"U7THT3TM3","ts":"1616374288.151800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2hOn4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"There are so many file formats out there, that I have a hard time keeping track of which one is the state of the art."}]}]}]},{"client_msg_id":"C77BCBD8-F672-4B7E-B465-89999EA9CD00","type":"message","text":"I guess by \"best\" I probably mean fastest. But I'm open to other definitions of \"best\".","user":"U7THT3TM3","ts":"1616374595.152700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iEE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I guess by \"best\" I probably mean fastest. But I'm open to other definitions of \"best\"."}]}]}]},{"client_msg_id":"bebbbf5f-de2b-47c6-a288-9115c02ae167","type":"message","text":"Fastest then probably Arrow and HDF5.\n\nId recommend whatever is most used in your field. I know the biology field tends to be a fan of HDF5.","user":"U01FAHWCMFF","ts":"1616375369.156200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"NShf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Fastest then probably Arrow and HDF5.\n\nId recommend whatever is most used in your field. I know the biology field tends to be a fan of HDF5."}]}]}]},{"client_msg_id":"a9b4a476-10d1-4321-b7a7-6ace38837f47","type":"message","text":"Im in bioinformatics and we tend to use parquet alot because we care more about data compression then speed.","user":"U01FAHWCMFF","ts":"1616375415.157400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"NFHb1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Im in bioinformatics and we tend to use parquet alot because we care more about data compression then speed."}]}]}]},{"client_msg_id":"08780282-95c4-4d1e-867a-0814a5437f31","type":"message","text":"is it possible to skip all rows below `comment='#'` while reading in a csv `CSV.read(file; comment=\"#\")`  ? i see the comment only skips the given row.","user":"UPH1M2MB2","ts":"1616402364.160500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ryXgP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is it possible to skip all rows below "},{"type":"text","text":"comment='#'","style":{"code":true}},{"type":"text","text":" while reading in a csv "},{"type":"text","text":"CSV.read(file; comment=\"#\")","style":{"code":true}},{"type":"text","text":"  ? i see the comment only skips the given row."}]}]}]},{"client_msg_id":"e938d26a-aec0-44f0-b39f-2e90fd41593c","type":"message","text":"Question for everyone regarding the handling of data and best practices:\n\nI have a few different datasets that I would like to keep backed up. I have access to a - for my purposes - nearly unlimited storage space for storing my data. I am using DrWatson.jl to organize my datasets (i.e. exp_raw, exp_pro, and exp_sim) and was considering about backing up these different datasets and breakdowns. Does anyone have any suggestions on how to best store/manage my data? I was thinking about using something like `borg` to back up the data and to browse snapshots. I think my storage location has a RAID V set up. Thanks!","user":"US64J0NPQ","ts":"1616422369.166500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LHNC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Question for everyone regarding the handling of data and best practices:\n\nI have a few different datasets that I would like to keep backed up. I have access to a - for my purposes - nearly unlimited storage space for storing my data. I am using DrWatson.jl to organize my datasets (i.e. exp_raw, exp_pro, and exp_sim) and was considering about backing up these different datasets and breakdowns. Does anyone have any suggestions on how to best store/manage my data? I was thinking about using something like "},{"type":"text","text":"borg","style":{"code":true}},{"type":"text","text":" to back up the data and to browse snapshots. I think my storage location has a RAID V set up. Thanks!"}]}]}]},{"client_msg_id":"ca234a52-d0c0-45bd-b29d-ba2494200aa7","type":"message","text":"Alright peeps - I have a huge array of structs were all the fields are strings.","user":"U6QGE7S86","ts":"1616435062.173100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5DMQp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Alright peeps - I have a huge array of structs were all the fields are strings."}]}]}]},{"client_msg_id":"1f5ee8e3-1d7e-47cf-b8dd-09c3fac75a98","type":"message","text":"How can I write that into a huge JSON3 array?","user":"U6QGE7S86","ts":"1616435070.173400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"F2S","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I write that into a huge JSON3 array?"}]}]}]},{"client_msg_id":"b0862568-d066-443b-b082-6a58baa15736","type":"message","text":"eg I have\n```v = [Foo(\"a\"), Foo(\"b\"), Foo(\"c\")]```","user":"U6QGE7S86","ts":"1616436032.175200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ZjALK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"eg I have\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"v = [Foo(\"a\"), Foo(\"b\"), Foo(\"c\")]"}]}]}]},{"client_msg_id":"6d5cfb62-8831-415a-878f-b620386a0aee","type":"message","text":"And I want to write it into a JSON array","user":"U6QGE7S86","ts":"1616436052.175600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vDOfD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And I want to write it into a JSON array"}]}]}]},{"client_msg_id":"114e30dc-b299-43ae-b5dc-f69a9508dc23","type":"message","text":"(Can I assume that they will remain in order?)","user":"U6QGE7S86","ts":"1616436210.176600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"o9e","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(Can I assume that they will remain in order?)"}]}]}],"thread_ts":"1616436210.176600","reply_count":2,"reply_users_count":2,"latest_reply":"1616436253.177800","reply_users":["U6BNE7LTZ","U6QGE7S86"],"subscribed":false},{"client_msg_id":"9b2f9ae9-aab7-4752-a2b0-c24fa0e04d13","type":"message","text":"do you want just the strings written in a JSON array? like `[\"a\", \"b\", \"c\"]`? or do you want the structs written as JSON objects in the array, like `[{\"field\": \"a\"}, {\"field\": \"b\"}]`","user":"U681ELA87","ts":"1616436213.176800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9PGy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"do you want just the strings written in a JSON array? like "},{"type":"text","text":"[\"a\", \"b\", \"c\"]","style":{"code":true}},{"type":"text","text":"? or do you want the structs written as JSON objects in the array, like "},{"type":"text","text":"[{\"field\": \"a\"}, {\"field\": \"b\"}]","style":{"code":true}}]}]}],"reactions":[{"name":"+1::skin-tone-5","users":["U6QGE7S86"],"count":1}]},{"client_msg_id":"30511eb1-c67f-4185-96a3-f81635afa4a6","type":"message","text":"The latter if possible <@U681ELA87>","user":"U6QGE7S86","ts":"1616436237.177400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uLW/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The latter if possible "},{"type":"user","user_id":"U681ELA87"}]}]}]},{"client_msg_id":"2cbbc0e1-2988-485d-b669-70f21bcadba2","type":"message","text":"simplest would just be:\n```using StructTypes, JSON3\nStructTypes.StructType(::Type{Foo}) = StructTypes.Struct()\njson = JSON3.write(v)\nv2 = JSON3.read(json, Vector{Foo})```","user":"U681ELA87","ts":"1616436297.178800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lltH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"simplest would just be:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using StructTypes, JSON3\nStructTypes.StructType(::Type{Foo}) = StructTypes.Struct()\njson = JSON3.write(v)\nv2 = JSON3.read(json, Vector{Foo})"}]}]}]},{"client_msg_id":"b2fa6f5a-4b27-4057-ace0-d4616d9be437","type":"message","text":"B E A Uuuuuutiful!","user":"U6QGE7S86","ts":"1616436464.179200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JiWXG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"B E A Uuuuuutiful!"}]}]}]},{"client_msg_id":"bb14c53b-8fa7-4984-b799-24d924cd9855","type":"message","text":"Thanks a bunch <@U681ELA87>, this stuff feels like magic.","user":"U6QGE7S86","ts":"1616436475.179500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vAH0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks a bunch "},{"type":"user","user_id":"U681ELA87"},{"type":"text","text":", this stuff feels like magic."}]}]}]},{"client_msg_id":"c89defc6-3ab0-439d-895b-7e641b6f78fa","type":"message","text":"You could also just use JSON:\n```julia&gt; JSON.json(v)\n\"[{\\\"field\\\":\\\"a\\\"},{\\\"field\\\":\\\"b\\\"},{\\\"field\\\":\\\"c\\\"}]\"```","user":"U6BNE7LTZ","ts":"1616436494.180000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"anF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You could also just use JSON:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> JSON.json(v)\n\"[{\\\"field\\\":\\\"a\\\"},{\\\"field\\\":\\\"b\\\"},{\\\"field\\\":\\\"c\\\"}]\""}]}]}]},{"client_msg_id":"00662971-56bf-4405-aab8-d3ccbb5090df","type":"message","text":"Oh lol. I'll write that down thx.","user":"U6QGE7S86","ts":"1616436538.180300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lBuoi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oh lol. I'll write that down thx."}]}]}]},{"client_msg_id":"1b7386b0-6393-4d8b-ae9e-d779da2e1957","type":"message","text":"unless you need to read it back in as a `Vector{Foo}`. JSON3.jl gives you both with the one `StructTypes` definition.","user":"U681ELA87","ts":"1616436639.180900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Jcb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"unless you need to read it back in as a "},{"type":"text","text":"Vector{Foo}","style":{"code":true}},{"type":"text","text":". JSON3.jl gives you both with the one "},{"type":"text","text":"StructTypes","style":{"code":true}},{"type":"text","text":" definition."}]}]}],"reactions":[{"name":"+1","users":["U6BNE7LTZ"],"count":1}]},{"client_msg_id":"d7117f02-ce42-4ee2-9a41-460c6142addc","type":"message","text":"Gotcha.","user":"U6QGE7S86","ts":"1616436683.181100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2O0z4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Gotcha."}]}]}]},{"client_msg_id":"547d7b50-bcc1-4352-88e3-bb8d997ed055","type":"message","text":"Uhh, how do people manage writing files into the directory where the `Pkg/src` is? I don't know my way around the file system with Julia yet.","user":"U6QGE7S86","ts":"1616436720.182000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lR0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Uhh, how do people manage writing files into the directory where the "},{"type":"text","text":"Pkg/src","style":{"code":true}},{"type":"text","text":" is? I don't know my way around the file system with Julia yet."}]}]}]},{"client_msg_id":"1f0af5f7-014d-4a49-ba20-690aeb7ef6c3","type":"message","text":"How can one create a new dataframe for every unique string value in a specific column of a 4 column dataframe? Each new dataframe should also contain the same 4 columns fields but only include the rows which belong to the string value for which its dataframe was created.","user":"U01QJ915TFD","ts":"1616438403.187300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ozY1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can one create a new dataframe for every unique string value in a specific column of a 4 column dataframe? Each new dataframe should also contain the same 4 columns fields but only include the rows which belong to the string value for which its dataframe was created."}]}]}],"thread_ts":"1616438403.187300","reply_count":1,"reply_users_count":1,"latest_reply":"1616438886.189600","reply_users":["UBF9YRB6H"],"subscribed":false},{"type":"message","text":"As this picture shows, the values for the first column are different. There are about 100 different strings and each of them repeats itself about 100 times. So I would like to extract those 100 occurences of one string and make that into its own dataframe but for every single different string.","files":[{"id":"F01SF8DGG3T","created":1616438682,"timestamp":1616438682,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01QJ915TFD","editable":false,"size":123891,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01SF8DGG3T/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01SF8DGG3T/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01SF8DGG3T-7b6dbc3035/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01SF8DGG3T-7b6dbc3035/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01SF8DGG3T-7b6dbc3035/image_360.png","thumb_360_w":360,"thumb_360_h":73,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01SF8DGG3T-7b6dbc3035/image_480.png","thumb_480_w":480,"thumb_480_h":97,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01SF8DGG3T-7b6dbc3035/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01SF8DGG3T-7b6dbc3035/image_720.png","thumb_720_w":720,"thumb_720_h":145,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01SF8DGG3T-7b6dbc3035/image_800.png","thumb_800_w":800,"thumb_800_h":161,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01SF8DGG3T-7b6dbc3035/image_960.png","thumb_960_w":960,"thumb_960_h":194,"thumb_1024":"https://files.slack.com/files-tmb/T68168MUP-F01SF8DGG3T-7b6dbc3035/image_1024.png","thumb_1024_w":1024,"thumb_1024_h":206,"original_w":1111,"original_h":224,"thumb_tiny":"AwAJADCmuT3IoKYGaRaU9DVECkE9SaBGPWigUAJjnb2pOq59KP4jQPumgD//2Q==","permalink":"https://julialang.slack.com/files/U01QJ915TFD/F01SF8DGG3T/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01SF8DGG3T-e7946a04be","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"Ryaq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"As this picture shows, the values for the first column are different. There are about 100 different strings and each of them repeats itself about 100 times. So I would like to extract those 100 occurences of one string and make that into its own dataframe but for every single different string."}]}]}],"user":"U01QJ915TFD","display_as_bot":false,"ts":"1616438794.189300"},{"client_msg_id":"0161dfa9-564f-465c-920b-592e004953ac","type":"message","text":"We should define a vararg `ismissing` for easy control flow. `ismissing(x,y, x) &amp;&amp; return false`.\n\nOr maybe a vararg `allmissing(x, y, z)` and `anymissing(x, y, z)`.","user":"UBF9YRB6H","ts":"1616450440.191300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"XLuOY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We should define a vararg "},{"type":"text","text":"ismissing","style":{"code":true}},{"type":"text","text":" for easy control flow. "},{"type":"text","text":"ismissing(x,y, x) && return false","style":{"code":true}},{"type":"text","text":".\n\nOr maybe a vararg "},{"type":"text","text":"allmissing(x, y, z)","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"anymissing(x, y, z)","style":{"code":true}},{"type":"text","text":"."}]}]}]},{"client_msg_id":"a13b146b-43df-4e33-89f4-8395e3551043","type":"message","text":"You can write `ismissing(coalesce(x, y, z))`","user":"U680THK2S","ts":"1616450847.191900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0f=R","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You can write "},{"type":"text","text":"ismissing(coalesce(x, y, z))","style":{"code":true}}]}]}],"thread_ts":"1616450847.191900","reply_count":1,"reply_users_count":1,"latest_reply":"1616451155.192100","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"bc3d079e-32d5-4c3b-a4be-20588501aa1c","type":"message","text":"Currently have something like\n\n```\tP_hired_match = 1 - cdf(F, sqrt(1 + σ_ϵ^2) * c_v)\n\n\tP_hired = V * P_hired_match\n\n\tE_y_hired = @chain begin\n\t\tNormal(0, sqrt(1 + σ_ϵ^2))\n\t\tTruncated(c_v, +Inf)\n\t\tmean\n\tend\n\n\tE_z_hired = (1 / (1 + σ_ϵ^2)) * E_y_hired\n\n\tY = P_hired * E_z_hired\n\n\tE_w_hired = β * (E_z_hred - c_v)\n\n\tE_w_hired = P_hired * E_w_hired\n\n\tE_π_hired = (1 - β) * (E_z_hired - c_v)\n\n\tE_π = P_hired_match * E_π_hired```\nand I want to collect all these values into something where a vector of that thing is a Table.\n\n1. I don't want to re-write all the variables I made into a named tuple like `(; Y, E_w_hired...)`. That's annoying\n2. I can't add to a Dict iteratively, because then I have to type more, like `D[\"E_w_hired\"] = ...`. Also that won't be converted to a Table.\n\nI have a package I've been working on offline that can do this, but I haven't finished tests so I don't want to rely on it right now. Is there package that already solves this problem?\n\nI think I remember something like a wrapper for `Dict`s that allows `getproperty`.","user":"UBF9YRB6H","ts":"1616526171.200600","team":"T68168MUP","edited":{"user":"UBF9YRB6H","ts":"1616526196.000000"},"blocks":[{"type":"rich_text","block_id":"sGQB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Currently have something like\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"\tP_hired_match = 1 - cdf(F, sqrt(1 + σ_ϵ^2) * c_v)\n\n\tP_hired = V * P_hired_match\n\n\tE_y_hired = @chain begin\n\t\tNormal(0, sqrt(1 + σ_ϵ^2))\n\t\tTruncated(c_v, +Inf)\n\t\tmean\n\tend\n\n\tE_z_hired = (1 / (1 + σ_ϵ^2)) * E_y_hired\n\n\tY = P_hired * E_z_hired\n\n\tE_w_hired = β * (E_z_hred - c_v)\n\n\tE_w_hired = P_hired * E_w_hired\n\n\tE_π_hired = (1 - β) * (E_z_hired - c_v)\n\n\tE_π = P_hired_match * E_π_hired"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"and I want to collect all these values into something where a vector of that thing is a Table.\n\n1. I don't want to re-write all the variables I made into a named tuple like "},{"type":"text","text":"(; Y, E_w_hired...)","style":{"code":true}},{"type":"text","text":". That's annoying\n2. I can't add to a Dict iteratively, because then I have to type more, like "},{"type":"text","text":"D[\"E_w_hired\"] = ...","style":{"code":true}},{"type":"text","text":". Also that won't be converted to a Table.\n\nI have a package I've been working on offline that can do this, but I haven't finished tests so I don't want to rely on it right now. Is there package that already solves this problem?\n\nI think I remember something like a wrapper for `Dict`s that allows "},{"type":"text","text":"getproperty","style":{"code":true}},{"type":"text","text":"."}]}]}],"thread_ts":"1616526171.200600","reply_count":2,"reply_users_count":1,"latest_reply":"1616526543.201300","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"f61a1d4a-c808-498d-b4b6-67571ce9b70a","type":"message","text":"How do I replace all the values of a dataframe column to a substring of the current values?\nFor example, my column's title is Name and I'd like to only keep the last five numbers. In the first row below that would be just `52000`\n\n `Name`\n`TSLAL042052000.U`\n`TSLAL042062000.U`\n\n\n`replace!(df.Name, ?????? ) =&gt; [10:15])`\nThis is my current incomplete attempt. I'm guessing I'm close but I can't figure out how to reference the values","user":"U01QJ915TFD","ts":"1616558594.210800","team":"T68168MUP","edited":{"user":"U01QJ915TFD","ts":"1616558650.000000"},"blocks":[{"type":"rich_text","block_id":"ji8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How do I replace all the values of a dataframe column to a substring of the current values?\nFor example, my column's title is Name and I'd like to only keep the last five numbers. In the first row below that would be just "},{"type":"text","text":"52000","style":{"code":true}},{"type":"text","text":"\n\n "},{"type":"text","text":"Name","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"TSLAL042052000.U","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"TSLAL042062000.U","style":{"code":true}},{"type":"text","text":"\n\n\n"},{"type":"text","text":"replace!(df.Name, ?????? ) => [10:15])","style":{"code":true}},{"type":"text","text":"\nThis is my current incomplete attempt. I'm guessing I'm close but I can't figure out how to reference the values"}]}]}]},{"client_msg_id":"f7cf2066-00a3-4f85-ac82-b211926277dd","type":"message","text":"I'm trying to add a character in between all my values of a specific column.  I've tried the following but it's not being applied to my data frame. It's just outputting an example of the desired result.\n\n```dot = \".\"\nfor name in df.Name \n  return name[1:3] * \"$(dot)\" * name[4:end]\nend```\n This is the output I'm getting:\n```\"520.00\"```\nThis is exactly the format I'd like but applied to all values of the column","user":"U01QJ915TFD","ts":"1616610232.218700","team":"T68168MUP","edited":{"user":"U01QJ915TFD","ts":"1616610255.000000"},"blocks":[{"type":"rich_text","block_id":"m49","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm trying to add a character in between all my values of a specific column.  I've tried the following but it's not being applied to my data frame. It's just outputting an example of the desired result.\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"dot = \".\"\nfor name in df.Name \n  return name[1:3] * \"$(dot)\" * name[4:end]\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":" This is the output I'm getting:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"\"520.00\""}]},{"type":"rich_text_section","elements":[{"type":"text","text":"This is exactly the format I'd like but applied to all values of the column"}]}]}],"thread_ts":"1616610232.218700","reply_count":9,"reply_users_count":3,"latest_reply":"1616610916.220600","reply_users":["UBF9YRB6H","U01C3624SGJ","U01QJ915TFD"],"is_locked":false,"subscribed":false},{"client_msg_id":"6de9b234-3c3f-40a6-bc98-fb48f3d45f15","type":"message","text":"Hi, I am trying to create a new column in my data frame. The source column is a `Date` and the target column should be a `string` with the year and the month. I have the following code\n```transform(cxvolume, :CREATED_ON =&gt; (a -&gt; Dates.format(a, \"mm yyyy\")) =&gt; :CREATED_ON_MONTH_YEAR)```\nbut i have this error message\n```MethodError: no method matching format(::SentinelArrays.ChainedVector{Dates.Date,Array{Dates.Date,1}}, ::String)```\nAny idea why?","user":"U01EZ6VN118","ts":"1616612004.222100","team":"T68168MUP","edited":{"user":"U01EZ6VN118","ts":"1616612042.000000"},"blocks":[{"type":"rich_text","block_id":"al5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I am trying to create a new column in my data frame. The source column is a "},{"type":"text","text":"Date","style":{"code":true}},{"type":"text","text":" and the target column should be a "},{"type":"text","text":"string","style":{"code":true}},{"type":"text","text":" with the year and the month. I have the following code\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"transform(cxvolume, :CREATED_ON => (a -> Dates.format(a, \"mm yyyy\")) => :CREATED_ON_MONTH_YEAR)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"but i have this error message\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"MethodError: no method matching format(::SentinelArrays.ChainedVector{Dates.Date,Array{Dates.Date,1}}, ::String)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Any idea why?"}]}]}],"thread_ts":"1616612004.222100","reply_count":4,"reply_users_count":2,"latest_reply":"1616612355.223300","reply_users":["U67SCG4HG","U680THK2S"],"is_locked":false,"subscribed":false},{"client_msg_id":"5fa97f4b-4a20-4b36-915f-5c7e995fd810","type":"message","text":"Playing around with the <https://docs.juliaplots.org/latest/|JuliaPlots> package -- which plot can I use to have bar / histogram chart with categorical x-axis? It looks like `bar` and `histogram` expect numbers in x-axis","user":"U01EZ6VN118","ts":"1616626920.225200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7EI0K","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Playing around with the "},{"type":"link","url":"https://docs.juliaplots.org/latest/","text":"JuliaPlots"},{"type":"text","text":" package -- which plot can I use to have bar / histogram chart with categorical x-axis? It looks like "},{"type":"text","text":"bar","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"histogram","style":{"code":true}},{"type":"text","text":" expect numbers in x-axis"}]}]}],"thread_ts":"1616626920.225200","reply_count":8,"reply_users_count":3,"latest_reply":"1616628040.226800","reply_users":["U6A936746","U01EZ6VN118","U8JP5B9T2"],"is_locked":false,"subscribed":false},{"client_msg_id":"dc4ad704-27d5-4c5a-bedc-9ed7a8459dae","type":"message","text":"How can I rename multiple columns in my dataframe to be the same as the first row below them? In my case, I'd like to do this to every 3rd column. I'm considering using something like this if it works but not sure where to take it from here:\n\n```rename!(df, [Symbol(\"col$i\") for i in 2:3:94]```\n","user":"U01QJ915TFD","ts":"1616637610.236900","team":"T68168MUP","edited":{"user":"U01QJ915TFD","ts":"1616639503.000000"},"blocks":[{"type":"rich_text","block_id":"MSz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I rename multiple columns in my dataframe to be the same as the first row below them? In my case, I'd like to do this to every 3rd column. I'm considering using something like this if it works but not sure where to take it from here:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"rename!(df, [Symbol(\"col$i\") for i in 2:3:94]"}]},{"type":"rich_text_section","elements":[]}]}]},{"type":"message","text":"Here's what my df looks like","files":[{"id":"F01SJN0S868","created":1616639489,"timestamp":1616639489,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01QJ915TFD","editable":false,"size":132401,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01SJN0S868/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01SJN0S868/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01SJN0S868-da5337096b/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01SJN0S868-da5337096b/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01SJN0S868-da5337096b/image_360.png","thumb_360_w":360,"thumb_360_h":75,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01SJN0S868-da5337096b/image_480.png","thumb_480_w":480,"thumb_480_h":99,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01SJN0S868-da5337096b/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01SJN0S868-da5337096b/image_720.png","thumb_720_w":720,"thumb_720_h":149,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01SJN0S868-da5337096b/image_800.png","thumb_800_w":800,"thumb_800_h":166,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01SJN0S868-da5337096b/image_960.png","thumb_960_w":960,"thumb_960_h":199,"thumb_1024":"https://files.slack.com/files-tmb/T68168MUP-F01SJN0S868-da5337096b/image_1024.png","thumb_1024_w":1024,"thumb_1024_h":212,"original_w":1212,"original_h":251,"thumb_tiny":"AwAJADCorMe9IVzyTSLS9qogMfTkUg7H8KPT6UDoPrQAevTilxyRx+VJ60vf8KAP/9k=","permalink":"https://julialang.slack.com/files/U01QJ915TFD/F01SJN0S868/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01SJN0S868-d35981d2ca","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"JPp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Here's what my df looks like"}]}]}],"user":"U01QJ915TFD","display_as_bot":false,"ts":"1616639494.239000","edited":{"user":"U01QJ915TFD","ts":"1616639513.000000"}},{"client_msg_id":"a42304f2-9347-47b2-82d2-62083190bff4","type":"message","text":"If what I posted above is not really possible, something else I'd like to learn how to do would be add something to the end of all column names or to a range of them. For example, say I wanted to add the letter \"a\" to the end of all column names.","user":"U01QJ915TFD","ts":"1616640354.242100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vjN7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If what I posted above is not really possible, something else I'd like to learn how to do would be add something to the end of all column names or to a range of them. For example, say I wanted to add the letter \"a\" to the end of all column names."}]}]}]},{"client_msg_id":"94c63e73-e51f-4606-ac90-6bb4cdbc0613","type":"message","text":"Are there some established JSON serialization formats for DataFrames?","user":"UB2QSHWPN","ts":"1616669419.246700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SEZXd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Are there some established JSON serialization formats for DataFrames?"}]}]}]},{"client_msg_id":"4036d67a-8c3f-47e3-9eb7-f388ceb9bcca","type":"message","text":"Why can we `push!` a row into a DataFrame, but can’t `pushfirst!`? I assume there’s a reason it is not defined","user":"ULMSCCJ4C","ts":"1616675092.249600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kST","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Why can we "},{"type":"text","text":"push!","style":{"code":true}},{"type":"text","text":" a row into a DataFrame, but can’t "},{"type":"text","text":"pushfirst!","style":{"code":true}},{"type":"text","text":"? I assume there’s a reason it is not defined"}]}]}],"thread_ts":"1616675092.249600","reply_count":3,"reply_users_count":2,"latest_reply":"1616677823.252600","reply_users":["U8JAMQGQY","ULMSCCJ4C"],"is_locked":false,"subscribed":false},{"client_msg_id":"c9c1884b-1c9d-435d-ad2d-33b9ac168493","type":"message","text":"We have just made a DataFrames 0.22.6 release. Here are the release notes: <https://discourse.julialang.org/t/release-announcements-for-dataframes-jl/18258/124>","user":"U8JAMQGQY","ts":"1616676602.251900","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"Release announcements for DataFrames.jl","title_link":"https://discourse.julialang.org/t/release-announcements-for-dataframes-jl/18258/124","text":"We have just made a 0.22.6 patch release. Hopefully it is the last release before 1.0 release. We have decided to make 0.22.6 release to deprecate some outstanding things that should be removed in 1.0 release, but were missed in the 0.22 release process. These functionalities are on the border of being an error, but since they worked in the past we have decided to have a release that will allow users to go through deprecation cycle (although we assume that most likely the deprecated methods are...","fallback":"JuliaLang: Release announcements for DataFrames.jl","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","ts":1616676542,"from_url":"https://discourse.julialang.org/t/release-announcements-for-dataframes-jl/18258/124","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/release-announcements-for-dataframes-jl/18258/124"}],"blocks":[{"type":"rich_text","block_id":"mf=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We have just made a DataFrames 0.22.6 release. Here are the release notes: "},{"type":"link","url":"https://discourse.julialang.org/t/release-announcements-for-dataframes-jl/18258/124"}]}]}]},{"client_msg_id":"7db74b6b-e58d-41e3-8ab0-15e4b2f2adfc","type":"message","text":"I feel kinda silly but is it possible to groupby, transform without ungrouping, and then sort by this new column?","user":"U01C2E6TYEM","ts":"1616679254.253700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nAly","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I feel kinda silly but is it possible to groupby, transform without ungrouping, and then sort by this new column?"}]}]}],"thread_ts":"1616679254.253700","reply_count":1,"reply_users_count":1,"latest_reply":"1616681554.254400","reply_users":["U8JAMQGQY"],"is_locked":false,"subscribed":false},{"client_msg_id":"32864029-da67-40de-aa9f-e3994ceea357","type":"message","text":"so just to be clear the new reworked `join` is *not* included yet, is that correct?","user":"U9VG1AYSG","ts":"1616680913.254100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SBj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"so just to be clear the new reworked "},{"type":"text","text":"join","style":{"code":true}},{"type":"text","text":" is "},{"type":"text","text":"not","style":{"bold":true}},{"type":"text","text":" included yet, is that correct?"}]}]}],"thread_ts":"1616680913.254100","reply_count":1,"reply_users_count":1,"latest_reply":"1616681310.254200","reply_users":["U8JAMQGQY"],"is_locked":false,"subscribed":false},{"client_msg_id":"5997204e-5ea2-4267-9ad3-c4ac2a423007","type":"message","text":"I am using Julia 1.6 now and CategoricalArrays v0.9.3. I get this error `CategoricalArray only supports AbstractString, AbstractChar and Number element types (got element type NamedTuple{(:Arm,), Tuple{String}})` . when trying to make a dataframe where one of the columns is a NamedTuple type which is then supposed to be converted to Categorical so that it can be transferred to R for ggplot. It used to work before so what happened now? Does CategoricalArrays not support NamedTuple in a column now? The code I have was working before a few weeks ago.","user":"U01EF0QVAB0","ts":"1616711434.260700","team":"T68168MUP","edited":{"user":"U01EF0QVAB0","ts":"1616712089.000000"},"blocks":[{"type":"rich_text","block_id":"3EYrb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am using Julia 1.6 now and CategoricalArrays v0.9.3. I get this error "},{"type":"text","text":"CategoricalArray only supports AbstractString, AbstractChar and Number element types (got element type NamedTuple{(:Arm,), Tuple{String}})","style":{"code":true}},{"type":"text","text":" . when trying to make a dataframe where one of the columns is a NamedTuple type which is then supposed to be converted to Categorical so that it can be transferred to R for ggplot. It used to work before so what happened now? Does CategoricalArrays not support NamedTuple in a column now? The code I have was working before a few weeks ago."}]}]}],"thread_ts":"1616711434.260700","reply_count":2,"reply_users_count":2,"latest_reply":"1616714182.266000","reply_users":["U8JAMQGQY","U01EF0QVAB0"],"is_locked":false,"subscribed":false},{"type":"message","text":"Could you help me with an unexpected result in a DataFrame?\n\nI'm trying to extract some information from a gzip CSV file and I found out a way to extract the values this way:\n\n```a_copy = load(File(format\"CSV\", file), delim=';') |&gt; DataFrame\n\na_cols = select(a_copy, :descritivo_item, :valor_total) # 592473x2 DataFrame\n\nfiltered_copy = a_cols[ismissing.(a_cols[!, :descritivo_item]), :] # 0x2 DataFrame```\nHowever, I'm struggling to understand why the DataFrame returned with cells that has no value instead of missing (see the screenshot)\n\nAny ideas of what I may be doing wrong here?","files":[{"id":"F01T6AQNGUQ","created":1616712760,"timestamp":1616712760,"name":"Screenshot Julia.png","title":"Screenshot Julia.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U0152N9899D","editable":false,"size":63588,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01T6AQNGUQ/screenshot_julia.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01T6AQNGUQ/download/screenshot_julia.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01T6AQNGUQ-dbc58e4086/screenshot_julia_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01T6AQNGUQ-dbc58e4086/screenshot_julia_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01T6AQNGUQ-dbc58e4086/screenshot_julia_360.png","thumb_360_w":360,"thumb_360_h":253,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01T6AQNGUQ-dbc58e4086/screenshot_julia_480.png","thumb_480_w":480,"thumb_480_h":337,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01T6AQNGUQ-dbc58e4086/screenshot_julia_160.png","original_w":604,"original_h":424,"thumb_tiny":"AwAhADCoUAGeaNq+tOP3RRxn2pgM2r60bR/eFO/DNH/AaAGn0zQVwAaD1pW6CgBx+6KCfeg/do2j0oAM5H/16Q7c8ijA44pcDPSgBh9qVugpD1pW6CgBT92lpD92loAQdqB1H0oHagdR9KAGmlakNK1AH//Z","permalink":"https://julialang.slack.com/files/U0152N9899D/F01T6AQNGUQ/screenshot_julia.png","permalink_public":"https://slack-files.com/T68168MUP-F01T6AQNGUQ-2e1af0e3eb","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"pe5Z0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Could you help me with an unexpected result in a DataFrame?\n\nI'm trying to extract some information from a gzip CSV file and I found out a way to extract the values this way:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"a_copy = load(File(format\"CSV\", file), delim=';') |> DataFrame\n\na_cols = select(a_copy, :descritivo_item, :valor_total) # 592473x2 DataFrame\n\nfiltered_copy = a_cols[ismissing.(a_cols[!, :descritivo_item]), :] # 0x2 DataFrame"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nHowever, I'm struggling to understand why the DataFrame returned with cells that has no value instead of missing (see the screenshot)\n\nAny ideas of what I may be doing wrong here?"}]}]}],"user":"U0152N9899D","display_as_bot":false,"ts":"1616712789.265000","thread_ts":"1616712789.265000","reply_count":2,"reply_users_count":2,"latest_reply":"1616713981.265800","reply_users":["U8JAMQGQY","U0152N9899D"],"is_locked":false,"subscribed":false},{"client_msg_id":"da0c85b0-d279-4aba-b470-a1e5a7b5edd9","type":"message","text":"I have a grouped and sorted dataframe and want to add a column with `transform` that is just an integer from 1 to n_groups. How do I do that?\n\nThere must be something like `nrow`, maybe `igroup` . data.table has `.GRP` for example\n```DT[ , i := .GRP, by = key(DT)]```","user":"UK1BNFHFV","ts":"1616752551.272700","team":"T68168MUP","edited":{"user":"UK1BNFHFV","ts":"1616752780.000000"},"blocks":[{"type":"rich_text","block_id":"oIy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a grouped and sorted dataframe and want to add a column with "},{"type":"text","text":"transform","style":{"code":true}},{"type":"text","text":" that is just an integer from 1 to n_groups. How do I do that?\n\nThere must be something like "},{"type":"text","text":"nrow","style":{"code":true}},{"type":"text","text":", maybe "},{"type":"text","text":"igroup","style":{"code":true}},{"type":"text","text":" . data.table has "},{"type":"text","text":".GRP","style":{"code":true}},{"type":"text","text":" for example\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"DT[ , i := .GRP, by = key(DT)]"}]}]}]},{"client_msg_id":"9ff64acb-e9c2-475a-b379-d4c00bcfb103","type":"message","text":"I am using ODBC.jl (Julia x86, win10) to connect to an old MS Access DB *.mde file and query dataframes from it, but seem there are cells with Cyrillic text, and I get wrong encoding from it. Setting locale identifiers into connection string doesn't help. Can it be solved on Julia side, or should I dig into Access properties?\n```using ODBC\n\nconn_str  = \"Driver={Microsoft Access Driver (*.mdb)};Dbq=$dbfile;Uid=Admin;Pwd=;\"\n#conn_str  = \"Driver={Microsoft Access Driver (*.mdb)};Dbq=C:\\mydatabase.mde;Locale Identifier=1049;Uid=Admin;Pwd=;\" # ru-RU locales\n\nconn = ODBC.Connection(conn_str)\n\ndf = DBInterface.execute(conn, \"SELECT * from DefFil\") |&gt; DataFrame```\nI get the following data:\n```julia&gt; select(df, :text_cnp)\n22042×1 DataFrame\n   Row │ text_cnp\n       │ String?\n───────┼───────────────────────────────────\n     1 │ \\r\\n\\r\\n   \\xc8\\xf1\\xf1\\xeb\\xe5\\…\n     2 │ \\r\\n\\r\\n   \\xc8\\xf1\\xf1\\xeb\\xe5\\…\n     3 │ \\r\\n\\r\\n   \\xc8\\xf1\\xf1\\xeb\\xe5\\…\n     4 │ \\r\\n\\r\\n   \\xc8\\xf1\\xf1\\xeb\\xe5\\…\n     5 │ \\r\\n\\r\\n   \\xc8\\xf1\\xf1\\xeb\\xe5\\…\n     6 │ \\r\\n\\r\\n   \\xc8\\xf1\\xf1\\xeb\\xe5\\…\n     7 │ \\r\\n\\r\\n   \\xc8\\xf1\\xf1\\xeb\\xe5\\…\n     8 │ \\r\\n\\r\\n   \\xc8\\xf1\\xf1\\xeb\\xe5\\…```","user":"UB2QSHWPN","ts":"1616752672.273200","team":"T68168MUP","edited":{"user":"UB2QSHWPN","ts":"1616752740.000000"},"blocks":[{"type":"rich_text","block_id":"oMP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am using ODBC.jl (Julia x86, win10) to connect to an old MS Access DB *.mde file and query dataframes from it, but seem there are cells with Cyrillic text, and I get wrong encoding from it. Setting locale identifiers into connection string doesn't help. Can it be solved on Julia side, or should I dig into Access properties?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using ODBC\n\nconn_str  = \"Driver={Microsoft Access Driver (*.mdb)};Dbq=$dbfile;Uid=Admin;Pwd=;\"\n#conn_str  = \"Driver={Microsoft Access Driver (*.mdb)};Dbq=C:\\mydatabase.mde;Locale Identifier=1049;Uid=Admin;Pwd=;\" # ru-RU locales\n\nconn = ODBC.Connection(conn_str)\n\ndf = DBInterface.execute(conn, \"SELECT * from DefFil\") |> DataFrame"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nI get the following data:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> select(df, :text_cnp)\n22042×1 DataFrame\n   Row │ text_cnp\n       │ String?\n───────┼───────────────────────────────────\n     1 │ \\r\\n\\r\\n   \\xc8\\xf1\\xf1\\xeb\\xe5\\…\n     2 │ \\r\\n\\r\\n   \\xc8\\xf1\\xf1\\xeb\\xe5\\…\n     3 │ \\r\\n\\r\\n   \\xc8\\xf1\\xf1\\xeb\\xe5\\…\n     4 │ \\r\\n\\r\\n   \\xc8\\xf1\\xf1\\xeb\\xe5\\…\n     5 │ \\r\\n\\r\\n   \\xc8\\xf1\\xf1\\xeb\\xe5\\…\n     6 │ \\r\\n\\r\\n   \\xc8\\xf1\\xf1\\xeb\\xe5\\…\n     7 │ \\r\\n\\r\\n   \\xc8\\xf1\\xf1\\xeb\\xe5\\…\n     8 │ \\r\\n\\r\\n   \\xc8\\xf1\\xf1\\xeb\\xe5\\…"}]}]}]},{"client_msg_id":"79ad86ea-6812-4122-ae1e-a1fa208fbc27","type":"message","text":"any suggestions on how can I use `|&gt;` for `select` with dataframes?\n```julia&gt; df = DataFrame(:a =&gt; [1, 2], :b =&gt; [1, 2])\n2×2 DataFrame\n Row │ a      b     \n     │ Int64  Int64 \n─────┼──────────────\n   1 │     1      1\n   2 │     2      2\n\njulia&gt; select(df, :a)\n2×1 DataFrame\n Row │ a     \n     │ Int64 \n─────┼───────\n   1 │     1\n   2 │     2\n\njulia&gt; df |&gt; select(:a)\nERROR: MethodError: no method matching select(::Symbol)\nClosest candidates are:\n  select(::AbstractDataFrame, ::Any...; copycols, renamecols) at C:\\Users\\deyan\\.julia\\packages\\DataFrames\\oQ5c7\\src\\abstractdataframe\\selection.jl:847\n  select(::Union{Function, Type}, ::AbstractDataFrame; renamecols) at C:\\Users\\deyan\\.julia\\packages\\DataFrames\\oQ5c7\\src\\abstractdataframe\\selection.jl:850\n  select(::Union{Function, Type}, ::GroupedDataFrame; copycols, keepkeys, ungroup, renamecols) at C:\\Users\\deyan\\.julia\\packages\\DataFrames\\oQ5c7\\src\\groupeddataframe\\splitapplycombine.jl:642\n  ...\nStacktrace:\n [1] top-level scope\n   @ REPL[15]:1\n\njulia&gt; ```","user":"U8WEJ293L","ts":"1616837802.287900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HjYd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"any suggestions on how can I use "},{"type":"text","text":"|>","style":{"code":true}},{"type":"text","text":" for "},{"type":"text","text":"select","style":{"code":true}},{"type":"text","text":" with dataframes?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> df = DataFrame(:a => [1, 2], :b => [1, 2])\n2×2 DataFrame\n Row │ a      b     \n     │ Int64  Int64 \n─────┼──────────────\n   1 │     1      1\n   2 │     2      2\n\njulia> select(df, :a)\n2×1 DataFrame\n Row │ a     \n     │ Int64 \n─────┼───────\n   1 │     1\n   2 │     2\n\njulia> df |> select(:a)\nERROR: MethodError: no method matching select(::Symbol)\nClosest candidates are:\n  select(::AbstractDataFrame, ::Any...; copycols, renamecols) at C:\\Users\\deyan\\.julia\\packages\\DataFrames\\oQ5c7\\src\\abstractdataframe\\selection.jl:847\n  select(::Union{Function, Type}, ::AbstractDataFrame; renamecols) at C:\\Users\\deyan\\.julia\\packages\\DataFrames\\oQ5c7\\src\\abstractdataframe\\selection.jl:850\n  select(::Union{Function, Type}, ::GroupedDataFrame; copycols, keepkeys, ungroup, renamecols) at C:\\Users\\deyan\\.julia\\packages\\DataFrames\\oQ5c7\\src\\groupeddataframe\\splitapplycombine.jl:642\n  ...\nStacktrace:\n [1] top-level scope\n   @ REPL[15]:1\n\njulia> "}]}]}],"thread_ts":"1616837802.287900","reply_count":3,"reply_users_count":2,"latest_reply":"1616838876.288400","reply_users":["U017J1FHTSA","U8WEJ293L"],"is_locked":false,"subscribed":false},{"client_msg_id":"ed99a910-496c-4cb9-847c-7fee6c47bca7","type":"message","text":"Has anyone tried using deepnote? Their free version looks good","user":"U01GXNFKY6R","ts":"1616840664.289200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RRX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Has anyone tried using deepnote? Their free version looks good"}]}]}]},{"client_msg_id":"82c5fee4-f9c3-4971-81e2-252b05521e2d","type":"message","text":"They have support for a Julia kernel too <https://docs.deepnote.com/environment/custom-environments/running-your-own-kernel#julia-kernel>","user":"U01GXNFKY6R","ts":"1616842321.289700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6Lz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"They have support for a Julia kernel too "},{"type":"link","url":"https://docs.deepnote.com/environment/custom-environments/running-your-own-kernel#julia-kernel"}]}]}]},{"client_msg_id":"015b4958-e704-46f2-af9a-34047a6ae901","type":"message","text":"I have a Dataframe with these column types:\n```eltype.(eachcol(planets))\n\n4-element Vector{DataType}:\n String\n Quantity{Float64, 𝐌, Unitful.FreeUnits{(kg,), 𝐌, nothing}}\n Quantity{Int64, 𝐋, Unitful.FreeUnits{(km,), 𝐋, nothing}}\n Quantity{Float64, 𝐓, Unitful.FreeUnits{(d,), 𝐓, nothing}}```\nI want to save this to disk, and read it to another Julia program while retaining the units.\nI can save it to CSV:\n```planet,mass,diameter,orbital_period\nMercury,3.3000000000000004e24 kg,4879 km,88.0 d\nVenus,4.8700000000000005e25 kg,12104 km,224.7 d\n...```\nBut when I read it in, it’s all strings.  This is not surprising, but I’d like to fix it.\nSuggestions? Just ‘remember’ the types and don’t store them in CSV, or somehow parse the “4879 km” back to unitful, which I didn’t notice in the docs for that package.","user":"U01CQTKB86N","ts":"1617013731.294200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WCvJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a Dataframe with these column types:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"eltype.(eachcol(planets))\n\n4-element Vector{DataType}:\n String\n Quantity{Float64, 𝐌, Unitful.FreeUnits{(kg,), 𝐌, nothing}}\n Quantity{Int64, 𝐋, Unitful.FreeUnits{(km,), 𝐋, nothing}}\n Quantity{Float64, 𝐓, Unitful.FreeUnits{(d,), 𝐓, nothing}}"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I want to save this to disk, and read it to another Julia program while retaining the units.\nI can save it to CSV:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"planet,mass,diameter,orbital_period\nMercury,3.3000000000000004e24 kg,4879 km,88.0 d\nVenus,4.8700000000000005e25 kg,12104 km,224.7 d\n..."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"But when I read it in, it’s all strings.  This is not surprising, but I’d like to fix it.\nSuggestions? Just ‘remember’ the types and don’t store them in CSV, or somehow parse the “4879 km” back to unitful, which I didn’t notice in the docs for that package."}]}]}]},{"client_msg_id":"0B86897B-7DE4-4ACE-BFF4-3444F93538AC","type":"message","text":"Haven’t gotten an answer over on discourse, so thought I’d see if anyone here has any thoughts on using JSONTable to turn a json with multiple nesting levels into a dataframe (sample code in the post): <https://discourse.julialang.org/t/parsing-nested-json-into-dataframe-using-jsontable/58127|https://discourse.julialang.org/t/parsing-nested-json-into-dataframe-using-jsontable/58127>","user":"U01399SPFT5","ts":"1617018621.296800","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"Parsing Nested JSON into DataFrame using JSONTable","title_link":"https://discourse.julialang.org/t/parsing-nested-json-into-dataframe-using-jsontable/58127","text":"I’m trying to parse academic publication data from Open Academic Graph, which stores the data in JSON format, but am running into an issue with parsing the below sample JSON they provide on the website because it is nested. I’m fairly new to working with JSON and could use some help. Any tips for how to parse it? using JSONTables, DataFrames, CSV, JSON3 js_string = \"{ \\\"id\\\":\\\"53e9ab9eb7602d970354a97e\\\", \\\"title\\\":\\\"Data mining: concepts and techniques\\\", \\\"authors\\\":[ { ...","fallback":"JuliaLang: Parsing Nested JSON into DataFrame using JSONTable","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","ts":1616953539,"from_url":"https://discourse.julialang.org/t/parsing-nested-json-into-dataframe-using-jsontable/58127","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/parsing-nested-json-into-dataframe-using-jsontable/58127"}],"blocks":[{"type":"rich_text","block_id":"1dT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Haven’t gotten an answer over on discourse, so thought I’d see if anyone here has any thoughts on using JSONTable to turn a json with multiple nesting levels into a dataframe (sample code in the post): "},{"type":"link","url":"https://discourse.julialang.org/t/parsing-nested-json-into-dataframe-using-jsontable/58127","text":"https://discourse.julialang.org/t/parsing-nested-json-into-dataframe-using-jsontable/58127"}]}]}]}]}