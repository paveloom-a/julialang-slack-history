{"cursor": 3, "messages": [{"client_msg_id":"579280a8-f860-41cf-beec-deca9a066441","type":"message","text":"when making a DataFrame from a vector of JSON objects, is it possible to NOT include a column? (current approach is drop! it after the fact","user":"UH8A351DJ","ts":"1614498908.105000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kWv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"when making a DataFrame from a vector of JSON objects, is it possible to NOT include a column? (current approach is drop! it after the fact"}]}]}]},{"client_msg_id":"4454cc48-3118-4fc2-b527-f1b3f9841df4","type":"message","text":"Hi guys,\n\nAny insight on this error? Is this due to weird thread issue (I'm using Julia 1.3.1).\n```[20:15:28] ERROR: LoadError: TaskFailedException:\n[20:15:28] IOError: close: i/o error (EIO)\n[20:15:28] Stacktrace:\n[20:15:28]  [1] uv_error at ./libuv.jl:97 [inlined]\n[20:15:28]  [2] close at ./filesystem.jl:107 [inlined]\n[20:15:28]  [3] sendfile(::String, ::String) at ./file.jl:814\n[20:15:28]  [4] #cp#12(::Bool, ::Bool, ::typeof(cp), ::String, ::String) at ./file.jl:344\n[20:15:28]  [5] (::Base.Filesystem.var\"#kw##cp\")(::NamedTuple{(:force,),Tuple{Bool}}, ::typeof(cp), ::String, ::String) at ./none:0\n[20:15:28]  [6] macro expansion at /home/darren/project/MRVerify/src/MRVerify.jl:856 [inlined]\n[20:15:28]  [7] (::Main.MRVerify.var\"#232#threadsfor_fun#44\"{NamedTuple{(:ses_dir, :config, :out_path, :sub, :ses, :ses_report_path, :labnotes_path, :issue_path, :anon_rda, :overwrite),Tuple{String,Dict{String,Any},String,String,String,Nothing,Nothing,Nothing,Bool,Bool}},Array{Tuple{Int64,Pair{Any,Any}},1}})(::Bool) at ./threadingconstructs.jl:61\n[20:15:28]  [8] (::Main.MRVerify.var\"#232#threadsfor_fun#44\"{NamedTuple{(:ses_dir, :config, :out_path, :sub, :ses, :ses_report_path, :labnotes_path, :issue_path, :anon_rda, :overwrite),Tuple{String,Dict{String,Any},String,String,String,Nothing,Nothing,Nothing,Bool,Bool}},Array{Tuple{Int64,Pair{Any,Any}},1}})() at ./threadingconstructs.jl:28\n[20:15:29] Stacktrace:\n[20:15:29]  [1] wait(::Task) at ./task.jl:251\n[20:15:29]  [2] macro expansion at ./threadingconstructs.jl:69 [inlined]\n[20:15:29]  [3] #process_session_data#15(::String, ::String, ::String, ::Nothing, ::Nothing, ::Nothing, ::Bool, ::Bool, ::typeof(Main.MRVerify.process_session_data), ::String, ::Dict{String,Any}) at /home/darren/project/MRVerify/src/MRVerify.jl:852\n[20:15:29]  [4] (::Main.MRVerify.var\"#kw##process_session_data\")(::NamedTuple{(:out_path, :sub, :ses, :issue_path, :ses_report_path, :labnotes_path, :anon_rda, :overwrite),Tuple{String,String,String,Nothing,Nothing,Nothing,Bool,Bool}}, ::typeof(Main.MRVerify.process_session_data), ::String, ::Dict{String,Any}) at ./none:0\n[20:15:29]  [5] top-level scope at /home/darren/project/MRVerify/src/MRVerify.jl:1329\n[20:15:29]  [6] include at ./boot.jl:328 [inlined]\n[20:15:29]  [7] include_relative(::Module, ::String) at ./loading.jl:1105\n[20:15:29]  [8] include(::Module, ::String) at ./Base.jl:31\n[20:15:29]  [9] exec_options(::Base.JLOptions) at ./client.jl:287\n[20:15:29]  [10] _start() at ./client.jl:460\n[20:15:29] in expression starting at /home/darren/project/MRVerify/src/MRVerify.jl:1326```\nMy code in Line 852 is the `Threads.@threads` , i.e.\n```Threads.@threads for (idx, (k, v)) in collect(enumerate(twix_result))```\nAny help would be appreciated!","user":"UUT4VGTE2","ts":"1614524183.106600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"q+2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi guys,\n\nAny insight on this error? Is this due to weird thread issue (I'm using Julia 1.3.1).\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"[20:15:28] ERROR: LoadError: TaskFailedException:\n[20:15:28] IOError: close: i/o error (EIO)\n[20:15:28] Stacktrace:\n[20:15:28]  [1] uv_error at ./libuv.jl:97 [inlined]\n[20:15:28]  [2] close at ./filesystem.jl:107 [inlined]\n[20:15:28]  [3] sendfile(::String, ::String) at ./file.jl:814\n[20:15:28]  [4] #cp#12(::Bool, ::Bool, ::typeof(cp), ::String, ::String) at ./file.jl:344\n[20:15:28]  [5] (::Base.Filesystem.var\"#kw##cp\")(::NamedTuple{(:force,),Tuple{Bool}}, ::typeof(cp), ::String, ::String) at ./none:0\n[20:15:28]  [6] macro expansion at /home/darren/project/MRVerify/src/MRVerify.jl:856 [inlined]\n[20:15:28]  [7] (::Main.MRVerify.var\"#232#threadsfor_fun#44\"{NamedTuple{(:ses_dir, :config, :out_path, :sub, :ses, :ses_report_path, :labnotes_path, :issue_path, :anon_rda, :overwrite),Tuple{String,Dict{String,Any},String,String,String,Nothing,Nothing,Nothing,Bool,Bool}},Array{Tuple{Int64,Pair{Any,Any}},1}})(::Bool) at ./threadingconstructs.jl:61\n[20:15:28]  [8] (::Main.MRVerify.var\"#232#threadsfor_fun#44\"{NamedTuple{(:ses_dir, :config, :out_path, :sub, :ses, :ses_report_path, :labnotes_path, :issue_path, :anon_rda, :overwrite),Tuple{String,Dict{String,Any},String,String,String,Nothing,Nothing,Nothing,Bool,Bool}},Array{Tuple{Int64,Pair{Any,Any}},1}})() at ./threadingconstructs.jl:28\n[20:15:29] Stacktrace:\n[20:15:29]  [1] wait(::Task) at ./task.jl:251\n[20:15:29]  [2] macro expansion at ./threadingconstructs.jl:69 [inlined]\n[20:15:29]  [3] #process_session_data#15(::String, ::String, ::String, ::Nothing, ::Nothing, ::Nothing, ::Bool, ::Bool, ::typeof(Main.MRVerify.process_session_data), ::String, ::Dict{String,Any}) at /home/darren/project/MRVerify/src/MRVerify.jl:852\n[20:15:29]  [4] (::Main.MRVerify.var\"#kw##process_session_data\")(::NamedTuple{(:out_path, :sub, :ses, :issue_path, :ses_report_path, :labnotes_path, :anon_rda, :overwrite),Tuple{String,String,String,Nothing,Nothing,Nothing,Bool,Bool}}, ::typeof(Main.MRVerify.process_session_data), ::String, ::Dict{String,Any}) at ./none:0\n[20:15:29]  [5] top-level scope at /home/darren/project/MRVerify/src/MRVerify.jl:1329\n[20:15:29]  [6] include at ./boot.jl:328 [inlined]\n[20:15:29]  [7] include_relative(::Module, ::String) at ./loading.jl:1105\n[20:15:29]  [8] include(::Module, ::String) at ./Base.jl:31\n[20:15:29]  [9] exec_options(::Base.JLOptions) at ./client.jl:287\n[20:15:29]  [10] _start() at ./client.jl:460\n[20:15:29] in expression starting at /home/darren/project/MRVerify/src/MRVerify.jl:1326"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"My code in Line 852 is the "},{"type":"text","text":"Threads.@threads","style":{"code":true}},{"type":"text","text":" , i.e.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Threads.@threads for (idx, (k, v)) in collect(enumerate(twix_result))"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Any help would be appreciated!"}]}]}]},{"client_msg_id":"d36a934f-b4a3-49c3-b474-f6f1136c651a","type":"message","text":"can I select columns by eltype?","user":"U01ARRMLM7E","ts":"1614544183.109600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"S3U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"can I select columns by eltype?"}]}]}],"thread_ts":"1614544183.109600","reply_count":1,"reply_users_count":1,"latest_reply":"1614545542.112300","reply_users":["U011QC7QLPL"],"subscribed":false},{"client_msg_id":"bd1ca99a-71e9-4d97-8e45-854e07bc1d9e","type":"message","text":"I've that idealistic thought in my mind, about streamlining ProtoBuf.jl and StructTypes.jl such that the former is generating code from proto files that matches the API of the latter. But for that I rn am not entirely sure I understood it correctly. StructTypes is a valid API for both, binary as well as textual de/serialization, isn't it?","user":"UMWFZF5DW","ts":"1614544751.112000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"e0K66","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I've that idealistic thought in my mind, about streamlining ProtoBuf.jl and StructTypes.jl such that the former is generating code from proto files that matches the API of the latter. But for that I rn am not entirely sure I understood it correctly. StructTypes is a valid API for both, binary as well as textual de/serialization, isn't it?"}]}]}]},{"client_msg_id":"ccd048aa-a288-4948-bea2-8cc061df944d","type":"message","text":"We could probably have `describe` on tables be it's own package that acts on tables and returns a named tuple of vectors. and then we override it for `DataFrame`s to return a DataFrame. It's a self-contained function for the most part.","user":"UBF9YRB6H","ts":"1614548440.115100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sXhi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We could probably have "},{"type":"text","text":"describe","style":{"code":true}},{"type":"text","text":" on tables be it's own package that acts on tables and returns a named tuple of vectors. and then we override it for `DataFrame`s to return a DataFrame. It's a self-contained function for the most part."}]}]}]},{"client_msg_id":"6c86d8b1-c334-4cc9-ade6-00ed68f4603f","type":"message","text":"what is the function to call on a `DataFrame` to say how big the `CSV.write` will be?","user":"UM8JUNJG7","ts":"1614549529.115900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rz4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"what is the function to call on a "},{"type":"text","text":"DataFrame","style":{"code":true}},{"type":"text","text":" to say how big the "},{"type":"text","text":"CSV.write","style":{"code":true}},{"type":"text","text":" will be?"}]}]}],"thread_ts":"1614549529.115900","reply_count":7,"reply_users_count":3,"latest_reply":"1614550189.117400","reply_users":["UBF9YRB6H","UM8JUNJG7","U01ARRMLM7E"],"subscribed":false},{"client_msg_id":"ee62cb5a-9fbc-4646-ae6e-f82f8eaa4fe0","type":"message","text":"is there a version of `lag` from ShiftedArrays that doesn't include the `missing`s? Just shortens the array","user":"UBF9YRB6H","ts":"1614552197.119400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Lgv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is there a version of "},{"type":"text","text":"lag","style":{"code":true}},{"type":"text","text":" from ShiftedArrays that doesn't include the `missing`s? Just shortens the array"}]}]}]},{"client_msg_id":"efb4739c-95b9-48ee-b779-0ad087e0727a","type":"message","text":"Does anyone know the situation with JSONTables. It worked when I used it about a month ago, but running the same program after an update fails. in expression starting at /Users/impero/.julia/packages/JSONTables/g5bSA/src/JSONTables.jl:3 it loads JSON3, but then: WARNING: could not import StructTypes.OrderedStruct into JSON3\nERROR: LoadError: LoadError: UndefVarError: OrderedStruct not defined","user":"U01GFAJRZ44","ts":"1614617954.140200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JkOfQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does anyone know the situation with JSONTables. It worked when I used it about a month ago, but running the same program after an update fails. in expression starting at /Users/impero/.julia/packages/JSONTables/g5bSA/src/JSONTables.jl:3 it loads JSON3, but then: WARNING: could not import StructTypes.OrderedStruct into JSON3\nERROR: LoadError: LoadError: UndefVarError: OrderedStruct not defined"}]}]}]},{"client_msg_id":"b08aef7f-1a17-462d-aec1-7f408d6fc2bc","type":"message","text":"I have got around this for now by removing JSON3 and loading JSON3@1.5.1. 1.7.1 seems to fail","user":"U01GFAJRZ44","ts":"1614618262.140800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"53vZ2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have got around this for now by removing JSON3 and loading JSON3@1.5.1. 1.7.1 seems to fail"}]}]}]},{"client_msg_id":"c4074f43-4358-4e53-824c-d78545ebe73b","type":"message","text":"Sounds like your versions are getting mismatched? JSON3 v1.7+ requires StructTypes v1.4+. If you somehow updated JSON3 without updating StructTypes (which your error makes it sound like), then things would get out of sync","user":"U681ELA87","ts":"1614618530.141900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"m5U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Sounds like your versions are getting mismatched? JSON3 v1.7+ requires StructTypes v1.4+. If you somehow updated JSON3 without updating StructTypes (which your error makes it sound like), then things would get out of sync"}]}]}]},{"client_msg_id":"393e9fbe-b581-45a1-b3d8-54d1c9f24b68","type":"message","text":"JSONTables.jl just depends on JSON3 “1” and StructTypes “1\", so it’s not imposing any hard requirements","user":"U681ELA87","ts":"1614618577.142400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jbA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"JSONTables.jl just depends on JSON3 “1” and StructTypes “1\", so it’s not imposing any hard requirements"}]}]}]},{"client_msg_id":"7e3a2579-b423-4a02-82af-90cbb2e79867","type":"message","text":"StructTypes.= 1.4.0. I just did an ‘update’ in the package manager, and got JSON3 1.7.1 (was 1.5.1) and StructTypes 1.4.0.","user":"U01GFAJRZ44","ts":"1614618648.143200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+Dmia","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"StructTypes.= 1.4.0. I just did an ‘update’ in the package manager, and got JSON3 1.7.1 (was 1.5.1) and StructTypes 1.4.0."}]}]}]},{"client_msg_id":"755faa4c-6faa-46fb-b7f3-84a89463f07c","type":"message","text":"so if you restart Julia and do `using JSON3`, what happens?","user":"U681ELA87","ts":"1614618989.143600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pvDtp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"so if you restart Julia and do "},{"type":"text","text":"using JSON3","style":{"code":true}},{"type":"text","text":", what happens?"}]}]}]},{"client_msg_id":"08975616-7976-46e2-bef5-2874dd8c1ea3","type":"message","text":"(and are you using a plain terminal REPL or some other Juno/Atom/VSCode/Jupyter/IJulia setup?)","user":"U681ELA87","ts":"1614619019.144200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JndDe","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(and are you using a plain terminal REPL or some other Juno/Atom/VSCode/Jupyter/IJulia setup?)"}]}]}]},{"client_msg_id":"50ca2c18-b93b-4543-983c-12eae6e32a83","type":"message","text":"Can I initialize a `CategoricalArray{String}` with levels from `Dict{String,Int}` and an array of encoded values `Vector{Int}`, without creating array of strings?","user":"UB2QSHWPN","ts":"1614620425.146100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TcHd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can I initialize a "},{"type":"text","text":"CategoricalArray{String}","style":{"code":true}},{"type":"text","text":" with levels from "},{"type":"text","text":"Dict{String,Int}","style":{"code":true}},{"type":"text","text":" and an array of encoded values "},{"type":"text","text":"Vector{Int}","style":{"code":true}},{"type":"text","text":", without creating array of strings?"}]}]}],"thread_ts":"1614620425.146100","reply_count":1,"reply_users_count":1,"latest_reply":"1614620528.146200","reply_users":["U67431ELR"],"subscribed":false},{"client_msg_id":"9c15a255-685e-4603-8b21-506757ddab2b","type":"message","text":"I'm confused; is the DataFrames syntax `df.a = v` deprecated or not?  becuase I could have sworn it was but I don't seem to be getting deprecation warnings anymore","user":"U9VG1AYSG","ts":"1614623444.147100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yHAx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm confused; is the DataFrames syntax "},{"type":"text","text":"df.a = v","style":{"code":true}},{"type":"text","text":" deprecated or not?  becuase I could have sworn it was but I don't seem to be getting deprecation warnings anymore"}]}]}],"thread_ts":"1614623444.147100","reply_count":4,"reply_users_count":3,"latest_reply":"1614624697.148000","reply_users":["U8JAMQGQY","U6A936746","U9VG1AYSG"],"subscribed":false},{"client_msg_id":"86e6fdea-468d-45eb-b816-f910049f34d7","type":"message","text":"I have a 3x76 Array{Any,2}. Row1 is string, rest are float64. How can I sort it according to ascending order of row2?","user":"U01NX6V5A2D","ts":"1614674539.155300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GpOT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a 3x76 Array{Any,2}. Row1 is string, rest are float64. How can I sort it according to ascending order of row2?"}]}]}],"thread_ts":"1614674539.155300","reply_count":3,"reply_users_count":2,"latest_reply":"1614675907.155800","reply_users":["U01NX6V5A2D","U6A936746"],"subscribed":false},{"client_msg_id":"0575e023-7aed-46b2-969c-67b502fbaba9","type":"message","text":"what's the best way to keep *all* columns in the result of a `combine`?  I'm surprised there seems to be no keyword argument for this","user":"U9VG1AYSG","ts":"1614699376.161300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"A7U1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"what's the best way to keep "},{"type":"text","text":"all","style":{"bold":true}},{"type":"text","text":" columns in the result of a "},{"type":"text","text":"combine","style":{"code":true}},{"type":"text","text":"?  I'm surprised there seems to be no keyword argument for this"}]}]}],"thread_ts":"1614699376.161300","reply_count":5,"reply_users_count":3,"latest_reply":"1614699731.162200","reply_users":["U67431ELR","U8JAMQGQY","U9VG1AYSG"],"subscribed":false},{"client_msg_id":"0e37cc79-a383-452c-bb34-1bfb2ba9dec7","type":"message","text":"thank you <@U8JAMQGQY> and <@U67431ELR> for putting me right on using `DataFrames: transform, transform!`; somehow I had all but forgotten about their existence.  One thing that I sometimes find myself needing to do that these functions make very easy (and seemingly even very efficient?) is adding a column to a dataframe while it's split into groups.  Giant pain in the ass without `transform!`","user":"U9VG1AYSG","ts":"1614711247.164300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"utP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"thank you "},{"type":"user","user_id":"U8JAMQGQY"},{"type":"text","text":" and "},{"type":"user","user_id":"U67431ELR"},{"type":"text","text":" for putting me right on using "},{"type":"text","text":"DataFrames: transform, transform!","style":{"code":true}},{"type":"text","text":"; somehow I had all but forgotten about their existence.  One thing that I sometimes find myself needing to do that these functions make very easy (and seemingly even very efficient?) is adding a column to a dataframe while it's split into groups.  Giant pain in the ass without "},{"type":"text","text":"transform!","style":{"code":true}}]}]}]},{"client_msg_id":"272b8303-8e0d-4120-9124-868ea94d856a","type":"message","text":"overall the DataFrames API is so wonderful, you guys did a really amazing job carefully thinking it through","user":"U9VG1AYSG","ts":"1614711309.164800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=gN8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"overall the DataFrames API is so wonderful, you guys did a really amazing job carefully thinking it through"}]}]}],"reactions":[{"name":"heart","users":["U681ELA87","U8JAMQGQY"],"count":2}]},{"client_msg_id":"e9ae3e13-3df4-49be-bafe-f3acd0275895","type":"message","text":"really, look at this:\n```transform!(groupby(df, :A), :A=&gt;(x -&gt; (D=x .+ 1, E=x .+ 2))=&gt;[:D, :E])```\nit's just so cool that that's trivial to do","user":"U9VG1AYSG","ts":"1614711546.165300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8z=Eb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"really, look at this:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"transform!(groupby(df, :A), :A=>(x -> (D=x .+ 1, E=x .+ 2))=>[:D, :E])"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"it's just so cool that that's trivial to do"}]}]}]},{"client_msg_id":"91b71397-80eb-4859-b765-babbde3a3304","type":"message","text":"(btw, is there a way to do that without returning a `NamedTuple` from the intermediate function? it seems redundant because it already tells it what the column names are)","user":"U9VG1AYSG","ts":"1614711575.165900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pXOfS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(btw, is there a way to do that without returning a "},{"type":"text","text":"NamedTuple","style":{"code":true}},{"type":"text","text":" from the intermediate function? it seems redundant because it already tells it what the column names are)"}]}]}]},{"client_msg_id":"85c5e84b-5a68-4bb8-ae11-433fa7e1bad4","type":"message","text":"```transform!(groupby(df, :A), :A=&gt; ByRow(x -&gt; x .+ (1,2)) =&gt; [:D, :E])```\nshould work and be fast (I am writing it from my head so please check :smile:)","user":"U8JAMQGQY","ts":"1614711947.167200","team":"T68168MUP","edited":{"user":"U8JAMQGQY","ts":"1614712020.000000"},"blocks":[{"type":"rich_text","block_id":"Lbxn","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"transform!(groupby(df, :A), :A=> ByRow(x -> x .+ (1,2)) => [:D, :E])"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"should work and be fast (I am writing it from my head so please check "},{"type":"emoji","name":"smile"},{"type":"text","text":")"}]}]}]},{"client_msg_id":"9cb79740-b3eb-43e5-a46f-cdf8e411fd12","type":"message","text":"wait what about `=&gt; AsTable`?","user":"UBF9YRB6H","ts":"1614712001.167500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hVp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"wait what about "},{"type":"text","text":"=> AsTable","style":{"code":true}},{"type":"text","text":"?"}]}]}],"reactions":[{"name":"point_up","users":["U9VG1AYSG"],"count":1}]},{"client_msg_id":"82ba6a7c-f068-4bcd-ac07-87152947d7da","type":"message","text":"<@U9VG1AYSG> did not want to return a `NamedTuple`","user":"U8JAMQGQY","ts":"1614712032.168200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"z7n","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U9VG1AYSG"},{"type":"text","text":" did not want to return a "},{"type":"text","text":"NamedTuple","style":{"code":true}}]}]}]},{"client_msg_id":"d613be86-21de-4bc7-a362-48f114ec2c3c","type":"message","text":"thanks, I think the `AsTable` is what I was looking for...","user":"U9VG1AYSG","ts":"1614712043.168500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9q/DV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"thanks, I think the "},{"type":"text","text":"AsTable","style":{"code":true}},{"type":"text","text":" is what I was looking for..."}]}]}]},{"client_msg_id":"a88b38e9-9edf-4961-bcaa-8d35dd74f2d9","type":"message","text":"well, what I relaly wanted was to elminate the redundancy of specifying table columns in two separate places, that's all","user":"U9VG1AYSG","ts":"1614712061.169000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"cfb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"well, what I relaly wanted was to elminate the redundancy of specifying table columns in two separate places, that's all"}]}]}]},{"client_msg_id":"53ea8e1b-f950-4a65-a54a-1b0337cf5688","type":"message","text":"so I think dumping to `AsTable` is the easiest way to do that","user":"U9VG1AYSG","ts":"1614712074.169600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FtI9L","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"so I think dumping to "},{"type":"text","text":"AsTable","style":{"code":true}},{"type":"text","text":" is the easiest way to do that"}]}]}]},{"client_msg_id":"c254830a-326a-46b8-8e1f-17b625c6b312","type":"message","text":"So as you can see - you can do it in two ways :slightly_smiling_face:.","user":"U8JAMQGQY","ts":"1614712078.169700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0CDyu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So as you can see - you can do it in two ways "},{"type":"emoji","name":"slightly_smiling_face"},{"type":"text","text":"."}]}]}],"reactions":[{"name":"+1","users":["U9VG1AYSG"],"count":1}]},{"client_msg_id":"35aa4ec7-9d80-49e8-b50d-8f31e15e51b3","type":"message","text":"it might be good if we add a section to the docs including lots of examples on the really \"fancy\" stuff one can do with the column pair syntax.  It wasn't at all obvious to me that you can do `cols=&gt;f=&gt;AsTable`","user":"U9VG1AYSG","ts":"1614712182.170900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"45dTK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it might be good if we add a section to the docs including lots of examples on the really \"fancy\" stuff one can do with the column pair syntax.  It wasn't at all obvious to me that you can do "},{"type":"text","text":"cols=>f=>AsTable","style":{"code":true}}]}]}]},{"client_msg_id":"9cbf5403-39c2-4431-92df-6e064f0007a2","type":"message","text":"in particular, it might be really nice to have a section on the underlying magic that makes the pairs syntax work.  Right now it seems very magical to me, but I assume it's all built around a few core, primitive functions.  Understanding what those are I think would be very enlightening","user":"U9VG1AYSG","ts":"1614712356.172000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/Ur","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"in particular, it might be really nice to have a section on the underlying magic that makes the pairs syntax work.  Right now it seems very magical to me, but I assume it's all built around a few core, primitive functions.  Understanding what those are I think would be very enlightening"}]}]}]},{"client_msg_id":"e19e3b7a-fb06-41ce-91f3-c591cbacbbb1","type":"message","text":"Those would be 2 different piece of documentation.\nUnderstanding based Explination for the later.\nand either learning based tutorials, or goal based guides for the former.\nI agree though. both would be valuable\n(4 kinds of docs <https://documentation.divio.com/introduction/>)","user":"U6A936746","ts":"1614712506.173600","team":"T68168MUP","edited":{"user":"U6A936746","ts":"1614712514.000000"},"blocks":[{"type":"rich_text","block_id":"S9ZC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Those would be 2 different piece of documentation.\nUnderstanding based Explination for the later.\nand either learning based tutorials, or goal based guides for the former.\nI agree though. both would be valuable\n(4 kinds of docs "},{"type":"link","url":"https://documentation.divio.com/introduction/"},{"type":"text","text":")"}]}]}]},{"client_msg_id":"69f67d7e-7ebc-4e8a-bf67-60c884ead749","type":"message","text":"<@U9VG1AYSG> there's this blog-post that has some of that: <https://bkamins.github.io/julialang/2020/12/24/minilanguage.html>","user":"U8JP5B9T2","ts":"1614713663.174400","team":"T68168MUP","attachments":[{"service_name":"Blog by Bogumił Kamiński","title":"DataFrames.jl minilanguage explained","title_link":"https://bkamins.github.io/julialang/2020/12/24/minilanguage.html","text":"Introduction","fallback":"Blog by Bogumił Kamiński: DataFrames.jl minilanguage explained","ts":1608785495,"from_url":"https://bkamins.github.io/julialang/2020/12/24/minilanguage.html","service_icon":"https://bkamins.github.io/favicon.ico","id":1,"original_url":"https://bkamins.github.io/julialang/2020/12/24/minilanguage.html"}],"blocks":[{"type":"rich_text","block_id":"IoLg","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U9VG1AYSG"},{"type":"text","text":" there's this blog-post that has some of that: "},{"type":"link","url":"https://bkamins.github.io/julialang/2020/12/24/minilanguage.html"}]}]}]},{"client_msg_id":"bc66637f-17d9-4263-9f37-c3b42d1d0df8","type":"message","text":"<@UL1475QDN> will soon work on a new version of DataFrames.jl documentation. However, I think we need to start with learning based tutorial part (as it is very hard to do all four kinds of docs in one shot).","user":"U8JAMQGQY","ts":"1614714018.175800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vcjc","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UL1475QDN"},{"type":"text","text":" will soon work on a new version of DataFrames.jl documentation. However, I think we need to start with learning based tutorial part (as it is very hard to do all four kinds of docs in one shot)."}]}]}]},{"client_msg_id":"4d2794e7-e24a-4a49-9c06-5b0d5fcc584b","type":"message","text":"came up on twitter that R `read_csv` can introduce missing values if extreme values of a numeric column are introduced deep in the file.  this repo has a reproducible example (I haven't checked myself but I trust the referrer...) <https://github.com/ebergelson/read_csv_issue_quickeg>  might be useful to add to our CSV tests/benchmarking","user":"U66M57AN4","ts":"1614738858.177500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0Zf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"came up on twitter that R "},{"type":"text","text":"read_csv","style":{"code":true}},{"type":"text","text":" can introduce missing values if extreme values of a numeric column are introduced deep in the file.  this repo has a reproducible example (I haven't checked myself but I trust the referrer...) "},{"type":"link","url":"https://github.com/ebergelson/read_csv_issue_quickeg"},{"type":"text","text":"  might be useful to add to our CSV tests/benchmarking"}]}]}]},{"client_msg_id":"54ac8e98-76a2-4fee-aaf2-dcacbe96f003","type":"message","text":"<https://twitter.com/bergelsonlab/status/1366866851884589056>","user":"U66M57AN4","ts":"1614738893.177700","team":"T68168MUP","attachments":[{"fallback":"<https://twitter.com/bergelsonlab|@bergelsonlab>: uhoh i love the #tidyverse &amp; all the <https://twitter.com/hadleywickham|@hadleywickham> tools but danger alert, read_csv() can make a column look full of NAs when it's not if you don't use guess_max() wisely (i.e. if u let it default when u have a rare thing 10k rows down) !!! ( read.csv() doesn't do this) #rstats","ts":1614721427,"author_name":"Bergelson Lab","author_link":"https://twitter.com/bergelsonlab/status/1366866851884589056","author_icon":"https://pbs.twimg.com/profile_images/981987616148066304/CP77wY4m_normal.jpg","author_subname":"@bergelsonlab","text":"uhoh i love the #tidyverse &amp; all the <https://twitter.com/hadleywickham|@hadleywickham> tools but danger alert, read_csv() can make a column look full of NAs when it's not if you don't use guess_max() wisely (i.e. if u let it default when u have a rare thing 10k rows down) !!! ( read.csv() doesn't do this) #rstats","service_name":"twitter","service_url":"https://twitter.com/","from_url":"https://twitter.com/bergelsonlab/status/1366866851884589056","id":1,"original_url":"https://twitter.com/bergelsonlab/status/1366866851884589056","footer":"Twitter","footer_icon":"https://a.slack-edge.com/80588/img/services/twitter_pixel_snapped_32.png"}],"blocks":[{"type":"rich_text","block_id":"9XCu","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://twitter.com/bergelsonlab/status/1366866851884589056"}]}]}]},{"client_msg_id":"8fb9e664-58c6-4b21-9097-498a84b5278b","type":"message","text":"After a hiatus off in computer simulation land I am back to processing some experimental data which I have loaded into a DataFrame. When I went looking I found that most traces of categorical arrays have been removed. What is the current state of affairs for this? I don't *need* to do anything as my DataFrame isn't huge (~500k rows and 5 columns. Of which 2 are Floats and 3 are \"categorical\") And so it fits into RAM regardless but in the past I would've used the function to convert columns to categorical arrays for faster filtering and such. I also found references to PooledArrays which I'm not familiar with. Thanks in advanced.","user":"U9L5Y89H7","ts":"1614788062.183000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nxZkU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"After a hiatus off in computer simulation land I am back to processing some experimental data which I have loaded into a DataFrame. When I went looking I found that most traces of categorical arrays have been removed. What is the current state of affairs for this? I don't "},{"type":"text","text":"need","style":{"bold":true}},{"type":"text","text":" to do anything as my DataFrame isn't huge (~500k rows and 5 columns. Of which 2 are Floats and 3 are \"categorical\") And so it fits into RAM regardless but in the past I would've used the function to convert columns to categorical arrays for faster filtering and such. I also found references to PooledArrays which I'm not familiar with. Thanks in advanced."}]}]}],"thread_ts":"1614788062.183000","reply_count":2,"reply_users_count":1,"latest_reply":"1614788239.183300","reply_users":["U67431ELR"],"subscribed":false},{"client_msg_id":"f653e31a-6aab-4410-9ecb-555efd546c7a","type":"message","text":"Is there a way to use the GroupKey from a grouped dataframe in a dict? I am able to put the GroupKey from the grouped df as the key in a dict, but I am having trouble hashing it. It seems like the GroupKey from a grouped df is like its own type. I want to make it easy to hash after I have transferred stuff into a dict.\n```n = 10\nA= rand(Normal(0,1),n)\nB= rand(Normal(1,1),n)\nC = rand(Normal(-1,1),n)\nD = rand(Normal(0.5,1),n)\n\nsimdata = DataFrame(Arm = vcat(fill(\"A\",n),fill(\"B\",n),fill(\"C\",n),fill(\"D\",n)),\n                    Y = vcat(A,B,C,D))\n\n\nsimgdf = groupby(simdata,:Arm)\n\nfunction CreateBandit(data,groups,response)\n    gdf = groupby(data,groups)\n    QN = Dict(key=&gt;(Q=0,n=0) for key in keys(gdf))\n    return(QN)\nend \n\na=CreateBandit(simdata,:Arm,:Y)\na[\"A\"] #doesn't work \na[(Arm=\"A\",)] #doesn't work ```\n","user":"U01EF0QVAB0","ts":"1614792955.186300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/7KfI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to use the GroupKey from a grouped dataframe in a dict? I am able to put the GroupKey from the grouped df as the key in a dict, but I am having trouble hashing it. It seems like the GroupKey from a grouped df is like its own type. I want to make it easy to hash after I have transferred stuff into a dict.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"n = 10\nA= rand(Normal(0,1),n)\nB= rand(Normal(1,1),n)\nC = rand(Normal(-1,1),n)\nD = rand(Normal(0.5,1),n)\n\nsimdata = DataFrame(Arm = vcat(fill(\"A\",n),fill(\"B\",n),fill(\"C\",n),fill(\"D\",n)),\n                    Y = vcat(A,B,C,D))\n\n\nsimgdf = groupby(simdata,:Arm)\n\nfunction CreateBandit(data,groups,response)\n    gdf = groupby(data,groups)\n    QN = Dict(key=>(Q=0,n=0) for key in keys(gdf))\n    return(QN)\nend \n\na=CreateBandit(simdata,:Arm,:Y)\na[\"A\"] #doesn't work \na[(Arm=\"A\",)] #doesn't work "}]},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"3a76cd94-1660-4f93-87bb-bff37d2c261e","type":"message","text":"convert it to a `NamedTuple` with `NamedTuple`","user":"U9VG1AYSG","ts":"1614792999.186600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PaF6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"convert it to a "},{"type":"text","text":"NamedTuple","style":{"code":true}},{"type":"text","text":" with "},{"type":"text","text":"NamedTuple","style":{"code":true}}]}]}],"thread_ts":"1614792999.186600","reply_count":2,"reply_users_count":1,"latest_reply":"1614793208.189600","reply_users":["U01EF0QVAB0"],"subscribed":false},{"client_msg_id":"20ffe7eb-f87f-4a0a-8cfe-523de192b786","type":"message","text":"that is, turn it into a `NamedTuple` when you construct the dict `Dict(NamedTuple(k)=&gt;(Q=0,n=0) for k \\in keys(gdf))`","user":"U9VG1AYSG","ts":"1614793047.187400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9L6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"that is, turn it into a "},{"type":"text","text":"NamedTuple","style":{"code":true}},{"type":"text","text":" when you construct the dict "},{"type":"text","text":"Dict(NamedTuple(k)=>(Q=0,n=0) for k \\in keys(gdf))","style":{"code":true}}]}]}],"reactions":[{"name":"correct_answer","users":["UBF9YRB6H","U01EF0QVAB0"],"count":2}]},{"client_msg_id":"93338f14-7974-4357-a38f-ada67a713531","type":"message","text":"yeah, `keys(gdf)` returns GroupKeys objects, which are unique to DataFrames.","user":"UBF9YRB6H","ts":"1614793136.188100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"u3v6U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yeah, "},{"type":"text","text":"keys(gdf)","style":{"code":true}},{"type":"text","text":" returns GroupKeys objects, which are unique to DataFrames."}]}]}]},{"client_msg_id":"8e8d785b-78e7-444d-8104-e381a8b8ac9a","type":"message","text":"I wonder if it would be worth it to ensure that the `hash` of the grouped dataframe key is the same as the hash of the NamedTuple.  Kind of hard to think through whether that would solve this in all cases or if it would cause something else to go wrong","user":"U9VG1AYSG","ts":"1614793198.189500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=aW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I wonder if it would be worth it to ensure that the "},{"type":"text","text":"hash","style":{"code":true}},{"type":"text","text":" of the grouped dataframe key is the same as the hash of the NamedTuple.  Kind of hard to think through whether that would solve this in all cases or if it would cause something else to go wrong"}]}]}]},{"client_msg_id":"29e79e6b-e2ea-4622-9ca0-ebffb1f4f821","type":"message","text":"We touched related issues at <https://github.com/JuliaData/DataFrames.jl/issues/2305>, but I don't see anything that says that `hash` shouldn't give the same result for `GroupKey` and `NamedTuple`. <@U8JAMQGQY>?","user":"U67431ELR","ts":"1614802744.190900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qGRNw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We touched related issues at "},{"type":"link","url":"https://github.com/JuliaData/DataFrames.jl/issues/2305"},{"type":"text","text":", but I don't see anything that says that "},{"type":"text","text":"hash","style":{"code":true}},{"type":"text","text":" shouldn't give the same result for "},{"type":"text","text":"GroupKey","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"NamedTuple","style":{"code":true}},{"type":"text","text":". "},{"type":"user","user_id":"U8JAMQGQY"},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"00076222-a352-4b51-b947-8a3c8c6fa4fd","type":"message","text":"The crucial thing is that two `GroupKey`s that have different parents are currently not equal according to `==` nor `isequal`:\n```julia&gt; df1 = DataFrame(a=[1, 2, missing]);\n\njulia&gt; df2 = copy(df1);\n\njulia&gt; gdf1 = groupby(df1, :a);\n\njulia&gt; gdf2 = groupby(df2, :a);\n\njulia&gt; gk1 = keys(gdf1);\n\njulia&gt; gk2 = keys(gdf2);\n\njulia&gt; gk1[1] == gk2[1]\nfalse\n\njulia&gt; isequal(gk1[1], gk2[1])\nfalse```\nI am not sure what would be the use-case of making `hash` of `NamedTuple` and `GroupKey` equal (it can be done, but what would be the benefit exactly?). In particular this would increase the cost of computing hash for `GroupKey` (but this is a secondary consideration a primary one is what would we want to use it for?).","user":"U8JAMQGQY","ts":"1614812288.194900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WMUG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The crucial thing is that two "},{"type":"text","text":"GroupKey","style":{"code":true}},{"type":"text","text":"s that have different parents are currently not equal according to "},{"type":"text","text":"==","style":{"code":true}},{"type":"text","text":" nor "},{"type":"text","text":"isequal","style":{"code":true}},{"type":"text","text":":\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> df1 = DataFrame(a=[1, 2, missing]);\n\njulia> df2 = copy(df1);\n\njulia> gdf1 = groupby(df1, :a);\n\njulia> gdf2 = groupby(df2, :a);\n\njulia> gk1 = keys(gdf1);\n\njulia> gk2 = keys(gdf2);\n\njulia> gk1[1] == gk2[1]\nfalse\n\njulia> isequal(gk1[1], gk2[1])\nfalse"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I am not sure what would be the use-case of making "},{"type":"text","text":"hash","style":{"code":true}},{"type":"text","text":" of "},{"type":"text","text":"NamedTuple","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"GroupKey","style":{"code":true}},{"type":"text","text":" equal (it can be done, but what would be the benefit exactly?). In particular this would increase the cost of computing hash for "},{"type":"text","text":"GroupKey","style":{"code":true}},{"type":"text","text":" (but this is a secondary consideration a primary one is what would we want to use it for?)."}]}]}],"reactions":[{"name":"+1","users":["U680THK2S","UBF9YRB6H","U6A936746"],"count":3}]},{"client_msg_id":"1033b565-3c72-425c-84ba-edd3a088570c","type":"message","text":"The use is just like the example above, suppose you want to use one group key in another `GroupedDataFrame` or any other dictionary","user":"U9VG1AYSG","ts":"1614813768.198600","team":"T68168MUP","edited":{"user":"U9VG1AYSG","ts":"1614813799.000000"},"blocks":[{"type":"rich_text","block_id":"NGvc4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The use is just like the example above, suppose you want to use one group key in another "},{"type":"text","text":"GroupedDataFrame","style":{"code":true}},{"type":"text","text":" or any other dictionary"}]}]}]},{"client_msg_id":"9361d011-4db3-40b1-9741-b1585f4728d9","type":"message","text":"but since you can already do this with conversion via `NamedTuple`, I'm not necessarily asserting that it's worth changing","user":"U9VG1AYSG","ts":"1614813783.199300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"83g0r","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"but since you can already do this with conversion via "},{"type":"text","text":"NamedTuple","style":{"code":true}},{"type":"text","text":", I'm not necessarily asserting that it's worth changing"}]}]}]},{"client_msg_id":"081a481c-cd77-4b6d-9919-b493f8e7e5bd","type":"message","text":"I suppose the question is partly do we want to make different kinds of \"row-like\" datastructures to be `isequal`? We don't really have any interface or supertype for \"row-like\" things (beyond the Tables.jl definition which is extremely light weight) so this could be pretty challenging. In other parts of Julia we don't e.g. compare `(1,2,3)` and `[1,2,3]` as `isequal` even though they iterate and index the same way, so it wouldn't make sense for all kinds of `NamedTuple` and `DataFrameRow` and `Dict{Symbol, ...}` and so-on all have the same hash. (Plus, really, it can only make sense on ordered containers, meaning `Dict{Symbol, ...}` can't be as row-like as the others).","user":"U66QZ3QF3","ts":"1614813801.199600","team":"T68168MUP","edited":{"user":"U66QZ3QF3","ts":"1614813823.000000"},"blocks":[{"type":"rich_text","block_id":"XhHYL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I suppose the question is partly do we want to make different kinds of \"row-like\" datastructures to be "},{"type":"text","text":"isequal","style":{"code":true}},{"type":"text","text":"? We don't really have any interface or supertype for \"row-like\" things (beyond the Tables.jl definition which is extremely light weight) so this could be pretty challenging. In other parts of Julia we don't e.g. compare "},{"type":"text","text":"(1,2,3)","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"[1,2,3]","style":{"code":true}},{"type":"text","text":" as "},{"type":"text","text":"isequal","style":{"code":true}},{"type":"text","text":" even though they iterate and index the same way, so it wouldn't make sense for all kinds of "},{"type":"text","text":"NamedTuple","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"DataFrameRow","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"Dict{Symbol, ...}","style":{"code":true}},{"type":"text","text":" and so-on all have the same hash. (Plus, really, it can only make sense on ordered containers, meaning "},{"type":"text","text":"Dict{Symbol, ...}","style":{"code":true}},{"type":"text","text":" can't be as row-like as the others)."}]}]}]},{"client_msg_id":"ee2049e6-596a-43ef-a671-e9aee306e81f","type":"message","text":"this was *NOT* my suggestion, I think maybe `GroupKey` should be equal to a NamedTuple because they are so similar and more crucially because this makes `GroupKey`s comparable across different dataframes, but I don't think that concept generalizes to all rows","user":"U9VG1AYSG","ts":"1614813880.200800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3tsr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"this was "},{"type":"text","text":"NOT","style":{"bold":true}},{"type":"text","text":" my suggestion, I think maybe "},{"type":"text","text":"GroupKey","style":{"code":true}},{"type":"text","text":" should be equal to a NamedTuple because they are so similar and more crucially because this makes `GroupKey`s comparable across different dataframes, but I don't think that concept generalizes to all rows"}]}]}]},{"client_msg_id":"cec61dea-c879-4706-bae7-fb5f0646f192","type":"message","text":"for what it's worth, I had already tried things like what <@U01EF0QVAB0> suggested above, so it may not be that rare a use case","user":"U9VG1AYSG","ts":"1614813934.201900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jgeC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"for what it's worth, I had already tried things like what "},{"type":"user","user_id":"U01EF0QVAB0"},{"type":"text","text":" suggested above, so it may not be that rare a use case"}]}]}]},{"client_msg_id":"bb37b899-cf56-4caf-aef7-f224d7ccf1b5","type":"message","text":"but again, just to emphasize, I don't see this as a serious problem, because it's so easy to just make it a `NamedTuple`.  Were that not the case it would seem like a much bigger deal","user":"U9VG1AYSG","ts":"1614813993.203200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"NQDGZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"but again, just to emphasize, I don't see this as a serious problem, because it's so easy to just make it a "},{"type":"text","text":"NamedTuple","style":{"code":true}},{"type":"text","text":".  Were that not the case it would seem like a much bigger deal"}]}]}]},{"client_msg_id":"5543d83e-0cee-4ce9-9b68-561223fe5eda","type":"message","text":"^ I don't think it's super relevant to the issue about indexing, but I would also like to see a more uniform concept of a Row. It's a tricky problem, see here: <https://github.com/JuliaData/DataFrames.jl/issues/2576>","user":"UBF9YRB6H","ts":"1614814173.204400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"AvVI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"^ I don't think it's super relevant to the issue about indexing, but I would also like to see a more uniform concept of a Row. It's a tricky problem, see here: "},{"type":"link","url":"https://github.com/JuliaData/DataFrames.jl/issues/2576"}]}]}]},{"client_msg_id":"d9a9db66-78bb-4b29-bc6b-d0c38cdf4d3a","type":"message","text":"to expand a bit more on my response to <@U66QZ3QF3> comment: I think the difference in `GroupKey`s case is that these objects *themselves* are *not* comparable across different dataframes.  This is *ONLY* true for `GroupKey`, even `DataFrameRow` gives the expected result:\n```julia&gt; df1 = DataFrame(A=[1,2], B=[3,4])\n2×2 DataFrame\n Row │ A      B\n     │ Int64  Int64\n─────┼──────────────\n   1 │     1      3\n   2 │     2      4\n\njulia&gt; df2 = DataFrame(A=[2,1], B=[4,3])\n2×2 DataFrame\n Row │ A      B\n     │ Int64  Int64\n─────┼──────────────\n   1 │     2      4\n   2 │     1      3\n\njulia&gt; DataFrameRow(df1, 1)\nDataFrameRow\n Row │ A      B\n     │ Int64  Int64\n─────┼──────────────\n   1 │     1      3\n\njulia&gt; DataFrameRow(df1, 1) == DataFrameRow(df2, 2)```\nSo I very much see this as a special case in which `GroupKey` isn't being quite consistent with other \"row-like\" concepts.","user":"U9VG1AYSG","ts":"1614814344.206000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6Q0Y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"to expand a bit more on my response to "},{"type":"user","user_id":"U66QZ3QF3"},{"type":"text","text":" comment: I think the difference in `GroupKey`s case is that these objects "},{"type":"text","text":"themselves","style":{"bold":true}},{"type":"text","text":" are "},{"type":"text","text":"not","style":{"bold":true}},{"type":"text","text":" comparable across different dataframes.  This is "},{"type":"text","text":"ONLY","style":{"bold":true}},{"type":"text","text":" true for "},{"type":"text","text":"GroupKey","style":{"code":true}},{"type":"text","text":", even "},{"type":"text","text":"DataFrameRow","style":{"code":true}},{"type":"text","text":" gives the expected result:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> df1 = DataFrame(A=[1,2], B=[3,4])\n2×2 DataFrame\n Row │ A      B\n     │ Int64  Int64\n─────┼──────────────\n   1 │     1      3\n   2 │     2      4\n\njulia> df2 = DataFrame(A=[2,1], B=[4,3])\n2×2 DataFrame\n Row │ A      B\n     │ Int64  Int64\n─────┼──────────────\n   1 │     2      4\n   2 │     1      3\n\njulia> DataFrameRow(df1, 1)\nDataFrameRow\n Row │ A      B\n     │ Int64  Int64\n─────┼──────────────\n   1 │     1      3\n\njulia> DataFrameRow(df1, 1) == DataFrameRow(df2, 2)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"So I very much see this as a special case in which "},{"type":"text","text":"GroupKey","style":{"code":true}},{"type":"text","text":" isn't being quite consistent with other \"row-like\" concepts."}]}]}]},{"client_msg_id":"89cc4259-9854-4796-92bd-823d801920c3","type":"message","text":"and I therefore think maybe something should be changed for `GroupKey`, but this does not necessarily imply that `DataFrameRow`, `GroupKey` and `NamedTuple` should all be comparable to each other","user":"U9VG1AYSG","ts":"1614814384.206700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ms9A","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"and I therefore think maybe something should be changed for "},{"type":"text","text":"GroupKey","style":{"code":true}},{"type":"text","text":", but this does not necessarily imply that "},{"type":"text","text":"DataFrameRow","style":{"code":true}},{"type":"text","text":", "},{"type":"text","text":"GroupKey","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"NamedTuple","style":{"code":true}},{"type":"text","text":" should all be comparable to each other"}]}]}],"reactions":[{"name":"+1","users":["UBF9YRB6H"],"count":1}]},{"client_msg_id":"3c6e8b2a-27bb-4321-aaf3-cbdf2f1d8b68","type":"message","text":"so the full implementation of this suggestion would mean that `GroupKey` would remain the same internally for sake of efficiency, but behave to everything else like a `NamedTuple`.  i.e. it's a special case because it needs the current representation for efficiency, but otherwise it's essentially just a named tuple","user":"U9VG1AYSG","ts":"1614814508.207800","team":"T68168MUP","edited":{"user":"U9VG1AYSG","ts":"1614814542.000000"},"blocks":[{"type":"rich_text","block_id":"6smc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"so the full implementation of this suggestion would mean that "},{"type":"text","text":"GroupKey","style":{"code":true}},{"type":"text","text":" would remain the same internally for sake of efficiency, but behave to everything else like a "},{"type":"text","text":"NamedTuple","style":{"code":true}},{"type":"text","text":".  i.e. it's a special case because it needs the current representation for efficiency, but otherwise it's essentially just a named tuple"}]}]}]},{"client_msg_id":"ceb08099-cd79-4c17-895d-5e6dcbb05374","type":"message","text":"this is already sort of how `DataFrameRow` works (though I haven't checked hashing)","user":"U9VG1AYSG","ts":"1614814571.208300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"r9qR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"this is already sort of how "},{"type":"text","text":"DataFrameRow","style":{"code":true}},{"type":"text","text":" works (though I haven't checked hashing)"}]}]}]},{"client_msg_id":"8a0e3de1-4715-4f31-af5b-412ddc3fae8e","type":"message","text":"The key question is: do we want to allow `GroupKey` from one data frame to be used in another data frame for lookup. The initial design assumed that the answer is NO for safety reasons and one should do such a process via `NamedTuple` explicitly. But - as usual - we can discuss it.","user":"U8JAMQGQY","ts":"1614814910.209800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ZBjr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The key question is: do we want to allow "},{"type":"text","text":"GroupKey","style":{"code":true}},{"type":"text","text":" from one data frame to be used in another data frame for lookup. The initial design assumed that the answer is NO for safety reasons and one should do such a process via "},{"type":"text","text":"NamedTuple","style":{"code":true}},{"type":"text","text":" explicitly. But - as usual - we can discuss it."}]}]}]},{"client_msg_id":"eb2fa8e4-454b-4bfd-a4f9-047cc9bedbc9","type":"message","text":"Ok, thanks, I think I understand.\n\n<@U8JAMQGQY> I suppose if the users can get at it, maybe you need to communicate that it is in fact a _token_ that is valid for a particular (grouped) data frame. Perhaps you could even have both a `GroupKey` (value-based, row-like, can be used in other data frames) and `GroupToken` (fast for internal algorithms, only valid for it's \"parent\" dataframe)?","user":"U66QZ3QF3","ts":"1614816258.213000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fvPsu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ok, thanks, I think I understand.\n\n"},{"type":"user","user_id":"U8JAMQGQY"},{"type":"text","text":" I suppose if the users can get at it, maybe you need to communicate that it is in fact a "},{"type":"text","text":"token","style":{"italic":true}},{"type":"text","text":" that is valid for a particular (grouped) data frame. Perhaps you could even have both a "},{"type":"text","text":"GroupKey","style":{"code":true}},{"type":"text","text":" (value-based, row-like, can be used in other data frames) and "},{"type":"text","text":"GroupToken","style":{"code":true}},{"type":"text","text":" (fast for internal algorithms, only valid for it's \"parent\" dataframe)?"}]}]}]},{"client_msg_id":"f6ad7cd5-06cc-405f-a88e-d9c29cbb33a6","type":"message","text":"well that's kind of the idea, there is the possibility that instead of forbidding this we'd just do the conversion automatically, that's all","user":"U9VG1AYSG","ts":"1614816347.213700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"a15","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"well that's kind of the idea, there is the possibility that instead of forbidding this we'd just do the conversion automatically, that's all"}]}]}]},{"client_msg_id":"ed30c9b8-5659-4eb0-9e73-29d39122805e","type":"message","text":"when we started talking about this I was fairly indifferent, but as I think more about the inconsistency with other \"row-like\" objects, all of which are comparable between different dataframes, I'm increasingly thinking that yes, `GroupKey` should also be by default","user":"U9VG1AYSG","ts":"1614816417.214800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3NhQw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"when we started talking about this I was fairly indifferent, but as I think more about the inconsistency with other \"row-like\" objects, all of which are comparable between different dataframes, I'm increasingly thinking that yes, "},{"type":"text","text":"GroupKey","style":{"code":true}},{"type":"text","text":" should also be by default"}]}]}]},{"client_msg_id":"1e5c1d23-e2a1-4630-8d94-a76602bb7cff","type":"message","text":"I haven't looked at the implementation - but if you can afford to carry the parent reference as well as the token in `GroupKey`, and do a quick `===`  check on the parent when indexing, then that would be OK. (Sometimes you can use `@inbounds` and `@boundscheck` as a hack to elide the `===` check for e.g. internal algorithms)","user":"U66QZ3QF3","ts":"1614816534.217200","team":"T68168MUP","edited":{"user":"U66QZ3QF3","ts":"1614816568.000000"},"blocks":[{"type":"rich_text","block_id":"UHp+S","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I haven't looked at the implementation - but if you can afford to carry the parent reference as well as the token in "},{"type":"text","text":"GroupKey","style":{"code":true}},{"type":"text","text":", and do a quick "},{"type":"text","text":"===","style":{"code":true}},{"type":"text","text":"  check on the parent when indexing, then that would be OK. (Sometimes you can use "},{"type":"text","text":"@inbounds","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"@boundscheck","style":{"code":true}},{"type":"text","text":" as a hack to elide the "},{"type":"text","text":"===","style":{"code":true}},{"type":"text","text":" check for e.g. internal algorithms)"}]}]}]},{"client_msg_id":"0495b4d8-cb00-4f9c-8cd1-16dce7b51eae","type":"message","text":"Could you open an issue for this please? I think it is not a problem to allow for cross-data frame usage of `GroupKey`. We simply used a more restrictive approach in the past. Also as <@U66QZ3QF3> notes - currently the design we use means that `GroupKey` is very fast (effectively it is just an integer with metadata linking it to a parent)","user":"U8JAMQGQY","ts":"1614816598.218400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rHG+v","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Could you open an issue for this please? I think it is not a problem to allow for cross-data frame usage of "},{"type":"text","text":"GroupKey","style":{"code":true}},{"type":"text","text":". We simply used a more restrictive approach in the past. Also as "},{"type":"user","user_id":"U66QZ3QF3"},{"type":"text","text":" notes - currently the design we use means that "},{"type":"text","text":"GroupKey","style":{"code":true}},{"type":"text","text":" is very fast (effectively it is just an integer with metadata linking it to a parent)"}]}]}],"reactions":[{"name":"+1","users":["UBF9YRB6H"],"count":1}]},{"client_msg_id":"03284e19-8560-42a1-8572-cc034fed1abf","type":"message","text":"issue here: <https://github.com/JuliaData/DataFrames.jl/issues/2639>","user":"U9VG1AYSG","ts":"1614816933.218700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7ML","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"issue here: "},{"type":"link","url":"https://github.com/JuliaData/DataFrames.jl/issues/2639"}]}]}]},{"client_msg_id":"ce0b3f50-978c-4c6f-a5ba-d557011ea19e","type":"message","text":"btw, what I said about hashing before was rather idiotic.  Surely the whole reason `GroupKey` exists is that it obviates the need for hashing at all.  I haven't formed an opinion yet about what the implmentation should look like, but I am coming around to the idea that all \"row-like\" objects of the same type should be comparable across dataframes","user":"U9VG1AYSG","ts":"1614817178.220000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KlAzl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"btw, what I said about hashing before was rather idiotic.  Surely the whole reason "},{"type":"text","text":"GroupKey","style":{"code":true}},{"type":"text","text":" exists is that it obviates the need for hashing at all.  I haven't formed an opinion yet about what the implmentation should look like, but I am coming around to the idea that all \"row-like\" objects of the same type should be comparable across dataframes"}]}]}]},{"client_msg_id":"6edd8a52-fcd4-4b19-b946-72f04ae841fe","type":"message","text":"Yeah, the row-like idea is good. But we shouldn't do anything that hurts performance of the lookup in a grouped data frame","user":"UBF9YRB6H","ts":"1614817263.220700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qBk=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah, the row-like idea is good. But we shouldn't do anything that hurts performance of the lookup in a grouped data frame"}]}]}]},{"client_msg_id":"cc92d17c-f931-4ecd-967f-cdf4ca915151","type":"message","text":"Yeah, I agree with that.  Ideally I think `GroupKey` should be allowed between DataFrames, but it would be silly to make anything more than a trivial performance sacrifice for it, as the desired functionality already exists with `NamedTuple` and is extremely simple once you understand what's going on","user":"U9VG1AYSG","ts":"1614817387.222000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Eju","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah, I agree with that.  Ideally I think "},{"type":"text","text":"GroupKey","style":{"code":true}},{"type":"text","text":" should be allowed between DataFrames, but it would be silly to make anything more than a trivial performance sacrifice for it, as the desired functionality already exists with "},{"type":"text","text":"NamedTuple","style":{"code":true}},{"type":"text","text":" and is extremely simple once you understand what's going on"}]}]}]},{"client_msg_id":"fd814d31-85de-4331-98dc-fbc02ff25578","type":"message","text":"So I am encountering a very strange bug while trying to implement the Multi Armed Bandit problem. My code randomly will say `ERROR: KeyError: key NamedTuple[(Arm = \"C\",)] not found`  but it breaks randomly on an iteration of the loop, and I can't figure out why that would happen considering that its clearly able to find the key in previous iterations but it can't find it all the time? This is very strange and I don't know whats going on or how to debug an error that happens randomly in an iteration but not others. Its all simulated data and basically my function Learn!(band,20) is randomly breaking. I wonder if its related to the hash <@U9VG1AYSG> mentioned before?\n\n```using Statistics, StatsBase, Distributions \nusing DataFrames\n\nmutable struct Bandit\n    QN::Dict{NamedTuple,Dict{Symbol,Real}}\n    Y::Dict{NamedTuple,Array{Float64,1}}\n    keylist::Vector{NamedTuple}\nend \n\nfunction CreateBandit(data,groups,response)\n    gdf = groupby(data,groups)\n    QN = Dict{NamedTuple,Dict{Symbol,Real}}()\n    Y = Dict{NamedTuple,Array{Float64,1}}()\n    for key in keys(gdf)\n        QN[NamedTuple(key)] = Dict(:Q=&gt;rand(Uniform(0,1e-10)),:n=&gt;0)\n        Y[NamedTuple(key)] = gdf[key][:,response]\n    end \n    return(Bandit(QN,Y,collect(keys(QN))))\nend \n\nBase.getindex(bandit::Bandit,key) = Base.getindex(bandit.QN,key)\n\nfunction Base.argmax(bandit::Bandit)\n    keylist = bandit.keylist\n    idx = argmax(getindex.(values(bandit.QN),:Q))\n    return(keylist[idx])\nend \n\nfunction Learn!(bandit::Bandit,steps;ϵ=0.05,η=nothing)\n    lrn = ifelse(isnothing(η),0,1)\n    for i=1:steps\n        u = rand(Uniform(0,1))\n        if u &lt;= ϵ\n            action = sample(bandit.keylist,1)\n        else\n            action = argmax(bandit)\n        end \n        Qcurrent = bandit[action][:Q]\n        Reward = sample(bandit.Y[action],1)[1]\n        diff = Reward - Qcurrent\n        curr = bandit[action]\n        curr[:n] += 1\n        if lrn == 0 \n            curr[:Q] += 1/curr[:n] .* diff\n        else\n            curr[:Q] += η .* diff\n        end\n    end\n    return(bandit)\nend \n\n#Simulate Data \nn = 100\nA= rand(Normal(0,1),n)\nB= rand(Normal(1,1),n)\nC = rand(Normal(-1,1),n)\nD = rand(Normal(0.5,1),n)\n\nsimdata = DataFrame(Arm = vcat(fill(\"A\",n),fill(\"B\",n),fill(\"C\",n),fill(\"D\",n)),\n                    Y = vcat(A,B,C,D))\n\n#Use Bandit \n\nband=CreateBandit(simdata,:Arm,:Y)            \nLearn!(band,20) #This breaks randomly on an iteration ```","user":"U01EF0QVAB0","ts":"1614836707.226300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"J23yu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So I am encountering a very strange bug while trying to implement the Multi Armed Bandit problem. My code randomly will say "},{"type":"text","text":"ERROR: KeyError: key NamedTuple[(Arm = \"C\",)] not found","style":{"code":true}},{"type":"text","text":"  but it breaks randomly on an iteration of the loop, and I can't figure out why that would happen considering that its clearly able to find the key in previous iterations but it can't find it all the time? This is very strange and I don't know whats going on or how to debug an error that happens randomly in an iteration but not others. Its all simulated data and basically my function Learn!(band,20) is randomly breaking. I wonder if its related to the hash "},{"type":"user","user_id":"U9VG1AYSG"},{"type":"text","text":" mentioned before?\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Statistics, StatsBase, Distributions \nusing DataFrames\n\nmutable struct Bandit\n    QN::Dict{NamedTuple,Dict{Symbol,Real}}\n    Y::Dict{NamedTuple,Array{Float64,1}}\n    keylist::Vector{NamedTuple}\nend \n\nfunction CreateBandit(data,groups,response)\n    gdf = groupby(data,groups)\n    QN = Dict{NamedTuple,Dict{Symbol,Real}}()\n    Y = Dict{NamedTuple,Array{Float64,1}}()\n    for key in keys(gdf)\n        QN[NamedTuple(key)] = Dict(:Q=>rand(Uniform(0,1e-10)),:n=>0)\n        Y[NamedTuple(key)] = gdf[key][:,response]\n    end \n    return(Bandit(QN,Y,collect(keys(QN))))\nend \n\nBase.getindex(bandit::Bandit,key) = Base.getindex(bandit.QN,key)\n\nfunction Base.argmax(bandit::Bandit)\n    keylist = bandit.keylist\n    idx = argmax(getindex.(values(bandit.QN),:Q))\n    return(keylist[idx])\nend \n\nfunction Learn!(bandit::Bandit,steps;ϵ=0.05,η=nothing)\n    lrn = ifelse(isnothing(η),0,1)\n    for i=1:steps\n        u = rand(Uniform(0,1))\n        if u <= ϵ\n            action = sample(bandit.keylist,1)\n        else\n            action = argmax(bandit)\n        end \n        Qcurrent = bandit[action][:Q]\n        Reward = sample(bandit.Y[action],1)[1]\n        diff = Reward - Qcurrent\n        curr = bandit[action]\n        curr[:n] += 1\n        if lrn == 0 \n            curr[:Q] += 1/curr[:n] .* diff\n        else\n            curr[:Q] += η .* diff\n        end\n    end\n    return(bandit)\nend \n\n#Simulate Data \nn = 100\nA= rand(Normal(0,1),n)\nB= rand(Normal(1,1),n)\nC = rand(Normal(-1,1),n)\nD = rand(Normal(0.5,1),n)\n\nsimdata = DataFrame(Arm = vcat(fill(\"A\",n),fill(\"B\",n),fill(\"C\",n),fill(\"D\",n)),\n                    Y = vcat(A,B,C,D))\n\n#Use Bandit \n\nband=CreateBandit(simdata,:Arm,:Y)            \nLearn!(band,20) #This breaks randomly on an iteration "}]}]}]},{"client_msg_id":"2689DED8-ADA0-4EC3-8023-A4CDBB666127","type":"message","text":"<https://stackoverflow.com/questions/38662826/dataframe-sorting-based-on-a-function-of-multiple-column-values|https://stackoverflow.com/questions/38662826/dataframe-sorting-based-on-a-function-of-multiple-column-values>\n\nHow can I do this operation with DataFrames.jl efficiently?","user":"U7V6YNG04","ts":"1614857685.227500","team":"T68168MUP","attachments":[{"service_name":"Stack Overflow","title":"DataFrame sorting based on a function of multiple column values","title_link":"https://stackoverflow.com/questions/38662826/dataframe-sorting-based-on-a-function-of-multiple-column-values","text":"Based on python, sort descending dataframe with pandas: Given: from pandas import DataFrame import pandas as pd d = {'x':[2,3,1,4,5], 'y':[5,4,3,2,1], 'letter':['a','a','b','b','c']} ...","fallback":"Stack Overflow: DataFrame sorting based on a function of multiple column values","thumb_url":"https://cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon@2.png?v=73d79a89bded","from_url":"https://stackoverflow.com/questions/38662826/dataframe-sorting-based-on-a-function-of-multiple-column-values","thumb_width":316,"thumb_height":316,"service_icon":"https://cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon.png?v=c78bd457575a","id":1,"original_url":"https://stackoverflow.com/questions/38662826/dataframe-sorting-based-on-a-function-of-multiple-column-values"}],"blocks":[{"type":"rich_text","block_id":"vtVxE","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://stackoverflow.com/questions/38662826/dataframe-sorting-based-on-a-function-of-multiple-column-values","text":"https://stackoverflow.com/questions/38662826/dataframe-sorting-based-on-a-function-of-multiple-column-values"},{"type":"text","text":"\n"},{"type":"text","text":"\nHow can I do this operation with DataFrames.jl efficiently?"}]}]}],"thread_ts":"1614857685.227500","reply_count":1,"reply_users_count":1,"latest_reply":"1614858206.231500","reply_users":["U7V6YNG04"],"subscribed":false},{"client_msg_id":"307aaac6-8644-42bd-83ab-fffcccd4c3e1","type":"message","text":"As there recently were questions here is a list of things needed for 1.0 release of DataFrames.jl: <https://github.com/JuliaData/DataFrames.jl/issues/2640>","user":"U8JAMQGQY","ts":"1614864159.232300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wz1I8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"As there recently were questions here is a list of things needed for 1.0 release of DataFrames.jl: "},{"type":"link","url":"https://github.com/JuliaData/DataFrames.jl/issues/2640"}]}]}]},{"client_msg_id":"505bf0af-1db3-492c-8b46-a4d1e05106f4","type":"message","text":"How do people actually do moving averages?\nLike this seems like a simple task, i just want my 1D time series plot to be smoother.","user":"U6A936746","ts":"1614944987.236100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fKh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How do people actually do moving averages?\nLike this seems like a simple task, i just want my 1D time series plot to be smoother."}]}]}],"thread_ts":"1614944987.236100","reply_count":6,"reply_users_count":4,"latest_reply":"1614945964.237800","reply_users":["U01C3624SGJ","U01G3TX4F9A","U6BJ9E351","U7JQGPGCQ"],"subscribed":false},{"client_msg_id":"803c4913-944c-4401-8039-abab4570e952","type":"message","text":"The docs for stack are confusing.","user":"U6A936746","ts":"1614946115.238300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hFgY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The docs for stack are confusing."}]}]}],"thread_ts":"1614946115.238300","reply_count":1,"reply_users_count":1,"latest_reply":"1614946174.238400","reply_users":["U6A936746"],"subscribed":false},{"client_msg_id":"9468fbb2-b53d-44ce-90dc-4d047177595c","type":"message","text":"This week I have written a post on what was recently changed in PooledArrays.jl: <https://bkamins.github.io/julialang/2021/03/05/pooledarrays.html>","user":"U8JAMQGQY","ts":"1614975573.240600","team":"T68168MUP","attachments":[{"service_name":"Blog by Bogumił Kamiński","title":"What is new in PooledArrays.jl?","title_link":"https://bkamins.github.io/julialang/2021/03/05/pooledarrays.html","text":"Introduction","fallback":"Blog by Bogumił Kamiński: What is new in PooledArrays.jl?","ts":1614941097,"from_url":"https://bkamins.github.io/julialang/2021/03/05/pooledarrays.html","service_icon":"https://bkamins.github.io/favicon.ico","id":1,"original_url":"https://bkamins.github.io/julialang/2021/03/05/pooledarrays.html"}],"blocks":[{"type":"rich_text","block_id":"xqbhm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This week I have written a post on what was recently changed in PooledArrays.jl: "},{"type":"link","url":"https://bkamins.github.io/julialang/2021/03/05/pooledarrays.html"}]}]}]},{"client_msg_id":"82d464af-d45c-443c-8647-99659ed0ac56","type":"message","text":"Is there a way to set the number of significant figures in the output of DataFrames’ `describe`?","user":"US8V7JSKB","ts":"1615007785.241600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jFRnz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to set the number of significant figures in the output of DataFrames’ "},{"type":"text","text":"describe","style":{"code":true}},{"type":"text","text":"?"}]}]}],"thread_ts":"1615007785.241600","reply_count":2,"reply_users_count":2,"latest_reply":"1615010190.242000","reply_users":["UBF9YRB6H","US8V7JSKB"],"subscribed":false},{"client_msg_id":"67191d21-e8de-4c41-ad7e-bbb0414f9678","type":"message","text":"Can anyone help me how I would convert a folder of CSVs files to a folder of arrow memory-mapped files using CSV.jl and/or Arrow.jl?\n\nI went through the docs and could not find anything about this.\n\nThat would be mostly for out-of-memory manipulation. Because each CSV would be 500MB (there is at least 60 of those) and I have 8GB RAM. I can do this in R using the `{arrow}` package, but I want to do this in Julia.","user":"U01QBF4PHKP","ts":"1615068696.247200","team":"T68168MUP","edited":{"user":"U01QBF4PHKP","ts":"1615068818.000000"},"blocks":[{"type":"rich_text","block_id":"STyl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can anyone help me how I would convert a folder of CSVs files to a folder of arrow memory-mapped files using CSV.jl and/or Arrow.jl?\n\nI went through the docs and could not find anything about this.\n\nThat would be mostly for out-of-memory manipulation. Because each CSV would be 500MB (there is at least 60 of those) and I have 8GB RAM. I can do this in R using the "},{"type":"text","text":"{arrow}","style":{"code":true}},{"type":"text","text":" package, but I want to do this in Julia."}]}]}]},{"client_msg_id":"bbe547f1-c2b1-4e55-8f6a-f7ae67aae455","type":"message","text":"Is there a place where I can find some information/comparisons between the common formats for saving data i.e CSV, Arrow, Parquet,HDF5 etc? I cant seem to understand when I should use one over the other.","user":"U01C3624SGJ","ts":"1615074224.256300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qzDh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a place where I can find some information/comparisons between the common formats for saving data i.e CSV, Arrow, Parquet,HDF5 etc? I cant seem to understand when I should use one over the other."}]}]}]},{"client_msg_id":"c7d674ae-7dc2-4cc5-aa7d-a0fe1a769dd2","type":"message","text":"That would certainly be helpful to have. Some day, I'd like to have an official JuliaData org website that helped point out things like this to help orient people getting into all things data in Julia.","user":"U681ELA87","ts":"1615074929.258100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RVovQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That would certainly be helpful to have. Some day, I'd like to have an official JuliaData org website that helped point out things like this to help orient people getting into all things data in Julia."}]}]}]},{"client_msg_id":"2213a731-6427-45f2-949c-2337dfe37a11","type":"message","text":"Briefly:\n• CSV: plain, human-readable text. semi-efficient storage wise, but can be gzipped for more compression. slowest for reading/writing, but most flexible cross languages. but different languages can have very different implementations for reading/writing; most languages have multiple.\n• Parquet: one of the older \"columnar\" storage formats. it has a ton of tricks for compressing data. pretty fast reading, partly variable on how many compression/encoding tricks were employed when writing. very non-breaking, so suitable for very long-term storage.\n• Arrow: new \"columnar\" storage format. has very simple compression supported, but not in most language implementations (I think currently only C++, Java, and Julia). very very fast reading; this is its bread and butter. writing is also decently fast. \n• HDF5: I'm least familiar w/ this format. I know there's been struggles across platforms (osx vs. linux vs. windows) for good support. I think it's pretty fast though for reading. probably the best support for really custom nested datastructure saving.","user":"U681ELA87","ts":"1615075313.265000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3S26","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Briefly:\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CSV: plain, human-readable text. semi-efficient storage wise, but can be gzipped for more compression. slowest for reading/writing, but most flexible cross languages. but different languages can have very different implementations for reading/writing; most languages have multiple."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Parquet: one of the older \"columnar\" storage formats. it has a ton of tricks for compressing data. pretty fast reading, partly variable on how many compression/encoding tricks were employed when writing. very non-breaking, so suitable for very long-term storage."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Arrow: new \"columnar\" storage format. has very simple compression supported, but not in most language implementations (I think currently only C++, Java, and Julia). very very fast reading; this is its bread and butter. writing is also decently fast. "}]},{"type":"rich_text_section","elements":[{"type":"text","text":"HDF5: I'm least familiar w/ this format. I know there's been struggles across platforms (osx vs. linux vs. windows) for good support. I think it's pretty fast though for reading. probably the best support for really custom nested datastructure saving."}]}],"style":"bullet","indent":0}]}]},{"client_msg_id":"2fdc971c-46d0-4476-a848-b5068367091d","type":"message","text":"Should probably also include JLD2 in the list.  It uses HDF5 under the hood, but also includes Julia type information (from what I understand) so it's kind of like .mat files for Matlab.  HDF5 is well suited for storing multiple multi-dimensional numeric arrays.  It's well established so I have to imagine that any struggles across platforms were related more to the HDF5.jl package itself (which BTW is great!) than to the HDF5 file format in general.","user":"U01FKQQ7J0J","ts":"1615080935.270200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jMd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Should probably also include JLD2 in the list.  It uses HDF5 under the hood, but also includes Julia type information (from what I understand) so it's kind of like .mat files for Matlab.  HDF5 is well suited for storing multiple multi-dimensional numeric arrays.  It's well established so I have to imagine that any struggles across platforms were related more to the HDF5.jl package itself (which BTW is great!) than to the HDF5 file format in general."}]}]}]},{"client_msg_id":"a293eabf-659f-4d15-a8d5-aa0d1581bb6b","type":"message","text":"Hello! I would like commnunity input from a change in DataFramesMeta.\n\nCurrently, the way to work with `Symbol`s is with `^(_)`, i.e.\n\n```julia&gt; @transform(df, b = ^(:A))\n2×2 DataFrame\n Row │ a      b      \n     │ Int64  Symbol \n─────┼───────────────\n   1 │     1  A\n   2 │     2  A```\nI want to change this, in favor of\n\n```julia&gt; @transform(df, b = syms(:A))\n2×2 DataFrame\n Row │ a      b      \n     │ Int64  Symbol \n─────┼───────────────\n   1 │     1  A\n   2 │     2  A```\nI think this will be less confusing and easier to work with `^` using math.\n\nDo people agree? Please comment here: <https://github.com/JuliaData/DataFramesMeta.jl/pull/228>","user":"UBF9YRB6H","ts":"1615149838.274000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OJU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello! I would like commnunity input from a change in DataFramesMeta.\n\nCurrently, the way to work with `Symbol`s is with "},{"type":"text","text":"^(_)","style":{"code":true}},{"type":"text","text":", i.e.\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> @transform(df, b = ^(:A))\n2×2 DataFrame\n Row │ a      b      \n     │ Int64  Symbol \n─────┼───────────────\n   1 │     1  A\n   2 │     2  A"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nI want to change this, in favor of\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> @transform(df, b = syms(:A))\n2×2 DataFrame\n Row │ a      b      \n     │ Int64  Symbol \n─────┼───────────────\n   1 │     1  A\n   2 │     2  A"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nI think this will be less confusing and easier to work with "},{"type":"text","text":"^","style":{"code":true}},{"type":"text","text":" using math.\n\nDo people agree? Please comment here: "},{"type":"link","url":"https://github.com/JuliaData/DataFramesMeta.jl/pull/228"}]}]}]},{"client_msg_id":"cfa654f8-b59b-418d-966c-d349d649e6ba","type":"message","text":"Not really familiar with DataFramesMeta, but perhaps `:(:A)` would work?","user":"UM30MT6RF","ts":"1615150541.275500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"e9+MP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not really familiar with DataFramesMeta, but perhaps "},{"type":"text","text":":(:A)","style":{"code":true}},{"type":"text","text":" would work?"}]}]}]},{"client_msg_id":"de94e5e5-f555-4d57-af72-75a9933c982a","type":"message","text":"hmmm that is currently an error, which is nice. But I like the expliciteness of `syms` and it's comparison to `cols`.","user":"UBF9YRB6H","ts":"1615151497.276100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FJ+wD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hmmm that is currently an error, which is nice. But I like the expliciteness of "},{"type":"text","text":"syms","style":{"code":true}},{"type":"text","text":" and it's comparison to "},{"type":"text","text":"cols","style":{"code":true}},{"type":"text","text":"."}]}]}]},{"client_msg_id":"83fe2686-5883-4672-b876-6c68a421be70","type":"message","text":"I took part of a discussion of a common pattern the other day: for some struct, say `MyStruct`, you have a DataFrame constructor. Now you end up with a `Vector{MyStruct}` and you’d like to construct a single `DataFrame` with all of them on top of each other, i.e. something like `reduce(vcat, DataFrame.(x))` where `x` is the vector just mentioned. However, you’d like the *implicit* index in the `Vector{MyStruct}` to become a new column in the `DataFrame`. Is there a non-verbose way to do that? It seems to me that it could be generally useful.","user":"U680T6770","ts":"1615152587.280500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"08+oe","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I took part of a discussion of a common pattern the other day: for some struct, say "},{"type":"text","text":"MyStruct","style":{"code":true}},{"type":"text","text":", you have a DataFrame constructor. Now you end up with a "},{"type":"text","text":"Vector{MyStruct}","style":{"code":true}},{"type":"text","text":" and you’d like to construct a single "},{"type":"text","text":"DataFrame","style":{"code":true}},{"type":"text","text":" with all of them on top of each other, i.e. something like "},{"type":"text","text":"reduce(vcat, DataFrame.(x))","style":{"code":true}},{"type":"text","text":" where "},{"type":"text","text":"x","style":{"code":true}},{"type":"text","text":" is the vector just mentioned. However, you’d like the "},{"type":"text","text":"implicit","style":{"bold":true}},{"type":"text","text":" index in the "},{"type":"text","text":"Vector{MyStruct}","style":{"code":true}},{"type":"text","text":" to become a new column in the "},{"type":"text","text":"DataFrame","style":{"code":true}},{"type":"text","text":". Is there a non-verbose way to do that? It seems to me that it could be generally useful."}]}]}],"reactions":[{"name":"+1","users":["UCZ7VBGUD"],"count":1}]},{"client_msg_id":"a0451c44-c53d-4b44-aed1-2ce3ce5c68c0","type":"message","text":"[Query.jl] Hey guys do you know how to map many variables to a single function?\n\nHere is the code in `DataFrames`:\n```@chain DataFrame(tbl) begin\n    dropmissing([:NT_GER, :NT_FG, :NT_CE])\n    groupby([:TP_SEXO, :CO_TURNO_GRADUACAO], skipmissing=true)\n    combine(nrow,\n            [:NT_GER, :NT_FG, :NT_CE] .=&gt; mean)\nend```\nhow do I do it in `Query.jl` but using a single instruction to `@map` to mean. I have to keep repeating myself like this:\n```Tables.datavaluerows(tbl) |&gt;\n    @dropna(:NT_GER, :NT_FG, :NT_CE, :TP_SEXO, :CO_TURNO_GRADUACAO) |&gt;\n    @groupby({_.TP_SEXO, _.CO_TURNO_GRADUACAO}) |&gt;\n    @map({\n        sexo = key(_)[1],\n        turno = key(_)[2],\n        nrows = length(_),\n        NT_GER_mean = mean(_.NT_GER),\n        NT_FG_mean = mean(_.NT_FG),\n        NT_CE_mean = mean(_.NT_CE)\n    }) |&gt; \n    DataFrame```\nIs there something like?:\n```@map({\n        sexo = key(_)[1],\n        turno = key(_)[2],\n        nrows = length(_),\n       {_.NT_GER, _.NT_FG, _.NT_CE} = mean(_)\n})```\n","user":"U01QBF4PHKP","ts":"1615195766.282900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"776","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"[Query.jl] Hey guys do you know how to map many variables to a single function?\n\nHere is the code in "},{"type":"text","text":"DataFrames","style":{"code":true}},{"type":"text","text":":\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@chain DataFrame(tbl) begin\n    dropmissing([:NT_GER, :NT_FG, :NT_CE])\n    groupby([:TP_SEXO, :CO_TURNO_GRADUACAO], skipmissing=true)\n    combine(nrow,\n            [:NT_GER, :NT_FG, :NT_CE] .=> mean)\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"how do I do it in "},{"type":"text","text":"Query.jl","style":{"code":true}},{"type":"text","text":" but using a single instruction to "},{"type":"text","text":"@map","style":{"code":true}},{"type":"text","text":" to mean. I have to keep repeating myself like this:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Tables.datavaluerows(tbl) |>\n    @dropna(:NT_GER, :NT_FG, :NT_CE, :TP_SEXO, :CO_TURNO_GRADUACAO) |>\n    @groupby({_.TP_SEXO, _.CO_TURNO_GRADUACAO}) |>\n    @map({\n        sexo = key(_)[1],\n        turno = key(_)[2],\n        nrows = length(_),\n        NT_GER_mean = mean(_.NT_GER),\n        NT_FG_mean = mean(_.NT_FG),\n        NT_CE_mean = mean(_.NT_CE)\n    }) |> \n    DataFrame"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Is there something like?:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@map({\n        sexo = key(_)[1],\n        turno = key(_)[2],\n        nrows = length(_),\n       {_.NT_GER, _.NT_FG, _.NT_CE} = mean(_)\n})"}]},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"7977eeac-3138-4591-916b-6a4d0f77d309","type":"message","text":"<https://github.com/JuliaData/DataFrames.jl/issues/2340> is done and you can use it on `main`. We would appreciate some correctness and performance tests before 1.0 release to make sure all works as expected.","user":"U8JAMQGQY","ts":"1615231436.284700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3S9x","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/JuliaData/DataFrames.jl/issues/2340"},{"type":"text","text":" is done and you can use it on "},{"type":"text","text":"main","style":{"code":true}},{"type":"text","text":". We would appreciate some correctness and performance tests before 1.0 release to make sure all works as expected."}]}]}],"thread_ts":"1615231436.284700","reply_count":1,"reply_users_count":1,"latest_reply":"1615233353.285300","reply_users":["U7JQGPGCQ"],"subscribed":false,"reactions":[{"name":"tada","users":["UBF9YRB6H","UFWQ6DP0S","U7PGB5DU3"],"count":3},{"name":"10000","users":["U7JQGPGCQ"],"count":1}]},{"client_msg_id":"43bb6b95-9440-496a-977a-b6ab3ac37f68","type":"message","text":"arrow related:\nwhen reading a feather file (~3.3G) compressed with lz4, after `Arrow.Table()` , I see the `RES` in `top` goes up to 21G, I thought the file is mmaped, but maybe it's not if there's compression?\n\nalso, is there a way to only read a few columns of the Arrow?","user":"UH8A351DJ","ts":"1615235932.287000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"m4tt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"arrow related:\nwhen reading a feather file (~3.3G) compressed with lz4, after "},{"type":"text","text":"Arrow.Table()","style":{"code":true}},{"type":"text","text":" , I see the "},{"type":"text","text":"RES","style":{"code":true}},{"type":"text","text":" in "},{"type":"text","text":"top","style":{"code":true}},{"type":"text","text":" goes up to 21G, I thought the file is mmaped, but maybe it's not if there's compression?\n\nalso, is there a way to only read a few columns of the Arrow?"}]}]}],"thread_ts":"1615235932.287000","reply_count":2,"reply_users_count":2,"latest_reply":"1615236893.287600","reply_users":["U681ELA87","UH8A351DJ"],"subscribed":false},{"client_msg_id":"9f48632c-102a-49b0-9c03-6f3e6e6574b4","type":"message","text":"data parallelism question. It seems `ThreadsX.map()` over a `TypedTables.Table(Arrow.Table( ...))` is no faster than normal map. Is this expected because the mmap?","user":"UH8A351DJ","ts":"1615247163.289400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hDO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"data parallelism question. It seems "},{"type":"text","text":"ThreadsX.map()","style":{"code":true}},{"type":"text","text":" over a "},{"type":"text","text":"TypedTables.Table(Arrow.Table( ...))","style":{"code":true}},{"type":"text","text":" is no faster than normal map. Is this expected because the mmap?"}]}]}],"thread_ts":"1615247163.289400","reply_count":7,"reply_users_count":3,"latest_reply":"1615249479.291400","reply_users":["U681ELA87","UH8A351DJ","U6A936746"],"subscribed":false},{"client_msg_id":"898197ea-56ce-41c8-be8a-059d8fab08c6","type":"message","text":"are there switch statements in julia similar to C#?","user":"U01G3BG7AFR","ts":"1615252232.292200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"AjmPn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"are there switch statements in julia similar to C#?"}]}]}]},{"client_msg_id":"a20c1176-de6b-45fe-80ef-ffaca05bad5d","type":"message","text":"Nope","user":"U680THK2S","ts":"1615252515.292500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/vbF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Nope"}]}]}]},{"type":"message","subtype":"thread_broadcast","text":"found part of the problem. I was doing:\n```for id in eachindex(dfc)\n   x = dfc[id]\n   f(x.col1, x.col2)\nend```\ninstant 5x speed up if I do:\n```for id in eachindex(dfc)\n\n   f(dfc.col1[id], dfc.col2[id])\nend```\n","user":"UH8A351DJ","ts":"1615256262.293000","thread_ts":"1615247163.289400","root":{"client_msg_id":"9f48632c-102a-49b0-9c03-6f3e6e6574b4","type":"message","text":"data parallelism question. It seems `ThreadsX.map()` over a `TypedTables.Table(Arrow.Table( ...))` is no faster than normal map. Is this expected because the mmap?","user":"UH8A351DJ","ts":"1615247163.289400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hDO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"data parallelism question. It seems "},{"type":"text","text":"ThreadsX.map()","style":{"code":true}},{"type":"text","text":" over a "},{"type":"text","text":"TypedTables.Table(Arrow.Table( ...))","style":{"code":true}},{"type":"text","text":" is no faster than normal map. Is this expected because the mmap?"}]}]}],"thread_ts":"1615247163.289400","reply_count":9,"reply_users_count":3,"latest_reply":"1615256262.293000","reply_users":["U681ELA87","UH8A351DJ","U6A936746"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"34osq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"found part of the problem. I was doing:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"for id in eachindex(dfc)\n   x = dfc[id]\n   f(x.col1, x.col2)\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"instant 5x speed up if I do:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"for id in eachindex(dfc)\n\n   f(dfc.col1[id], dfc.col2[id])\nend"}]},{"type":"rich_text_section","elements":[]}]}],"client_msg_id":"2db7db31-fe58-4aa5-b258-697740c2aa12"},{"client_msg_id":"f746f96e-ec87-45b4-b734-da3630baf006","type":"message","text":"Question about best practices for SQLite and Julia: when inserting data into the SQLite db (using SQLite.jl) --- in my case, from xml files, though it prob doesn’t mattter ---  is it better to prepare a Tables.jl table and then load that into the sqlite table *if* you have more than 1 row of data to insert? as opposed to just repeatedly using INSERT statements, that is","user":"US8V7JSKB","ts":"1615258279.295100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"U627","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Question about best practices for SQLite and Julia: when inserting data into the SQLite db (using SQLite.jl) --- in my case, from xml files, though it prob doesn’t mattter ---  is it better to prepare a Tables.jl table and then load that into the sqlite table "},{"type":"text","text":"if","style":{"bold":true}},{"type":"text","text":" you have more than 1 row of data to insert? as opposed to just repeatedly using INSERT statements, that is"}]}]}],"thread_ts":"1615258279.295100","reply_count":1,"reply_users_count":1,"latest_reply":"1615258845.295300","reply_users":["U01FKQQ7J0J"],"subscribed":false},{"client_msg_id":"5f2f7e45-77de-4b9e-8760-760cf8fcd73f","type":"message","text":"Anybody want to help bikeshed this little Tables.jl utility name? <https://github.com/JuliaData/Tables.jl/pull/233>\n\nI think <@U681ELA87>’s `rowmerge` suggestion is perfect for the underlying non-kwarg function here but seems unintuitive with the kwarg method: `rowmerge(row; x=1, y=2, z=3)`\n\nmaybe we should use `rowmerge` for the underlying non-kwarg function here, and then define a more appropriately named function for the kwarg version?\n\nmaybe `rowfrom`? `rowfrom(row; x=1, y=2)`\nor `copyrow`? `copyrow(row; x=1, y=2)`\n\nthoughts?","user":"U674T0Y9Z","ts":"1615305358.305900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=fA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Anybody want to help bikeshed this little Tables.jl utility name? "},{"type":"link","url":"https://github.com/JuliaData/Tables.jl/pull/233"},{"type":"text","text":"\n\nI think "},{"type":"user","user_id":"U681ELA87"},{"type":"text","text":"’s "},{"type":"text","text":"rowmerge","style":{"code":true}},{"type":"text","text":" suggestion is perfect for the underlying non-kwarg function here but seems unintuitive with the kwarg method: "},{"type":"text","text":"rowmerge(row; x=1, y=2, z=3)","style":{"code":true}},{"type":"text","text":"\n\nmaybe we should use "},{"type":"text","text":"rowmerge","style":{"code":true}},{"type":"text","text":" for the underlying non-kwarg function here, and then define a more appropriately named function for the kwarg version?\n\nmaybe "},{"type":"text","text":"rowfrom","style":{"code":true}},{"type":"text","text":"? "},{"type":"text","text":"rowfrom(row; x=1, y=2)","style":{"code":true}},{"type":"text","text":"\nor "},{"type":"text","text":"copyrow","style":{"code":true}},{"type":"text","text":"? "},{"type":"text","text":"copyrow(row; x=1, y=2)","style":{"code":true}},{"type":"text","text":"\n\nthoughts?"}]}]}],"thread_ts":"1615305358.305900","reply_count":3,"reply_users_count":2,"latest_reply":"1615306827.306800","reply_users":["U67431ELR","U681ELA87"],"subscribed":false},{"client_msg_id":"cf209a56-a431-4293-a7a7-65d1da3c3918","type":"message","text":"does anyone here know XML/Xpaths decently well? I'm trying to use EzXML but I can't seem to get the `find` functions to work with Xpaths how I expect. here is a <https://github.com/JuliaIO/EzXML.jl/issues/155|mwe issue> I posted. This could just be personal confusion but the xpath `//foo` should get all of the elements of foo right?","user":"UM8JUNJG7","ts":"1615310684.309800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yfA+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"does anyone here know XML/Xpaths decently well? I'm trying to use EzXML but I can't seem to get the "},{"type":"text","text":"find","style":{"code":true}},{"type":"text","text":" functions to work with Xpaths how I expect. here is a "},{"type":"link","url":"https://github.com/JuliaIO/EzXML.jl/issues/155","text":"mwe issue"},{"type":"text","text":" I posted. This could just be personal confusion but the xpath "},{"type":"text","text":"//foo","style":{"code":true}},{"type":"text","text":" should get all of the elements of foo right?"}]}]}]},{"client_msg_id":"b517bbf6-19b8-4acd-928a-3adec13a7f3c","type":"message","text":"<@U681ELA87>\n```Arrow.write(\"/tmp/data.arrow\", DataFrame(days=[Day(i) for i in 1:1:10]))\nrun(`Rscript -e \"library(arrow); df = read_feather('/tmp/data.arrow')\"`);\n\nError in Table__to_dataframe(x, use_threads = option_use_threads()) : \n  cannot handle Array of type \nCalls: read_feather ... as.data.frame -&gt; as.data.frame.Table -&gt; Table__to_dataframe\nExecution halted\nERROR: failed process: Process(`Rscript -e \"library(arrow); df = read_feather('/tmp/data.arrow')\"`, ProcessExited(1)) [1]```\nraises an error in R. would that be an R `feather` bug?","user":"U01ARRMLM7E","ts":"1615336375.318300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zwI","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U681ELA87"},{"type":"text","text":"\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Arrow.write(\"/tmp/data.arrow\", DataFrame(days=[Day(i) for i in 1:1:10]))\nrun(`Rscript -e \"library(arrow); df = read_feather('/tmp/data.arrow')\"`);\n\nError in Table__to_dataframe(x, use_threads = option_use_threads()) : \n  cannot handle Array of type \nCalls: read_feather ... as.data.frame -> as.data.frame.Table -> Table__to_dataframe\nExecution halted\nERROR: failed process: Process(`Rscript -e \"library(arrow); df = read_feather('/tmp/data.arrow')\"`, ProcessExited(1)) [1]"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"raises an error in R. would that be an R "},{"type":"text","text":"feather","style":{"code":true}},{"type":"text","text":" bug?"}]}]}]},{"client_msg_id":"06937dc8-874f-4ffc-b799-7574fcba16f7","type":"message","text":"so JSON.jl vs JSON3.jl","user":"U67BJLYCS","ts":"1615338060.318700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0G0U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"so JSON.jl vs JSON3.jl"}]}]}]},{"client_msg_id":"5955a477-4079-480b-9c87-73ef360f47b8","type":"message","text":"which one hsould I use :wink:","user":"U67BJLYCS","ts":"1615338067.319100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"R++je","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"which one hsould I use "},{"type":"emoji","name":"wink"}]}]}]},{"client_msg_id":"c114bb0b-06a8-4670-b62b-722e8dd1014e","type":"message","text":"does JSON3 assume that the data passed to `read` continues to exists?","user":"U67BJLYCS","ts":"1615338248.319600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SXq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"does JSON3 assume that the data passed to "},{"type":"text","text":"read","style":{"code":true}},{"type":"text","text":" continues to exists?"}]}]}],"thread_ts":"1615338248.319600","reply_count":9,"reply_users_count":2,"latest_reply":"1615338472.324100","reply_users":["U681ELA87","U67BJLYCS"],"subscribed":false},{"client_msg_id":"2d5792e5-0316-4a23-9a37-fe29e0c8e0d1","type":"message","text":"Does anyone know of a good tutorial/source for classifying an unbalanced dataset (1:15)","user":"U01C3624SGJ","ts":"1615385675.328200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HoNx=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does anyone know of a good tutorial/source for classifying an unbalanced dataset (1:15)"}]}]}]},{"client_msg_id":"0263d830-da66-42ee-bfa9-08cc4a910cd4","type":"message","text":"I have an arrow file with a couple of date columns which are `String`. I now want to parse them as dates and save the file back to disk, so I do:\n```df = DataFrame(Arrow.Table(\"myarrowfile.arrow\"))\ndf.date1 = Date.(df.date1)\ndf.date2 = Date.(df.date2)\nArrow.write(\"myarrowfile_dates.arrow\", df)```\nbut now when I do:\n```Arrow.Table(\"myarrowfile_dates.arrow\")```\nI get:\n```TaskFailedException\n\n    nested task error: TaskFailedException\n    Stacktrace:\n     [1] wait\n       @ .\\task.jl:317 [inlined]\n     [2] fetch\n       @ .\\task.jl:332 [inlined]\n     [3] macro expansion\n       @ ~\\.julia\\packages\\Arrow\\Re9EM\\src\\table.jl:198 [inlined]\n     [4] (::Arrow.var\"#82#88\"{Channel{Task}, Arrow.Table})()\n       @ Arrow .\\threadingconstructs.jl:169\n    \n        nested task error: KeyError: key 6 not found\n        Stacktrace:\n         [1] getindex\n           @ .\\dict.jl:482 [inlined]\n         [2] build(field::Arrow.Flatbuf.Field, batch::Arrow.Batch, rb::Arrow.Flatbuf.RecordBatch, de::Dict{Int64, Arrow.DictEncoding}, nodeidx::Int64, bufferidx::Int64, convert::Bool)\n           @ Arrow ~\\.julia\\packages\\Arrow\\Re9EM\\src\\table.jl:355\n         [3] iterate(x::Arrow.VectorIterator, ::Tuple{Int64, Int64, Int64})\n           @ Arrow ~\\.julia\\packages\\Arrow\\Re9EM\\src\\table.jl:336\n         [4] copyto!(dest::Vector{Any}, src::Arrow.VectorIterator)\n           @ Base .\\abstractarray.jl:841\n         [5] _collect\n           @ .\\array.jl:608 [inlined]\n         [6] collect\n           @ .\\array.jl:602 [inlined]\n         [7] macro expansion\n           @ ~\\.julia\\packages\\Arrow\\Re9EM\\src\\table.jl:254 [inlined]\n         [8] (::Arrow.var\"#86#92\"{Bool, Dict{Int64, Arrow.DictEncoding}, Arrow.Batch})()\n           @ Arrow .\\threadingconstructs.jl:169\n\nStacktrace:\n [1] wait\n   @ .\\task.jl:317 [inlined]\n [2] Arrow.Table(bytes::Vector{UInt8}, off::Int64, tlen::Nothing; convert::Bool)\n   @ Arrow ~\\.julia\\packages\\Arrow\\Re9EM\\src\\table.jl:261\n [3] Arrow.Table(str::String, pos::Int64, len::Nothing; convert::Bool)\n   @ Arrow ~\\.julia\\packages\\Arrow\\Re9EM\\src\\table.jl:179\n [4] Table\n   @ ~\\.julia\\packages\\Arrow\\Re9EM\\src\\table.jl:179 [inlined]\n [5] top-level scope\n   @ In[14]:1\n [6] eval\n   @ .\\boot.jl:360 [inlined]\n [7] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n   @ Base .\\loading.jl:1090```\nAny ideas what might be going on?","user":"U7JQGPGCQ","ts":"1615408617.337100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Lrj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have an arrow file with a couple of date columns which are "},{"type":"text","text":"String","style":{"code":true}},{"type":"text","text":". I now want to parse them as dates and save the file back to disk, so I do:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"df = DataFrame(Arrow.Table(\"myarrowfile.arrow\"))\ndf.date1 = Date.(df.date1)\ndf.date2 = Date.(df.date2)\nArrow.write(\"myarrowfile_dates.arrow\", df)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"but now when I do:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Arrow.Table(\"myarrowfile_dates.arrow\")"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I get:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"TaskFailedException\n\n    nested task error: TaskFailedException\n    Stacktrace:\n     [1] wait\n       @ .\\task.jl:317 [inlined]\n     [2] fetch\n       @ .\\task.jl:332 [inlined]\n     [3] macro expansion\n       @ ~\\.julia\\packages\\Arrow\\Re9EM\\src\\table.jl:198 [inlined]\n     [4] (::Arrow.var\"#82#88\"{Channel{Task}, Arrow.Table})()\n       @ Arrow .\\threadingconstructs.jl:169\n    \n        nested task error: KeyError: key 6 not found\n        Stacktrace:\n         [1] getindex\n           @ .\\dict.jl:482 [inlined]\n         [2] build(field::Arrow.Flatbuf.Field, batch::Arrow.Batch, rb::Arrow.Flatbuf.RecordBatch, de::Dict{Int64, Arrow.DictEncoding}, nodeidx::Int64, bufferidx::Int64, convert::Bool)\n           @ Arrow ~\\.julia\\packages\\Arrow\\Re9EM\\src\\table.jl:355\n         [3] iterate(x::Arrow.VectorIterator, ::Tuple{Int64, Int64, Int64})\n           @ Arrow ~\\.julia\\packages\\Arrow\\Re9EM\\src\\table.jl:336\n         [4] copyto!(dest::Vector{Any}, src::Arrow.VectorIterator)\n           @ Base .\\abstractarray.jl:841\n         [5] _collect\n           @ .\\array.jl:608 [inlined]\n         [6] collect\n           @ .\\array.jl:602 [inlined]\n         [7] macro expansion\n           @ ~\\.julia\\packages\\Arrow\\Re9EM\\src\\table.jl:254 [inlined]\n         [8] (::Arrow.var\"#86#92\"{Bool, Dict{Int64, Arrow.DictEncoding}, Arrow.Batch})()\n           @ Arrow .\\threadingconstructs.jl:169\n\nStacktrace:\n [1] wait\n   @ .\\task.jl:317 [inlined]\n [2] Arrow.Table(bytes::Vector{UInt8}, off::Int64, tlen::Nothing; convert::Bool)\n   @ Arrow ~\\.julia\\packages\\Arrow\\Re9EM\\src\\table.jl:261\n [3] Arrow.Table(str::String, pos::Int64, len::Nothing; convert::Bool)\n   @ Arrow ~\\.julia\\packages\\Arrow\\Re9EM\\src\\table.jl:179\n [4] Table\n   @ ~\\.julia\\packages\\Arrow\\Re9EM\\src\\table.jl:179 [inlined]\n [5] top-level scope\n   @ In[14]:1\n [6] eval\n   @ .\\boot.jl:360 [inlined]\n [7] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n   @ Base .\\loading.jl:1090"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Any ideas what might be going on?"}]}]}],"thread_ts":"1615408617.337100","reply_count":3,"reply_users_count":3,"latest_reply":"1615409383.337600","reply_users":["U9VG1AYSG","U681ELA87","U7JQGPGCQ"],"subscribed":false},{"client_msg_id":"2bc08edd-b3c2-421e-99cf-1ad11e1d7785","type":"message","text":"How do I resample data in Julia? I have tick data and I'd like to resample it so that I only keep the rows with the first instance of each second? For example in what is shown below I would only like to keep the first row and the others would be discarded. Thanks.\n\n2021-01-04 09:30:02:006\n2021-01-04 09:30:02:008\n2021-01-04 09:30:02:012\n2021-01-04 09:30:02:014","user":"U01QJ915TFD","ts":"1615483254.341100","team":"T68168MUP","edited":{"user":"U01QJ915TFD","ts":"1615483363.000000"},"blocks":[{"type":"rich_text","block_id":"o=4r","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How do I resample data in Julia? I have tick data and I'd like to resample it so that I only keep the rows with the first instance of each second? For example in what is shown below I would only like to keep the first row and the others would be discarded. Thanks.\n\n2021-01-04 09:30:02:006\n2021-01-04 09:30:02:008\n2021-01-04 09:30:02:012\n2021-01-04 09:30:02:014"}]}]}],"thread_ts":"1615483254.341100","reply_count":2,"reply_users_count":1,"latest_reply":"1615483879.341500","reply_users":["U011V2YN59N"],"subscribed":false},{"client_msg_id":"e1e3a942-ce56-4cb8-b8ce-4b46876c8e06","type":"message","text":"related: has there been discussion about a version of `unique` that accepts an equivalence relation?","user":"U011V2YN59N","ts":"1615484645.343400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lV0Ky","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"related: has there been discussion about a version of "},{"type":"text","text":"unique","style":{"code":true}},{"type":"text","text":" that accepts an equivalence relation?"}]}]}],"thread_ts":"1615484645.343400","reply_count":15,"reply_users_count":3,"latest_reply":"1615486530.347800","reply_users":["UBF9YRB6H","U011V2YN59N","U8JAMQGQY"],"subscribed":false},{"client_msg_id":"415d87ad-a821-4a20-b31d-0a05e35eaafd","type":"message","text":"Re DataFramesMeta’s @transform macro: is there a way to bind a ‘partially applied’ `@transform` to a name? i.e., the macro equivalent of `t = blah_blah -&gt; transform(df, blah_blah)` ? I basically don’t want to have to keep referencing the same df if I want to do a bunch of transforms consecutively","user":"US8V7JSKB","ts":"1615533036.354000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fWz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Re DataFramesMeta’s @transform macro: is there a way to bind a ‘partially applied’ "},{"type":"text","text":"@transform","style":{"code":true}},{"type":"text","text":" to a name? i.e., the macro equivalent of "},{"type":"text","text":"t = blah_blah -> transform(df, blah_blah)","style":{"code":true}},{"type":"text","text":" ? I basically don’t want to have to keep referencing the same df if I want to do a bunch of transforms consecutively"}]}]}]},{"client_msg_id":"82af0166-9839-4172-aec4-06fb6051421d","type":"message","text":"Is there any chance to read a tsv file with multiple `delim` s using CSV.jl?\n\nThis file <https://ec.europa.eu/eurostat/estat-navtree-portlet-prod/BulkDownloadListing?file=data/nama_10r_2hhinc.tsv.gz> from Eurostat uses `\\t` ,   `\\t` and `,`  in one file.","user":"U836PQXSN","ts":"1615562563.361000","team":"T68168MUP","edited":{"user":"U836PQXSN","ts":"1615562577.000000"},"blocks":[{"type":"rich_text","block_id":"jLQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there any chance to read a tsv file with multiple "},{"type":"text","text":"delim","style":{"code":true}},{"type":"text","text":" s using CSV.jl?\n\nThis file "},{"type":"link","url":"https://ec.europa.eu/eurostat/estat-navtree-portlet-prod/BulkDownloadListing?file=data/nama_10r_2hhinc.tsv.gz"},{"type":"text","text":" from Eurostat uses "},{"type":"text","text":"\\t","style":{"code":true}},{"type":"text","text":" ,  "},{"type":"text","text":" \\t","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":",","style":{"code":true}},{"type":"text","text":"  in one file."}]}]}],"thread_ts":"1615562563.361000","reply_count":1,"reply_users_count":1,"latest_reply":"1615562719.361200","reply_users":["U836PQXSN"],"subscribed":false},{"client_msg_id":"eb703cc1-1ec0-4d6f-93a2-2d6566c5ad18","type":"message","text":"Could interested people have a look at <https://github.com/JuliaData/DataFrames.jl/pull/2649> so that we do not regret the design after we implement it? (we are now getting more restrictive with breaking things so experimenting is risky :smile:) Thank you!","user":"U8JAMQGQY","ts":"1615563217.362600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TcH0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Could interested people have a look at "},{"type":"link","url":"https://github.com/JuliaData/DataFrames.jl/pull/2649"},{"type":"text","text":" so that we do not regret the design after we implement it? (we are now getting more restrictive with breaking things so experimenting is risky "},{"type":"emoji","name":"smile"},{"type":"text","text":") Thank you!"}]}]}]},{"client_msg_id":"2563e67e-c56e-401e-88c5-d2c327e6ac31","type":"message","text":"In Julia do changes on a copy of a variable get applied to the original automatically?\nIn python I would generally assign data to a new variable in case I was going to experiment so as to not affect the original.\n\nI just tried doing that in Julia and I now see that my original is showing up just as my copy.\nFor reference, I was trying to remove the \"T\" from the DateTime Format and my column's data changed from datetime to Any.","user":"U01QJ915TFD","ts":"1615574484.366100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"AE6mV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In Julia do changes on a copy of a variable get applied to the original automatically?\nIn python I would generally assign data to a new variable in case I was going to experiment so as to not affect the original.\n\nI just tried doing that in Julia and I now see that my original is showing up just as my copy.\nFor reference, I was trying to remove the \"T\" from the DateTime Format and my column's data changed from datetime to Any."}]}]}],"thread_ts":"1615574484.366100","reply_count":37,"reply_users_count":4,"latest_reply":"1615577198.374000","reply_users":["UBF9YRB6H","U01QJ915TFD","USU9FRPEU","UH24GRBLL"],"subscribed":false},{"type":"message","text":"How can I remove the T from the middle of my DateTime rows?","files":[{"id":"F01R4JRBAAF","created":1615580327,"timestamp":1615580327,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01QJ915TFD","editable":false,"size":50464,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01R4JRBAAF/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01R4JRBAAF/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01R4JRBAAF-b877d7adbb/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01R4JRBAAF-b877d7adbb/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01R4JRBAAF-b877d7adbb/image_360.png","thumb_360_w":360,"thumb_360_h":254,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01R4JRBAAF-b877d7adbb/image_480.png","thumb_480_w":480,"thumb_480_h":339,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01R4JRBAAF-b877d7adbb/image_160.png","original_w":658,"original_h":465,"thumb_tiny":"AwAhADCruI70u7nPP502lFMALE96Snd6P896AG0HHalNKelADaUdqSlFAC9//wBdL/nvSZGaMj/IoAG6fjQen/66QkUEigBKKKKACiiigAooooA//9k=","permalink":"https://julialang.slack.com/files/U01QJ915TFD/F01R4JRBAAF/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01R4JRBAAF-df04c37dda","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"2M6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I remove the T from the middle of my DateTime rows?"}]}]}],"user":"U01QJ915TFD","display_as_bot":false,"ts":"1615580400.376200"},{"client_msg_id":"02c1b767-fcab-4040-a616-31a0d607e734","type":"message","text":"That’s the default of how `DateTime` objects are printed. If you’d rather transform it to a custom-formatted string column, you could do something like:\n```df.dt = Dates.format.(dt.dt, \"yyyy-mm-dd HH:MM:SS.s\")```","user":"U681ELA87","ts":"1615580706.378300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"B8X/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That’s the default of how "},{"type":"text","text":"DateTime","style":{"code":true}},{"type":"text","text":" objects are printed. If you’d rather transform it to a custom-formatted string column, you could do something like:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"df.dt = Dates.format.(dt.dt, \"yyyy-mm-dd HH:MM:SS.s\")"}]}]}],"thread_ts":"1615580706.378300","reply_count":2,"reply_users_count":2,"latest_reply":"1615583508.385800","reply_users":["U01QJ915TFD","UAGUENL2Y"],"subscribed":false,"reactions":[{"name":"slightly_smiling_face","users":["U01QJ915TFD"],"count":1}]},{"client_msg_id":"1864bc9e-8a69-46d3-8dad-20585cfa7482","type":"message","text":"the `T` is part of ISO 8601, the objectively best datetime format standard","user":"UH24GRBLL","ts":"1615580736.378800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FbdJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"the "},{"type":"text","text":"T","style":{"code":true}},{"type":"text","text":" is part of ISO 8601, the objectively best datetime format standard"}]}]}],"thread_ts":"1615580736.378800","reply_count":5,"reply_users_count":2,"latest_reply":"1615582668.381300","reply_users":["U01QJ915TFD","UH24GRBLL"],"subscribed":false,"reactions":[{"name":"slightly_smiling_face","users":["U01QJ915TFD","U82LX4ACB"],"count":2},{"name":"+1","users":["U01FKQQ7J0J"],"count":1}]},{"client_msg_id":"35f4ff51-5238-409f-9d03-481b1988837f","type":"message","text":"it's also just a representation thing","user":"UH24GRBLL","ts":"1615580784.379100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"V+B","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it's also just a representation thing"}]}]}]},{"type":"message","text":"a final conversion from dataframe to Array is usually ugly because I want to be in column-major for numerical reasons.\n\nIs there an elegant way to convert to an Array and have it be column-major?","user":"U90JR0C80","ts":"1615581633.379800","team":"T68168MUP","attachments":[{"fallback":"[March 12th, 2021 3:21 PM] jessebett: saving and loading csv data with DataFrames keeps things row-major.\nafter manipulating the data, e.g. for selection, I am prepared to do numerical work where I want my data as an Array stored column-major.\n\nIs there an elegant way to do this transformation? Currently converting to matrix, transposing, and collecting from the LinearAlgebra.Adjoint type.","ts":"1615580474.019200","author_id":"U90JR0C80","author_subname":"Jesse Bettencourt","channel_id":"C6A044SQH","channel_name":"helpdesk","is_msg_unfurl":true,"is_thread_root_unfurl":true,"text":"saving and loading csv data with DataFrames keeps things row-major.\nafter manipulating the data, e.g. for selection, I am prepared to do numerical work where I want my data as an Array stored column-major.\n\nIs there an elegant way to do this transformation? Currently converting to matrix, transposing, and collecting from the LinearAlgebra.Adjoint type.","author_name":"Jesse Bettencourt","author_link":"https://julialang.slack.com/team/U90JR0C80","author_icon":"https://avatars.slack-edge.com/2018-01-30/307291696386_d856e2350ce251cee88a_48.jpg","mrkdwn_in":["text"],"color":"D0D0D0","from_url":"https://julialang.slack.com/archives/C6A044SQH/p1615580474019200?thread_ts=1615580474019200&cid=C6A044SQH","is_share":true,"footer":"Thread in #helpdesk"}],"blocks":[{"type":"rich_text","block_id":"enyK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"a final conversion from dataframe to Array is usually ugly because I want to be in column-major for numerical reasons.\n\nIs there an elegant way to convert to an Array and have it be column-major?"}]}]}],"thread_ts":"1615581633.379800","reply_count":2,"reply_users_count":1,"latest_reply":"1615581969.380100","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"f2e29bfd-3c26-4f76-8638-baf7e5c4eea6","type":"message","text":"How can I set my DateTime rows to only include rows between between two times? The file I'm working with has data from 4:00AM to 8:00PM(20:00) but I only want to have data from 9:30AM to 4:00PM on my dataframe.","user":"U01QJ915TFD","ts":"1615590197.395100","team":"T68168MUP","edited":{"user":"U01QJ915TFD","ts":"1615590246.000000"},"blocks":[{"type":"rich_text","block_id":"gTV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I set my DateTime rows to only include rows between between two times? The file I'm working with has data from 4:00AM to 8:00PM(20:00) but I only want to have data from 9:30AM to 4:00PM on my dataframe."}]}]}],"thread_ts":"1615590197.395100","reply_count":1,"reply_users_count":1,"latest_reply":"1615590364.395300","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"0a205e6b-8499-41c3-a719-57251ca7c83f","type":"message","text":"<@U681ELA87> was typing this up in slack here before I realized it might be better to preserve in GitHub <https://github.com/JuliaData/Arrow.jl/issues/88#issuecomment-798806038>\n\na Slack-amenable tangent to that, though - I wonder if there's a better way to support/replace the `Foo`/`_FooArrow` pattern I'm demo'ing in that comment. Instead of:\n\n```struct _FooArrow ... end\n\nFoo(::_FooArrow) = ...\n\nArrow.ArrowTypes.registertype!(Foo, _FooArrow)\n\nArrow.ArrowTypes.arrowconvert(::Type{_FooArrow}, f::Foo) = ...```\nIt'd be nice if I could just define\n\n```toarrow(::Foo)::NamedTuple = ...\nfromarrow(Type{&lt;:Foo}, ::NamedTuple) = ...```\nwithout needing to define a `_FooArrow` type at all or dynamically register stuff\n\nhaven't thought too hard about it though","user":"U674T0Y9Z","ts":"1615681523.427700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"n5vVH","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U681ELA87"},{"type":"text","text":" was typing this up in slack here before I realized it might be better to preserve in GitHub "},{"type":"link","url":"https://github.com/JuliaData/Arrow.jl/issues/88#issuecomment-798806038"},{"type":"text","text":"\n\na Slack-amenable tangent to that, though - I wonder if there's a better way to support/replace the "},{"type":"text","text":"Foo","style":{"code":true}},{"type":"text","text":"/"},{"type":"text","text":"_FooArrow","style":{"code":true}},{"type":"text","text":" pattern I'm demo'ing in that comment. Instead of:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"struct _FooArrow ... end\n\nFoo(::_FooArrow) = ...\n\nArrow.ArrowTypes.registertype!(Foo, _FooArrow)\n\nArrow.ArrowTypes.arrowconvert(::Type{_FooArrow}, f::Foo) = ..."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nIt'd be nice if I could just define\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"toarrow(::Foo)::NamedTuple = ...\nfromarrow(Type{<:Foo}, ::NamedTuple) = ..."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nwithout needing to define a "},{"type":"text","text":"_FooArrow","style":{"code":true}},{"type":"text","text":" type at all or dynamically register stuff\n\nhaven't thought too hard about it though"}]}]}]},{"client_msg_id":"cd2a8008-c348-4ca1-904d-c2cbe7ef2d85","type":"message","text":"Yes, I've been slowly forming my thoughts around this over the last week. I would add a `ArrowTypes.lower` function (like `JSON.lower`) that would allow a hook into the serialization process to say how your custom struct should be serialized. You would overload `ArrowTypes.lower(::MyCustomStruct)` and it would need to return standard arrow supported structs.","user":"U681ELA87","ts":"1615681710.429600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wapF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes, I've been slowly forming my thoughts around this over the last week. I would add a "},{"type":"text","text":"ArrowTypes.lower","style":{"code":true}},{"type":"text","text":" function (like "},{"type":"text","text":"JSON.lower","style":{"code":true}},{"type":"text","text":") that would allow a hook into the serialization process to say how your custom struct should be serialized. You would overload "},{"type":"text","text":"ArrowTypes.lower(::MyCustomStruct)","style":{"code":true}},{"type":"text","text":" and it would need to return standard arrow supported structs."}]}]}],"thread_ts":"1615681710.429600","reply_count":1,"reply_users_count":1,"latest_reply":"1615681830.430600","reply_users":["U674T0Y9Z"],"subscribed":false},{"client_msg_id":"8d32632d-cf05-4c50-a996-bbc3446bee2a","type":"message","text":"What I haven't quite worked out the details on is still serializing the metadata of the pre-lowered struct so you can still get it back out automatically..","user":"U681ELA87","ts":"1615681758.430500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"M4V8B","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What I haven't quite worked out the details on is still serializing the metadata of the pre-lowered struct so you can still get it back out automatically.."}]}]}],"thread_ts":"1615681758.430500","reply_count":1,"reply_users_count":1,"latest_reply":"1615682229.430800","reply_users":["U674T0Y9Z"],"subscribed":false},{"client_msg_id":"892e951a-5673-4378-97e0-34d2d030910c","type":"message","text":"<https://h2oai.github.io/db-benchmark/|https://h2oai.github.io/db-benchmark/>","user":"UKG4WF8PJ","ts":"1615689460.433200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5qfr2","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://h2oai.github.io/db-benchmark/","text":"https://h2oai.github.io/db-benchmark/"}]}]}]},{"client_msg_id":"aa72022e-cc0c-400b-a690-e9a6d5ca564a","type":"message","text":"<https://www.ritchievink.com/blog/2021/02/28/i-wrote-one-of-the-fastest-dataframe-libraries/>","user":"UDGT4PM41","ts":"1615699697.434400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FGe","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://www.ritchievink.com/blog/2021/02/28/i-wrote-one-of-the-fastest-dataframe-libraries/"}]}]}],"thread_ts":"1615699697.434400","reply_count":1,"reply_users_count":1,"latest_reply":"1615700380.434500","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"fd4560a3-453c-4ed0-867a-2bfd99fdcc72","type":"message","text":"What’s the best way to read Parquet files from Google Cloud Storage?","user":"U01GXNFKY6R","ts":"1615711455.437000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eGHd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What’s the best way to read Parquet files from Google Cloud Storage?"}]}]}]},{"client_msg_id":"93ef6318-1ffc-49ac-b4a3-6301f17d1cf0","type":"message","text":"I have a DataFrame with a column of tuples, each containing three elements. How can I split that up into three columns where each one has the corresponding tuple elements?","user":"U011V2YN59N","ts":"1615749713.444300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"44J","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a DataFrame with a column of tuples, each containing three elements. How can I split that up into three columns where each one has the corresponding tuple elements?"}]}]}]},{"client_msg_id":"37e15e1c-4265-4863-9c6e-ee39f124fb3d","type":"message","text":"I can't seem to find a nice way to do it with the dataframes api","user":"U011V2YN59N","ts":"1615749755.445100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"NLH1+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I can't seem to find a nice way to do it with the dataframes api"}]}]}]},{"client_msg_id":"c893e776-7a59-499e-a655-7d670234d91c","type":"message","text":"```julia&gt; using DataFrames\n\njulia&gt; df = DataFrame(a = [(1, 2, 3), (4, 5, 6)])\n2×1 DataFrame\n Row │ a         \n     │ Tuple…    \n─────┼───────────\n   1 │ (1, 2, 3)\n   2 │ (4, 5, 6)\n\njulia&gt; transform(df, :a =&gt; identity =&gt; AsTable)\n2×4 DataFrame\n Row │ a          x1     x2     x3    \n     │ Tuple…     Int64  Int64  Int64 \n─────┼────────────────────────────────\n   1 │ (1, 2, 3)      1      2      3\n   2 │ (4, 5, 6)      4      5      6```\nTo give them informative names, change `identity` to be a function returning a named tuple or a DataFrame","user":"UBF9YRB6H","ts":"1615749845.446000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/mJ+0","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> using DataFrames\n\njulia> df = DataFrame(a = [(1, 2, 3), (4, 5, 6)])\n2×1 DataFrame\n Row │ a         \n     │ Tuple…    \n─────┼───────────\n   1 │ (1, 2, 3)\n   2 │ (4, 5, 6)\n\njulia> transform(df, :a => identity => AsTable)\n2×4 DataFrame\n Row │ a          x1     x2     x3    \n     │ Tuple…     Int64  Int64  Int64 \n─────┼────────────────────────────────\n   1 │ (1, 2, 3)      1      2      3\n   2 │ (4, 5, 6)      4      5      6"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nTo give them informative names, change "},{"type":"text","text":"identity","style":{"code":true}},{"type":"text","text":" to be a function returning a named tuple or a DataFrame"}]}]}]},{"client_msg_id":"4c881f55-8ef9-45b3-b00d-f77ffa96b91e","type":"message","text":"ah wow that is so much nicer than my mess of `map`  and `zip`","user":"U011V2YN59N","ts":"1615749880.446500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Q+r","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ah wow that is so much nicer than my mess of "},{"type":"text","text":"map","style":{"code":true}},{"type":"text","text":"  and "},{"type":"text","text":"zip","style":{"code":true}}]}]}],"reactions":[{"name":"+1","users":["UBF9YRB6H"],"count":1}]},{"client_msg_id":"1397af91-79bb-4096-83e5-b4ec0f921017","type":"message","text":"thanks!","user":"U011V2YN59N","ts":"1615749890.446800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"c+s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"thanks!"}]}]}]},{"client_msg_id":"6b784b2e-2280-4ecf-a053-c811949a1d00","type":"message","text":"hmm I am getting this error with that code","user":"U011V2YN59N","ts":"1615750127.447100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6sqT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hmm I am getting this error with that code"}]}]}]},{"client_msg_id":"2d1ee8a4-5727-473f-bf15-18dadc62d96a","type":"message","text":"```ERROR: ArgumentError: keys of the returned elements must be identical\nStacktrace:\n [1] _expand_to_table(res::Vector{String})\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:380\n [2] select_transform!(nc::Union{Function, Pair{var\"#s267\", var\"#s266\"} where {var\"#s267\"&lt;:Union{Int64, AsTable, AbstractVector{Int64}}, var\"#s266\"&lt;:(Pair{var\"#s164\", var\"#s163\"} where {var\"#s164\"&lt;:Union{Function, Type}, var\"#s163\"&lt;:Union{DataType, Symbol, AbstractVector{Symbol}}})}, Type}, df::DataFrame, newdf::DataFrame, transformed_cols::Set{Symbol}, copycols::Bool, allow_resizing_newdf::Base.RefValue{Bool})\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:522\n [3] _manipulate(df::DataFrame, normalized_cs::Any, copycols::Bool, keeprows::Bool)\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:1279\n [4] manipulate(::DataFrame, ::Any, ::Vararg{Any, N} where N; copycols::Bool, keeprows::Bool, renamecols::Bool)\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:1209\n [5] #select#384\n   @ ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:847 [inlined]\n [6] #transform#386\n   @ ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:913 [inlined]```","user":"U011V2YN59N","ts":"1615750146.447800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"H6l","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: ArgumentError: keys of the returned elements must be identical\nStacktrace:\n [1] _expand_to_table(res::Vector{String})\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:380\n [2] select_transform!(nc::Union{Function, Pair{var\"#s267\", var\"#s266\"} where {var\"#s267\"<:Union{Int64, AsTable, AbstractVector{Int64}}, var\"#s266\"<:(Pair{var\"#s164\", var\"#s163\"} where {var\"#s164\"<:Union{Function, Type}, var\"#s163\"<:Union{DataType, Symbol, AbstractVector{Symbol}}})}, Type}, df::DataFrame, newdf::DataFrame, transformed_cols::Set{Symbol}, copycols::Bool, allow_resizing_newdf::Base.RefValue{Bool})\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:522\n [3] _manipulate(df::DataFrame, normalized_cs::Any, copycols::Bool, keeprows::Bool)\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:1279\n [4] manipulate(::DataFrame, ::Any, ::Vararg{Any, N} where N; copycols::Bool, keeprows::Bool, renamecols::Bool)\n   @ DataFrames ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:1209\n [5] #select#384\n   @ ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:847 [inlined]\n [6] #transform#386\n   @ ~/.julia/packages/DataFrames/oQ5c7/src/abstractdataframe/selection.jl:913 [inlined]"}]}]}]},{"client_msg_id":"d201c99b-5b62-4078-85f6-ed665ff5583a","type":"message","text":"Do you have a mix of tuples and named tuples? Or some tuples that have 4 elements?","user":"UBF9YRB6H","ts":"1615750564.448400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RLU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do you have a mix of tuples and named tuples? Or some tuples that have 4 elements?"}]}]}]},{"client_msg_id":"33cc3e7f-5886-4b2b-b929-ae2116c852f3","type":"message","text":"ah I see what was happening, my tuples weren't be parsed from the CSV properly.","user":"U011V2YN59N","ts":"1615750777.448800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yy/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ah I see what was happening, my tuples weren't be parsed from the CSV properly."}]}]}]},{"client_msg_id":"2c023030-8b94-48ac-a652-0b63cf64acff","type":"message","text":"I am really trying to get a Pandas dataframe where one column is full of tuples","user":"U011V2YN59N","ts":"1615750794.449300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iYd5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am really trying to get a Pandas dataframe where one column is full of tuples"}]}]}]},{"client_msg_id":"ccd93f57-56fa-48e9-883f-0c0f9c4d91aa","type":"message","text":"from python to julia","user":"U011V2YN59N","ts":"1615750799.449500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uzLMm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"from python to julia"}]}]}]},{"client_msg_id":"f49a5572-fdc0-433a-805f-7b7de4cc6a74","type":"message","text":"Pandas.read_csv does not parse tuples though","user":"U011V2YN59N","ts":"1615750811.449800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"EeGo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Pandas.read_csv does not parse tuples though"}]}]}]},{"client_msg_id":"899013a1-90e5-451a-92a8-21281f661f8b","type":"message","text":"(Pandas being `Pandas.jl`)","user":"U011V2YN59N","ts":"1615750837.450200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qTPd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(Pandas being "},{"type":"text","text":"Pandas.jl","style":{"code":true}},{"type":"text","text":")"}]}]}]},{"client_msg_id":"bf8d2b46-bcbb-4872-853c-4f674690b29c","type":"message","text":"thanks!","user":"U011V2YN59N","ts":"1615750940.450600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eGebC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"thanks!"}]}]}]},{"client_msg_id":"33acb2e6-d817-44dc-ab76-b6e8e7546b86","type":"message","text":"dataframes qn: Why is it that\n```checkEntry(col) = map(cell-&gt;(cell == \"&lt;5\" ? rand([1,2,3,4]) : cell), col)\nmapcols!(checkEntry, df)```\nworks, but\n```for col in eachcol(df)\n  col = map(checkEntry, col)\nend```\ndoesn’t?\n(The first is obviously much nicer, but I’d expected the latter to work too.)","user":"US8V7JSKB","ts":"1615792371.454700","team":"T68168MUP","edited":{"user":"US8V7JSKB","ts":"1615792422.000000"},"blocks":[{"type":"rich_text","block_id":"/hS+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"dataframes qn: Why is it that\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"checkEntry(col) = map(cell->(cell == \"<5\" ? rand([1,2,3,4]) : cell), col)\nmapcols!(checkEntry, df)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"works, but\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"for col in eachcol(df)\n  col = map(checkEntry, col)\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"doesn’t?\n(The first is obviously much nicer, but I’d expected the latter to work too.)"}]}]}]},{"client_msg_id":"237c5f8f-4177-46fc-9204-0d892cbdf536","type":"message","text":"Trying to get the rows with the top N items from some column. Can anyone think of a better way than this?\n\n```julia&gt; df = DataFrame(x = 1:10, y = rand(10));\n\njulia&gt; function findmaxn(v, n)\n           srt = invperm(sortperm(v,rev=true))\n           findall(&lt;=(n), srt)\n       end\nfindmaxn (generic function with 1 method)\n\njulia&gt; df[findmaxn(df.y, 3), :]\n3×2 DataFrame\n Row │ x      y        \n     │ Int64  Float64  \n─────┼─────────────────\n   1 │     3  0.555818\n   2 │     7  0.818728\n   3 │     8  0.792478```","user":"U8JP5B9T2","ts":"1615822342.456800","team":"T68168MUP","edited":{"user":"U8JP5B9T2","ts":"1615822368.000000"},"blocks":[{"type":"rich_text","block_id":"=ty","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Trying to get the rows with the top N items from some column. Can anyone think of a better way than this?\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> df = DataFrame(x = 1:10, y = rand(10));\n\njulia> function findmaxn(v, n)\n           srt = invperm(sortperm(v,rev=true))\n           findall(<=(n), srt)\n       end\nfindmaxn (generic function with 1 method)\n\njulia> df[findmaxn(df.y, 3), :]\n3×2 DataFrame\n Row │ x      y        \n     │ Int64  Float64  \n─────┼─────────────────\n   1 │     3  0.555818\n   2 │     7  0.818728\n   3 │     8  0.792478"}]}]}],"thread_ts":"1615822342.456800","reply_count":4,"reply_users_count":2,"latest_reply":"1615822660.458100","reply_users":["U67431ELR","U8JP5B9T2"],"subscribed":false},{"client_msg_id":"258894bb-23cb-4523-b26f-e2047fcd9ae6","type":"message","text":"in particular, it seems like there should be something like `findmaxn` somewhere","user":"U8JP5B9T2","ts":"1615822388.457400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"aOSj0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"in particular, it seems like there should be something like "},{"type":"text","text":"findmaxn","style":{"code":true}},{"type":"text","text":" somewhere"}]}]}]},{"client_msg_id":"02e14eb2-d3ae-4c7d-8b3a-20ff931123c6","type":"message","text":"Is there a way to check if a high-resolution timestamp is contained within a low-resolution one?\n```using Dates\nt_month = floor(Dates.now(), Dates.Month)\nt_sec = floor(Dates.now(), Dates.Second)\nt_sec in t_month == true # method error```","user":"UB2QSHWPN","ts":"1615822928.459900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tlwq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to check if a high-resolution timestamp is contained within a low-resolution one?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Dates\nt_month = floor(Dates.now(), Dates.Month)\nt_sec = floor(Dates.now(), Dates.Second)\nt_sec in t_month == true # method error"}]}]}],"thread_ts":"1615822928.459900","reply_count":1,"reply_users_count":1,"latest_reply":"1615823034.460000","reply_users":["UBF9YRB6H"],"subscribed":false},{"client_msg_id":"de8e5604-a952-4497-8070-8166c472a03d","type":"message","text":"hi guys! is there an equivalent of <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html> ? I’m trying to put something similar together than quantopian’s alphalens, and would need to bin the data effectively. I wrote my own function to do it (as I couldn’t find anything similar), but would love to compare it to some kind of official implementation","user":"U01N351DMT9","ts":"1615903158.002300","team":"T68168MUP","edited":{"user":"U01N351DMT9","ts":"1615903162.000000"},"blocks":[{"type":"rich_text","block_id":"yjw2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hi guys! is there an equivalent of "},{"type":"link","url":"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html"},{"type":"text","text":" ? I’m trying to put something similar together than quantopian’s alphalens, and would need to bin the data effectively. I wrote my own function to do it (as I couldn’t find anything similar), but would love to compare it to some kind of official implementation"}]}]}],"thread_ts":"1615903158.002300","reply_count":9,"reply_users_count":2,"latest_reply":"1615904212.005600","reply_users":["U01N351DMT9","U67431ELR"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"if there’s no such function, I’d love to collaborate with someone who is more experienced with Julia to make this happen as part of any package!","user":"U01N351DMT9","ts":"1615903350.004000","thread_ts":"1615903158.002300","root":{"client_msg_id":"de8e5604-a952-4497-8070-8166c472a03d","type":"message","text":"hi guys! is there an equivalent of <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html> ? I’m trying to put something similar together than quantopian’s alphalens, and would need to bin the data effectively. I wrote my own function to do it (as I couldn’t find anything similar), but would love to compare it to some kind of official implementation","user":"U01N351DMT9","ts":"1615903158.002300","team":"T68168MUP","edited":{"user":"U01N351DMT9","ts":"1615903162.000000"},"blocks":[{"type":"rich_text","block_id":"yjw2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hi guys! is there an equivalent of "},{"type":"link","url":"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html"},{"type":"text","text":" ? I’m trying to put something similar together than quantopian’s alphalens, and would need to bin the data effectively. I wrote my own function to do it (as I couldn’t find anything similar), but would love to compare it to some kind of official implementation"}]}]}],"thread_ts":"1615903158.002300","reply_count":9,"reply_users_count":2,"latest_reply":"1615904212.005600","reply_users":["U01N351DMT9","U67431ELR"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"8IDTe","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"if there’s no such function, I’d love to collaborate with someone who is more experienced with Julia to make this happen as part of any package!"}]}]}],"client_msg_id":"00db99f9-c6d7-4284-9ef0-f12f9553dd34"},{"type":"message","subtype":"thread_broadcast","text":"nothing! my own crappy implementation also uses it. but I couldn’t find a code snippet to easily use it with dataframes’ groupby / combine methods. do you have a direction to point me towards?","user":"U01N351DMT9","ts":"1615903618.004500","thread_ts":"1615903158.002300","root":{"client_msg_id":"de8e5604-a952-4497-8070-8166c472a03d","type":"message","text":"hi guys! is there an equivalent of <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html> ? I’m trying to put something similar together than quantopian’s alphalens, and would need to bin the data effectively. I wrote my own function to do it (as I couldn’t find anything similar), but would love to compare it to some kind of official implementation","user":"U01N351DMT9","ts":"1615903158.002300","team":"T68168MUP","edited":{"user":"U01N351DMT9","ts":"1615903162.000000"},"blocks":[{"type":"rich_text","block_id":"yjw2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hi guys! is there an equivalent of "},{"type":"link","url":"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html"},{"type":"text","text":" ? I’m trying to put something similar together than quantopian’s alphalens, and would need to bin the data effectively. I wrote my own function to do it (as I couldn’t find anything similar), but would love to compare it to some kind of official implementation"}]}]}],"thread_ts":"1615903158.002300","reply_count":9,"reply_users_count":2,"latest_reply":"1615904212.005600","reply_users":["U01N351DMT9","U67431ELR"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"+7/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"nothing! my own crappy implementation also uses it. but I couldn’t find a code snippet to easily use it with dataframes’ groupby / combine methods. do you have a direction to point me towards?"}]}]}],"client_msg_id":"7924922b-d1b9-415a-bbeb-2eb77f1b7cd2"},{"client_msg_id":"3034bcb7-8669-4da2-a73e-e3d69e6fb222","type":"message","text":"How would you merge 2 dataframes in the following situation? :\nDataFrame A has 2 columns, DataFrame B has 3 columns but both DataFrame A and B share some rows in their first column.\n\nAfter merging, I'd like to have a DataFrame with a total of 4 columns which only keeps the rows that DataFrame A and B have in common in their first column.","user":"U01QJ915TFD","ts":"1615916916.011400","team":"T68168MUP","edited":{"user":"U01QJ915TFD","ts":"1615917014.000000"},"blocks":[{"type":"rich_text","block_id":"Tuu2G","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How would you merge 2 dataframes in the following situation? :\nDataFrame A has 2 columns, DataFrame B has 3 columns but both DataFrame A and B share some rows in their first column.\n\nAfter merging, I'd like to have a DataFrame with a total of 4 columns which only keeps the rows that DataFrame A and B have in common in their first column."}]}]}],"thread_ts":"1615916916.011400","reply_count":2,"reply_users_count":2,"latest_reply":"1615917490.012300","reply_users":["U8JAMQGQY","U01QJ915TFD"],"subscribed":false},{"client_msg_id":"c1c57145-31dd-48d9-b475-af50b054bae3","type":"message","text":"Hello hello data peeps","user":"U6QGE7S86","ts":"1615944507.000300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YIvMt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello hello data peeps"}]}]}]},{"client_msg_id":"d5b861fe-e0ee-4a26-9b14-6903d759dc61","type":"message","text":"Are there more recent benchmarks vs other tabular readers than <https://h2oai.github.io/db-benchmark/>?","user":"U6QGE7S86","ts":"1615944526.000700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gcC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Are there more recent benchmarks vs other tabular readers than "},{"type":"link","url":"https://h2oai.github.io/db-benchmark/"},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"1e0685f3-8260-45ca-9d77-5ab1d4122e52","type":"message","text":"They're using 1.5.3 Julia so I'm guessing there will be some awesome speedups for their new round of benchmarking.","user":"U6QGE7S86","ts":"1615944547.001200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sCsp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"They're using 1.5.3 Julia so I'm guessing there will be some awesome speedups for their new round of benchmarking."}]}]}]},{"client_msg_id":"62c081a1-ffe8-4bb8-a54c-98ec3418e332","type":"message","text":"The speedups probably won't come from 1.6, but rather through multithreading in grouping and combining","user":"UBF9YRB6H","ts":"1615945131.001800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mI4p","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The speedups probably won't come from 1.6, but rather through multithreading in grouping and combining"}]}]}],"reactions":[{"name":"+1::skin-tone-5","users":["U6QGE7S86"],"count":1}]},{"client_msg_id":"d7c0ac02-fff0-4745-9304-ec4e1d481d7c","type":"message","text":"Ah, also found <https://bkamins.github.io/julialang/2021/03/05/pooledarrays.html>.","user":"U6QGE7S86","ts":"1615945175.002100","team":"T68168MUP","attachments":[{"service_name":"Blog by Bogumił Kamiński","title":"What is new in PooledArrays.jl?","title_link":"https://bkamins.github.io/julialang/2021/03/05/pooledarrays.html","text":"Introduction","fallback":"Blog by Bogumił Kamiński: What is new in PooledArrays.jl?","ts":1614941097,"from_url":"https://bkamins.github.io/julialang/2021/03/05/pooledarrays.html","service_icon":"https://bkamins.github.io/favicon.ico","id":1,"original_url":"https://bkamins.github.io/julialang/2021/03/05/pooledarrays.html"}],"blocks":[{"type":"rich_text","block_id":"SBd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah, also found "},{"type":"link","url":"https://bkamins.github.io/julialang/2021/03/05/pooledarrays.html"},{"type":"text","text":"."}]}]}]}]}