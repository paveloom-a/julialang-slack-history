{"cursor": 0, "messages": [{"type":"message","subtype":"channel_join","ts":"1612997896.000200","user":"ULL3KSGBS","text":"<@ULL3KSGBS> has joined the channel"},{"type":"message","subtype":"channel_purpose","ts":"1612997896.000300","user":"ULL3KSGBS","text":"<@ULL3KSGBS> set the channel purpose: ECP project ExaSGD","purpose":"ECP project ExaSGD"},{"type":"message","subtype":"channel_join","ts":"1612997915.000500","user":"UN2U72Q3F","text":"<@UN2U72Q3F> has joined the channel","inviter":"ULL3KSGBS"},{"type":"message","subtype":"channel_join","ts":"1612997915.000700","user":"U67BJLYCS","text":"<@U67BJLYCS> has joined the channel","inviter":"ULL3KSGBS"},{"type":"message","subtype":"channel_join","ts":"1612997916.000900","user":"U6A0PD8CR","text":"<@U6A0PD8CR> has joined the channel","inviter":"ULL3KSGBS"},{"client_msg_id":"2c29382a-ec89-489c-8023-efae5a7228da","type":"message","text":"Hi <@U67BJLYCS>! We have two construction sites at the moment. 1) Porting to AMDGPU.jl and 2) Moving to second-order Hessians. We use KernelAbstractions in ExaPF together with ForwardDiff and moved to Hessians using AD now. What we observe is that a small test case uses a lot of compile time with KA:\n `julia --project --optimize=0 --check-bounds=no test/Polar/hessian.jl  157.00s user 3.38s system 101% cpu 2:38.77 total`\nvs\n `julia --project --optimize=0 --check-bounds=no test/Polar/hessian.jl  57.49s user 3.22s system 102% cpu 59.058 total`\nwithout KA on the CPU. The difference is all only compile time. The actual kernel is computationally extremely small. A quick `@profile` shows a scary number of Cassette potholes. Francois was spending quite some time on making our code less inference heavy. So we are aware that we may just be doing something wrong. Do you have any idea? Basically what makes it blow up is that we are using a float type of\n`t1s{N,V} = ForwardDiff.Dual{Nothing,V, N} where {N,V}` and go second order with `t2s{M,N,V} =  ForwardDiff.Dual{Nothing,t1s{N,V}, M} where {M,N,V}` . `M` and `N` grow with larger problems. I fear that some expressions may just explode in size.","user":"ULL3KSGBS","ts":"1612998631.007900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"L2l","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":"! We have two construction sites at the moment. 1) Porting to AMDGPU.jl and 2) Moving to second-order Hessians. We use KernelAbstractions in ExaPF together with ForwardDiff and moved to Hessians using AD now. What we observe is that a small test case uses a lot of compile time with KA:\n "},{"type":"text","text":"julia --project --optimize=0 --check-bounds=no test/Polar/hessian.jl  157.00s user 3.38s system 101% cpu 2:38.77 total","style":{"code":true}},{"type":"text","text":"\nvs\n "},{"type":"text","text":"julia --project --optimize=0 --check-bounds=no test/Polar/hessian.jl  57.49s user 3.22s system 102% cpu 59.058 total","style":{"code":true}},{"type":"text","text":"\nwithout KA on the CPU. The difference is all only compile time. The actual kernel is computationally extremely small. A quick "},{"type":"text","text":"@profile","style":{"code":true}},{"type":"text","text":" shows a scary number of Cassette potholes. Francois was spending quite some time on making our code less inference heavy. So we are aware that we may just be doing something wrong. Do you have any idea? Basically what makes it blow up is that we are using a float type of\n"},{"type":"text","text":"t1s{N,V} = ForwardDiff.Dual{Nothing,V, N} where {N,V}","style":{"code":true}},{"type":"text","text":" and go second order with "},{"type":"text","text":"t2s{M,N,V} =  ForwardDiff.Dual{Nothing,t1s{N,V}, M} where {M,N,V}","style":{"code":true}},{"type":"text","text":" . "},{"type":"text","text":"M","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"N","style":{"code":true}},{"type":"text","text":" grow with larger problems. I fear that some expressions may just explode in size."}]}]}]},{"client_msg_id":"41f28b9d-73f4-45e3-aaf2-913a7b7c4b12","type":"message","text":"Hi!","user":"U67BJLYCS","ts":"1613000665.008100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wg4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi!"}]}]}]},{"client_msg_id":"add8c510-e387-4672-9dc9-c339cfeb31cf","type":"message","text":"I can't say that I am surprised","user":"U67BJLYCS","ts":"1613000721.008400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nEvJi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I can't say that I am surprised"}]}]}]},{"client_msg_id":"7ea3d9a7-2497-45db-b927-01b49e79c812","type":"message","text":"using Cassette means that we hit an empty inference cache for everything","user":"U67BJLYCS","ts":"1613000747.009000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"esz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"using Cassette means that we hit an empty inference cache for everything"}]}]}]},{"client_msg_id":"49ec9328-75a0-4bc2-9378-6f2ad113f5f3","type":"message","text":"Can you remind me where the code lives?","user":"U67BJLYCS","ts":"1613000777.009400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QHJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can you remind me where the code lives?"}]}]}]},{"client_msg_id":"627653f3-fe43-4e80-b595-ece08115a230","type":"message","text":"This is the full code. Though, we should try to build a MWE. Let me know if you'd be interested in that. It's the main power flow kernel:\n <https://github.com/exanauts/ExaPF.jl/blob/ab577acb9ddbb470b3d9fe606423a168028df6a2/src/polar/kernels.jl#L65-L96>\n\nAnd it's called from here with second-order types:\n<https://github.com/exanauts/ExaPF.jl/blob/ab577acb9ddbb470b3d9fe606423a168028df6a2/src/autodiff.jl#L583-L590>\n\nThere's a bit reordering going on due the application background before calling the kernel. This is implemented through views. Again, once compiled, the code flies. So that's good. If you think there's a way to fix it we'll invest some time in a MWE and try to isolate it there.","user":"ULL3KSGBS","ts":"1613056192.002500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jQn=d","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This is the full code. Though, we should try to build a MWE. Let me know if you'd be interested in that. It's the main power flow kernel:\n "},{"type":"link","url":"https://github.com/exanauts/ExaPF.jl/blob/ab577acb9ddbb470b3d9fe606423a168028df6a2/src/polar/kernels.jl#L65-L96"},{"type":"text","text":"\n\nAnd it's called from here with second-order types:\n"},{"type":"link","url":"https://github.com/exanauts/ExaPF.jl/blob/ab577acb9ddbb470b3d9fe606423a168028df6a2/src/autodiff.jl#L583-L590"},{"type":"text","text":"\n\nThere's a bit reordering going on due the application background before calling the kernel. This is implemented through views. Again, once compiled, the code flies. So that's good. If you think there's a way to fix it we'll invest some time in a MWE and try to isolate it there."}]}]}]},{"client_msg_id":"2c08bae1-f5af-4da9-af4b-e225abdf60d4","type":"message","text":"<@U67BJLYCS> Sorry I missed your message here in the Julia slack. I have still to organize my notifications here.","user":"ULL3KSGBS","ts":"1613056947.003200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+cmSS","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":" Sorry I missed your message here in the Julia slack. I have still to organize my notifications here."}]}]}]},{"client_msg_id":"ef1d07dd-c39a-4431-8ff2-998f43835c0d","type":"message","text":"Valentin, forget about it. I've just today wrote a handwritten adjoint and we are doing forward over reverse now. It's fast, and once Enzyme digests GPU code, we can just flip the switch.","user":"ULL3KSGBS","ts":"1613102362.004100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8sbJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Valentin, forget about it. I've just today wrote a handwritten adjoint and we are doing forward over reverse now. It's fast, and once Enzyme digests GPU code, we can just flip the switch."}]}]}]}]}