{"cursor": 0, "messages": [{"type":"message","subtype":"channel_join","ts":"1612997896.000200","user":"ULL3KSGBS","text":"<@ULL3KSGBS> has joined the channel"},{"type":"message","subtype":"channel_purpose","ts":"1612997896.000300","user":"ULL3KSGBS","text":"<@ULL3KSGBS> set the channel purpose: ECP project ExaSGD","purpose":"ECP project ExaSGD"},{"type":"message","subtype":"channel_join","ts":"1612997915.000500","user":"UN2U72Q3F","text":"<@UN2U72Q3F> has joined the channel","inviter":"ULL3KSGBS"},{"type":"message","subtype":"channel_join","ts":"1612997915.000700","user":"U67BJLYCS","text":"<@U67BJLYCS> has joined the channel","inviter":"ULL3KSGBS"},{"type":"message","subtype":"channel_join","ts":"1612997916.000900","user":"U6A0PD8CR","text":"<@U6A0PD8CR> has joined the channel","inviter":"ULL3KSGBS"},{"client_msg_id":"2c29382a-ec89-489c-8023-efae5a7228da","type":"message","text":"Hi <@U67BJLYCS>! We have two construction sites at the moment. 1) Porting to AMDGPU.jl and 2) Moving to second-order Hessians. We use KernelAbstractions in ExaPF together with ForwardDiff and moved to Hessians using AD now. What we observe is that a small test case uses a lot of compile time with KA:\n `julia --project --optimize=0 --check-bounds=no test/Polar/hessian.jl  157.00s user 3.38s system 101% cpu 2:38.77 total`\nvs\n `julia --project --optimize=0 --check-bounds=no test/Polar/hessian.jl  57.49s user 3.22s system 102% cpu 59.058 total`\nwithout KA on the CPU. The difference is all only compile time. The actual kernel is computationally extremely small. A quick `@profile` shows a scary number of Cassette potholes. Francois was spending quite some time on making our code less inference heavy. So we are aware that we may just be doing something wrong. Do you have any idea? Basically what makes it blow up is that we are using a float type of\n`t1s{N,V} = ForwardDiff.Dual{Nothing,V, N} where {N,V}` and go second order with `t2s{M,N,V} =  ForwardDiff.Dual{Nothing,t1s{N,V}, M} where {M,N,V}` . `M` and `N` grow with larger problems. I fear that some expressions may just explode in size.","user":"ULL3KSGBS","ts":"1612998631.007900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"L2l","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":"! We have two construction sites at the moment. 1) Porting to AMDGPU.jl and 2) Moving to second-order Hessians. We use KernelAbstractions in ExaPF together with ForwardDiff and moved to Hessians using AD now. What we observe is that a small test case uses a lot of compile time with KA:\n "},{"type":"text","text":"julia --project --optimize=0 --check-bounds=no test/Polar/hessian.jl  157.00s user 3.38s system 101% cpu 2:38.77 total","style":{"code":true}},{"type":"text","text":"\nvs\n "},{"type":"text","text":"julia --project --optimize=0 --check-bounds=no test/Polar/hessian.jl  57.49s user 3.22s system 102% cpu 59.058 total","style":{"code":true}},{"type":"text","text":"\nwithout KA on the CPU. The difference is all only compile time. The actual kernel is computationally extremely small. A quick "},{"type":"text","text":"@profile","style":{"code":true}},{"type":"text","text":" shows a scary number of Cassette potholes. Francois was spending quite some time on making our code less inference heavy. So we are aware that we may just be doing something wrong. Do you have any idea? Basically what makes it blow up is that we are using a float type of\n"},{"type":"text","text":"t1s{N,V} = ForwardDiff.Dual{Nothing,V, N} where {N,V}","style":{"code":true}},{"type":"text","text":" and go second order with "},{"type":"text","text":"t2s{M,N,V} =  ForwardDiff.Dual{Nothing,t1s{N,V}, M} where {M,N,V}","style":{"code":true}},{"type":"text","text":" . "},{"type":"text","text":"M","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"N","style":{"code":true}},{"type":"text","text":" grow with larger problems. I fear that some expressions may just explode in size."}]}]}]}]}