{"cursor": 1, "messages": [{"client_msg_id":"25d32f7d-905a-4dc4-8afd-71aef844ce6c","type":"message","text":"I know they are sometimes called various things so not sure if I am just searching for the wrong thing in Distributions docs etc.","user":"UMS7H0ASG","ts":"1611753424.002900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"i0cP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I know they are sometimes called various things so not sure if I am just searching for the wrong thing in Distributions docs etc."}]}]}]},{"client_msg_id":"db0b43dc-75d9-4f27-8e2c-730d4ac7d1f6","type":"message","text":"What's the problem here?\n","user":"U01ARRMLM7E","ts":"1611785736.004000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j4f","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What's the problem here?\n"}]}]}]},{"client_msg_id":"db0b43dc-75d9-4f27-8e2c-730d4ac7d1f6","type":"message","text":"```using Distributions\nusing Turing\nusing Random\nusing MCMCChains\n\n\n\nrng = Random.MersenneTwister(1)\n\n\n@model function modelfn(y)\n    y .~ Normal(0,1)\nend\n\n\nxs = rand(rng, Normal(5, 3), 1000)\n\nmod = modelfn(xs)\n\nsample(mod, NUTS(), 1000)\n\n \nERROR: LoadError: MethodError: no method matching phasepoint(::Random._GLOBAL_RNG, ::Array{Any,1}, ::AdvancedHMC.Hamiltonian{AdvancedHMC.DiagEuclideanMetric{Float64,Array{Float64,1}},Turing.Inference.var\"#logπ#52\"{DynamicPPL.VarInfo{NamedTuple{(),Tuple{}},Float64},DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}},Turing.Inference.var\"#∂logπ∂θ#51\"{DynamicPPL.VarInfo{NamedTuple{(),Tuple{}},Float64},DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}}})\nClosest candidates are:\n  phasepoint(::Union{AbstractRNG, AbstractArray{var\"#s46\",1} where var\"#s46\"&lt;:AbstractRNG}, ::Union{AbstractArray{T,1}, AbstractArray{T,2}}, ::AdvancedHMC.Hamiltonian) where T&lt;:Real at \n  phasepoint(::AdvancedHMC.Hamiltonian, ::T, ::T; ℓπ, ℓκ) where T&lt;:(Union{AbstractArray{T,1}, AbstractArray{T,2}} where T) at \n  phasepoint(::AdvancedHMC.Hamiltonian, ::T1, ::T2; r, ℓπ, ℓκ) where {T1&lt;:(Union{AbstractArray{T,1}, AbstractArray{T,2}} where T), T2&lt;:(Union{AbstractArray{T,1}, AbstractArray{T,2}} where T)} at \nStacktrace:\n [1] initialstep(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}, ::DynamicPPL.VarInfo{NamedTuple{(),Tuple{}},Float64}; init_params::Nothing, nadapts::Int64, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:nchains,),Tuple{Int64}}}) at \n [2] step(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}; resume_from::Nothing, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:nadapts, :nchains),Tuple{Int64,Int64}}}) at \n [3] macro expansion at AbstractMCMC/Nw3Wn/src/sample.jl:78 [inlined]\n [4] macro expansion at ProgressLogging/BBN0b/src/ProgressLogging.jl:328 [inlined]\n [5] macro expansion at AbstractMCMC/Nw3Wn/src/logging.jl:8 [inlined]\n [6] mcmcsample(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}, ::Int64; progress::Bool, progressname::String, callback::Nothing, discard_initial::Int64, thinning::Int64, chain_type::Type{T} where T, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:nadapts, :nchains),Tuple{Int64,Int64}}}) at /AbstractMCMC/Nw3Wn/src/sample.jl:76\n [7] sample(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}, ::Int64; chain_type::Type{T} where T, resume_from::Nothing, progress::Bool, nadapts::Int64, discard_adapt::Bool, discard_initial::Int64, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:nchains,),Tuple{Int64}}}) at /home/user/src/project/julia/packages/Turing/O1Pn0/src/inference/hmc.jl:140\n [8] #sample#2 at /home/user/src/project/julia/packages/Turing/O1Pn0/src/inference/Inference.jl:143 [inlined]\n [9] #sample#1 at /home/user/src/project/julia/packages/Turing/O1Pn0/src/inference/Inference.jl:133 [inlined]\n [10] top-level scope at /home/user/src/project/src/tools/DistributionFinder.jl:20\n [11] include_string(::Function, ::Module, ::String, ::String) at /nix/store/l899vbb7z6w01xl2mkn7xzc9s8bx2msp-julia-1.5.2/lib/julia/sys.so:?\n [12] invokelatest(::Any, ::Any, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at ./essentials.jl:710\n [13] invokelatest(::Any, ::Any, ::Vararg{Any,N} where N) at ./essentials.jl:709\n [14] inlineeval(::Module, ::String, ::Int64, ::Int64, ::String; softscope::Bool) at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:185\n [15] (::VSCodeServer.var\"#61#65\"{String,Int64,Int64,String,Module,Bool,VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:144\n [16] withpath(::VSCodeServer.var\"#61#65\"{String,Int64,Int64,String,Module,Bool,VSCodeServer.ReplRunCodeRequestParams}, ::String) at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/repl.jl:124\n [17] (::VSCodeServer.var\"#60#64\"{String,Int64,Int64,String,Module,Bool,Bool,VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:142\n [18] hideprompt(::VSCodeServer.var\"#60#64\"{String,Int64,Int64,String,Module,Bool,Bool,VSCodeServer.ReplRunCodeRequestParams}) at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/repl.jl:36\n [19] (::VSCodeServer.var\"#59#63\"{String,Int64,Int64,String,Module,Bool,Bool,VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:110\n [20] with_logstate(::Function, ::Any) at /nix/store/l899vbb7z6w01xl2mkn7xzc9s8bx2msp-julia-1.5.2/lib/julia/sys.so:?\n [21] with_logger at ./logging.jl:514 [inlined]\n [22] (::VSCodeServer.var\"#58#62\"{VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:109\n [23] #invokelatest#1 at ./essentials.jl:710 [inlined]\n [24] invokelatest(::Any) at ./essentials.jl:709\n [25] macro expansion at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:27 [inlined]\n [26] (::VSCodeServer.var\"#56#57\")() at ./task.jl:356\nin expression starting at /home/user/src/project/src/tools/```","user":"U01ARRMLM7E","ts":"1611785736.004100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j4f-I=/G","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Distributions\nusing Turing\nusing Random\nusing MCMCChains\n\n\n\nrng = Random.MersenneTwister(1)\n\n\n@model function modelfn(y)\n    y .~ Normal(0,1)\nend\n\n\nxs = rand(rng, Normal(5, 3), 1000)\n\nmod = modelfn(xs)\n\nsample(mod, NUTS(), 1000)\n\n \nERROR: LoadError: MethodError: no method matching phasepoint(::Random._GLOBAL_RNG, ::Array{Any,1}, ::AdvancedHMC.Hamiltonian{AdvancedHMC.DiagEuclideanMetric{Float64,Array{Float64,1}},Turing.Inference.var\"#logπ#52\"{DynamicPPL.VarInfo{NamedTuple{(),Tuple{}},Float64},DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}},Turing.Inference.var\"#∂logπ∂θ#51\"{DynamicPPL.VarInfo{NamedTuple{(),Tuple{}},Float64},DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}}})\nClosest candidates are:\n  phasepoint(::Union{AbstractRNG, AbstractArray{var\"#s46\",1} where var\"#s46\"<:AbstractRNG}, ::Union{AbstractArray{T,1}, AbstractArray{T,2}}, ::AdvancedHMC.Hamiltonian) where T<:Real at \n  phasepoint(::AdvancedHMC.Hamiltonian, ::T, ::T; ℓπ, ℓκ) where T<:(Union{AbstractArray{T,1}, AbstractArray{T,2}} where T) at \n  phasepoint(::AdvancedHMC.Hamiltonian, ::T1, ::T2; r, ℓπ, ℓκ) where {T1<:(Union{AbstractArray{T,1}, AbstractArray{T,2}} where T), T2<:(Union{AbstractArray{T,1}, AbstractArray{T,2}} where T)} at \nStacktrace:\n [1] initialstep(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}, ::DynamicPPL.VarInfo{NamedTuple{(),Tuple{}},Float64}; init_params::Nothing, nadapts::Int64, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:nchains,),Tuple{Int64}}}) at \n [2] step(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}; resume_from::Nothing, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:nadapts, :nchains),Tuple{Int64,Int64}}}) at \n [3] macro expansion at AbstractMCMC/Nw3Wn/src/sample.jl:78 [inlined]\n [4] macro expansion at ProgressLogging/BBN0b/src/ProgressLogging.jl:328 [inlined]\n [5] macro expansion at AbstractMCMC/Nw3Wn/src/logging.jl:8 [inlined]\n [6] mcmcsample(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}, ::Int64; progress::Bool, progressname::String, callback::Nothing, discard_initial::Int64, thinning::Int64, chain_type::Type{T} where T, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:nadapts, :nchains),Tuple{Int64,Int64}}}) at /AbstractMCMC/Nw3Wn/src/sample.jl:76\n [7] sample(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}, ::Int64; chain_type::Type{T} where T, resume_from::Nothing, progress::Bool, nadapts::Int64, discard_adapt::Bool, discard_initial::Int64, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:nchains,),Tuple{Int64}}}) at /home/user/src/project/julia/packages/Turing/O1Pn0/src/inference/hmc.jl:140\n [8] #sample#2 at /home/user/src/project/julia/packages/Turing/O1Pn0/src/inference/Inference.jl:143 [inlined]\n [9] #sample#1 at /home/user/src/project/julia/packages/Turing/O1Pn0/src/inference/Inference.jl:133 [inlined]\n [10] top-level scope at /home/user/src/project/src/tools/DistributionFinder.jl:20\n [11] include_string(::Function, ::Module, ::String, ::String) at /nix/store/l899vbb7z6w01xl2mkn7xzc9s8bx2msp-julia-1.5.2/lib/julia/sys.so:?\n [12] invokelatest(::Any, ::Any, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at ./essentials.jl:710\n [13] invokelatest(::Any, ::Any, ::Vararg{Any,N} where N) at ./essentials.jl:709\n [14] inlineeval(::Module, ::String, ::Int64, ::Int64, ::String; softscope::Bool) at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:185\n [15] (::VSCodeServer.var\"#61#65\"{String,Int64,Int64,String,Module,Bool,VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:144\n [16] withpath(::VSCodeServer.var\"#61#65\"{String,Int64,Int64,String,Module,Bool,VSCodeServer.ReplRunCodeRequestParams}, ::String) at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/repl.jl:124\n [17] (::VSCodeServer.var\"#60#64\"{String,Int64,Int64,String,Module,Bool,Bool,VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:142\n [18] hideprompt(::VSCodeServer.var\"#60#64\"{String,Int64,Int64,String,Module,Bool,Bool,VSCodeServer.ReplRunCodeRequestParams}) at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/repl.jl:36\n [19] (::VSCodeServer.var\"#59#63\"{String,Int64,Int64,String,Module,Bool,Bool,VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:110\n [20] with_logstate(::Function, ::Any) at /nix/store/l899vbb7z6w01xl2mkn7xzc9s8bx2msp-julia-1.5.2/lib/julia/sys.so:?\n [21] with_logger at ./logging.jl:514 [inlined]\n [22] (::VSCodeServer.var\"#58#62\"{VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:109\n [23] #invokelatest#1 at ./essentials.jl:710 [inlined]\n [24] invokelatest(::Any) at ./essentials.jl:709\n [25] macro expansion at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:27 [inlined]\n [26] (::VSCodeServer.var\"#56#57\")() at ./task.jl:356\nin expression starting at /home/user/src/project/src/tools/"}]}]}]},{"client_msg_id":"a830c95b-6de3-49a1-bf77-3c3b06b5d9b5","type":"message","text":"Hey all I am having really slow sampling and I am not sure what it is, seems like a simple model. I am using VScode and the sampling is stuck at 0.0% for long time and then after several minutes (~10) the sampling suddently shoots to 100%, but maybe thats just a vscode problem with showing the progress. I am trying ReverseDiff now but it seems the same (if not slower). I tried `chain2 = sample(model2, NUTS(0.65), 10000)` but even if I try to just sample 100 samples it takes a long time. Here is model:\n\n```@model function m3(damage, plate, species, trt, n1, n2, n3, ::Type{T} = Float64) where {T}\n    mu0 ~ Normal(0,10)\n    trt_effects ~ filldist(Normal(0,10), n1)\n    species_effects ~ filldist(Normal(0,10), n2)\n    interaction_effects ~ filldist(Normal(0,10), n1*n2) \n    plate_effects ~ filldist(Normal(0,10), n3)\n    sigma ~ Gamma(1,1)\n    mu = Array{T,3}(undef, n1+1,n2+1,n3+1)\n    mu .= 0.0\n    for i in 1:(n1+1)\n        for j in 1:(n2+1)\n            for k in 1:(n3+1)\n                mu[i,j,k] += mu0\n                if i != 1\n                    mu[i,j,k] += trt_effects[i-1]\n                end\n                if j != 1\n                    mu[i,j,k] += species_effects[j-1]\n                end\n                if k != 1\n                    mu[i,j,k] += plate_effects[k-1]\n                end\n                if (i != 1) &amp;&amp; (j != 1)\n                    mu[i,j,k] += interaction_effects[i-1 + n1*(j-2)]\n                end\n            end\n        end\n    end\n    residuals = Array{T,1}(undef, length(damage))\n    residuals .= 0.0\n\n    for i in 1:length(damage)\n        residuals[i] = damage[i] - mu[trt[i],species[i],plate[i]]\n        damage[i] ~ Normal(mu[trt[i],species[i],plate[i]],sigma)\n    end\n    return mu, residuals\nend```","user":"U6CFMFM2R","ts":"1611940926.006300","team":"T68168MUP","edited":{"user":"U6CFMFM2R","ts":"1611941634.000000"},"blocks":[{"type":"rich_text","block_id":"G4Yc9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey all I am having really slow sampling and I am not sure what it is, seems like a simple model. I am using VScode and the sampling is stuck at 0.0% for long time and then after several minutes (~10) the sampling suddently shoots to 100%, but maybe thats just a vscode problem with showing the progress. I am trying ReverseDiff now but it seems the same (if not slower). I tried "},{"type":"text","text":"chain2 = sample(model2, NUTS(0.65), 10000)","style":{"code":true}},{"type":"text","text":" but even if I try to just sample 100 samples it takes a long time. Here is model:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@model function m3(damage, plate, species, trt, n1, n2, n3, ::Type{T} = Float64) where {T}\n    mu0 ~ Normal(0,10)\n    trt_effects ~ filldist(Normal(0,10), n1)\n    species_effects ~ filldist(Normal(0,10), n2)\n    interaction_effects ~ filldist(Normal(0,10), n1*n2) \n    plate_effects ~ filldist(Normal(0,10), n3)\n    sigma ~ Gamma(1,1)\n    mu = Array{T,3}(undef, n1+1,n2+1,n3+1)\n    mu .= 0.0\n    for i in 1:(n1+1)\n        for j in 1:(n2+1)\n            for k in 1:(n3+1)\n                mu[i,j,k] += mu0\n                if i != 1\n                    mu[i,j,k] += trt_effects[i-1]\n                end\n                if j != 1\n                    mu[i,j,k] += species_effects[j-1]\n                end\n                if k != 1\n                    mu[i,j,k] += plate_effects[k-1]\n                end\n                if (i != 1) && (j != 1)\n                    mu[i,j,k] += interaction_effects[i-1 + n1*(j-2)]\n                end\n            end\n        end\n    end\n    residuals = Array{T,1}(undef, length(damage))\n    residuals .= 0.0\n\n    for i in 1:length(damage)\n        residuals[i] = damage[i] - mu[trt[i],species[i],plate[i]]\n        damage[i] ~ Normal(mu[trt[i],species[i],plate[i]],sigma)\n    end\n    return mu, residuals\nend"}]}]}],"thread_ts":"1611940926.006300","reply_count":3,"reply_users_count":2,"latest_reply":"1611952475.018100","reply_users":["U8T9JUA5R","U6CFMFM2R"],"subscribed":false},{"client_msg_id":"c8082a4f-2c23-4ef6-a19e-2e1b1be65829","type":"message","text":"with tape compilation?","user":"U69BL50BF","ts":"1611941089.006500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+mLa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"with tape compilation?"}]}]}],"thread_ts":"1611941089.006500","reply_count":1,"reply_users_count":1,"latest_reply":"1611941452.007200","reply_users":["U6CFMFM2R"],"subscribed":false},{"client_msg_id":"d4672b15-01be-4925-ae05-4025ab09dfe6","type":"message","text":"I also can't seem to kill the sampling with `CTRL+C` or `ESC`","user":"U6CFMFM2R","ts":"1611941934.008000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"C1he","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I also can't seem to kill the sampling with "},{"type":"text","text":"CTRL+C","style":{"code":true}},{"type":"text","text":" or "},{"type":"text","text":"ESC","style":{"code":true}}]}]}]},{"client_msg_id":"1b72a136-0eab-470e-b5a8-7f8ff6e188b7","type":"message","text":"what's the scale of n1,n2,n3?","user":"U01H36BUDJB","ts":"1611943191.008200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lIBB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"what's the scale of n1,n2,n3?"}]}]}],"thread_ts":"1611943191.008200","reply_count":5,"reply_users_count":2,"latest_reply":"1611953539.024500","reply_users":["U6CFMFM2R","U01H36BUDJB"],"subscribed":false},{"client_msg_id":"0835c6dc-2742-4e73-a5e4-f0fced3cdcae","type":"message","text":"maybe also do `@macroexpand` before `@model` and post the output as a reply here","user":"U01H36BUDJB","ts":"1611943529.010400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LTpzi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"maybe also do "},{"type":"text","text":"@macroexpand","style":{"code":true}},{"type":"text","text":" before "},{"type":"text","text":"@model","style":{"code":true}},{"type":"text","text":" and post the output as a reply here"}]}]}],"thread_ts":"1611943529.010400","reply_count":1,"reply_users_count":1,"latest_reply":"1611947119.017100","reply_users":["U6CFMFM2R"],"subscribed":false},{"client_msg_id":"8fbfb201-73a1-488c-ad62-2602437406b3","type":"message","text":"might also be worth trying with Metropolis-Hastings just to check whether or not its the gradient computations that are the problem","user":"U01H36BUDJB","ts":"1611943635.011200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"R2IX6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"might also be worth trying with Metropolis-Hastings just to check whether or not its the gradient computations that are the problem"}]}]}]},{"client_msg_id":"9dec6fe7-ec23-480c-9a9a-a783c6066295","type":"message","text":"maybe the conditionals are the problem? <@U69BL50BF> I thought that the DiffEqFlux docs said tape compilation didn't work in this case.","user":"U01H36BUDJB","ts":"1611943862.012500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"01B2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"maybe the conditionals are the problem? "},{"type":"user","user_id":"U69BL50BF"},{"type":"text","text":" I thought that the DiffEqFlux docs said tape compilation didn't work in this case."}]}]}]},{"client_msg_id":"c0955daa-7a73-4c4e-afcf-5d7d5ccd3425","type":"message","text":"I know it will break your model definition, but maybe try stripping out the conditionals and see if it improves the speed.","user":"U01H36BUDJB","ts":"1611943942.013300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5W2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I know it will break your model definition, but maybe try stripping out the conditionals and see if it improves the speed."}]}]}]},{"client_msg_id":"59d3d938-a7c8-4338-be5f-71b54f8275d7","type":"message","text":"The most frustrating thing is I can't kill the sampling. So when I want to change something and try again to see if it \"fixes it\" I have to kill julia, rerun all my code. This is making the try-test-change loop very difficult","user":"U6CFMFM2R","ts":"1611945382.014100","team":"T68168MUP","edited":{"user":"U6CFMFM2R","ts":"1611945397.000000"},"blocks":[{"type":"rich_text","block_id":"s+rsr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The most frustrating thing is I can't kill the sampling. So when I want to change something and try again to see if it \"fixes it\" I have to kill julia, rerun all my code. This is making the try-test-change loop very difficult"}]}]}]},{"client_msg_id":"25ce0c51-e944-48d7-a4d1-59070471a47a","type":"message","text":"It seems to kill just 1 iteration, and it goes on to the next one","user":"U6CFMFM2R","ts":"1611945423.014500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TYt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It seems to kill just 1 iteration, and it goes on to the next one"}]}]}]},{"client_msg_id":"9794CD9A-9CC9-4FBA-9C05-E9BF8A431FEB","type":"message","text":"Could you set the max iterations to 2 or something like that?","user":"U7THT3TM3","ts":"1611945541.015100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"CS31","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Could you set the max iterations to 2 or something like that?"}]}]}]},{"client_msg_id":"21deb547-b065-43db-8f98-3cd13bf02b7b","type":"message","text":"&gt; maybe the conditionals are the problem? <@U69BL50BF> I thought that the DiffEqFlux docs said tape compilation didn't work in this case.\nThese conditions are not value dependent","user":"U69BL50BF","ts":"1611946047.015500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/L6Se","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"maybe the conditionals are the problem? "},{"type":"user","user_id":"U69BL50BF"},{"type":"text","text":" I thought that the DiffEqFlux docs said tape compilation didn't work in this case."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"These conditions are not value dependent"}]}]}]},{"client_msg_id":"fd941454-da63-4a28-820c-1e9724609423","type":"message","text":"it still represents a static graph","user":"U69BL50BF","ts":"1611946053.015800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"NDJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it still represents a static graph"}]}]}]},{"client_msg_id":"6689b9c0-0e2f-4c5a-b294-a3c71bc0994b","type":"message","text":"Another way of saying it is, for these conditions, the order of true and false is independent of what values you give to the function, so their truth values can all be determined at compile time","user":"U69BL50BF","ts":"1611946104.016500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"I3Bm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Another way of saying it is, for these conditions, the order of true and false is independent of what values you give to the function, so their truth values can all be determined at compile time"}]}]}],"thread_ts":"1611946104.016500","reply_count":1,"reply_users_count":1,"latest_reply":"1611952688.018800","reply_users":["U01H36BUDJB"],"subscribed":false},{"client_msg_id":"c079921d-d777-43e7-a682-a724ea56dd6a","type":"message","text":"that would work with tape compilation.","user":"U69BL50BF","ts":"1611946109.016700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OSlu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"that would work with tape compilation."}]}]}]},{"client_msg_id":"3e41759b-ff7d-4005-9614-f71a0d5a6859","type":"message","text":".35 secs for 10 iterations, 130sec for 100 iterations","user":"U6CFMFM2R","ts":"1611946673.017000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"T18KZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":".35 secs for 10 iterations, 130sec for 100 iterations"}]}]}],"thread_ts":"1611946673.017000","reply_count":4,"reply_users_count":2,"latest_reply":"1611953255.024300","reply_users":["U01H36BUDJB","U6CFMFM2R"],"subscribed":false},{"client_msg_id":"9248f9b1-da21-4338-ac2e-c21edba74831","type":"message","text":"12 secs for 10_000 iterations with MH()","user":"U6CFMFM2R","ts":"1611952311.018000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lRXq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"12 secs for 10_000 iterations with MH()"}]}]}],"thread_ts":"1611952311.018000","reply_count":1,"reply_users_count":1,"latest_reply":"1611952788.019000","reply_users":["U01H36BUDJB"],"subscribed":false},{"client_msg_id":"6e61aac2-cddd-4a9e-a0f1-016404f13884","type":"message","text":"I guess Zygote wouldn't work with this model as written, but you could try Tracker. Beyond that, sounds like an issue needs to be created. The question is whether it's a Turing or ReverseDiff problem.","user":"U01H36BUDJB","ts":"1611952883.021800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7Bi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I guess Zygote wouldn't work with this model as written, but you could try Tracker. Beyond that, sounds like an issue needs to be created. The question is whether it's a Turing or ReverseDiff problem."}]}]}]},{"client_msg_id":"68491fd4-9d9a-4e19-9f99-d6b4fc4a7760","type":"message","text":"whats an easy way to make a MWE for people? The data has like ~800 rows","user":"U6CFMFM2R","ts":"1611952940.022500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MYj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"whats an easy way to make a MWE for people? The data has like ~800 rows"}]}]}],"thread_ts":"1611952940.022500","reply_count":1,"reply_users_count":1,"latest_reply":"1611954811.025300","reply_users":["U01H36BUDJB"],"subscribed":false},{"client_msg_id":"b4a633b3-c6cc-4d70-b01e-0d6820589b9b","type":"message","text":"Did you try removing the conditionals yet? I know it *shouldn't* matter... but still.","user":"U01H36BUDJB","ts":"1611953197.024200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hhO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Did you try removing the conditionals yet? I know it "},{"type":"text","text":"shouldn't","style":{"bold":true}},{"type":"text","text":" matter... but still."}]}]}],"thread_ts":"1611953197.024200","reply_count":12,"reply_users_count":3,"latest_reply":"1611958695.030700","reply_users":["U6CFMFM2R","U01H36BUDJB","U69BL50BF"],"subscribed":false},{"client_msg_id":"2930b2a8-4006-460e-8b77-f972be51342e","type":"message","text":"The loop variable order also should be reversed for column-major, but this doesn't seem to affect anything in my quick experiments. I suppose Julia's compiler is probably smart enough to do this automatically.","user":"U01H36BUDJB","ts":"1611957921.027700","team":"T68168MUP","edited":{"user":"U01H36BUDJB","ts":"1611957941.000000"},"blocks":[{"type":"rich_text","block_id":"4Gq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The loop variable order also should be reversed for column-major, but this doesn't seem to affect anything in my quick experiments. I suppose Julia's compiler is probably smart enough to do this automatically."}]}]}]},{"client_msg_id":"005c85a9-e567-4f02-b7df-6ed8217d805a","type":"message","text":"the cost of AD is probably just more than that.","user":"U69BL50BF","ts":"1611957944.028100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Kuk7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"the cost of AD is probably just more than that."}]}]}]},{"client_msg_id":"708116ee-342b-4c05-9e05-407a925dfa94","type":"message","text":"yeah but I tested it on MH just to see if it did anything","user":"U01H36BUDJB","ts":"1611957957.028500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"chHL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yeah but I tested it on MH just to see if it did anything"}]}]}]},{"client_msg_id":"684f2ea5-b270-4833-89f9-df57dc2884aa","type":"message","text":"reverse mode AD doesn't play by the rules.","user":"U69BL50BF","ts":"1611957958.028700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GL1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"reverse mode AD doesn't play by the rules."}]}]}]},{"client_msg_id":"5d88fa16-18d7-4746-9ee4-5a052b0fd5bc","type":"message","text":"oh okay","user":"U69BL50BF","ts":"1611957963.028900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hXvKk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"oh okay"}]}]}]},{"client_msg_id":"65c96dab-c6e3-4ddb-b553-04811c7f8579","type":"message","text":"it doesn't reverse it, but sometimes you can still SIMD with gaps as of LLVM X (I forget the version but it's quite recent and impacted the ForwardDiff2.jl design)","user":"U69BL50BF","ts":"1611957995.029600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j97Yy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it doesn't reverse it, but sometimes you can still SIMD with gaps as of LLVM X (I forget the version but it's quite recent and impacted the ForwardDiff2.jl design)"}]}]}]},{"client_msg_id":"652dbe54-06b2-405d-b910-cefe0a44e05b","type":"message","text":"oh interesting","user":"U01H36BUDJB","ts":"1611958022.029800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"y4K1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"oh interesting"}]}]}]},{"client_msg_id":"2197c8ea-0447-4978-846c-0c16c166a618","type":"message","text":"Hello, I want to transform prior distribution for downstream use.\n\n```@model function gdemo()\n    sigma ~ Exponential(2)\n    sigmaTransformed = sigma + 1\nend\n\nchn = sample(gdemo(), HMC(0.05, 10), 10)```\n1. What is the Turing way of transforming prior `sigma` to `sigmaTransformed`? Is my code correct or should I be looking into Bijectors.jl?\n2. What is the Turing way of returning `sigmaTransformed` for example for inspecting the posterior? Should i be looking into `generated_quantities` ?\nThank you","user":"U011PPW7K53","ts":"1612171908.036000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iLbM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello, I want to transform prior distribution for downstream use.\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@model function gdemo()\n    sigma ~ Exponential(2)\n    sigmaTransformed = sigma + 1\nend\n\nchn = sample(gdemo(), HMC(0.05, 10), 10)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What is the Turing way of transforming prior "},{"type":"text","text":"sigma","style":{"code":true}},{"type":"text","text":" to "},{"type":"text","text":"sigmaTransformed","style":{"code":true}},{"type":"text","text":"? Is my code correct or should I be looking into Bijectors.jl?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"What is the Turing way of returning "},{"type":"text","text":"sigmaTransformed","style":{"code":true}},{"type":"text","text":" for example for inspecting the posterior? Should i be looking into "},{"type":"text","text":"generated_quantities","style":{"code":true}},{"type":"text","text":" ?"}]}],"style":"ordered","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"\nThank you"}]}]}]},{"client_msg_id":"a581ab5d-0aed-4ccf-9ae5-5e24f4469f40","type":"message","text":"`generated_quantities` is your friend. The model implementation looks correct.","user":"UC0SY9JFP","ts":"1612173376.036600","team":"T68168MUP","edited":{"user":"UC0SY9JFP","ts":"1612269340.000000"},"blocks":[{"type":"rich_text","block_id":"sqe","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"generated_quantities","style":{"code":true}},{"type":"text","text":" is your friend. The model implementation looks correct."}]}]}],"thread_ts":"1612173376.036600","reply_count":1,"reply_users_count":1,"latest_reply":"1612269316.037000","reply_users":["U011PPW7K53"],"subscribed":false,"reactions":[{"name":"+1","users":["U011PPW7K53"],"count":1}]},{"client_msg_id":"64311dc9-4a0a-4a94-9294-23be3950d7e4","type":"message","text":"Hey folks. I was trying the tutorial on Bayesian Differential equations in a Pluto notebook. I am getting an MCMCChains method error -- Failed to show value. Here is the code and the message. I am not sure how I should read the error, like what is the cause of the issue. Any suggestions are appreciated.\n```Turing.setadbackend(:forwarddiff)\n\n@model function fitlv(data, prob1)\n    σ ~ InverseGamma(2, 3) # ~ is the tilde character\n    α ~ truncated(Normal(1.5,0.5),0.5,2.5)\n    β ~ truncated(Normal(1.2,0.5),0,2)\n    γ ~ truncated(Normal(3.0,0.5),1,4)\n    δ ~ truncated(Normal(1.0,0.5),0,2)\n\n    p = [α,β,γ,δ]\n    prob = remake(prob1, p=p)\n    predicted = solve(prob,Tsit5(),saveat=0.1)\n\n    for i = 1:length(predicted)\n        data[:,i] ~ MvNormal(predicted[i], σ)\n    end\nend\n\nmodel = fitlv(odedata, prob1)\n\n# This next command runs 3 independent chains without using multithreading. \nchain = mapreduce(c -&gt; sample(model, NUTS(.65),1000), chainscat, 1:3)```\nHere is the message.\n```Failed to show value:\n\nMethodError: no method matching iterate(::MCMCChains.Chains{Float64,AxisArrays.AxisArray{Float64,3,Array{Float64,3},Tuple{AxisArrays.Axis{:iter,StepRange{Int64,Int64}},AxisArrays.Axis{:var,Array{Symbol,1}},AxisArrays.Axis{:chain,UnitRange{Int64}}}},Missing,NamedTuple{(:parameters, :internals),Tuple{Array{Symbol,1},Array{Symbol,1}}},NamedTuple{(),Tuple{}}})\n\nClosest candidates are:\n\niterate(!Matched::Base.RegexMatchIterator) at regex.jl:552\n\niterate(!Matched::Base.RegexMatchIterator, !Matched::Any) at regex.jl:552\n\niterate(!Matched::Libtask.TRef, !Matched::Any...) at /home/krishnab/.julia/packages/Libtask/00Il9/src/tref.jl:86\n\n...```","user":"UDDSTBX19","ts":"1612292883.038900","team":"T68168MUP","edited":{"user":"UDDSTBX19","ts":"1612293101.000000"},"blocks":[{"type":"rich_text","block_id":"niA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey folks. I was trying the tutorial on Bayesian Differential equations in a Pluto notebook. I am getting an MCMCChains method error -- Failed to show value. Here is the code and the message. I am not sure how I should read the error, like what is the cause of the issue. Any suggestions are appreciated.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Turing.setadbackend(:forwarddiff)\n\n@model function fitlv(data, prob1)\n    σ ~ InverseGamma(2, 3) # ~ is the tilde character\n    α ~ truncated(Normal(1.5,0.5),0.5,2.5)\n    β ~ truncated(Normal(1.2,0.5),0,2)\n    γ ~ truncated(Normal(3.0,0.5),1,4)\n    δ ~ truncated(Normal(1.0,0.5),0,2)\n\n    p = [α,β,γ,δ]\n    prob = remake(prob1, p=p)\n    predicted = solve(prob,Tsit5(),saveat=0.1)\n\n    for i = 1:length(predicted)\n        data[:,i] ~ MvNormal(predicted[i], σ)\n    end\nend\n\nmodel = fitlv(odedata, prob1)\n\n# This next command runs 3 independent chains without using multithreading. \nchain = mapreduce(c -> sample(model, NUTS(.65),1000), chainscat, 1:3)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nHere is the message.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Failed to show value:\n\nMethodError: no method matching iterate(::MCMCChains.Chains{Float64,AxisArrays.AxisArray{Float64,3,Array{Float64,3},Tuple{AxisArrays.Axis{:iter,StepRange{Int64,Int64}},AxisArrays.Axis{:var,Array{Symbol,1}},AxisArrays.Axis{:chain,UnitRange{Int64}}}},Missing,NamedTuple{(:parameters, :internals),Tuple{Array{Symbol,1},Array{Symbol,1}}},NamedTuple{(),Tuple{}}})\n\nClosest candidates are:\n\niterate(!Matched::Base.RegexMatchIterator) at regex.jl:552\n\niterate(!Matched::Base.RegexMatchIterator, !Matched::Any) at regex.jl:552\n\niterate(!Matched::Libtask.TRef, !Matched::Any...) at /home/krishnab/.julia/packages/Libtask/00Il9/src/tref.jl:86\n\n..."}]}]}],"thread_ts":"1612292883.038900","reply_count":15,"reply_users_count":2,"latest_reply":"1612369526.042200","reply_users":["UDDSTBX19","U01H36BUDJB"],"subscribed":false,"reactions":[{"name":"face_with_monocle","users":["UC0SY9JFP"],"count":1}]},{"client_msg_id":"10605262-adea-4cf8-a838-525ba435ab49","type":"message","text":"Hi all-\n\nI noticed that the adaption steps are no longer removed from the chains. Where can I submit an issue?","user":"UH08DT0JU","ts":"1612372127.043400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IdCD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi all-\n\nI noticed that the adaption steps are no longer removed from the chains. Where can I submit an issue?"}]}]}],"thread_ts":"1612372127.043400","reply_count":6,"reply_users_count":2,"latest_reply":"1612430927.044600","reply_users":["UH08DT0JU","U8T9JUA5R"],"subscribed":false},{"client_msg_id":"2a980d06-c441-4c52-b733-30498db59d4e","type":"message","text":"Hi All,\nJust a quick question about syntax. From what I understand, using filldist is more performant than for loops. I am unsure if/how to translate this to use filldist:\n\n`for i ∈ 1:length(predicted)`\n        `data[:,i] ~ MvNormal(predicted[i], σ)` \n`end` \nwhere `predicted` is the solution to an ODEProblem and has size `(5,101)` .\n\nAny suggestions appreciated! :slightly_smiling_face:","user":"U01M641BZEY","ts":"1612464277.049800","team":"T68168MUP","edited":{"user":"U01M641BZEY","ts":"1612464333.000000"},"blocks":[{"type":"rich_text","block_id":"nFDJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi All,\nJust a quick question about syntax. From what I understand, using filldist is more performant than for loops. I am unsure if/how to translate this to use filldist:\n\n"},{"type":"text","text":"for i ∈ 1:length(predicted)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"        data[:,i] ~ MvNormal(predicted[i], σ) ","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"end ","style":{"code":true}},{"type":"text","text":"\nwhere "},{"type":"text","text":"predicted","style":{"code":true}},{"type":"text","text":" is the solution to an ODEProblem and has size "},{"type":"text","text":"(5,101)","style":{"code":true}},{"type":"text","text":" .\n\nAny suggestions appreciated! "},{"type":"emoji","name":"slightly_smiling_face"}]}]}],"thread_ts":"1612464277.049800","reply_count":4,"reply_users_count":2,"latest_reply":"1612466366.050800","reply_users":["U01H36BUDJB","U01M641BZEY"],"subscribed":false},{"client_msg_id":"d3e56c16-9fd2-442c-b044-caf2096fc853","type":"message","text":"Say, I find that whenever I use the command `Turing.turnprogress(false)`, I get an error message\n```ERROR: UndefVarError: turnprogress not defined\nStacktrace:\n [1] getproperty(::Module, ::Symbol) at ./Base.jl:26\n [2] top-level scope at REPL[27]:1```\nDoes this make sense to anyone. I could not find a reference to `turnprogress` in the documentation, and no issues are posted. So perhaps I am missing something simple.","user":"UDDSTBX19","ts":"1612471590.052100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bACP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Say, I find that whenever I use the command "},{"type":"text","text":"Turing.turnprogress(false)","style":{"code":true}},{"type":"text","text":", I get an error message\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: UndefVarError: turnprogress not defined\nStacktrace:\n [1] getproperty(::Module, ::Symbol) at ./Base.jl:26\n [2] top-level scope at REPL[27]:1"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Does this make sense to anyone. I could not find a reference to "},{"type":"text","text":"turnprogress","style":{"code":true}},{"type":"text","text":" in the documentation, and no issues are posted. So perhaps I am missing something simple."}]}]}],"thread_ts":"1612471590.052100","reply_count":2,"reply_users_count":2,"latest_reply":"1612473084.052900","reply_users":["UC0SY9JFP","UDDSTBX19"],"subscribed":false},{"client_msg_id":"7b7c4906-02b1-4001-962c-e6a1bfeac2b1","type":"message","text":"Hi! New user question: I'm having a really hard time navigating the documentation for Turing. I'm learning Julia from scratch, coming from R and taking a Bayesian class using RStan at the moment. (I'm a grad student, and I'm looking to learn Julia on the side.) I'm noticing the API is sort of spread out across different packages, and I was hoping for something a bit more comprehensive if it exists, similar to Stan's manuals. Any pointers would be highly appreciated! :)\n\n(One example: it took me a bit to discover Distributions.jl was a thing, since I kept trying to locate different distributions on the <http://turing.ml|turing.ml> website, with no luck! Another example is figuring out how to initialize my parameters: I see one brief mention of \"init_theta\" on the official guide at <http://turing.ml|turing.ml>, and some GitHub issues here and there, but as far as I can tell, there's no API documentation that tells me what I actually pass in)","user":"U01M951C4JX","ts":"1612680426.059800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ZMk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi! New user question: I'm having a really hard time navigating the documentation for Turing. I'm learning Julia from scratch, coming from R and taking a Bayesian class using RStan at the moment. (I'm a grad student, and I'm looking to learn Julia on the side.) I'm noticing the API is sort of spread out across different packages, and I was hoping for something a bit more comprehensive if it exists, similar to Stan's manuals. Any pointers would be highly appreciated! :)\n\n(One example: it took me a bit to discover Distributions.jl was a thing, since I kept trying to locate different distributions on the "},{"type":"link","url":"http://turing.ml","text":"turing.ml"},{"type":"text","text":" website, with no luck! Another example is figuring out how to initialize my parameters: I see one brief mention of \"init_theta\" on the official guide at "},{"type":"link","url":"http://turing.ml","text":"turing.ml"},{"type":"text","text":", and some GitHub issues here and there, but as far as I can tell, there's no API documentation that tells me what I actually pass in)"}]}]}]},{"client_msg_id":"e787a32e-3e94-479e-99d3-c25631b73eb2","type":"message","text":"Hey Folks. I was trying to understand how to use the `describe()` function on a `ChainDataFrame`. So I used\n`chain = mapreduce(c -&gt; sample(model, NUTS(.65),1000), chainscat, 1:3);`\nto generate multiple chains. But I can't figure out how to use the `describe` function on the chains. When I try `describe(chain)` that just shows `MCMCChains.ChainDataFrame[,]` . When I try `describe[chain[:,:,1]` that does not work either. So I am missing something in the syntax somewhere.","user":"UDDSTBX19","ts":"1612716147.063700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yEA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey Folks. I was trying to understand how to use the "},{"type":"text","text":"describe()","style":{"code":true}},{"type":"text","text":" function on a "},{"type":"text","text":"ChainDataFrame","style":{"code":true}},{"type":"text","text":". So I used\n"},{"type":"text","text":"chain = mapreduce(c -> sample(model, NUTS(.65),1000), chainscat, 1:3);","style":{"code":true}},{"type":"text","text":"\nto generate multiple chains. But I can't figure out how to use the "},{"type":"text","text":"describe","style":{"code":true}},{"type":"text","text":" function on the chains. When I try "},{"type":"text","text":"describe(chain)","style":{"code":true}},{"type":"text","text":" that just shows "},{"type":"text","text":"MCMCChains.ChainDataFrame[,]","style":{"code":true}},{"type":"text","text":" . When I try "},{"type":"text","text":"describe[chain[:,:,1]","style":{"code":true}},{"type":"text","text":" that does not work either. So I am missing something in the syntax somewhere."}]}]}],"thread_ts":"1612716147.063700","reply_count":11,"reply_users_count":2,"latest_reply":"1612717666.067100","reply_users":["U9JNHB83X","UDDSTBX19"],"subscribed":false},{"client_msg_id":"fe21c14c-7992-40ea-a430-ad3126a2ca25","type":"message","text":"Note I am using Pluto. So that might be a little difference between the Pluto output and the command line output?","user":"UDDSTBX19","ts":"1612716184.064300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jURM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Note I am using Pluto. So that might be a little difference between the Pluto output and the command line output?"}]}]}]},{"client_msg_id":"6b3872d7-8865-4f1b-bfd0-d834a8ccca02","type":"message","text":"The full model is:\n```Turing.setadbackend(:forwarddiff)\nfunction lotka_volterra(du,u,p,t)\n  x, y = u\n  α, β, γ, δ  = p\n  du[1] = (α - β*y)x # dx =\n  du[2] = (δ*x - γ)y # dy = \nend\np = [1.5, 1.0, 3.0, 1.0]\nu0 = [1.0,1.0]\n@model function fitlv(data, prob1)\n    σ ~ InverseGamma(2, 3) # ~ is the tilde character\n    α ~ truncated(Normal(1.5,0.5),0.5,2.5)\n    β ~ truncated(Normal(1.2,0.5),0,2)\n    γ ~ truncated(Normal(3.0,0.5),1,4)\n    δ ~ truncated(Normal(1.0,0.5),0,2)\n\n    p = [α,β,γ,δ]\n    prob = remake(prob1, p=p)\n    predicted = solve(prob,Tsit5(),saveat=0.1)\n\n    for i = 1:length(predicted)\n        data[:,i] ~ MvNormal(predicted[i], σ)\n    end\nend\n\nmodel = fitlv(odedata, prob1)\n\n# This next command runs 3 independent chains without using multithreading. \nchain = mapreduce(c -&gt; sample(model, NUTS(.65),1000), chainscat, 1:3)```","user":"UDDSTBX19","ts":"1612716510.065400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dNx+8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The full model is:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Turing.setadbackend(:forwarddiff)\nfunction lotka_volterra(du,u,p,t)\n  x, y = u\n  α, β, γ, δ  = p\n  du[1] = (α - β*y)x # dx =\n  du[2] = (δ*x - γ)y # dy = \nend\np = [1.5, 1.0, 3.0, 1.0]\nu0 = [1.0,1.0]\n@model function fitlv(data, prob1)\n    σ ~ InverseGamma(2, 3) # ~ is the tilde character\n    α ~ truncated(Normal(1.5,0.5),0.5,2.5)\n    β ~ truncated(Normal(1.2,0.5),0,2)\n    γ ~ truncated(Normal(3.0,0.5),1,4)\n    δ ~ truncated(Normal(1.0,0.5),0,2)\n\n    p = [α,β,γ,δ]\n    prob = remake(prob1, p=p)\n    predicted = solve(prob,Tsit5(),saveat=0.1)\n\n    for i = 1:length(predicted)\n        data[:,i] ~ MvNormal(predicted[i], σ)\n    end\nend\n\nmodel = fitlv(odedata, prob1)\n\n# This next command runs 3 independent chains without using multithreading. \nchain = mapreduce(c -> sample(model, NUTS(.65),1000), chainscat, 1:3)"}]}]}]},{"client_msg_id":"15c166f2-e967-46bb-990f-fc766cfe9570","type":"message","text":"Hi,\n\n I’m trying to get von Mises distributions to work with Turing and would like some advise on how best to proceed. I’ve got a simple FowardDiff implementation of SpecialFunctions besselix to enable HMC/NUTS to work with the distribution, but I’ve now got a more fundamental problem with the definition in Distributions.jl.\n\n If I test a distribution with a low κ (i.e. large variance) almost all the test proposals fail the isinfinite check. Looking through the code, the problem appears to be the definition of the range of the von Mises function:\n\nWith low κ the observation values (y) cover the whole range from -π to π, so when a test point has a non zero mean (μ) there’s always some observations where |y-μ|&gt;π. This violates the definition given in Distributions.jl\n\n`@distr_support VonMises d.μ - π d.μ + π`\n\nIf I remove the value check in the Distributions.jl logpdf definition, sampling works well. And, at least for my use case, this is perfectly valid definition. von Mises is then a continuous, repeating, distribution, and any limitations on the range of μ are defined in the prior.\n\nI’ve been trying to understand why Distributions.jl has this range definition, but haven’t been able to find much. It was introduced in #273 with this <https://github.com/JuliaStats/Distributions.jl/issues/273#issuecomment-61982908|comment> but without further discussion, or explanation in the quoted source (wikipedia).\n\nSo, does anyone know why von Mises has this range? And what would be the best way to get it working with Turing? A fix for Distributions.jl or an import of the logpdf function within Turing?\n\nThanks.","user":"U01JL6RGKU7","ts":"1612784378.068100","team":"T68168MUP","edited":{"user":"U01JL6RGKU7","ts":"1612784870.000000"},"blocks":[{"type":"rich_text","block_id":"ZG5Py","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi,\n\n I’m trying to get von Mises distributions to work with Turing and would like some advise on how best to proceed. I’ve got a simple FowardDiff implementation of SpecialFunctions besselix to enable HMC/NUTS to work with the distribution, but I’ve now got a more fundamental problem with the definition in Distributions.jl.\n\n If I test a distribution with a low κ (i.e. large variance) almost all the test proposals fail the isinfinite check. Looking through the code, the problem appears to be the definition of the range of the von Mises function:\n\nWith low κ the observation values (y) cover the whole range from -π to π, so when a test point has a non zero mean (μ) there’s always some observations where |y-μ|>π. This violates the definition given in Distributions.jl\n\n"},{"type":"text","text":"@distr_support VonMises d.μ - π d.μ + π","style":{"code":true}},{"type":"text","text":"\n\nIf I remove the value check in the Distributions.jl logpdf definition, sampling works well. And, at least for my use case, this is perfectly valid definition. von Mises is then a continuous, repeating, distribution, and any limitations on the range of μ are defined in the prior.\n\nI’ve been trying to understand why Distributions.jl has this range definition, but haven’t been able to find much. It was introduced in #273 with this "},{"type":"link","url":"https://github.com/JuliaStats/Distributions.jl/issues/273#issuecomment-61982908","text":"comment"},{"type":"text","text":" but without further discussion, or explanation in the quoted source (wikipedia).\n\nSo, does anyone know why von Mises has this range? And what would be the best way to get it working with Turing? A fix for Distributions.jl or an import of the logpdf function within Turing?\n\nThanks."}]}]}],"thread_ts":"1612784378.068100","reply_count":1,"reply_users_count":1,"latest_reply":"1612786123.068300","reply_users":["UC0SY9JFP"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"Another issue with circular distributions parameterized by an angle with NUTS is that you either are supported on the entire real line, in which case the distribution has infinite modes (and if you can transition between nodes in NUTS, then this will cause poor adaptation), or you are supported only on the the range [0, pi], in which case you could bifurcate a single mode and only explore one of them. The same is true whether we're talking about the random variable or the mean parameter of von Mises. The way Stan handles this is to internally represent circular variables in terms of two normally distributed cartesian coordinates x and y such that theta = atan(y, x). theta is then uniformly distributed on the circle. To apply a von Mises distribution, to theta, you'd want to increment the logpdf manually. Does Turing give us access to this? In brief something like\n```# equivalent to `μ ~ UniformOnCircle(); θ ~ VonMises(μ)` if we had such a thing\n@model function vMmodel(θ)\n    μx ~ Normal()\n    μy ~ Normal()\n    μ = atan(μy, μx)\n    lp += logpdf(VonMises(μ), θ) # is there a syntax for this?\nend```\n","user":"UHDQQ4GN6","ts":"1612825842.070100","thread_ts":"1612784378.068100","root":{"client_msg_id":"15c166f2-e967-46bb-990f-fc766cfe9570","type":"message","text":"Hi,\n\n I’m trying to get von Mises distributions to work with Turing and would like some advise on how best to proceed. I’ve got a simple FowardDiff implementation of SpecialFunctions besselix to enable HMC/NUTS to work with the distribution, but I’ve now got a more fundamental problem with the definition in Distributions.jl.\n\n If I test a distribution with a low κ (i.e. large variance) almost all the test proposals fail the isinfinite check. Looking through the code, the problem appears to be the definition of the range of the von Mises function:\n\nWith low κ the observation values (y) cover the whole range from -π to π, so when a test point has a non zero mean (μ) there’s always some observations where |y-μ|&gt;π. This violates the definition given in Distributions.jl\n\n`@distr_support VonMises d.μ - π d.μ + π`\n\nIf I remove the value check in the Distributions.jl logpdf definition, sampling works well. And, at least for my use case, this is perfectly valid definition. von Mises is then a continuous, repeating, distribution, and any limitations on the range of μ are defined in the prior.\n\nI’ve been trying to understand why Distributions.jl has this range definition, but haven’t been able to find much. It was introduced in #273 with this <https://github.com/JuliaStats/Distributions.jl/issues/273#issuecomment-61982908|comment> but without further discussion, or explanation in the quoted source (wikipedia).\n\nSo, does anyone know why von Mises has this range? And what would be the best way to get it working with Turing? A fix for Distributions.jl or an import of the logpdf function within Turing?\n\nThanks.","user":"U01JL6RGKU7","ts":"1612784378.068100","team":"T68168MUP","edited":{"user":"U01JL6RGKU7","ts":"1612784870.000000"},"blocks":[{"type":"rich_text","block_id":"ZG5Py","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi,\n\n I’m trying to get von Mises distributions to work with Turing and would like some advise on how best to proceed. I’ve got a simple FowardDiff implementation of SpecialFunctions besselix to enable HMC/NUTS to work with the distribution, but I’ve now got a more fundamental problem with the definition in Distributions.jl.\n\n If I test a distribution with a low κ (i.e. large variance) almost all the test proposals fail the isinfinite check. Looking through the code, the problem appears to be the definition of the range of the von Mises function:\n\nWith low κ the observation values (y) cover the whole range from -π to π, so when a test point has a non zero mean (μ) there’s always some observations where |y-μ|>π. This violates the definition given in Distributions.jl\n\n"},{"type":"text","text":"@distr_support VonMises d.μ - π d.μ + π","style":{"code":true}},{"type":"text","text":"\n\nIf I remove the value check in the Distributions.jl logpdf definition, sampling works well. And, at least for my use case, this is perfectly valid definition. von Mises is then a continuous, repeating, distribution, and any limitations on the range of μ are defined in the prior.\n\nI’ve been trying to understand why Distributions.jl has this range definition, but haven’t been able to find much. It was introduced in #273 with this "},{"type":"link","url":"https://github.com/JuliaStats/Distributions.jl/issues/273#issuecomment-61982908","text":"comment"},{"type":"text","text":" but without further discussion, or explanation in the quoted source (wikipedia).\n\nSo, does anyone know why von Mises has this range? And what would be the best way to get it working with Turing? A fix for Distributions.jl or an import of the logpdf function within Turing?\n\nThanks."}]}]}],"thread_ts":"1612784378.068100","reply_count":4,"reply_users_count":4,"latest_reply":"1612825842.070100","reply_users":["UC0SY9JFP","U01H36BUDJB","U01JL6RGKU7","UHDQQ4GN6"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"sjC9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Another issue with circular distributions parameterized by an angle with NUTS is that you either are supported on the entire real line, in which case the distribution has infinite modes (and if you can transition between nodes in NUTS, then this will cause poor adaptation), or you are supported only on the the range [0, pi], in which case you could bifurcate a single mode and only explore one of them. The same is true whether we're talking about the random variable or the mean parameter of von Mises. The way Stan handles this is to internally represent circular variables in terms of two normally distributed cartesian coordinates x and y such that theta = atan(y, x). theta is then uniformly distributed on the circle. To apply a von Mises distribution, to theta, you'd want to increment the logpdf manually. Does Turing give us access to this? In brief something like\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"# equivalent to `μ ~ UniformOnCircle(); θ ~ VonMises(μ)` if we had such a thing\n@model function vMmodel(θ)\n    μx ~ Normal()\n    μy ~ Normal()\n    μ = atan(μy, μx)\n    lp += logpdf(VonMises(μ), θ) # is there a syntax for this?\nend"}]},{"type":"rich_text_section","elements":[]}]}],"client_msg_id":"eaebf69d-e77b-4b42-9a18-927892bb3470"},{"client_msg_id":"5fdcced9-aa47-46ff-bcd2-e2e5c8fbe98e","type":"message","text":"Could someone help with : <https://discourse.julialang.org/t/sampling-from-power-posterior/54924>","user":"U7QLM6E2E","ts":"1612881181.074600","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"Sampling from power posterior","title_link":"https://discourse.julialang.org/t/sampling-from-power-posterior/54924","text":"Hey, I was wondering if someone could guide me (or direct me to the relevant code) on how to sample from the power posterior given a Turing model. By power posterior, I mean the posterior to modified joint : p(x|y; b) \\propto p(y|x)^b p(x), where b \\in [0,1].","fallback":"JuliaLang: Sampling from power posterior","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","fields":[{"title":"Reading time","value":"1 mins :clock2:","short":true},{"title":"Likes","value":"3 :heart:","short":true}],"ts":1612876144,"from_url":"https://discourse.julialang.org/t/sampling-from-power-posterior/54924","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/sampling-from-power-posterior/54924"}],"blocks":[{"type":"rich_text","block_id":"MNZsE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Could someone help with : "},{"type":"link","url":"https://discourse.julialang.org/t/sampling-from-power-posterior/54924"}]}]}],"thread_ts":"1612881181.074600","reply_count":4,"reply_users_count":2,"latest_reply":"1612882233.075600","reply_users":["UC0SY9JFP","U7QLM6E2E"],"subscribed":false},{"client_msg_id":"2ed1f7fd-5b7e-4550-a5fd-b159646976cb","type":"message","text":"I think the searchbar on <http://turing.ml|turing.ml> is broken btw.","user":"U01C2AJ9F63","ts":"1612963752.082700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6qbz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think the searchbar on "},{"type":"link","url":"http://turing.ml","text":"turing.ml"},{"type":"text","text":" is broken btw."}]}]}],"reactions":[{"name":"confused","users":["UC0SY9JFP"],"count":1}]},{"client_msg_id":"a532a28b-272a-4f9d-8d89-4f61f7e48edf","type":"message","text":"multiple hierarchical models with different sample sizes","user":"U01HSP1E1NW","ts":"1613004206.084100","team":"T68168MUP","edited":{"user":"U01HSP1E1NW","ts":"1613004670.000000"},"blocks":[{"type":"rich_text","block_id":"y7F","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"multiple hierarchical models with different sample sizes"}]}]}],"thread_ts":"1613004206.084100","reply_count":1,"reply_users_count":1,"latest_reply":"1613004561.084900","reply_users":["U01HSP1E1NW"],"subscribed":false},{"client_msg_id":"177374a5-6705-43ac-a9f5-6a51e9a79639","type":"message","text":"At the suggestion of someone very helpful at “helpdesk” I’m writing to ask about using Turing for Bayesian linear regression.  I’ve found the great documentation example: <https://turing.ml/dev/tutorials/5-linearregression/> and am working to use that a starter for the problem I’m looking to solve.\n\nThat problem is a Bayesian linear regression case where the with priors on the parameters are drawn from an underlying power-law distribution.  But I haven’t been able to find a power law distribution in Distributions.jl so I’m not sure how to go about trying implement this.\n\nThoughts and guidance much appreciated!","user":"UHRBR18HH","ts":"1613016923.085400","team":"T68168MUP","attachments":[{"title":"Linear Regression","title_link":"https://turing.ml/dev/tutorials/5-linearregression/","text":"Linear Regression","fallback":"Linear Regression","from_url":"https://turing.ml/dev/tutorials/5-linearregression/","service_icon":"https://turing.ml/dev/assets/img/favicon.ico","service_name":"turing.ml","id":1,"original_url":"https://turing.ml/dev/tutorials/5-linearregression/"}],"blocks":[{"type":"rich_text","block_id":"0+4yd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"At the suggestion of someone very helpful at “helpdesk” I’m writing to ask about using Turing for Bayesian linear regression.  I’ve found the great documentation example: "},{"type":"link","url":"https://turing.ml/dev/tutorials/5-linearregression/"},{"type":"text","text":" and am working to use that a starter for the problem I’m looking to solve.\n\nThat problem is a Bayesian linear regression case where the with priors on the parameters are drawn from an underlying power-law distribution.  But I haven’t been able to find a power law distribution in Distributions.jl so I’m not sure how to go about trying implement this.\n\nThoughts and guidance much appreciated!"}]}]}]},{"client_msg_id":"e103a083-9064-40e0-872e-3b8bd7befdee","type":"message","text":"quick question: can variational inference benefit from multithreading?","user":"U01M641BZEY","ts":"1613070473.090000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"W+YL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"quick question: can variational inference benefit from multithreading?"}]}]}]},{"client_msg_id":"869be028-f8de-486b-bd75-e21fc35b0711","type":"message","text":"Is there a way to infer from a Turing model if the likelihood is factorizable, i.e.  : `p(y|x) = \\prod_i p(y|x_i)` ?","user":"U7QLM6E2E","ts":"1613125563.093900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OFy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to infer from a Turing model if the likelihood is factorizable, i.e.  : "},{"type":"text","text":"p(y|x) = \\prod_i p(y|x_i)","style":{"code":true}},{"type":"text","text":" ?"}]}]}],"thread_ts":"1613125563.093900","reply_count":3,"reply_users_count":2,"latest_reply":"1613125764.094400","reply_users":["UC0SY9JFP","U7QLM6E2E"],"subscribed":false},{"client_msg_id":"255dcfc8-baf8-4c14-adf5-0ebb1174849a","type":"message","text":"Is there currently an predefined dense MvNormal variational posterior that I can set using something like vi(model, advi, q=DenseMvNormal) ?","user":"U01M641BZEY","ts":"1613149586.096000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jC8L","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there currently an predefined dense MvNormal variational posterior that I can set using something like vi(model, advi, q=DenseMvNormal) ?"}]}]}],"thread_ts":"1613149586.096000","reply_count":1,"reply_users_count":1,"latest_reply":"1613149665.096100","reply_users":["U01M641BZEY"],"subscribed":false},{"client_msg_id":"cf4e740c-d1d7-4c3a-b89b-cca8113d802d","type":"message","text":"Hi guys, trying to extend the GMM example into something a bit more general and running into problems, roughly:\n1. `filldist` doesn’t work on `InverseWishart` for learning cov matrices of the mixture components, have bumped an issue on DistributionsAD I found regarding this, is this an easy fix, not familiar with the underlying code, in the mean time what is the best way to jerry-rig something?\n2. Is doing a for loop as in the example actually efficient, I am not sure what formulation is preferable, is there some way to do a filldist on the `MvNormal` ’s for `x` to cycle over the drawn `k` ?\n3. If I want to say add some multiplier parameter to the `MvNormal` distribution, is it preferable to define a custom distribution or to use `Turing.@addlogprob!` as I am doing now to just add the log pdf multiplied with something I pass in? Mainly bothered about performance implications but also what comprises “good practice” here?\n4. Slight aside as I am sure I am missing something dumb but for some reason there is no progress bar in the Julia prompt when I run this model, I have tried `Turing.setprogress!(true)` etc to no avail\nMore generally I would appreciate any suggested improvements, as I plan on using this with quite a big dataset so any modest gains would be nice :slightly_smiling_face:\n\n``` @model LikelihoodWeightedGMM(x, K, ws) = begin\n    \n    D, N = size(x)\n\n    # Draw the parameters for the clusters.\n    # μ ~ filldist(MvNormal([0.5, 0.5], 0.5 * I(2)), K)\n    μ1 ~ MvNormal(repeat([0.5], K), 0.5)\n    μ2 ~ MvNormal(repeat([0.5], K), 0.5)\n    μ = [[μ1[i], μ2[i]] for i in 1:length(μ1)]\n\n    # Σ ~ filldist(InverseWishart(3, Matrix{Float64}([[2,0.5] [0.5,2]])), K)\n\n    # Draw the weights for the K clusters from a Dirichlet distribution.\n    α = 1.0\n    w ~ Dirichlet(K, α)\n    \n    # Comment out this line if you instead want to have uniform weights.\n    # w = repeat([1 / K], K)\n    \n    # Draw assignments for each datum and generate it from a multivariate normal.\n    k ~ filldist(Categorical(w), N)\n\n    # x .~ MvNormal.(μ[k], repeat([0.05], length(k)))\n    for i in 1:N\n        Turing.@addlogprob! logpdf(MvNormal([μ1[k[i]], μ2[k[i]]], 0.05), x[:, i]) * ws[i]\n        # Turing.@addlogprob! logpdf(MvNormal([μ1[k[i]], μ2[k[i]]], Σ[k[i]]), x[:, i]) * ws[i]\n    end\n    return k\nend```","user":"UMS7H0ASG","ts":"1613211463.103000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WXc1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi guys, trying to extend the GMM example into something a bit more general and running into problems, roughly:\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"filldist","style":{"code":true}},{"type":"text","text":" doesn’t work on "},{"type":"text","text":"InverseWishart","style":{"code":true}},{"type":"text","text":" for learning cov matrices of the mixture components, have bumped an issue on DistributionsAD I found regarding this, is this an easy fix, not familiar with the underlying code, in the mean time what is the best way to jerry-rig something?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Is doing a for loop as in the example actually efficient, I am not sure what formulation is preferable, is there some way to do a filldist on the "},{"type":"text","text":"MvNormal","style":{"code":true}},{"type":"text","text":" ’s for "},{"type":"text","text":"x","style":{"code":true}},{"type":"text","text":" to cycle over the drawn "},{"type":"text","text":"k","style":{"code":true}},{"type":"text","text":" ?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"If I want to say add some multiplier parameter to the "},{"type":"text","text":"MvNormal","style":{"code":true}},{"type":"text","text":" distribution, is it preferable to define a custom distribution or to use "},{"type":"text","text":"Turing.@addlogprob!","style":{"code":true}},{"type":"text","text":" as I am doing now to just add the log pdf multiplied with something I pass in? Mainly bothered about performance implications but also what comprises “good practice” here?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Slight aside as I am sure I am missing something dumb but for some reason there is no progress bar in the Julia prompt when I run this model, I have tried "},{"type":"text","text":"Turing.setprogress!(true)","style":{"code":true}},{"type":"text","text":" etc to no avail"}]}],"style":"ordered","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"More generally I would appreciate any suggested improvements, as I plan on using this with quite a big dataset so any modest gains would be nice "},{"type":"emoji","name":"slightly_smiling_face"},{"type":"text","text":"\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":" @model LikelihoodWeightedGMM(x, K, ws) = begin\n    \n    D, N = size(x)\n\n    # Draw the parameters for the clusters.\n    # μ ~ filldist(MvNormal([0.5, 0.5], 0.5 * I(2)), K)\n    μ1 ~ MvNormal(repeat([0.5], K), 0.5)\n    μ2 ~ MvNormal(repeat([0.5], K), 0.5)\n    μ = [[μ1[i], μ2[i]] for i in 1:length(μ1)]\n\n    # Σ ~ filldist(InverseWishart(3, Matrix{Float64}([[2,0.5] [0.5,2]])), K)\n\n    # Draw the weights for the K clusters from a Dirichlet distribution.\n    α = 1.0\n    w ~ Dirichlet(K, α)\n    \n    # Comment out this line if you instead want to have uniform weights.\n    # w = repeat([1 / K], K)\n    \n    # Draw assignments for each datum and generate it from a multivariate normal.\n    k ~ filldist(Categorical(w), N)\n\n    # x .~ MvNormal.(μ[k], repeat([0.05], length(k)))\n    for i in 1:N\n        Turing.@addlogprob! logpdf(MvNormal([μ1[k[i]], μ2[k[i]]], 0.05), x[:, i]) * ws[i]\n        # Turing.@addlogprob! logpdf(MvNormal([μ1[k[i]], μ2[k[i]]], Σ[k[i]]), x[:, i]) * ws[i]\n    end\n    return k\nend"}]}]}]},{"client_msg_id":"655266ff-507a-4dcd-8fde-439d05d18c66","type":"message","text":"I would use a loop until it’s too slow. No need to over-complicate a model if it’s fast enough. Similarly, use `ForwardDiff` until it’s too slow. Then try `ReverseDiff` or `Zygote`. If nothing else works, analyse bottlenecks. Which parameter that when removed or fixed speeds up your inference? Then look into speeding this one up. You can even write your own distribution for it with un-normalised logpdf. `filldist` essentially builds a custom distribution. The scaling of the logpdf can also be handled using a custom distribution. CC: <@U7QLM6E2E> who was trying to solve a similar problem not too long ago but I didn’t think of this solution back then.","user":"U85JBUGGP","ts":"1613216679.107100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QucID","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I would use a loop until it’s too slow. No need to over-complicate a model if it’s fast enough. Similarly, use "},{"type":"text","text":"ForwardDiff","style":{"code":true}},{"type":"text","text":" until it’s too slow. Then try "},{"type":"text","text":"ReverseDiff","style":{"code":true}},{"type":"text","text":" or "},{"type":"text","text":"Zygote","style":{"code":true}},{"type":"text","text":". If nothing else works, analyse bottlenecks. Which parameter that when removed or fixed speeds up your inference? Then look into speeding this one up. You can even write your own distribution for it with un-normalised logpdf. "},{"type":"text","text":"filldist","style":{"code":true}},{"type":"text","text":" essentially builds a custom distribution. The scaling of the logpdf can also be handled using a custom distribution. CC: "},{"type":"user","user_id":"U7QLM6E2E"},{"type":"text","text":" who was trying to solve a similar problem not too long ago but I didn’t think of this solution back then."}]}]}],"thread_ts":"1613216679.107100","reply_count":4,"reply_users_count":2,"latest_reply":"1613217749.107900","reply_users":["UMS7H0ASG","U85JBUGGP"],"subscribed":false},{"client_msg_id":"fd371119-693e-4a23-b731-a3913943636e","type":"message","text":"Hello, how do I make my prior ordered? MWE\n\n```@model function gdemo(y)\n    theta ~ filldist(Normal(1, 2),6)\n    y .~ OrderedLogistic(0, theta) \nend```\nTo be specific, I want to order my `theta` priors for `OrderedLogistic` so `theta[6] &gt; theta[5] &gt; theta[4] &gt; theta[3] &gt; theta[2] &gt; theta[1]` ?\n\nIf this isn't supported yet, is there any dirty workaround? I am stucked with this for a long time so I don't mind some workaround at the moment :eyes:","user":"U011PPW7K53","ts":"1613219560.111200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ZfIG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello, how do I make my prior ordered? MWE\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@model function gdemo(y)\n    theta ~ filldist(Normal(1, 2),6)\n    y .~ OrderedLogistic(0, theta) \nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nTo be specific, I want to order my "},{"type":"text","text":"theta","style":{"code":true}},{"type":"text","text":" priors for "},{"type":"text","text":"OrderedLogistic","style":{"code":true}},{"type":"text","text":" so "},{"type":"text","text":"theta[6] > theta[5] > theta[4] > theta[3] > theta[2] > theta[1]","style":{"code":true}},{"type":"text","text":" ?\n\nIf this isn't supported yet, is there any dirty workaround? I am stucked with this for a long time so I don't mind some workaround at the moment "},{"type":"emoji","name":"eyes"}]}]}],"thread_ts":"1613219560.111200","reply_count":1,"reply_users_count":1,"latest_reply":"1613220383.111300","reply_users":["U011PPW7K53"],"subscribed":false},{"client_msg_id":"e996e0fb-e9ce-41e7-8d5b-92ec5227d6d6","type":"message","text":"I remember a while ago talking to a couple of people about initialising parameters in Turing, has that been properly implemented yet? It is mentioned in the docs but cannot see a way to actually do it looking at the codebase","user":"UMS7H0ASG","ts":"1613250657.113200","team":"T68168MUP","edited":{"user":"UMS7H0ASG","ts":"1613250666.000000"},"blocks":[{"type":"rich_text","block_id":"wQCn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I remember a while ago talking to a couple of people about initialising parameters in Turing, has that been properly implemented yet? It is mentioned in the docs but cannot see a way to actually do it looking at the codebase"}]}]}]},{"type":"message","subtype":"thread_broadcast","text":"So I have noticed that when I try to fit a GMM where the components have low variance the sampling fails in that I guess perhaps nothing is meeting acceptance criteria for HMC? Does anyone see a fix for the problem I have outlined properly here with a comparison of graphs at different component sigmas: <https://discourse.julialang.org/t/strange-issues-fitting-gmms-in-turing-jl-by-extending-a-fairly-simple-example/55232> ? :slightly_smiling_face:","user":"UMS7H0ASG","ts":"1613255469.119800","thread_ts":"1613250657.113200","root":{"client_msg_id":"e996e0fb-e9ce-41e7-8d5b-92ec5227d6d6","type":"message","text":"I remember a while ago talking to a couple of people about initialising parameters in Turing, has that been properly implemented yet? It is mentioned in the docs but cannot see a way to actually do it looking at the codebase","user":"UMS7H0ASG","ts":"1613250657.113200","team":"T68168MUP","edited":{"user":"UMS7H0ASG","ts":"1613250666.000000"},"blocks":[{"type":"rich_text","block_id":"wQCn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I remember a while ago talking to a couple of people about initialising parameters in Turing, has that been properly implemented yet? It is mentioned in the docs but cannot see a way to actually do it looking at the codebase"}]}]}],"thread_ts":"1613250657.113200","reply_count":26,"reply_users_count":2,"latest_reply":"1613255469.119800","reply_users":["U9JNHB83X","UMS7H0ASG"],"subscribed":false},"attachments":[{"service_name":"JuliaLang","title":"Strange issues fitting GMMs in Turing.jl by extending a fairly simple example","title_link":"https://discourse.julialang.org/t/strange-issues-fitting-gmms-in-turing-jl-by-extending-a-fairly-simple-example/55232","text":"I have been extending the Turing example on Bayesian GMMs today in a number of ways, trying it out with non-symmetric 2D Gaussian locations and more clusters than the two in the example I have linked, and namely with some more tightly distributed components. This is where I have run into a problem I am struggling to diagnose and wondering if it is something looking into further. I first pick my parameters and generate some data: using Distributions, StatsPlots, Random, KernelDensity # Set a ra...","fallback":"JuliaLang: Strange issues fitting GMMs in Turing.jl by extending a fairly simple example","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/3X/d/0/d0f10d047cf6628e840b3f6498595ebaaa87ae50_2_1024x358.jpeg","ts":1613255325,"from_url":"https://discourse.julialang.org/t/strange-issues-fitting-gmms-in-turing-jl-by-extending-a-fairly-simple-example/55232","thumb_width":1024,"thumb_height":358,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/strange-issues-fitting-gmms-in-turing-jl-by-extending-a-fairly-simple-example/55232"}],"blocks":[{"type":"rich_text","block_id":"1dWa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So I have noticed that when I try to fit a GMM where the components have low variance the sampling fails in that I guess perhaps nothing is meeting acceptance criteria for HMC? Does anyone see a fix for the problem I have outlined properly here with a comparison of graphs at different component sigmas: "},{"type":"link","url":"https://discourse.julialang.org/t/strange-issues-fitting-gmms-in-turing-jl-by-extending-a-fairly-simple-example/55232"},{"type":"text","text":" ? "},{"type":"emoji","name":"slightly_smiling_face"}]}]}],"client_msg_id":"8cbcc685-b692-469a-b3f8-621ec21c76c9"}]}