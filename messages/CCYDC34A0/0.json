{"cursor": 2, "messages": [{"client_msg_id":"9a88c7ef-ffb0-4920-87ff-eca0f0810633","type":"message","text":"Approximate Bayesian Computation.","user":"U69BL50BF","ts":"1615300364.002800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4Iqd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Approximate Bayesian Computation."}]}]}]},{"client_msg_id":"a231f94d-56ec-41cc-a0f1-fb2499101798","type":"message","text":"ah, right","user":"U01H36BUDJB","ts":"1615300607.003000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8HF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ah, right"}]}]}]},{"client_msg_id":"98579e6d-1ba2-466c-a0e7-a8dd02f683c4","type":"message","text":"approximate bayes... never really worked much with that before","user":"U01H36BUDJB","ts":"1615300629.003300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dSb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"approximate bayes... never really worked much with that before"}]}]}]},{"client_msg_id":"45fcb3ae-01ef-47b6-bcc2-ba476543aef3","type":"message","text":"Is there an effort atm to include likelihood-free/simulation based inference in Turing (or elsewhere?)","user":"U01M641BZEY","ts":"1615300873.005100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RCx07","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there an effort atm to include likelihood-free/simulation based inference in Turing (or elsewhere?)"}]}]}]},{"client_msg_id":"b7d828eb-89e6-47cb-b819-c66d8c0302a5","type":"message","text":"as for e.g. is described nicely here by Miles Cranmer: <https://astroautomata.com/blog/simulation-based-inference/>","user":"U01M641BZEY","ts":"1615301140.006600","team":"T68168MUP","attachments":[{"service_name":"astro automata","title":"A tutorial on simulation-based inference","title_link":"https://astroautomata.com/blog/simulation-based-inference/","text":"Personal website of Miles Cranmer","fallback":"astro automata: A tutorial on simulation-based inference","image_url":"http://astroautomata.com//assets/images/nflow.png","ts":1595808000,"from_url":"https://astroautomata.com/blog/simulation-based-inference/","image_width":425,"image_height":250,"image_bytes":60223,"service_icon":"https://astroautomata.com/apple-icon-57x57.png","id":1,"original_url":"https://astroautomata.com/blog/simulation-based-inference/"}],"blocks":[{"type":"rich_text","block_id":"+kLa0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"as for e.g. is described nicely here by Miles Cranmer: "},{"type":"link","url":"https://astroautomata.com/blog/simulation-based-inference/"}]}]}]},{"type":"message","subtype":"channel_join","ts":"1615364009.010700","user":"U01QXPZ8SR2","text":"<@U01QXPZ8SR2> has joined the channel","inviter":"U01JA0D7L56"},{"type":"message","subtype":"channel_join","ts":"1615367004.011000","user":"U01QB1S7C0P","text":"<@U01QB1S7C0P> has joined the channel","inviter":"U01JA0D7L56"},{"client_msg_id":"d7c4394a-6d19-4ea7-a320-8dc0fdd8b53c","type":"message","text":"how is reversediff so much faster?! argh spent like an hour optimizing a Turing model and then realized that I had forgot to change the ad backend and sampling went from 2hrs for 1000 samples to 2 min","user":"U6CFMFM2R","ts":"1615488171.019400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"CRR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"how is reversediff so much faster?! argh spent like an hour optimizing a Turing model and then realized that I had forgot to change the ad backend and sampling went from 2hrs for 1000 samples to 2 min"}]}]}]},{"client_msg_id":"06f3535a-7525-4c78-b59d-c2e51c13eee1","type":"message","text":"Reverse-mode AD vs forward-mode AD. If you have N parameters and your model is properly vectorised, you can expect reverse-mode to be ~N times faster. Less in practice but ideally not by much.","user":"U85JBUGGP","ts":"1615494112.021200","team":"T68168MUP","edited":{"user":"U85JBUGGP","ts":"1615494122.000000"},"blocks":[{"type":"rich_text","block_id":"aGP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Reverse-mode AD vs forward-mode AD. If you have N parameters and your model is properly vectorised, you can expect reverse-mode to be ~N times faster. Less in practice but ideally not by much."}]}]}]},{"client_msg_id":"eafb4e2c-c5a2-4862-a196-59727bed2b4d","type":"message","text":"`N` needs to be large enough though","user":"U85JBUGGP","ts":"1615494133.021500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bMh6v","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"N","style":{"code":true}},{"type":"text","text":" needs to be large enough though"}]}]}]},{"client_msg_id":"3c9d0a4e-3833-4e12-b83f-8c584db8ad39","type":"message","text":"When Diffractor is out and Turing uses it, you can expect AD to be even faster! But that won’t be for a while.","user":"U85JBUGGP","ts":"1615494210.022100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2N+q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"When Diffractor is out and Turing uses it, you can expect AD to be even faster! But that won’t be for a while."}]}]}]},{"client_msg_id":"6e63c40a-f166-43e2-b44c-e15bfd498a5b","type":"message","text":"its really incredible","user":"U6CFMFM2R","ts":"1615497774.023000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"M+t","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"its really incredible"}]}]}]},{"client_msg_id":"e04f1568-3a34-4297-896e-f1d2d6fa2e47","type":"message","text":"I’m trying to implement this simple hierarchical model in Turing:\ny_j | θ_j, σ_j ~ N(θ_j, σ_{j}^2$\n θ_j | μ, τ ~ N(μ, τ^2)$\nwith priors p(μ, τ)\n\nWould\n```@model meta(y) = begin\n    μ ~ Uniform(0, 1)\n    τ ~ Uniform(0, 1)\n    θ ~ Normal(μ, τ^2)\n\n    for j in eachindex(y) \n      y[j] ~  Normal(θ, τ)\n    end\nend```\nbe correct? Do I have to do anything to specify that the theta is specific to / potentially different for each y_j?","user":"US8V7JSKB","ts":"1615541250.026300","team":"T68168MUP","edited":{"user":"US8V7JSKB","ts":"1615541347.000000"},"blocks":[{"type":"rich_text","block_id":"JPOp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’m trying to implement this simple hierarchical model in Turing:\ny_j | θ_j, σ_j ~ N(θ_j, σ_{j}^2$\n θ_j | μ, τ ~ N(μ, τ^2)$\nwith priors p(μ, τ)\n\nWould\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@model meta(y) = begin\n    μ ~ Uniform(0, 1)\n    τ ~ Uniform(0, 1)\n    θ ~ Normal(μ, τ^2)\n\n    for j in eachindex(y) \n      y[j] ~  Normal(θ, τ)\n    end\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"be correct? Do I have to do anything to specify that the theta is specific to / potentially different for each y_j?"}]}]}],"thread_ts":"1615541250.026300","reply_count":4,"reply_users_count":2,"latest_reply":"1615541586.027200","reply_users":["U01C2AJ9F63","US8V7JSKB"],"subscribed":false},{"client_msg_id":"022f8bc2-934c-4e01-935a-83eaa47fe49d","type":"message","text":"Is there a trick to working with parameter vectors and callbacks? For example, say I have a vector of parameters `κ ~ MvNormal(Fill(0.0,5),1.0)` . I've tried TuringCallbacks (without setting a variable_filter) and a few different things with Turkie but no luck so far.","user":"U017YGFQTE3","ts":"1615544384.035400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Bt97","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a trick to working with parameter vectors and callbacks? For example, say I have a vector of parameters "},{"type":"text","text":"κ ~ MvNormal(Fill(0.0,5),1.0)","style":{"code":true}},{"type":"text","text":" . I've tried TuringCallbacks (without setting a variable_filter) and a few different things with Turkie but no luck so far."}]}]}]},{"type":"message","text":"No signs of slowing down!","files":[{"id":"F01RTKCCRJL","created":1615571882,"timestamp":1615571882,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U85JBUGGP","editable":false,"size":462351,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01RTKCCRJL/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01RTKCCRJL/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_360.png","thumb_360_w":360,"thumb_360_h":230,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_480.png","thumb_480_w":480,"thumb_480_h":307,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_720.png","thumb_720_w":720,"thumb_720_h":460,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_800.png","thumb_800_w":800,"thumb_800_h":511,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_960.png","thumb_960_w":960,"thumb_960_h":614,"thumb_1024":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_1024.png","thumb_1024_w":1024,"thumb_1024_h":655,"original_w":1658,"original_h":1060,"thumb_tiny":"AwAeADDSJOeKPm9qOppaADrR0oooAKOv0pOv0paAE/ipaQg9jijB9aAFzSYz1o5x1o59aAFopOfX9KXn1oA//9k=","permalink":"https://julialang.slack.com/files/U85JBUGGP/F01RTKCCRJL/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01RTKCCRJL-a5dde4754b","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"4RB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"No signs of slowing down!"}]}]}],"user":"U85JBUGGP","display_as_bot":false,"ts":"1615571913.037400","reactions":[{"name":"fire","users":["UC0SY9JFP","U85JBUGGP"],"count":2}]},{"client_msg_id":"c8f3f275-b1ae-46c1-98f6-4d8f0c64ffbd","type":"message","text":"Hey I was one of those stars!","user":"U01H36BUDJB","ts":"1615590217.044200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"cxlDp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey I was one of those stars!"}]}]}]},{"client_msg_id":"e2d45524-80bf-41b6-b072-77b304ba67e1","type":"message","text":"I was really excited when I found Turing. You guys have done awesome work :)","user":"U01H36BUDJB","ts":"1615590280.044600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pEjw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I was really excited when I found Turing. You guys have done awesome work :)"}]}]}]},{"client_msg_id":"9cadb051-c86e-4ed0-b465-4aad3451e810","type":"message","text":"Would any of you happen to have any teaching materials for Turing? I’m trying to learn it for a Bayesian Stats class (that is unfortunately R-based), and I’m trying to find as many Turing examples as I can. I’m already aware of the <http://Turing.ml|Turing.ml> tutorials and Rob Goedman’s Stat Rethinking code.","user":"US8V7JSKB","ts":"1615599144.047400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wrc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Would any of you happen to have any teaching materials for Turing? I’m trying to learn it for a Bayesian Stats class (that is unfortunately R-based), and I’m trying to find as many Turing examples as I can. I’m already aware of the "},{"type":"link","url":"http://Turing.ml","text":"Turing.ml"},{"type":"text","text":" tutorials and Rob Goedman’s Stat Rethinking code."}]}]}]},{"client_msg_id":"6f680ca7-37b2-4ab5-b365-3d26f7b4138c","type":"message","text":"I can't think of anything outside those materials","user":"U9JNHB83X","ts":"1615608332.048200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HiCk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I can't think of anything outside those materials"}]}]}]},{"client_msg_id":"60475152-19c8-47e6-a071-6bdf0f5748e9","type":"message","text":"They tend to be mostly comprehensive","user":"U9JNHB83X","ts":"1615608341.048600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xFi1Z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"They tend to be mostly comprehensive"}]}]}]},{"client_msg_id":"0f786972-ef30-40b8-8462-9b03fea01a25","type":"message","text":"I will be adding later this semester portuguese bayesian materials using Turing. I have a opensource creative-commons content for Bayesian Stats using R and Stan (mostly rstanarm and brms for PhD candidates in Social Sciences). All the materials are being reformulated now in a branch (2021-02 -- i.e next semester) that will be taught next semester but they are in Portuguese. You can access them here (<https://storopoli.io/Estatistica-Bayesiana/>) and the github repo (<https://github.com/storopoli/Estatistica-Bayesiana>). My ideia is to update the R and Stan materials and create also a Julia version using Turing.","user":"U01QBF4PHKP","ts":"1615634458.052600","team":"T68168MUP","attachments":[{"service_name":"Estatística Bayesiana com R e RStan","title":"Estatística Bayesiana com R e RStan: Estatística Bayesiana com R e Stan","title_link":"https://storopoli.io/Estatistica-Bayesiana/","text":"Companion para a disciplina de Estatística Bayesiana para alunos de Mestrado e Doutorado da UNINOVE","fallback":"Estatística Bayesiana com R e RStan: Estatística Bayesiana com R e RStan: Estatística Bayesiana com R e Stan","from_url":"https://storopoli.io/Estatistica-Bayesiana/","id":1,"original_url":"https://storopoli.io/Estatistica-Bayesiana/"}],"blocks":[{"type":"rich_text","block_id":"9QOIV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I will be adding later this semester portuguese bayesian materials using Turing. I have a opensource creative-commons content for Bayesian Stats using R and Stan (mostly rstanarm and brms for PhD candidates in Social Sciences). All the materials are being reformulated now in a branch (2021-02 -- i.e next semester) that will be taught next semester but they are in Portuguese. You can access them here ("},{"type":"link","url":"https://storopoli.io/Estatistica-Bayesiana/"},{"type":"text","text":") and the github repo ("},{"type":"link","url":"https://github.com/storopoli/Estatistica-Bayesiana"},{"type":"text","text":"). My ideia is to update the R and Stan materials and create also a Julia version using Turing."}]}]}]},{"client_msg_id":"47f0d9ca-2d67-4e56-b186-e048125ee57c","type":"message","text":"I am defining a custom distribution to define parameter constraints of one parameter by a second parameter. The new custom distribution version compared to my initial `Turing` model which worked by rejection of parameter combination, which don't satisfy these conditions, doesn't seem to take the prior into account.\n\nSo I wanted to ask if that is to be expected. The MWE below hopefully illustrates the scenario, if parameter falls outside the condition/range of support you get `-Inf` but for the a supported parameter set you get different `logp`. The `reject` function is akin to the scenario where one uses `@addlogp -Inf` to reject a sample. Am I missing something here?\n```julia&gt; function reject(y)\n           x = similar(y)\n           x[1] = logpdf(Uniform(0.0,2.0), y[1])\n           x[2] = ifelse(y[2] &lt; y[1], -Inf, logpdf(Normal(0.6,.3), y[2]))\n           return x\n       end\n\njulia&gt; function cust_dist(y)\n           x = similar(y)\n           x[1] = logpdf(Uniform(0.0,2.0), y[1])\n           x[2] = logpdf(truncated(Normal(0.6,.3), y[2]), y[1], Inf)\n           return x\n       end\n\njulia&gt; rv = [0.1,.05];\n\njulia&gt; cust_dist(rv)\n2-element Array{Float64,1}:\n  -0.6931471805599453\n -Inf\n\njulia&gt; reject(rv)\n2-element Array{Float64,1}:\n  -0.6931471805599453\n -Inf\n\njulia&gt; rv = [0.1,.3]\n\njulia&gt; reject(rv)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -0.21496572887873655\n\njulia&gt; cust_dist(rv)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -0.16599567864432105```\n","user":"UGFMDAMC3","ts":"1615643956.064600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ozeta","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am defining a custom distribution to define parameter constraints of one parameter by a second parameter. The new custom distribution version compared to my initial "},{"type":"text","text":"Turing","style":{"code":true}},{"type":"text","text":" model which worked by rejection of parameter combination, which don't satisfy these conditions, doesn't seem to take the prior into account.\n\nSo I wanted to ask if that is to be expected. The MWE below hopefully illustrates the scenario, if parameter falls outside the condition/range of support you get "},{"type":"text","text":"-Inf","style":{"code":true}},{"type":"text","text":" but for the a supported parameter set you get different "},{"type":"text","text":"logp","style":{"code":true}},{"type":"text","text":". The "},{"type":"text","text":"reject","style":{"code":true}},{"type":"text","text":" function is akin to the scenario where one uses "},{"type":"text","text":"@addlogp -Inf","style":{"code":true}},{"type":"text","text":" to reject a sample. Am I missing something here?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> function reject(y)\n           x = similar(y)\n           x[1] = logpdf(Uniform(0.0,2.0), y[1])\n           x[2] = ifelse(y[2] < y[1], -Inf, logpdf(Normal(0.6,.3), y[2]))\n           return x\n       end\n\njulia> function cust_dist(y)\n           x = similar(y)\n           x[1] = logpdf(Uniform(0.0,2.0), y[1])\n           x[2] = logpdf(truncated(Normal(0.6,.3), y[2]), y[1], Inf)\n           return x\n       end\n\njulia> rv = [0.1,.05];\n\njulia> cust_dist(rv)\n2-element Array{Float64,1}:\n  -0.6931471805599453\n -Inf\n\njulia> reject(rv)\n2-element Array{Float64,1}:\n  -0.6931471805599453\n -Inf\n\njulia> rv = [0.1,.3]\n\njulia> reject(rv)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -0.21496572887873655\n\njulia> cust_dist(rv)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -0.16599567864432105"}]},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"6ba62d7a-cee0-4b5a-9347-85712ce0cdb2","type":"message","text":"The “rejection” approach doesn’t include log of the normalisation constant when you “truncate”. The logpdf of `truncated` includes `log` the normalisation constant.","user":"U85JBUGGP","ts":"1615648152.067100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jzQm5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The “rejection” approach doesn’t include log of the normalisation constant when you “truncate”. The logpdf of "},{"type":"text","text":"truncated","style":{"code":true}},{"type":"text","text":" includes "},{"type":"text","text":"log","style":{"code":true}},{"type":"text","text":" the normalisation constant."}]}]}]},{"client_msg_id":"81bf003e-c84a-42b4-831d-b806ab43f592","type":"message","text":"if you try different `rv` values, you will see that the difference in the second number is always constant up to machine precision.","user":"U85JBUGGP","ts":"1615648201.068000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4Eu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"if you try different "},{"type":"text","text":"rv","style":{"code":true}},{"type":"text","text":" values, you will see that the difference in the second number is always constant up to machine precision."}]}]}]},{"client_msg_id":"d9b17ee2-74bd-4ca2-adcc-ab4a4ea9bd13","type":"message","text":"I see, yeah you are right with varying rv there is a constant difference. I guess I am surprised by the following scenario where the prior of the second parameter is a `Normal(0.6, 0.3)` and prior probability of second parameter depends on the value of the first and increases the closer `rv[1]` gets to `rv[2]`  . It seems a bit un-intuitive to me but I am also somewhat out of my depth here.","user":"UGFMDAMC3","ts":"1615649311.074900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HB7j","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I see, yeah you are right with varying rv there is a constant difference. I guess I am surprised by the following scenario where the prior of the second parameter is a "},{"type":"text","text":"Normal(0.6, 0.3)","style":{"code":true}},{"type":"text","text":" and prior probability of second parameter depends on the value of the first and increases the closer "},{"type":"text","text":"rv[1]","style":{"code":true}},{"type":"text","text":" gets to "},{"type":"text","text":"rv[2]","style":{"code":true}},{"type":"text","text":"  . It seems a bit un-intuitive to me but I am also somewhat out of my depth here."}]}]}]},{"client_msg_id":"e3968cdb-053c-4344-9aca-06ced6eab388","type":"message","text":"plot the pdf surface to visualise what’s happening better","user":"U85JBUGGP","ts":"1615649480.075200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QZf08","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"plot the pdf surface to visualise what’s happening better"}]}]}]},{"type":"message","text":"Yeah, I had plotted the pdf surface before and I can see how the density increases near the truncation.","files":[{"id":"F01RVKGQBU0","created":1615654034,"timestamp":1615654034,"name":"pdf_plots.svg","title":"pdf_plots.svg","mimetype":"image/svg+xml","filetype":"svg","pretty_type":"SVG","user":"UGFMDAMC3","editable":false,"size":35902,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01RVKGQBU0/pdf_plots.svg","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01RVKGQBU0/download/pdf_plots.svg","permalink":"https://julialang.slack.com/files/UGFMDAMC3/F01RVKGQBU0/pdf_plots.svg","permalink_public":"https://slack-files.com/T68168MUP-F01RVKGQBU0-4d245ddc14","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"VAG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah, I had plotted the pdf surface before and I can see how the density increases near the truncation."}]}]}],"user":"UGFMDAMC3","display_as_bot":false,"ts":"1615654039.077800"},{"client_msg_id":"d132e30c-4d2f-4948-b653-9563df1d554e","type":"message","text":"The above the scenario outlined gives the below logprobs. which is counter-intuitve in the sense that prior probability of the second parameter depends on the first even though the prior was defined independent of the first parameter. That means that `rv2` has a higher prior probability then `rv3` , which pretty much lies near the mean of the prior. Whereas, for the \"rejection\" approach `rv3` has the highest prior probability. Btw, thanks for taking the time to discuss this here.\n\n```julia&gt; rv1 = [0.1,1.5]\n2-element Array{Float64,1}:\n 0.1\n 1.5\n\njulia&gt; rv2 = [1.4,1.5]\n2-element Array{Float64,1}:\n 1.4\n 1.5\n\njulia&gt; reject(rv1)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -4.214965728878737\n\njulia&gt; reject(rv2)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -4.214965728878737\n\njulia&gt; cust_dist(rv2)\n2-element Array{Float64,1}:\n -0.6931471805599453\n  1.349825386941264\n\njulia&gt; cust_dist(rv1)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -4.165995678644322\n\njulia&gt; rv3 = [0.1, 0.6]\n2-element Array{Float64,1}:\n 0.1\n 0.6\n\njulia&gt; cust_dist(rv3)\n2-element Array{Float64,1}:\n -0.6931471805599453\n  0.33400432135567887\n\njulia&gt; reject(rv3)\n2-element Array{Float64,1}:\n -0.6931471805599453\n  0.28503427112126334```\n","user":"UGFMDAMC3","ts":"1615655045.085200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"81H8h","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The above the scenario outlined gives the below logprobs. which is counter-intuitve in the sense that prior probability of the second parameter depends on the first even though the prior was defined independent of the first parameter. That means that "},{"type":"text","text":"rv2","style":{"code":true}},{"type":"text","text":" has a higher prior probability then "},{"type":"text","text":"rv3","style":{"code":true}},{"type":"text","text":" , which pretty much lies near the mean of the prior. Whereas, for the \"rejection\" approach "},{"type":"text","text":"rv3","style":{"code":true}},{"type":"text","text":" has the highest prior probability. Btw, thanks for taking the time to discuss this here.\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> rv1 = [0.1,1.5]\n2-element Array{Float64,1}:\n 0.1\n 1.5\n\njulia> rv2 = [1.4,1.5]\n2-element Array{Float64,1}:\n 1.4\n 1.5\n\njulia> reject(rv1)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -4.214965728878737\n\njulia> reject(rv2)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -4.214965728878737\n\njulia> cust_dist(rv2)\n2-element Array{Float64,1}:\n -0.6931471805599453\n  1.349825386941264\n\njulia> cust_dist(rv1)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -4.165995678644322\n\njulia> rv3 = [0.1, 0.6]\n2-element Array{Float64,1}:\n 0.1\n 0.6\n\njulia> cust_dist(rv3)\n2-element Array{Float64,1}:\n -0.6931471805599453\n  0.33400432135567887\n\njulia> reject(rv3)\n2-element Array{Float64,1}:\n -0.6931471805599453\n  0.28503427112126334"}]},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"02ffdfe6-90db-4fde-8afb-fd8f4069e32a","type":"message","text":"Note that the normalisation constant is a function of `y[1]`. So it’s only constant if you fix `y[1]`. The rejection method is not correct because it ignores this term.","user":"U85JBUGGP","ts":"1615659267.086200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FYQu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Note that the normalisation constant is a function of "},{"type":"text","text":"y[1]","style":{"code":true}},{"type":"text","text":". So it’s only constant if you fix "},{"type":"text","text":"y[1]","style":{"code":true}},{"type":"text","text":". The rejection method is not correct because it ignores this term."}]}]}]},{"client_msg_id":"db1ce0e2-75b2-4bf5-9113-cf2a89c9368e","type":"message","text":"I see. So the enforcing parameter constraints by rejection in a `Turing.jl` is nonsensical? So that `rv2 = [1.4,1.5]` has a higher prior probability then `rv3 = [0.1, 0.6]` is correct if the priors such that `p1 ~ Uniform(0.0,2.0)` and `p2 ~ truncated(Normal(0.6,0.3))` if the constraint is such that `p2 &gt; p1` . What I want to achieve with the prior on `p2`  is that p2 is unlikely to take bigger values such as `1.5` and beyond. The prior was derived from data where the relationship between `p1` and `p2` was not existing. Do I have to define the prior on `p2` differently?","user":"UGFMDAMC3","ts":"1615667108.095700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iD0f","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I see. So the enforcing parameter constraints by rejection in a "},{"type":"text","text":"Turing.jl","style":{"code":true}},{"type":"text","text":" is nonsensical? So that "},{"type":"text","text":"rv2 = [1.4,1.5]","style":{"code":true}},{"type":"text","text":" has a higher prior probability then "},{"type":"text","text":"rv3 = [0.1, 0.6]","style":{"code":true}},{"type":"text","text":" is correct if the priors such that "},{"type":"text","text":"p1 ~ Uniform(0.0,2.0)","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"p2 ~ truncated(Normal(0.6,0.3))","style":{"code":true}},{"type":"text","text":" if the constraint is such that "},{"type":"text","text":"p2 > p1","style":{"code":true}},{"type":"text","text":" . What I want to achieve with the prior on "},{"type":"text","text":"p2","style":{"code":true}},{"type":"text","text":"  is that p2 is unlikely to take bigger values such as "},{"type":"text","text":"1.5","style":{"code":true}},{"type":"text","text":" and beyond. The prior was derived from data where the relationship between "},{"type":"text","text":"p1","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"p2","style":{"code":true}},{"type":"text","text":" was not existing. Do I have to define the prior on "},{"type":"text","text":"p2","style":{"code":true}},{"type":"text","text":" differently?"}]}]}]},{"client_msg_id":"41c88383-5653-4d9f-918f-a3c235ffa02a","type":"message","text":"&gt; I see. So the enforcing parameter constraints by rejection in a `Turing.jl` is nonsensical?\nIn case of stochastic support, I think so yes. I don’t understand the rest of your comment.","user":"U85JBUGGP","ts":"1615669809.098500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"T/MwG","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"I see. So the enforcing parameter constraints by rejection in a "},{"type":"text","text":"Turing.jl","style":{"code":true}},{"type":"text","text":" is nonsensical?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"In case of stochastic support, I think so yes. I don’t understand the rest of your comment."}]}]}]},{"client_msg_id":"f22b7b47-3e4e-42b2-8c85-fda1ae988c38","type":"message","text":"Maybe it is just counter-intuitive to me but let consider the following parameter sets: If `parset=[p1,p2]` and  `parset4 = [1.999, 1.999111]` and `parset5 = [0.00001, 0.6]` and with priors such that `p1 ~ Uniform(0.0,1.0)` and `p2~ Normal(0.6,0.3)` you get:\n\n```julia&gt; function cust_dist(y)\n           x = similar(y)\n           x[1] = logpdf(Uniform(0.0,2.0), y[1])\n           x[2] = logpdf(truncated(Normal(0.6,.3), y[2]), y[1], Inf)\n           return x\n       end\n\njulia&gt; parset4 = [1.999, 1.999111]\n2-element Array{Float64,1}:\n 1.999\n 1.999111\n\njulia&gt; cust_dist(parset4)\n2-element Array{Float64,1}:\n -0.6931471805599453\n  2.7835928275731323\n\njulia&gt; parset5 = [0.00001, 0.6]\n2-element Array{Float64,1}:\n 1.0e-5\n 0.6\n\njulia&gt; cust_dist(parset5)\n2-element Array{Float64,1}:\n -0.6931471805599453\n  0.3080490221087328```\nAnd thus merely from a prior probability perspective `parset4` has a higher probability then `parset5` , which would mean in a situation where the prior dominates and little information coming from the likelihood the posterior estimates would lie near the neighbourhood of `parset4`, right? That is even though prior of `p2` would suggest that the highest probability density is near `mu=0.6` . I hope this is a a bit clearer. Let me know if not","user":"UGFMDAMC3","ts":"1615675026.113300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"63+I","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Maybe it is just counter-intuitive to me but let consider the following parameter sets: If "},{"type":"text","text":"parset=[p1,p2]","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":" parset4 = [1.999, 1.999111]","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"parset5 = [0.00001, 0.6]","style":{"code":true}},{"type":"text","text":" and with priors such that "},{"type":"text","text":"p1 ~ Uniform(0.0,1.0)","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"p2~ Normal(0.6,0.3)","style":{"code":true}},{"type":"text","text":" you get:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> function cust_dist(y)\n           x = similar(y)\n           x[1] = logpdf(Uniform(0.0,2.0), y[1])\n           x[2] = logpdf(truncated(Normal(0.6,.3), y[2]), y[1], Inf)\n           return x\n       end\n\njulia> parset4 = [1.999, 1.999111]\n2-element Array{Float64,1}:\n 1.999\n 1.999111\n\njulia> cust_dist(parset4)\n2-element Array{Float64,1}:\n -0.6931471805599453\n  2.7835928275731323\n\njulia> parset5 = [0.00001, 0.6]\n2-element Array{Float64,1}:\n 1.0e-5\n 0.6\n\njulia> cust_dist(parset5)\n2-element Array{Float64,1}:\n -0.6931471805599453\n  0.3080490221087328"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"And thus merely from a prior probability perspective "},{"type":"text","text":"parset4","style":{"code":true}},{"type":"text","text":" has a higher probability then "},{"type":"text","text":"parset5","style":{"code":true}},{"type":"text","text":" , which would mean in a situation where the prior dominates and little information coming from the likelihood the posterior estimates would lie near the neighbourhood of "},{"type":"text","text":"parset4","style":{"code":true}},{"type":"text","text":", right? That is even though prior of "},{"type":"text","text":"p2","style":{"code":true}},{"type":"text","text":" would suggest that the highest probability density is near "},{"type":"text","text":"mu=0.6","style":{"code":true}},{"type":"text","text":" . I hope this is a a bit clearer. Let me know if not"}]}]}]},{"client_msg_id":"592cc44d-be2a-4a05-8959-44fcd3a96056","type":"message","text":"Note that the prior probability of `p2 = 0.6` is not as high as you may think it is. This is a multivariate distribution after all so you need to look at the probability of both variables simultaneously. If you analyse the effect of `p1`, you see that `p2` can only be 0.6 if `p1 &lt; 0.6` which has a probability of `0.3`. `P(p2 = 0.6 | p1)` is itself a function of `p1` because of the normalisation constant. Perhaps, the semantics you are going for are not compatible with a truncated normal distribution for `p2` with stochastic support. Maybe you want to have the mean of `p2` offset from `p1` by a constant to maintain a constant shape in the truncated distribution? Or you may not even want normal distribution but exponential.","user":"U85JBUGGP","ts":"1615676100.118900","team":"T68168MUP","edited":{"user":"U85JBUGGP","ts":"1615676177.000000"},"blocks":[{"type":"rich_text","block_id":"TjW9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Note that the prior probability of "},{"type":"text","text":"p2 = 0.6","style":{"code":true}},{"type":"text","text":" is not as high as you may think it is. This is a multivariate distribution after all so you need to look at the probability of both variables simultaneously. If you analyse the effect of "},{"type":"text","text":"p1","style":{"code":true}},{"type":"text","text":", you see that "},{"type":"text","text":"p2","style":{"code":true}},{"type":"text","text":" can only be 0.6 if "},{"type":"text","text":"p1 < 0.6","style":{"code":true}},{"type":"text","text":" which has a probability of "},{"type":"text","text":"0.3","style":{"code":true}},{"type":"text","text":". "},{"type":"text","text":"P(p2 = 0.6 | p1)","style":{"code":true}},{"type":"text","text":" is itself a function of "},{"type":"text","text":"p1","style":{"code":true}},{"type":"text","text":" because of the normalisation constant. Perhaps, the semantics you are going for are not compatible with a truncated normal distribution for "},{"type":"text","text":"p2","style":{"code":true}},{"type":"text","text":" with stochastic support. Maybe you want to have the mean of "},{"type":"text","text":"p2","style":{"code":true}},{"type":"text","text":" offset from "},{"type":"text","text":"p1","style":{"code":true}},{"type":"text","text":" by a constant to maintain a constant shape in the truncated distribution? Or you may not even want normal distribution but exponential."}]}]}],"thread_ts":"1615676100.118900","reply_count":4,"reply_users_count":2,"latest_reply":"1615677521.122100","reply_users":["UGFMDAMC3","U85JBUGGP"],"subscribed":false},{"client_msg_id":"402fe937-6ce8-459e-981f-359b92d630af","type":"message","text":"So if the correct pdf doesn’t look like what you want, then change the distribution to make it look like what you want. But rejection sampling assumes that the “normalisation constant” is constant wrt `p1` which is not always the case with stochastic support.","user":"U85JBUGGP","ts":"1615676758.120900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tCH7t","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So if the correct pdf doesn’t look like what you want, then change the distribution to make it look like what you want. But rejection sampling assumes that the “normalisation constant” is constant wrt "},{"type":"text","text":"p1","style":{"code":true}},{"type":"text","text":" which is not always the case with stochastic support."}]}]}]},{"client_msg_id":"49b42c15-4ee7-46aa-a23b-0aefe04cbe0d","type":"message","text":"Hello Turing community, how do I compute predictions for `Turing.addlogprob!` ? MWE\n\n```@model function gdemo(y)\n    mu ~ Normal(0,2)\n    for i in eachindex(y)\n        y[i] ~ Normal(mu,1)\n    end\nend\n\n@model function gdemoFail(y)\n    mu ~ Normal(0,2)\n    for i in eachindex(y)\n        Turing.@addlogprob! logpdf(Normal(mu, 1), y[i])\n    end\nend\n\ny = randn(50)\nchn = sample(gdemo(y), NUTS(), 100)\nyTest = Vector{Union{Missing,Float64}}(undef, length(y))\n\ntestModel = gdemo(yTest)\npredictions = predict(testModel, chn)\n\ntestModelFail = gdemoFail(yTest)\npredictionsFail = predict(testModelFail, chn)\njulia&gt; ERROR: MethodError: no method matching logpdf(::Normal{Float64}, ::Missing).....................```","user":"U011PPW7K53","ts":"1615717913.125700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/Ec+Q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello Turing community, how do I compute predictions for "},{"type":"text","text":"Turing.addlogprob!","style":{"code":true}},{"type":"text","text":" ? MWE\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@model function gdemo(y)\n    mu ~ Normal(0,2)\n    for i in eachindex(y)\n        y[i] ~ Normal(mu,1)\n    end\nend\n\n@model function gdemoFail(y)\n    mu ~ Normal(0,2)\n    for i in eachindex(y)\n        Turing.@addlogprob! logpdf(Normal(mu, 1), y[i])\n    end\nend\n\ny = randn(50)\nchn = sample(gdemo(y), NUTS(), 100)\nyTest = Vector{Union{Missing,Float64}}(undef, length(y))\n\ntestModel = gdemo(yTest)\npredictions = predict(testModel, chn)\n\ntestModelFail = gdemoFail(yTest)\npredictionsFail = predict(testModelFail, chn)\njulia> ERROR: MethodError: no method matching logpdf(::Normal{Float64}, ::Missing)....................."}]}]}]},{"client_msg_id":"0913ebd4-edeb-492a-81f3-31eb69f72101","type":"message","text":"Some help with this type stability / AD issue would be greatly appreciated (details in thread)","user":"U017YGFQTE3","ts":"1615732094.126800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uFVsZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Some help with this type stability / AD issue would be greatly appreciated (details in thread)"}]}]}],"thread_ts":"1615732094.126800","reply_count":2,"reply_users_count":1,"latest_reply":"1615732504.127300","reply_users":["U017YGFQTE3"],"subscribed":false},{"client_msg_id":"fc6dcc54-34c2-4728-a794-3415120e58e8","type":"message","text":"A question to Turing Specialists.\n\nI performed a QR decomposition on my model matrix `X`.\n\n```#### QR Decomposition ####\nQ, R = qr(X)```\nThen i fitted a Turing Model to my Q Matrix, casting it to a `Matrix`:\n```model_qr = varying_intercept(Matrix(Q), idx, float(y));\nchn_qr = sample(model_qr, NUTS(1_000, 0.65), MCMCThreads(), 2_000, 4);```\nThis is the output:\n```julia&gt; chn_qr\nChains MCMC chain (2000×24×4 Array{Float64,3}):\n\nIterations        = 1:2000\nThinning interval = 1\nChains            = 1, 2, 3, 4\nSamples per chain = 2000\nparameters        = β[1], β[2], μ, μⱼ[1], μⱼ[2], μⱼ[3], μⱼ[4], μⱼ[5], μⱼ[6], μⱼ[7], σ, σⱼ\ninternals         = acceptance_rate, hamiltonian_energy, hamiltonian_energy_error, is_accept, log_density, lp, max_hamiltonian_energy_error, n_steps, nom_step_size, numerical_error, step_size, tree_depth\n\nSummary Statistics\n  parameters       mean       std   naive_se      mcse         ess      rhat \n      Symbol    Float64   Float64    Float64   Float64     Float64   Float64 \n\n        β[1]     0.0615    1.6131     0.0180    0.0242   5229.9895    0.9997\n        β[2]   -46.6065    4.4243     0.0495    0.0556   6779.8184    1.0009\n           μ    23.2964    1.7568     0.0196    0.0354   1548.2673    1.0012\n       μⱼ[1]     1.3817    1.8077     0.0202    0.0364   1599.0513    1.0011\n       μⱼ[2]     1.6890    1.7905     0.0200    0.0362   1604.2076    1.0009\n       μⱼ[3]    -4.0131    1.7790     0.0199    0.0355   1565.3781    1.0010\n       μⱼ[4]     5.8102    2.0615     0.0230    0.0377   2110.1449    1.0009\n       μⱼ[5]    -2.0791    1.8817     0.0210    0.0356   1732.1206    1.0011\n       μⱼ[6]    -5.2884    1.7996     0.0201    0.0362   1600.3742    1.0012\n       μⱼ[7]     1.9578    1.8095     0.0202    0.0365   1607.9793    1.0011\n           σ     2.6639    0.1189     0.0013    0.0014   7292.1689    1.0006\n          σⱼ     4.2218    1.4394     0.0161    0.0266   3021.6447    1.0011\n\nQuantiles\n  parameters       2.5%      25.0%      50.0%      75.0%      97.5% \n      Symbol    Float64    Float64    Float64    Float64    Float64 \n\n        β[1]    -2.9792    -0.7044     0.0580     0.8240     3.1977\n        β[2]   -55.0442   -49.6002   -46.6371   -43.6904   -37.9659\n           μ    19.9347    22.2450    23.2709    24.3002    26.9628\n       μⱼ[1]    -2.3245     0.3304     1.3992     2.4828     4.9164\n       μⱼ[2]    -2.0222     0.6622     1.6990     2.7723     5.1757\n       μⱼ[3]    -7.7450    -5.0484    -3.9772    -2.9457    -0.6303\n       μⱼ[4]     1.7884     4.5075     5.7806     7.0934     9.8737\n       μⱼ[5]    -6.0375    -3.1619    -2.0521    -0.9130     1.5270\n       μⱼ[6]    -9.0518    -6.3368    -5.2189    -4.1916    -1.8506\n       μⱼ[7]    -1.8358     0.9237     1.9936     3.0302     5.4514\n           σ     2.4414     2.5788     2.6598     2.7444     2.9033\n          σⱼ     2.4002     3.2597     3.9161     4.8519     7.7719```\nI've managed to reconstruct the quantiles of β by multiplying R^-1 * β (in a very ugly non-elegant way):\n```quantiles_beta = select(DataFrame(quantile(group(chn_qr, :β))), r\"%\");\nmapcols(x -&gt; R^-1 * x, quantiles_beta)\n2×5 DataFrame\n Row │ 2.5%         25.0%        50.0%        75.0%        97.5%       \n     │ Float64      Float64      Float64      Float64      Float64     \n─────┼─────────────────────────────────────────────────────────────────\n   1 │ -2.56661     -2.34772     -2.22018     -2.09349     -1.86299\n   2 │  0.00516392   0.00465319   0.00437521   0.00409877   0.00356173```\n*How would I apply R^-1 * β to all the β in the `chn_qr` object so that I can create a new \"`chn_qr_reconstructed`\" object and perform all the methods that uses `Chain` to it?*\n\nBTW: QR reparameterization is 19.6s sampling and using X is 135.6s sampling (with a way better ESS overall)","user":"U01QBF4PHKP","ts":"1615799946.141000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WmSC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"A question to Turing Specialists.\n\nI performed a QR decomposition on my model matrix "},{"type":"text","text":"X","style":{"code":true}},{"type":"text","text":".\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"#### QR Decomposition ####\nQ, R = qr(X)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nThen i fitted a Turing Model to my Q Matrix, casting it to a "},{"type":"text","text":"Matrix","style":{"code":true}},{"type":"text","text":":\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"model_qr = varying_intercept(Matrix(Q), idx, float(y));\nchn_qr = sample(model_qr, NUTS(1_000, 0.65), MCMCThreads(), 2_000, 4);"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nThis is the output:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> chn_qr\nChains MCMC chain (2000×24×4 Array{Float64,3}):\n\nIterations        = 1:2000\nThinning interval = 1\nChains            = 1, 2, 3, 4\nSamples per chain = 2000\nparameters        = β[1], β[2], μ, μⱼ[1], μⱼ[2], μⱼ[3], μⱼ[4], μⱼ[5], μⱼ[6], μⱼ[7], σ, σⱼ\ninternals         = acceptance_rate, hamiltonian_energy, hamiltonian_energy_error, is_accept, log_density, lp, max_hamiltonian_energy_error, n_steps, nom_step_size, numerical_error, step_size, tree_depth\n\nSummary Statistics\n  parameters       mean       std   naive_se      mcse         ess      rhat \n      Symbol    Float64   Float64    Float64   Float64     Float64   Float64 \n\n        β[1]     0.0615    1.6131     0.0180    0.0242   5229.9895    0.9997\n        β[2]   -46.6065    4.4243     0.0495    0.0556   6779.8184    1.0009\n           μ    23.2964    1.7568     0.0196    0.0354   1548.2673    1.0012\n       μⱼ[1]     1.3817    1.8077     0.0202    0.0364   1599.0513    1.0011\n       μⱼ[2]     1.6890    1.7905     0.0200    0.0362   1604.2076    1.0009\n       μⱼ[3]    -4.0131    1.7790     0.0199    0.0355   1565.3781    1.0010\n       μⱼ[4]     5.8102    2.0615     0.0230    0.0377   2110.1449    1.0009\n       μⱼ[5]    -2.0791    1.8817     0.0210    0.0356   1732.1206    1.0011\n       μⱼ[6]    -5.2884    1.7996     0.0201    0.0362   1600.3742    1.0012\n       μⱼ[7]     1.9578    1.8095     0.0202    0.0365   1607.9793    1.0011\n           σ     2.6639    0.1189     0.0013    0.0014   7292.1689    1.0006\n          σⱼ     4.2218    1.4394     0.0161    0.0266   3021.6447    1.0011\n\nQuantiles\n  parameters       2.5%      25.0%      50.0%      75.0%      97.5% \n      Symbol    Float64    Float64    Float64    Float64    Float64 \n\n        β[1]    -2.9792    -0.7044     0.0580     0.8240     3.1977\n        β[2]   -55.0442   -49.6002   -46.6371   -43.6904   -37.9659\n           μ    19.9347    22.2450    23.2709    24.3002    26.9628\n       μⱼ[1]    -2.3245     0.3304     1.3992     2.4828     4.9164\n       μⱼ[2]    -2.0222     0.6622     1.6990     2.7723     5.1757\n       μⱼ[3]    -7.7450    -5.0484    -3.9772    -2.9457    -0.6303\n       μⱼ[4]     1.7884     4.5075     5.7806     7.0934     9.8737\n       μⱼ[5]    -6.0375    -3.1619    -2.0521    -0.9130     1.5270\n       μⱼ[6]    -9.0518    -6.3368    -5.2189    -4.1916    -1.8506\n       μⱼ[7]    -1.8358     0.9237     1.9936     3.0302     5.4514\n           σ     2.4414     2.5788     2.6598     2.7444     2.9033\n          σⱼ     2.4002     3.2597     3.9161     4.8519     7.7719"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nI've managed to reconstruct the quantiles of β by multiplying R^-1 * β (in a very ugly non-elegant way):\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"quantiles_beta = select(DataFrame(quantile(group(chn_qr, :β))), r\"%\");\nmapcols(x -> R^-1 * x, quantiles_beta)\n2×5 DataFrame\n Row │ 2.5%         25.0%        50.0%        75.0%        97.5%       \n     │ Float64      Float64      Float64      Float64      Float64     \n─────┼─────────────────────────────────────────────────────────────────\n   1 │ -2.56661     -2.34772     -2.22018     -2.09349     -1.86299\n   2 │  0.00516392   0.00465319   0.00437521   0.00409877   0.00356173"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\n"},{"type":"text","text":"How would I apply R^-1 * β to all the β in the ","style":{"bold":true}},{"type":"text","text":"chn_qr","style":{"bold":true,"code":true}},{"type":"text","text":" object so that I can create a new \"","style":{"bold":true}},{"type":"text","text":"chn_qr_reconstructed","style":{"bold":true,"code":true}},{"type":"text","text":"\" object and perform all the methods that uses ","style":{"bold":true}},{"type":"text","text":"Chain","style":{"bold":true,"code":true}},{"type":"text","text":" to it?","style":{"bold":true}},{"type":"text","text":"\n\nBTW: QR reparameterization is 19.6s sampling and using X is 135.6s sampling (with a way better ESS overall)"}]}]}]},{"client_msg_id":"597ce232-6a21-493d-aa06-4f87e9043153","type":"message","text":"Small question. I'm trying to stick as close to the mathematical definition as possible [here](<https://statisticalrethinkingjulia.github.io/TuringModels.jl/models/varying-intercepts-reedfrogs/>) and wonder if it is possible to use a function for the link. Thanks!","user":"U01BTNDCUBX","ts":"1615823316.144800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UnD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Small question. I'm trying to stick as close to the mathematical definition as possible [here]("},{"type":"link","url":"https://statisticalrethinkingjulia.github.io/TuringModels.jl/models/varying-intercepts-reedfrogs/"},{"type":"text","text":") and wonder if it is possible to use a function for the link. Thanks!"}]}]}],"thread_ts":"1615823316.144800","reply_count":5,"reply_users_count":3,"latest_reply":"1615824042.145900","reply_users":["U7QLM6E2E","U01C2AJ9F63","U01QBF4PHKP"],"subscribed":false},{"client_msg_id":"056c0b0e-0d3d-4ee5-bb25-0b7ea3452043","type":"message","text":"I thought the Bayesian logistic regression example didn't look very \"Turian\" so I made some updates to it. I also reparameterized it for even *more* speed and efficiency\n\n```train_F = qr([ones(size(train,1)) train]);\ntrain_Q_ast = Matrix(train_F.Q) * sqrt(size(train,1)-1)\ntrain_R_ast = train_F.R / sqrt(size(train,1)-1)\ntrain_R_ast_inv = inv(train_R_ast)\n\n# Bayesian logistic regression (LR)\n@model logistic_regression_new(Q_ast, y, p, σ) = begin\n    theta ~ MvNormal(zeros(p), σ)\n    y ~ Distributions.Product(Bernoulli.(logistic.(Q_ast * theta)))\nend;\n# Retrieve the number of observations.\nn, p = size([ones(size(train,1)) train])\n\n# Sample using HMC.\n@time chain = mapreduce(c -&gt; sample(logistic_regression_new(train_Q_ast, train_label, p, 1), HMC(0.05, 10), 1500),\n    chainscat,\n    1:3\n)\n\ndescribe(chain)\n\nbeta = mapslices(x-&gt; train_R_ast_inv * x, chain[:,namesingroup(chain, :theta),:].value.data, dims=[2])\ntransformed_chain = Chains(beta, [\"Intercept\", \"student\", \"balance\", \"income\"]```\nOn my machine the original code gives\n```  28.732436 seconds (304.93 M allocations: 10.426 GiB, 9.61% gc time)```\nWhile the new code gives\n```  4.446898 seconds (8.36 M allocations: 2.351 GiB, 10.44% gc time)```\nPlus almost 6X increase in ESS :slightly_smiling_face:","user":"U6CFMFM2R","ts":"1615827429.147800","team":"T68168MUP","edited":{"user":"U6CFMFM2R","ts":"1615827586.000000"},"blocks":[{"type":"rich_text","block_id":"xGm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I thought the Bayesian logistic regression example didn't look very \"Turian\" so I made some updates to it. I also reparameterized it for even "},{"type":"text","text":"more","style":{"bold":true}},{"type":"text","text":" speed and efficiency\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"train_F = qr([ones(size(train,1)) train]);\ntrain_Q_ast = Matrix(train_F.Q) * sqrt(size(train,1)-1)\ntrain_R_ast = train_F.R / sqrt(size(train,1)-1)\ntrain_R_ast_inv = inv(train_R_ast)\n\n# Bayesian logistic regression (LR)\n@model logistic_regression_new(Q_ast, y, p, σ) = begin\n    theta ~ MvNormal(zeros(p), σ)\n    y ~ Distributions.Product(Bernoulli.(logistic.(Q_ast * theta)))\nend;\n# Retrieve the number of observations.\nn, p = size([ones(size(train,1)) train])\n\n# Sample using HMC.\n@time chain = mapreduce(c -> sample(logistic_regression_new(train_Q_ast, train_label, p, 1), HMC(0.05, 10), 1500),\n    chainscat,\n    1:3\n)\n\ndescribe(chain)\n\nbeta = mapslices(x-> train_R_ast_inv * x, chain[:,namesingroup(chain, :theta),:].value.data, dims=[2])\ntransformed_chain = Chains(beta, [\"Intercept\", \"student\", \"balance\", \"income\"]"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"On my machine the original code gives\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"  28.732436 seconds (304.93 M allocations: 10.426 GiB, 9.61% gc time)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"While the new code gives\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"  4.446898 seconds (8.36 M allocations: 2.351 GiB, 10.44% gc time)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Plus almost 6X increase in ESS "},{"type":"emoji","name":"slightly_smiling_face"}]}]}],"thread_ts":"1615827429.147800","reply_count":1,"reply_users_count":1,"latest_reply":"1615829809.148100","reply_users":["U01QBF4PHKP"],"subscribed":false,"reactions":[{"name":"tada","users":["UC0SY9JFP","U01QBF4PHKP"],"count":2},{"name":"fast_parrot","users":["U01QBF4PHKP"],"count":1}]},{"client_msg_id":"d0e26400-7390-4850-bcd5-0fabb6ee3d61","type":"message","text":"Are there any examples floating around of doing prior updating with Turing? i.e. it's not clear to me how I would use my previous posterior samples to specify the priors of a subsequent fit.","user":"U01H36BUDJB","ts":"1615884517.003400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2rZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Are there any examples floating around of doing prior updating with Turing? i.e. it's not clear to me how I would use my previous posterior samples to specify the priors of a subsequent fit."}]}]}]},{"client_msg_id":"8d746320-140f-42e8-b68c-990185b938d7","type":"message","text":"I don't see an implementation of the p-lag autoregressive likelihood in Turing or Distributions.jl. Is there another package that has it, perhaps?","user":"U01H36BUDJB","ts":"1615973192.009500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hXdt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I don't see an implementation of the p-lag autoregressive likelihood in Turing or Distributions.jl. Is there another package that has it, perhaps?"}]}]}]},{"client_msg_id":"50ff929b-cbab-46fa-bbf9-418b8fe11ab6","type":"message","text":"Any state space modelling package allows you to do that such as <https://lampspuc.github.io/StateSpaceModels.jl/latest/manual/#StateSpaceModels.SARIMA> can’t find the likelihood function in that package though","user":"U6C937ENB","ts":"1615974793.010700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jLH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Any state space modelling package allows you to do that such as "},{"type":"link","url":"https://lampspuc.github.io/StateSpaceModels.jl/latest/manual/#StateSpaceModels.SARIMA"},{"type":"text","text":" can’t find the likelihood function in that package though"}]}]}],"reactions":[{"name":"+1","users":["U01H36BUDJB"],"count":1}]},{"client_msg_id":"c504b259-2026-471c-b270-7f6e7b0e6079","type":"message","text":"Kalman.jl if you set up your state space model yourself","user":"U6C937ENB","ts":"1615974804.011000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vv82","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Kalman.jl if you set up your state space model yourself"}]}]}]},{"client_msg_id":"21530b6b-1b09-42c9-87c9-ff88217702d9","type":"message","text":"Hi. I was trying to make a custom distribution for Turing to help avoid loops, and to make certain common operations easier. I have to define a bijector for the distribution. My distribution is a Continuous matrix-variate, so it should be an identity bijector, but I see things like Identity{0}, Identity{1}, etc. What does that number refer to? The dimensionality of the transformation? Something else?","user":"UGFD16K6D","ts":"1615986916.014600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ZPf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi. I was trying to make a custom distribution for Turing to help avoid loops, and to make certain common operations easier. I have to define a bijector for the distribution. My distribution is a Continuous matrix-variate, so it should be an identity bijector, but I see things like Identity{0}, Identity{1}, etc. What does that number refer to? The dimensionality of the transformation? Something else?"}]}]}],"thread_ts":"1615986916.014600","reply_count":1,"reply_users_count":1,"latest_reply":"1615986968.014700","reply_users":["UGFD16K6D"],"subscribed":false},{"client_msg_id":"d3c99072-9c1a-4570-80f4-55094fcc1c94","type":"message","text":"Hi! Could you send me a tutorial of customized drawing from MCMC for Turing inference?","user":"U01Q398M3QB","ts":"1616016461.016900","team":"T68168MUP","edited":{"user":"U01Q398M3QB","ts":"1616017564.000000"},"blocks":[{"type":"rich_text","block_id":"5W/U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi! Could you send me a tutorial of customized drawing from MCMC for Turing inference?"}]}]}]},{"client_msg_id":"dff1a8bc-0661-4171-a9a0-e65d8728c7b3","type":"message","text":"What kind of customized plotting are you looking for? Suppose you already know how to extract parameters from the chain (see <https://github.com/TuringLang/MCMCChains.jl>), you could do any plots on the results using Plots.jl; or if you want to define custom plots on the chain directly, see <https://github.com/TuringLang/MCMCChains.jl/blob/master/src/plot.jl>.","user":"U6H9SJKCH","ts":"1616017550.018700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ed6aN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What kind of customized plotting are you looking for? Suppose you already know how to extract parameters from the chain (see "},{"type":"link","url":"https://github.com/TuringLang/MCMCChains.jl"},{"type":"text","text":"), you could do any plots on the results using Plots.jl; or if you want to define custom plots on the chain directly, see "},{"type":"link","url":"https://github.com/TuringLang/MCMCChains.jl/blob/master/src/plot.jl"},{"type":"text","text":"."}]}]}]},{"client_msg_id":"f026a53a-f7f1-4c95-bcac-2f07663e479c","type":"message","text":"<@U6H9SJKCH> Sorry for the unclear description. I met some obstacles that MvNormal() distribution doesn't accept non-positive definite covariance matrix. I tried some other method to force it to be symmetry and positive definite. However, those methods could not apply to the dual type variables in Turing process. Thus, I want to define a customMvNormal distribtuion which use svd decomposition instead of Cholesky() in the original MvNormal()","user":"U01Q398M3QB","ts":"1616017805.019600","team":"T68168MUP","edited":{"user":"U01Q398M3QB","ts":"1616017963.000000"},"blocks":[{"type":"rich_text","block_id":"Rp1US","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U6H9SJKCH"},{"type":"text","text":" Sorry for the unclear description. I met some obstacles that MvNormal() distribution doesn't accept non-positive definite covariance matrix. I tried some other method to force it to be symmetry and positive definite. However, those methods could not apply to the dual type variables in Turing process. Thus, I want to define a customMvNormal distribtuion which use svd decomposition instead of Cholesky() in the original MvNormal()"}]}]}]},{"client_msg_id":"5174db21-4045-4548-8a4d-7990d1c9577d","type":"message","text":"<https://turing.ml/dev/docs/using-turing/advanced#how-to-define-a-customized-distribution>","user":"U6H9SJKCH","ts":"1616020342.020100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3wWuF","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://turing.ml/dev/docs/using-turing/advanced#how-to-define-a-customized-distribution"}]}]}]},{"client_msg_id":"db063ca1-f7c1-4317-8a1f-a3f964dfb50a","type":"message","text":"Also checkout examples in <https://github.com/TuringLang/DistributionsAD.jl>","user":"U6H9SJKCH","ts":"1616020400.020300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PsV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Also checkout examples in "},{"type":"link","url":"https://github.com/TuringLang/DistributionsAD.jl"}]}]}],"thread_ts":"1616020400.020300","reply_count":1,"reply_users_count":1,"latest_reply":"1616020572.020400","reply_users":["U01Q398M3QB"],"subscribed":false},{"client_msg_id":"47A9B395-85EE-431A-A4AC-691B83473644","type":"message","text":"We could try to get <https://github.com/mschauer/GaussianDistributions.jl|https://github.com/mschauer/GaussianDistributions.jl> working for Turing, that would also be a good exercise for the work on <https://github.com/cscherrer/MeasureTheory.jl|https://github.com/cscherrer/MeasureTheory.jl>","user":"U6C937ENB","ts":"1616052406.022000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jRh8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We could try to get "},{"type":"link","url":"https://github.com/mschauer/GaussianDistributions.jl","text":"https://github.com/mschauer/GaussianDistributions.jl"},{"type":"text","text":" working for Turing, that would also be a good exercise for the work on "},{"type":"link","url":"https://github.com/cscherrer/MeasureTheory.jl","text":"https://github.com/cscherrer/MeasureTheory.jl"}]}]}],"thread_ts":"1616052406.022000","reply_count":1,"reply_users_count":1,"latest_reply":"1616053032.022300","reply_users":["U8T9JUA5R"],"subscribed":false,"reactions":[{"name":"+1","users":["UC0SY9JFP","UE98VNG4U"],"count":2}]},{"client_msg_id":"0f48988d-dafd-48f9-93c0-7c61b1613f07","type":"message","text":"Is there a recommended way to deal with the integrator terminating early in a bayesian differential equation model? i.e. something similar to this <https://diffeqparamestim.sciml.ai/dev/methods/optimization_based_methods/#Note-About-Loss-Functions>","user":"U01C1H3GLLU","ts":"1616085082.024800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DhEv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a recommended way to deal with the integrator terminating early in a bayesian differential equation model? i.e. something similar to this "},{"type":"link","url":"https://diffeqparamestim.sciml.ai/dev/methods/optimization_based_methods/#Note-About-Loss-Functions"}]}]}]},{"client_msg_id":"eb3b0695-8054-4c63-80f8-9a6ce83174db","type":"message","text":"maybe a naive question, but I'm trying to understand whether Turing can be also used to compute expectations. I have a scalar-valued function `f(p, z)` where `p` are some parameters (which I would like to optimize) and `z` is distributed as, eg, a multivariate Gaussian (though other distributions may be possible).\n\nComputing this expectation (integrating over `z`) should ideally be fast, so I'd like to avoid sampling approaches in favor of faster approximate solutions. Does it make sense to try and use Turing to compute the expected value `f(p, z)`?","user":"U6BJ9E351","ts":"1616250233.029400","team":"T68168MUP","edited":{"user":"U6BJ9E351","ts":"1616250837.000000"},"blocks":[{"type":"rich_text","block_id":"zSH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"maybe a naive question, but I'm trying to understand whether Turing can be also used to compute expectations. I have a scalar-valued function "},{"type":"text","text":"f(p, z)","style":{"code":true}},{"type":"text","text":" where "},{"type":"text","text":"p","style":{"code":true}},{"type":"text","text":" are some parameters (which I would like to optimize) and "},{"type":"text","text":"z","style":{"code":true}},{"type":"text","text":" is distributed as, eg, a multivariate Gaussian (though other distributions may be possible).\n\nComputing this expectation (integrating over "},{"type":"text","text":"z","style":{"code":true}},{"type":"text","text":") should ideally be fast, so I'd like to avoid sampling approaches in favor of faster approximate solutions. Does it make sense to try and use Turing to compute the expected value "},{"type":"text","text":"f(p, z)","style":{"code":true}},{"type":"text","text":"?"}]}]}],"thread_ts":"1616250233.029400","reply_count":3,"reply_users_count":3,"latest_reply":"1616251298.030000","reply_users":["U6BJ9E351","UJDHH8CG4","U8T9JUA5R"],"subscribed":false},{"client_msg_id":"b0242599-d7d0-4ed7-aa72-18cf35c2067e","type":"message","text":"Hello everyone, I was wondering if the GSOC applications for Turing projects are still open for appying?","user":"U013EAW0Z1V","ts":"1616346883.031000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VBcS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello everyone, I was wondering if the GSOC applications for Turing projects are still open for appying?"}]}]}]},{"client_msg_id":"e6793018-7fc7-40e3-9033-3521330c672e","type":"message","text":"Hi <@UHDNY2YMA>, I was looking at the JuliaCon talk <https://www.youtube.com/watch?v=ekF8ApjPYWk> and was wondering where I can find the example code that is shown in the end, scripts/nf_banana.jl? Thanks.","user":"U018F5W2H24","ts":"1616371484.032500","team":"T68168MUP","attachments":[{"service_name":"YouTube","service_url":"https://www.youtube.com/","title":"JuliaCon 2020 | Bijectors.jl: Transforming probability distributions in Julia | Tor Erlend Fjelde","title_link":"https://www.youtube.com/watch?v=ekF8ApjPYWk","author_name":"The Julia Programming Language","author_link":"https://www.youtube.com/c/TheJuliaLanguage","thumb_url":"https://i.ytimg.com/vi/ekF8ApjPYWk/hqdefault.jpg","thumb_width":480,"thumb_height":360,"fallback":"YouTube Video: JuliaCon 2020 | Bijectors.jl: Transforming probability distributions in Julia | Tor Erlend Fjelde","video_html":"<iframe width=\"400\" height=\"225\" src=\"https://www.youtube.com/embed/ekF8ApjPYWk?feature=oembed&autoplay=1&iv_load_policy=3\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>","video_html_width":400,"video_html_height":225,"from_url":"https://www.youtube.com/watch?v=ekF8ApjPYWk","service_icon":"https://a.slack-edge.com/80588/img/unfurl_icons/youtube.png","id":1,"original_url":"https://www.youtube.com/watch?v=ekF8ApjPYWk"}],"blocks":[{"type":"rich_text","block_id":"8JIFj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi "},{"type":"user","user_id":"UHDNY2YMA"},{"type":"text","text":", I was looking at the JuliaCon talk "},{"type":"link","url":"https://www.youtube.com/watch?v=ekF8ApjPYWk"},{"type":"text","text":" and was wondering where I can find the example code that is shown in the end, scripts/nf_banana.jl? Thanks."}]}]}]},{"client_msg_id":"5873a25a-8a1b-4da1-8968-8d7b40e746ef","type":"message","text":"Does anyone have suggestions for debugging poor sampler performance? I have a relatively simply multivariate regression (all covariates are computed independently) and it's really ridiculously slow with NUTS and makes an enormous number of allocations.","user":"U01H36BUDJB","ts":"1616406595.034100","team":"T68168MUP","edited":{"user":"U01H36BUDJB","ts":"1616406607.000000"},"blocks":[{"type":"rich_text","block_id":"j+V","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does anyone have suggestions for debugging poor sampler performance? I have a relatively simply multivariate regression (all covariates are computed independently) and it's really ridiculously slow with NUTS and makes an enormous number of allocations."}]}]}],"thread_ts":"1616406595.034100","reply_count":7,"reply_users_count":2,"latest_reply":"1616408569.036400","reply_users":["UC0SY9JFP","U01H36BUDJB"],"subscribed":false},{"client_msg_id":"946b04cb-2263-4517-b847-dbe176e90a67","type":"message","text":"How do we access divergences data in NUTS? (Something like Stan diagnostics)","user":"U01QBF4PHKP","ts":"1616491111.039500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Yzc8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How do we access divergences data in NUTS? (Something like Stan diagnostics)"}]}]}],"thread_ts":"1616491111.039500","reply_count":2,"reply_users_count":1,"latest_reply":"1616491435.040400","reply_users":["U01C2AJ9F63"],"subscribed":false},{"client_msg_id":"d3b92171-68b7-4bf6-8876-67971f99c214","type":"message","text":"I'm having some troubles with the Turing API. Given a model (as for example the one described in <https://turing.ml/dev/docs/using-turing/get-started>), how do I get the joint likelihood of data and hidden variables? I've tried `logjoint` but I am not sure how to pass the values of the latent variables as `AbstractVarInfo`?","user":"U6BJ9E351","ts":"1616500136.042500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8zLt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm having some troubles with the Turing API. Given a model (as for example the one described in "},{"type":"link","url":"https://turing.ml/dev/docs/using-turing/get-started"},{"type":"text","text":"), how do I get the joint likelihood of data and hidden variables? I've tried "},{"type":"text","text":"logjoint","style":{"code":true}},{"type":"text","text":" but I am not sure how to pass the values of the latent variables as "},{"type":"text","text":"AbstractVarInfo","style":{"code":true}},{"type":"text","text":"?"}]}]}],"thread_ts":"1616500136.042500","reply_count":7,"reply_users_count":2,"latest_reply":"1616500661.043900","reply_users":["U6BJ9E351","U85JBUGGP"],"subscribed":false},{"type":"message","text":"Apologies if this is a basic question - I am trying do do bayesian inference on a DE (with NUTS) and I am running into a problem that sometimes the sampling appears to get \"stuck\" while at other times the sampling seems to behave as expected. I've attached two screen shots at a small number of iterations showing this - The only thing I changed when running was the number of iterations. Are there any simple issues that could be the cause of this? I assume it's a modelling issue :disappointed:","files":[{"id":"F01SMF5HQ65","created":1616575876,"timestamp":1616575876,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01C1H3GLLU","editable":false,"size":45278,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01SMF5HQ65/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01SMF5HQ65/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01SMF5HQ65-2e16f1c962/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01SMF5HQ65-2e16f1c962/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01SMF5HQ65-2e16f1c962/image_360.png","thumb_360_w":360,"thumb_360_h":91,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01SMF5HQ65-2e16f1c962/image_480.png","thumb_480_w":480,"thumb_480_h":122,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01SMF5HQ65-2e16f1c962/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01SMF5HQ65-2e16f1c962/image_720.png","thumb_720_w":720,"thumb_720_h":182,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01SMF5HQ65-2e16f1c962/image_800.png","thumb_800_w":800,"thumb_800_h":203,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01SMF5HQ65-2e16f1c962/image_960.png","thumb_960_w":960,"thumb_960_h":243,"original_w":1019,"original_h":258,"thumb_tiny":"AwAMADDSBoyKXFFACZoFFGKAFopPxpfxoA//2Q==","permalink":"https://julialang.slack.com/files/U01C1H3GLLU/F01SMF5HQ65/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01SMF5HQ65-d7d5b9c51f","is_starred":false,"has_rich_preview":false},{"id":"F01SYLNL08Y","created":1616575890,"timestamp":1616575890,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01C1H3GLLU","editable":false,"size":35024,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01SYLNL08Y/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01SYLNL08Y/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01SYLNL08Y-5f7663b8db/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01SYLNL08Y-5f7663b8db/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01SYLNL08Y-5f7663b8db/image_360.png","thumb_360_w":360,"thumb_360_h":91,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01SYLNL08Y-5f7663b8db/image_480.png","thumb_480_w":480,"thumb_480_h":122,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01SYLNL08Y-5f7663b8db/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01SYLNL08Y-5f7663b8db/image_720.png","thumb_720_w":720,"thumb_720_h":182,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01SYLNL08Y-5f7663b8db/image_800.png","thumb_800_w":800,"thumb_800_h":203,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01SYLNL08Y-5f7663b8db/image_960.png","thumb_960_w":960,"thumb_960_h":243,"original_w":1019,"original_h":258,"thumb_tiny":"AwAMADDRJ5p1GKMUAFJnmlpD0oAWik/GgdetAH//2Q==","permalink":"https://julialang.slack.com/files/U01C1H3GLLU/F01SYLNL08Y/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01SYLNL08Y-1c818c7c96","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"CUWm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Apologies if this is a basic question - I am trying do do bayesian inference on a DE (with NUTS) and I am running into a problem that sometimes the sampling appears to get \"stuck\" while at other times the sampling seems to behave as expected. I've attached two screen shots at a small number of iterations showing this - The only thing I changed when running was the number of iterations. Are there any simple issues that could be the cause of this? I assume it's a modelling issue "},{"type":"emoji","name":"disappointed"}]}]}],"user":"U01C1H3GLLU","ts":"1616576172.049300","thread_ts":"1616576172.049300","reply_count":2,"reply_users_count":2,"latest_reply":"1616577328.055000","reply_users":["U01C2AJ9F63","UHDQQ4GN6"],"is_locked":false,"subscribed":false},{"client_msg_id":"55b75784-2add-42b7-8adf-6833605e7ac8","type":"message","text":"<@U8T9JUA5R> Hi! David and everyone. I'm trying to create a \"CustomMvNormal\" distribution from the package Distributions.jl for Turing inference :\n\n```struct CustomMvNormal &lt;: AbstractMvNormal\nend```\nThe only thing I want to modify from MvNomal() is to replace the input covariance matrix d.Σ with a new correspond symmetry and positive definite covariance matrix  by calling functions from ProximalOperators.jl s.t.\n\n```d.Σ = prox(IndPSD(), Symmetric(d.Σ))[1]+1e-12*Matrix(I, size(d.Σ), size(d.Σ))```\nThe reason I do not replace covariance matrix externally is because that:  during the Turing process, the matrix variables are automatically changed into dual type which I could not apply Symmetric() or Eigen() on it. Thus, I'm trying to force the covariance matrix to be symmetry and PD inside the new distribution function CustomMvNormal().\n<https://github.com/JuliaStats/Distributions.jl/blob/master/src/multivariate/mvnormal.jl>\n\nCould you help me with this?\nThanks a lot in advance!","user":"U01Q398M3QB","ts":"1616576935.054400","team":"T68168MUP","edited":{"user":"U01Q398M3QB","ts":"1616577594.000000"},"blocks":[{"type":"rich_text","block_id":"4wa","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U8T9JUA5R"},{"type":"text","text":" Hi! David and everyone. I'm trying to create a \"CustomMvNormal\" distribution from the package Distributions.jl for Turing inference :\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"struct CustomMvNormal <: AbstractMvNormal\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"The only thing I want to modify from MvNomal() is to replace the input covariance matrix d.Σ with a new correspond symmetry and positive definite covariance matrix  by calling functions from ProximalOperators.jl s.t.\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"d.Σ = prox(IndPSD(), Symmetric(d.Σ))[1]+1e-12*Matrix(I, size(d.Σ), size(d.Σ))"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nThe reason I do not replace covariance matrix externally is because that:  during the Turing process, the matrix variables are automatically changed into dual type which I could not apply Symmetric() or Eigen() on it. Thus, I'm trying to force the covariance matrix to be symmetry and PD inside the new distribution function CustomMvNormal().\n"},{"type":"link","url":"https://github.com/JuliaStats/Distributions.jl/blob/master/src/multivariate/mvnormal.jl"},{"type":"text","text":"\n\nCould you help me with this?\nThanks a lot in advance!"}]}]}]},{"client_msg_id":"865232ee-c16a-498b-8906-3c53af6a9d74","type":"message","text":"The bugged code (if I try to force covariance matrix to be symmetry and positive definite externally) and its error message attaches:\n```using Random\nusing DifferentialEquations\nusing GenericLinearAlgebra\nusing Statistics\nusing Distributions\nusing Turing\n# data\nu_account = [1.0; 2.0];\n@model bayes_lna(y) = begin\n    log_delta ~ Normal(0.0,1.0) \n    delta = [exp(log_delta)]\n    phi = [delta  0; 10^-7 delta] # covariance matrix \n    D,V = GenericLinearAlgebra.eigen(GenericLinearAlgebra.Symmetric(phi))  # get eigen-values and eigen-vectos\n    D .= max.(D,1e-12) # force eigen values to be positive\n    phi_new = V*GenericLinearAlgebra.Diagonal(D)*V' # create new symmetry and positive definite matrix\n    y[:] ~ MvNormal([0.0,0.0],GenericLinearAlgebra.Symmetric(phi_new))\nend\nRandom.seed!(87654)\niterations = 20\nchain = sample(bayes_lna(u_account), NUTS(0.65),iterations)```","user":"U01Q398M3QB","ts":"1616578091.056700","team":"T68168MUP","edited":{"user":"U01Q398M3QB","ts":"1616578570.000000"},"blocks":[{"type":"rich_text","block_id":"Ynsh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The bugged code (if I try to force covariance matrix to be symmetry and positive definite externally) and its error message attaches:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Random\nusing DifferentialEquations\nusing GenericLinearAlgebra\nusing Statistics\nusing Distributions\nusing Turing\n# data\nu_account = [1.0; 2.0];\n@model bayes_lna(y) = begin\n    log_delta ~ Normal(0.0,1.0) \n    delta = [exp(log_delta)]\n    phi = [delta  0; 10^-7 delta] # covariance matrix \n    D,V = GenericLinearAlgebra.eigen(GenericLinearAlgebra.Symmetric(phi))  # get eigen-values and eigen-vectos\n    D .= max.(D,1e-12) # force eigen values to be positive\n    phi_new = V*GenericLinearAlgebra.Diagonal(D)*V' # create new symmetry and positive definite matrix\n    y[:] ~ MvNormal([0.0,0.0],GenericLinearAlgebra.Symmetric(phi_new))\nend\nRandom.seed!(87654)\niterations = 20\nchain = sample(bayes_lna(u_account), NUTS(0.65),iterations)"}]}]}]},{"type":"message","text":"","files":[{"id":"F01S9111Z34","created":1616578325,"timestamp":1616578325,"name":"error trace.pdf","title":"error trace.pdf","mimetype":"application/pdf","filetype":"pdf","pretty_type":"PDF","user":"U01Q398M3QB","editable":false,"size":34682,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01S9111Z34/error_trace.pdf","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01S9111Z34/download/error_trace.pdf","thumb_pdf":"https://files.slack.com/files-tmb/T68168MUP-F01S9111Z34-db62534ff9/error_trace_thumb_pdf.png","thumb_pdf_w":935,"thumb_pdf_h":1210,"permalink":"https://julialang.slack.com/files/U01Q398M3QB/F01S9111Z34/error_trace.pdf","permalink_public":"https://slack-files.com/T68168MUP-F01S9111Z34-646f3dab18","is_starred":false,"has_rich_preview":false}],"upload":false,"user":"U01Q398M3QB","display_as_bot":false,"ts":"1616578329.057200"},{"type":"message","subtype":"thread_broadcast","text":"I don't have any experience with ProximalOperators.jl, so I can't comment on this part. In general, if you have full control over the model and work with custom distributions, probably it would be much more efficient if you do not work with the covariance matrix `C` directly but instead with the lower-triangular matrix `L` in its decomposition `C = LL'`. In the example you show `phi` is actually already a lower-triangular matrix. Generally, you then just have to enforce that the diagonal entries of `L` are non-negative (e.g., by using `exp` or `log1pexp` ), the off-diagonal elements of `L` can be arbitrary. BTW I am a bit surprised you have to use `GenericLinearAlgebra` here - at least `Symmetric`  and `Diagonal` from `LinearAlgebra` should work fine with dual numbers (so maybe it is just due to `eigen`?). In fact, AFAIK PDMats (which is used by the constructors of `MvNormal`) only defines special dispatches for `LinearAlgebra.Symmetric` but not GenericLinearAlgebra.","user":"U8T9JUA5R","ts":"1616580167.061100","thread_ts":"1616578091.056700","root":{"client_msg_id":"865232ee-c16a-498b-8906-3c53af6a9d74","type":"message","text":"The bugged code (if I try to force covariance matrix to be symmetry and positive definite externally) and its error message attaches:\n```using Random\nusing DifferentialEquations\nusing GenericLinearAlgebra\nusing Statistics\nusing Distributions\nusing Turing\n# data\nu_account = [1.0; 2.0];\n@model bayes_lna(y) = begin\n    log_delta ~ Normal(0.0,1.0) \n    delta = [exp(log_delta)]\n    phi = [delta  0; 10^-7 delta] # covariance matrix \n    D,V = GenericLinearAlgebra.eigen(GenericLinearAlgebra.Symmetric(phi))  # get eigen-values and eigen-vectos\n    D .= max.(D,1e-12) # force eigen values to be positive\n    phi_new = V*GenericLinearAlgebra.Diagonal(D)*V' # create new symmetry and positive definite matrix\n    y[:] ~ MvNormal([0.0,0.0],GenericLinearAlgebra.Symmetric(phi_new))\nend\nRandom.seed!(87654)\niterations = 20\nchain = sample(bayes_lna(u_account), NUTS(0.65),iterations)```","user":"U01Q398M3QB","ts":"1616578091.056700","team":"T68168MUP","edited":{"user":"U01Q398M3QB","ts":"1616578570.000000"},"blocks":[{"type":"rich_text","block_id":"Ynsh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The bugged code (if I try to force covariance matrix to be symmetry and positive definite externally) and its error message attaches:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Random\nusing DifferentialEquations\nusing GenericLinearAlgebra\nusing Statistics\nusing Distributions\nusing Turing\n# data\nu_account = [1.0; 2.0];\n@model bayes_lna(y) = begin\n    log_delta ~ Normal(0.0,1.0) \n    delta = [exp(log_delta)]\n    phi = [delta  0; 10^-7 delta] # covariance matrix \n    D,V = GenericLinearAlgebra.eigen(GenericLinearAlgebra.Symmetric(phi))  # get eigen-values and eigen-vectos\n    D .= max.(D,1e-12) # force eigen values to be positive\n    phi_new = V*GenericLinearAlgebra.Diagonal(D)*V' # create new symmetry and positive definite matrix\n    y[:] ~ MvNormal([0.0,0.0],GenericLinearAlgebra.Symmetric(phi_new))\nend\nRandom.seed!(87654)\niterations = 20\nchain = sample(bayes_lna(u_account), NUTS(0.65),iterations)"}]}]}],"thread_ts":"1616578091.056700","reply_count":2,"reply_users_count":2,"latest_reply":"1616581529.062700","reply_users":["U8T9JUA5R","U01Q398M3QB"],"is_locked":false,"subscribed":false},"blocks":[{"type":"rich_text","block_id":"05o","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I don't have any experience with ProximalOperators.jl, so I can't comment on this part. In general, if you have full control over the model and work with custom distributions, probably it would be much more efficient if you do not work with the covariance matrix "},{"type":"text","text":"C","style":{"code":true}},{"type":"text","text":" directly but instead with the lower-triangular matrix "},{"type":"text","text":"L","style":{"code":true}},{"type":"text","text":" in its decomposition "},{"type":"text","text":"C = LL'","style":{"code":true}},{"type":"text","text":". In the example you show "},{"type":"text","text":"phi","style":{"code":true}},{"type":"text","text":" is actually already a lower-triangular matrix. Generally, you then just have to enforce that the diagonal entries of "},{"type":"text","text":"L","style":{"code":true}},{"type":"text","text":" are non-negative (e.g., by using "},{"type":"text","text":"exp","style":{"code":true}},{"type":"text","text":" or "},{"type":"text","text":"log1pexp","style":{"code":true}},{"type":"text","text":" ), the off-diagonal elements of "},{"type":"text","text":"L","style":{"code":true}},{"type":"text","text":" can be arbitrary. BTW I am a bit surprised you have to use "},{"type":"text","text":"GenericLinearAlgebra","style":{"code":true}},{"type":"text","text":" here - at least "},{"type":"text","text":"Symmetric","style":{"code":true}},{"type":"text","text":"  and "},{"type":"text","text":"Diagonal","style":{"code":true}},{"type":"text","text":" from "},{"type":"text","text":"LinearAlgebra","style":{"code":true}},{"type":"text","text":" should work fine with dual numbers (so maybe it is just due to "},{"type":"text","text":"eigen","style":{"code":true}},{"type":"text","text":"?). In fact, AFAIK PDMats (which is used by the constructors of "},{"type":"text","text":"MvNormal","style":{"code":true}},{"type":"text","text":") only defines special dispatches for "},{"type":"text","text":"LinearAlgebra.Symmetric","style":{"code":true}},{"type":"text","text":" but not GenericLinearAlgebra."}]}]}],"client_msg_id":"9b05f06e-d571-4bcc-8abc-95ddaceacc68"},{"type":"message","text":"It appears that there is something wrong with plotting sample values (there is a distinctly positive density for negative values which are not present in the chain, nor allowed by the priors). Is this because of some smoothing kernel used for making that plot? Can that be improved somehow?","files":[{"id":"F01S93VFYR0","created":1616580126,"timestamp":1616580126,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U9AHT3YM7","editable":false,"size":175615,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01S93VFYR0/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01S93VFYR0/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01S93VFYR0-f4ac8344ff/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01S93VFYR0-f4ac8344ff/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01S93VFYR0-f4ac8344ff/image_360.png","thumb_360_w":360,"thumb_360_h":179,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01S93VFYR0-f4ac8344ff/image_480.png","thumb_480_w":480,"thumb_480_h":239,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01S93VFYR0-f4ac8344ff/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01S93VFYR0-f4ac8344ff/image_720.png","thumb_720_w":720,"thumb_720_h":358,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01S93VFYR0-f4ac8344ff/image_800.png","thumb_800_w":800,"thumb_800_h":398,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01S93VFYR0-f4ac8344ff/image_960.png","thumb_960_w":960,"thumb_960_h":477,"thumb_1024":"https://files.slack.com/files-tmb/T68168MUP-F01S93VFYR0-f4ac8344ff/image_1024.png","thumb_1024_w":1024,"thumb_1024_h":509,"original_w":1322,"original_h":657,"thumb_tiny":"AwAXADDS7UgBoOPXmmPGZBw5XnPFAmSZPpRk+n61X+zNn/WtUscZQEFi3Pem0u4k32H5PpQScdKDRSKGnrQQ/wDBjOe9B+9TxQBF++6fJ+tPXfj58Z9qX+L8KWncVhCcUUj9qcaQz//Z","permalink":"https://julialang.slack.com/files/U9AHT3YM7/F01S93VFYR0/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01S93VFYR0-cba01e3789","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"yTA7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It appears that there is something wrong with plotting sample values (there is a distinctly positive density for negative values which are not present in the chain, nor allowed by the priors). Is this because of some smoothing kernel used for making that plot? Can that be improved somehow?"}]}]}],"user":"U9AHT3YM7","display_as_bot":false,"ts":"1616580193.061500","thread_ts":"1616580193.061500","reply_count":7,"reply_users_count":4,"latest_reply":"1616582213.063800","reply_users":["U8T9JUA5R","UC0SY9JFP","U9AHT3YM7","UHDQQ4GN6"],"is_locked":false,"subscribed":false},{"client_msg_id":"f3f9b73a-b611-4558-8694-7e7bec869a71","type":"message","text":"I'm trying to debug my Turing model:\n```@model function gdemo(datasets)\n    σ₀ ~ Gamma(1.0, 1/0.02) # μm^2\n    logκ ~ Normal(log(2000), 2) # NoUnits\n    cm ~ TruncatedNormal(2.35, 1.5, 0.0, Inf) # NoUnits\n    e0 ~ Gamma(1.0, 1/0.2) # Gy\n    logsf_sigma ~ Gamma(1.0, 1/0.2)\n    density = 1u\"g/cm^3\"\n    dlogs = map(d -&gt; d.log_sfs, datasets)\n\n    for (i, dataset) in enumerate(datasets)\n        doses = dataset.doses\n\n        cur_beta = e2beta(dataset.E)\n        cur_zeff = zeff(dataset.Z, cur_beta)\n        zkb = cur_zeff^2/(cur_beta^2 * exp(logκ))\n        p_ion_kill = (1.0 - exp(-zkb))^cm\n        σ = σ₀ * p_ion_kill\n\n        alet = glet(cur_beta, dataset.Z)\n        log_ion_kill = σ .* uconvert.(Ref(NoUnits), -density .* doses ./ alet .* 1.0u\"μm^2\")\n        \n        doses_Gy = uconvert.(Ref(NoUnits), doses ./ 1.0u\"Gy\")\n\n        log_gamma_kill = log.(1.0 .- (1.0 .- exp.(- (1.0 .- p_ion_kill) .* doses_Gy ./ e0)).^cm)\n        logsfs_mu = log_ion_kill + log_gamma_kill\n\n        dlogs[i] ~ MvNormal(logsfs_mu, logsf_sigma)\n    end\nend```\nIt seems to not fit to my data at all, that is posteriors for the first five variables in this model look more or less like priors. Yet `ess` and `rhat` in chain statistics look great. Is there any obvious error?","user":"U9AHT3YM7","ts":"1616600346.068100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"buE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm trying to debug my Turing model:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@model function gdemo(datasets)\n    σ₀ ~ Gamma(1.0, 1/0.02) # μm^2\n    logκ ~ Normal(log(2000), 2) # NoUnits\n    cm ~ TruncatedNormal(2.35, 1.5, 0.0, Inf) # NoUnits\n    e0 ~ Gamma(1.0, 1/0.2) # Gy\n    logsf_sigma ~ Gamma(1.0, 1/0.2)\n    density = 1u\"g/cm^3\"\n    dlogs = map(d -> d.log_sfs, datasets)\n\n    for (i, dataset) in enumerate(datasets)\n        doses = dataset.doses\n\n        cur_beta = e2beta(dataset.E)\n        cur_zeff = zeff(dataset.Z, cur_beta)\n        zkb = cur_zeff^2/(cur_beta^2 * exp(logκ))\n        p_ion_kill = (1.0 - exp(-zkb))^cm\n        σ = σ₀ * p_ion_kill\n\n        alet = glet(cur_beta, dataset.Z)\n        log_ion_kill = σ .* uconvert.(Ref(NoUnits), -density .* doses ./ alet .* 1.0u\"μm^2\")\n        \n        doses_Gy = uconvert.(Ref(NoUnits), doses ./ 1.0u\"Gy\")\n\n        log_gamma_kill = log.(1.0 .- (1.0 .- exp.(- (1.0 .- p_ion_kill) .* doses_Gy ./ e0)).^cm)\n        logsfs_mu = log_ion_kill + log_gamma_kill\n\n        dlogs[i] ~ MvNormal(logsfs_mu, logsf_sigma)\n    end\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"It seems to not fit to my data at all, that is posteriors for the first five variables in this model look more or less like priors. Yet "},{"type":"text","text":"ess","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"rhat","style":{"code":true}},{"type":"text","text":" in chain statistics look great. Is there any obvious error?"}]}]}],"thread_ts":"1616600346.068100","reply_count":7,"reply_users_count":2,"latest_reply":"1616601751.069500","reply_users":["UC0SY9JFP","U9AHT3YM7"],"is_locked":false,"subscribed":false},{"client_msg_id":"ae62e59a-ad13-45f2-81a6-265ede125c95","type":"message","text":"What is the best way to diagnose pathological behavior from MCMC chains in terms of plotting? Like for example the typical 'funnel' behavior. Especially when having more than 2 parameters that is (e.g. 10 to keep it simple for now). Are cornerplots a good way of doing this?","user":"U01C2AJ9F63","ts":"1616669326.079800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PJI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What is the best way to diagnose pathological behavior from MCMC chains in terms of plotting? Like for example the typical 'funnel' behavior. Especially when having more than 2 parameters that is (e.g. 10 to keep it simple for now). Are cornerplots a good way of doing this?"}]}]}],"thread_ts":"1616669326.079800","reply_count":1,"reply_users_count":1,"latest_reply":"1616671128.080300","reply_users":["UHDQQ4GN6"],"is_locked":false,"subscribed":false},{"client_msg_id":"e7357155-8fb3-4582-92d3-d0c15cb046e4","type":"message","text":"Is there anything I can do to speed sampling up here?\n```M = 10\nlength(y) == 500\nTuring.setadbackend(:zygote)\n\nTuring.@model function gp_logistic(y)\n    u ~ MvNormal(ones(M))\n    f ~ MvNormal(A' * u, K̃)\n    for i in eachindex(y)\n        y[i] ~ Bernoulli(logistic(f[i]))\n    end\nend\n\ntrue_samples = Turing.sample(gp_logistic(y_01), Turing.NUTS(0.85), 1000)```\nNote that K̃ is already defined as `PDMat` Right now the expected time is 6hours :cry:","user":"U7QLM6E2E","ts":"1616679981.082200","team":"T68168MUP","edited":{"user":"U7QLM6E2E","ts":"1616679997.000000"},"blocks":[{"type":"rich_text","block_id":"3wAUH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there anything I can do to speed sampling up here?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"M = 10\nlength(y) == 500\nTuring.setadbackend(:zygote)\n\nTuring.@model function gp_logistic(y)\n    u ~ MvNormal(ones(M))\n    f ~ MvNormal(A' * u, K̃)\n    for i in eachindex(y)\n        y[i] ~ Bernoulli(logistic(f[i]))\n    end\nend\n\ntrue_samples = Turing.sample(gp_logistic(y_01), Turing.NUTS(0.85), 1000)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Note that K̃ is already defined as "},{"type":"text","text":"PDMat","style":{"code":true}},{"type":"text","text":" Right now the expected time is 6hours "},{"type":"emoji","name":"cry"}]}]}],"thread_ts":"1616679981.082200","reply_count":14,"reply_users_count":3,"latest_reply":"1616680722.085300","reply_users":["U8T9JUA5R","U7QLM6E2E","UC0SY9JFP"],"is_locked":false,"subscribed":false},{"client_msg_id":"c5a36426-4fb4-4f35-a50f-8cd96a2d0398","type":"message","text":"Are there any examples of performing stacking with model weights in Turing as described in <https://arxiv.org/pdf/2006.12335.pdf|Stacking for Non-mixing Bayesian Computations>?","user":"U01C1H3GLLU","ts":"1616682535.086500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"cXAmG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Are there any examples of performing stacking with model weights in Turing as described in "},{"type":"link","url":"https://arxiv.org/pdf/2006.12335.pdf","text":"Stacking for Non-mixing Bayesian Computations"},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"7fd51181-d2b9-4dcf-ba8a-b4bbf697f08c","type":"message","text":"In the  example on the front page of the Turing site (<https://turing.ml/stable/>)\n``` @model gdemo(x, y) = begin\n  # Assumptions\n  σ ~ InverseGamma(2,3)\n  μ ~ Normal(0,sqrt(σ))\n  # Observations\n  x ~ Normal(μ, sqrt(σ))\n  y ~ Normal(μ, sqrt(σ))\nend```\nWhy would one use either \\sigma as a symbol for variance, or the square root of the standard deviation as a parameter in the normal distribution?","user":"U01G3TX4F9A","ts":"1616706107.090600","team":"T68168MUP","attachments":[{"title":"Turing.jl - Turing.jl","title_link":"https://turing.ml/stable/","text":"Turing: A robust, efficient and modular library for general-purpose probabilistic programming.","fallback":"Turing.jl - Turing.jl","from_url":"https://turing.ml/stable/","service_icon":"https://turing.ml/dev/assets/img/favicon.ico","service_name":"turing.ml","id":1,"original_url":"https://turing.ml/stable/"}],"blocks":[{"type":"rich_text","block_id":"qHBLA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In the  example on the front page of the Turing site ("},{"type":"link","url":"https://turing.ml/stable/"},{"type":"text","text":")\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":" @model gdemo(x, y) = begin\n  # Assumptions\n  σ ~ InverseGamma(2,3)\n  μ ~ Normal(0,sqrt(σ))\n  # Observations\n  x ~ Normal(μ, sqrt(σ))\n  y ~ Normal(μ, sqrt(σ))\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Why would one use either \\sigma as a symbol for variance, or the square root of the standard deviation as a parameter in the normal distribution?"}]}]}]},{"client_msg_id":"d91f0d33-5518-4fec-b4e1-13474eeb1d71","type":"message","text":"Hi. Since I am relatively new to Julia was wondering if it is possible to fit HMM model to my multivariate data where the first two columns are following Normal distribution and the last two columns as binomial (success and failures) ? For example using finite mixture model example stated in Turing.jl and than using HMM. Thanks!","user":"U01SDQ8NLP5","ts":"1616750554.093700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ia45Q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi. Since I am relatively new to Julia was wondering if it is possible to fit HMM model to my multivariate data where the first two columns are following Normal distribution and the last two columns as binomial (success and failures) ? For example using finite mixture model example stated in Turing.jl and than using HMM. Thanks!"}]}]}]},{"client_msg_id":"acc60e90-1f8c-40ac-ab50-428b1bc6ee9f","type":"message","text":"Is Turing not fully compatible with `Julia Version 1.6.0 Commit f9720dc2eb* on Linux`? I tried with a new directory\n\n```activate .\nadd Turing\nst Turing\n# It gives Turing v0.15.1 ```\n`Turing v0.15.1` is a few months old. The last release was `Turing v0.15.12`\n\nHow can I install the latest Turing version?","user":"U011PPW7K53","ts":"1617016445.100200","team":"T68168MUP","edited":{"user":"U011PPW7K53","ts":"1617016510.000000"},"blocks":[{"type":"rich_text","block_id":"WKDkK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is Turing not fully compatible with "},{"type":"text","text":"Julia Version 1.6.0 Commit f9720dc2eb* on Linux","style":{"code":true}},{"type":"text","text":"? I tried with a new directory\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"activate .\nadd Turing\nst Turing\n# It gives Turing v0.15.1 "}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Turing v0.15.1","style":{"code":true}},{"type":"text","text":" is a few months old. The last release was "},{"type":"text","text":"Turing v0.15.12","style":{"code":true}},{"type":"text","text":"\n\nHow can I install the latest Turing version?"}]}]}]},{"client_msg_id":"6f852e0d-c7b8-442b-8b03-a3d8298b665e","type":"message","text":"Libtask_jll is not compatible with Julia 1.6 yet, and therefore the particle filter methods do not work with Julia 1.6 yet and the latest version can't be installed currently. It will be fixed soon but we have to wait for libjulia 1.6 to be available via Yggdrasil since it is required by Libtask. The older version can be installed (but the particle filter methods are broken) since in older versions we did not provide pre-built binaries via Yggdrasil but Libtask was built locally. You can find more information and some links in this issue: <https://github.com/TuringLang/Turing.jl/issues/1554>","user":"U8T9JUA5R","ts":"1617016692.104500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+iU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Libtask_jll is not compatible with Julia 1.6 yet, and therefore the particle filter methods do not work with Julia 1.6 yet and the latest version can't be installed currently. It will be fixed soon but we have to wait for libjulia 1.6 to be available via Yggdrasil since it is required by Libtask. The older version can be installed (but the particle filter methods are broken) since in older versions we did not provide pre-built binaries via Yggdrasil but Libtask was built locally. You can find more information and some links in this issue: "},{"type":"link","url":"https://github.com/TuringLang/Turing.jl/issues/1554"}]}]}]},{"client_msg_id":"83e49849-6ce9-4e93-ba34-8f714be0f587","type":"message","text":"If you do not care about broken particle filter methods, I think you can work around the Libtask_jll requirement by installing it manually as described in <https://github.com/TuringLang/Libtask.jl#julia-nightly> (but I haven't tried it myself).","user":"U8T9JUA5R","ts":"1617016837.106000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wLYT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If you do not care about broken particle filter methods, I think you can work around the Libtask_jll requirement by installing it manually as described in "},{"type":"link","url":"https://github.com/TuringLang/Libtask.jl#julia-nightly"},{"type":"text","text":" (but I haven't tried it myself)."}]}]}],"reactions":[{"name":"+1","users":["U011PPW7K53"],"count":1}]},{"client_msg_id":"faebd6e5-fc91-43c6-a464-b32cc16356f8","type":"message","text":"NUTS seems to start at points very far from expected values of priors. That's a problem because at such extreme values numerical precision affects my model. How can I make it start sampling closer to expected values of priors?","user":"U9AHT3YM7","ts":"1617036088.108800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Q8/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"NUTS seems to start at points very far from expected values of priors. That's a problem because at such extreme values numerical precision affects my model. How can I make it start sampling closer to expected values of priors?"}]}]}]},{"client_msg_id":"06275461-68cd-4ece-ab27-39fda76a9ad4","type":"message","text":"Congratulations to <@UC0SY9JFP> in being interviewed by Alex Andorra in the Learning Bayesian Statistics podcast.!!\n\nThe topic is Developing Turing.\n\nHere is the link: <https://www.learnbayesstats.com/episode/36-bayesian-non-parametrics-developing-turing-julia-martin-trapp|#36 Bayesian Non-Parametrics &amp; Developing Turing.jl, with Martin Trapp>. Starting to listen now.","user":"U01QBF4PHKP","ts":"1617099115.114400","team":"T68168MUP","attachments":[{"title":"#36 Bayesian Non-Parametrics &amp; Developing Turing.jl, with Martin Trapp - Learning Bayesian Statistics","title_link":"https://www.learnbayesstats.com/episode/36-bayesian-non-parametrics-developing-turing-julia-martin-trapp","text":"Episode sponsored by Tidelift: tidelift.comI bet you already heard of Bayesian nonparametric models, at least on this very podcast. We already talked ab...","fallback":"#36 Bayesian Non-Parametrics &amp; Developing Turing.jl, with Martin Trapp - Learning Bayesian Statistics","image_url":"https://assets.captivate.fm/80092a3d-db4c-45db-9809-6b24ce1140a1/asset/1188d25e-2cd5-4070-8e47-cc272051ffad/bQQyHOC249PUj1nGliWL-4G-.jpg","image_width":478,"image_height":250,"from_url":"https://www.learnbayesstats.com/episode/36-bayesian-non-parametrics-developing-turing-julia-martin-trapp","image_bytes":76619,"service_icon":"https://www.learnbayesstats.com/images/small-artwork.jpg","service_name":"learnbayesstats.com","id":1,"original_url":"https://www.learnbayesstats.com/episode/36-bayesian-non-parametrics-developing-turing-julia-martin-trapp"}],"blocks":[{"type":"rich_text","block_id":"aNu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Congratulations to "},{"type":"user","user_id":"UC0SY9JFP"},{"type":"text","text":" in being interviewed by Alex Andorra in the Learning Bayesian Statistics podcast.!!\n\nThe topic is Developing Turing.\n\nHere is the link: "},{"type":"link","url":"https://www.learnbayesstats.com/episode/36-bayesian-non-parametrics-developing-turing-julia-martin-trapp","text":"#36 Bayesian Non-Parametrics & Developing Turing.jl, with Martin Trapp"},{"type":"text","text":". Starting to listen now."}]}]}],"reactions":[{"name":"+1","users":["UDXST8ARK","U9AHT3YM7"],"count":2}]},{"client_msg_id":"99b0a15a-9fc6-428c-ad56-772716b4bcb9","type":"message","text":"I hope I didn’t say too many stupid things. :sweat_smile:","user":"UC0SY9JFP","ts":"1617099169.114900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+AnP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I hope I didn’t say too many stupid things. "},{"type":"emoji","name":"sweat_smile"}]}]}]},{"client_msg_id":"02060d4e-1b5c-4c1e-8771-9fb328c8529a","type":"message","text":"Hi -- just some confusion. I was told earlier on this channel that `sample` expects to be passed the total number of steps (warmup + samples), and you pass the number of warmup steps to NUTS i.e. `sample(rng, model, NUTS(nwarmup, 0.8), MCMCThreads(), nwarmup + nsamples, nchains)` . On github I see <https://github.com/TuringLang/Turing.jl/issues/1537|an issue> which seems to say something contradictory - that you don't pass Nwarmup to sample, only to NUTS, i.e. in the below code block there are 2 000 total samples for each chain and the first 1 000 are removed.\n\n```nsamples = 1000\nnwarmup = 1000\nspecs = NUTS(nwarmup, .65)\nn_chains = 4\nchain = sample(model(y), specs, MCMCThreads(), nsamples, n_chains) ```\n What is the correct behaviour?","user":"U01C1H3GLLU","ts":"1617100087.118800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"g8k0z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi -- just some confusion. I was told earlier on this channel that "},{"type":"text","text":"sample","style":{"code":true}},{"type":"text","text":" expects to be passed the total number of steps (warmup + samples), and you pass the number of warmup steps to NUTS i.e. "},{"type":"text","text":"sample(rng, model, NUTS(nwarmup, 0.8), MCMCThreads(), nwarmup + nsamples, nchains)","style":{"code":true}},{"type":"text","text":" . On github I see "},{"type":"link","url":"https://github.com/TuringLang/Turing.jl/issues/1537","text":"an issue"},{"type":"text","text":" which seems to say something contradictory - that you don't pass Nwarmup to sample, only to NUTS, i.e. in the below code block there are 2 000 total samples for each chain and the first 1 000 are removed.\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"nsamples = 1000\nnwarmup = 1000\nspecs = NUTS(nwarmup, .65)\nn_chains = 4\nchain = sample(model(y), specs, MCMCThreads(), nsamples, n_chains) "}]},{"type":"rich_text_section","elements":[{"type":"text","text":" What is the correct behaviour?"}]}]}]},{"client_msg_id":"b4f80339-7d6a-4b37-b97e-c90c9ca9a070","type":"message","text":"are parameters in a Turing chain organised alphabetically?","user":"U01M641BZEY","ts":"1617111980.120900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"C1Np","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"are parameters in a Turing chain organised alphabetically?"}]}]}]}]}