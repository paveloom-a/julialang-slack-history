{"cursor": 2, "messages": [{"client_msg_id":"9a88c7ef-ffb0-4920-87ff-eca0f0810633","type":"message","text":"Approximate Bayesian Computation.","user":"U69BL50BF","ts":"1615300364.002800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4Iqd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Approximate Bayesian Computation."}]}]}]},{"client_msg_id":"a231f94d-56ec-41cc-a0f1-fb2499101798","type":"message","text":"ah, right","user":"U01H36BUDJB","ts":"1615300607.003000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8HF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ah, right"}]}]}]},{"client_msg_id":"98579e6d-1ba2-466c-a0e7-a8dd02f683c4","type":"message","text":"approximate bayes... never really worked much with that before","user":"U01H36BUDJB","ts":"1615300629.003300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dSb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"approximate bayes... never really worked much with that before"}]}]}]},{"client_msg_id":"45fcb3ae-01ef-47b6-bcc2-ba476543aef3","type":"message","text":"Is there an effort atm to include likelihood-free/simulation based inference in Turing (or elsewhere?)","user":"U01M641BZEY","ts":"1615300873.005100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RCx07","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there an effort atm to include likelihood-free/simulation based inference in Turing (or elsewhere?)"}]}]}]},{"client_msg_id":"b7d828eb-89e6-47cb-b819-c66d8c0302a5","type":"message","text":"as for e.g. is described nicely here by Miles Cranmer: <https://astroautomata.com/blog/simulation-based-inference/>","user":"U01M641BZEY","ts":"1615301140.006600","team":"T68168MUP","attachments":[{"service_name":"astro automata","title":"A tutorial on simulation-based inference","title_link":"https://astroautomata.com/blog/simulation-based-inference/","text":"Personal website of Miles Cranmer","fallback":"astro automata: A tutorial on simulation-based inference","image_url":"http://astroautomata.com//assets/images/nflow.png","ts":1595808000,"from_url":"https://astroautomata.com/blog/simulation-based-inference/","image_width":425,"image_height":250,"image_bytes":60223,"service_icon":"https://astroautomata.com/apple-icon-57x57.png","id":1,"original_url":"https://astroautomata.com/blog/simulation-based-inference/"}],"blocks":[{"type":"rich_text","block_id":"+kLa0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"as for e.g. is described nicely here by Miles Cranmer: "},{"type":"link","url":"https://astroautomata.com/blog/simulation-based-inference/"}]}]}]},{"type":"message","subtype":"channel_join","ts":"1615364009.010700","user":"U01QXPZ8SR2","text":"<@U01QXPZ8SR2> has joined the channel","inviter":"U01JA0D7L56"},{"type":"message","subtype":"channel_join","ts":"1615367004.011000","user":"U01QB1S7C0P","text":"<@U01QB1S7C0P> has joined the channel","inviter":"U01JA0D7L56"},{"client_msg_id":"d7c4394a-6d19-4ea7-a320-8dc0fdd8b53c","type":"message","text":"how is reversediff so much faster?! argh spent like an hour optimizing a Turing model and then realized that I had forgot to change the ad backend and sampling went from 2hrs for 1000 samples to 2 min","user":"U6CFMFM2R","ts":"1615488171.019400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"CRR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"how is reversediff so much faster?! argh spent like an hour optimizing a Turing model and then realized that I had forgot to change the ad backend and sampling went from 2hrs for 1000 samples to 2 min"}]}]}]},{"client_msg_id":"06f3535a-7525-4c78-b59d-c2e51c13eee1","type":"message","text":"Reverse-mode AD vs forward-mode AD. If you have N parameters and your model is properly vectorised, you can expect reverse-mode to be ~N times faster. Less in practice but ideally not by much.","user":"U85JBUGGP","ts":"1615494112.021200","team":"T68168MUP","edited":{"user":"U85JBUGGP","ts":"1615494122.000000"},"blocks":[{"type":"rich_text","block_id":"aGP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Reverse-mode AD vs forward-mode AD. If you have N parameters and your model is properly vectorised, you can expect reverse-mode to be ~N times faster. Less in practice but ideally not by much."}]}]}]},{"client_msg_id":"eafb4e2c-c5a2-4862-a196-59727bed2b4d","type":"message","text":"`N` needs to be large enough though","user":"U85JBUGGP","ts":"1615494133.021500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bMh6v","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"N","style":{"code":true}},{"type":"text","text":" needs to be large enough though"}]}]}]},{"client_msg_id":"3c9d0a4e-3833-4e12-b83f-8c584db8ad39","type":"message","text":"When Diffractor is out and Turing uses it, you can expect AD to be even faster! But that won’t be for a while.","user":"U85JBUGGP","ts":"1615494210.022100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2N+q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"When Diffractor is out and Turing uses it, you can expect AD to be even faster! But that won’t be for a while."}]}]}]},{"client_msg_id":"6e63c40a-f166-43e2-b44c-e15bfd498a5b","type":"message","text":"its really incredible","user":"U6CFMFM2R","ts":"1615497774.023000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"M+t","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"its really incredible"}]}]}]},{"client_msg_id":"e04f1568-3a34-4297-896e-f1d2d6fa2e47","type":"message","text":"I’m trying to implement this simple hierarchical model in Turing:\ny_j | θ_j, σ_j ~ N(θ_j, σ_{j}^2$\n θ_j | μ, τ ~ N(μ, τ^2)$\nwith priors p(μ, τ)\n\nWould\n```@model meta(y) = begin\n    μ ~ Uniform(0, 1)\n    τ ~ Uniform(0, 1)\n    θ ~ Normal(μ, τ^2)\n\n    for j in eachindex(y) \n      y[j] ~  Normal(θ, τ)\n    end\nend```\nbe correct? Do I have to do anything to specify that the theta is specific to / potentially different for each y_j?","user":"US8V7JSKB","ts":"1615541250.026300","team":"T68168MUP","edited":{"user":"US8V7JSKB","ts":"1615541347.000000"},"blocks":[{"type":"rich_text","block_id":"JPOp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’m trying to implement this simple hierarchical model in Turing:\ny_j | θ_j, σ_j ~ N(θ_j, σ_{j}^2$\n θ_j | μ, τ ~ N(μ, τ^2)$\nwith priors p(μ, τ)\n\nWould\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@model meta(y) = begin\n    μ ~ Uniform(0, 1)\n    τ ~ Uniform(0, 1)\n    θ ~ Normal(μ, τ^2)\n\n    for j in eachindex(y) \n      y[j] ~  Normal(θ, τ)\n    end\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"be correct? Do I have to do anything to specify that the theta is specific to / potentially different for each y_j?"}]}]}],"thread_ts":"1615541250.026300","reply_count":4,"reply_users_count":2,"latest_reply":"1615541586.027200","reply_users":["U01C2AJ9F63","US8V7JSKB"],"subscribed":false},{"client_msg_id":"022f8bc2-934c-4e01-935a-83eaa47fe49d","type":"message","text":"Is there a trick to working with parameter vectors and callbacks? For example, say I have a vector of parameters `κ ~ MvNormal(Fill(0.0,5),1.0)` . I've tried TuringCallbacks (without setting a variable_filter) and a few different things with Turkie but no luck so far.","user":"U017YGFQTE3","ts":"1615544384.035400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Bt97","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a trick to working with parameter vectors and callbacks? For example, say I have a vector of parameters "},{"type":"text","text":"κ ~ MvNormal(Fill(0.0,5),1.0)","style":{"code":true}},{"type":"text","text":" . I've tried TuringCallbacks (without setting a variable_filter) and a few different things with Turkie but no luck so far."}]}]}]},{"type":"message","text":"No signs of slowing down!","files":[{"id":"F01RTKCCRJL","created":1615571882,"timestamp":1615571882,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U85JBUGGP","editable":false,"size":462351,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01RTKCCRJL/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01RTKCCRJL/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_360.png","thumb_360_w":360,"thumb_360_h":230,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_480.png","thumb_480_w":480,"thumb_480_h":307,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_720.png","thumb_720_w":720,"thumb_720_h":460,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_800.png","thumb_800_w":800,"thumb_800_h":511,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_960.png","thumb_960_w":960,"thumb_960_h":614,"thumb_1024":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_1024.png","thumb_1024_w":1024,"thumb_1024_h":655,"original_w":1658,"original_h":1060,"thumb_tiny":"AwAeADDSJOeKPm9qOppaADrR0oooAKOv0pOv0paAE/ipaQg9jijB9aAFzSYz1o5x1o59aAFopOfX9KXn1oA//9k=","permalink":"https://julialang.slack.com/files/U85JBUGGP/F01RTKCCRJL/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01RTKCCRJL-a5dde4754b","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"4RB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"No signs of slowing down!"}]}]}],"user":"U85JBUGGP","display_as_bot":false,"ts":"1615571913.037400","reactions":[{"name":"fire","users":["UC0SY9JFP","U85JBUGGP"],"count":2}]},{"client_msg_id":"c8f3f275-b1ae-46c1-98f6-4d8f0c64ffbd","type":"message","text":"Hey I was one of those stars!","user":"U01H36BUDJB","ts":"1615590217.044200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"cxlDp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey I was one of those stars!"}]}]}]},{"client_msg_id":"e2d45524-80bf-41b6-b072-77b304ba67e1","type":"message","text":"I was really excited when I found Turing. You guys have done awesome work :)","user":"U01H36BUDJB","ts":"1615590280.044600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pEjw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I was really excited when I found Turing. You guys have done awesome work :)"}]}]}]},{"client_msg_id":"9cadb051-c86e-4ed0-b465-4aad3451e810","type":"message","text":"Would any of you happen to have any teaching materials for Turing? I’m trying to learn it for a Bayesian Stats class (that is unfortunately R-based), and I’m trying to find as many Turing examples as I can. I’m already aware of the <http://Turing.ml|Turing.ml> tutorials and Rob Goedman’s Stat Rethinking code.","user":"US8V7JSKB","ts":"1615599144.047400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wrc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Would any of you happen to have any teaching materials for Turing? I’m trying to learn it for a Bayesian Stats class (that is unfortunately R-based), and I’m trying to find as many Turing examples as I can. I’m already aware of the "},{"type":"link","url":"http://Turing.ml","text":"Turing.ml"},{"type":"text","text":" tutorials and Rob Goedman’s Stat Rethinking code."}]}]}]},{"client_msg_id":"6f680ca7-37b2-4ab5-b365-3d26f7b4138c","type":"message","text":"I can't think of anything outside those materials","user":"U9JNHB83X","ts":"1615608332.048200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HiCk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I can't think of anything outside those materials"}]}]}]},{"client_msg_id":"60475152-19c8-47e6-a071-6bdf0f5748e9","type":"message","text":"They tend to be mostly comprehensive","user":"U9JNHB83X","ts":"1615608341.048600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xFi1Z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"They tend to be mostly comprehensive"}]}]}]},{"client_msg_id":"0f786972-ef30-40b8-8462-9b03fea01a25","type":"message","text":"I will be adding later this semester portuguese bayesian materials using Turing. I have a opensource creative-commons content for Bayesian Stats using R and Stan (mostly rstanarm and brms for PhD candidates in Social Sciences). All the materials are being reformulated now in a branch (2021-02 -- i.e next semester) that will be taught next semester but they are in Portuguese. You can access them here (<https://storopoli.io/Estatistica-Bayesiana/>) and the github repo (<https://github.com/storopoli/Estatistica-Bayesiana>). My ideia is to update the R and Stan materials and create also a Julia version using Turing.","user":"U01QBF4PHKP","ts":"1615634458.052600","team":"T68168MUP","attachments":[{"service_name":"Estatística Bayesiana com R e RStan","title":"Estatística Bayesiana com R e RStan: Estatística Bayesiana com R e Stan","title_link":"https://storopoli.io/Estatistica-Bayesiana/","text":"Companion para a disciplina de Estatística Bayesiana para alunos de Mestrado e Doutorado da UNINOVE","fallback":"Estatística Bayesiana com R e RStan: Estatística Bayesiana com R e RStan: Estatística Bayesiana com R e Stan","from_url":"https://storopoli.io/Estatistica-Bayesiana/","id":1,"original_url":"https://storopoli.io/Estatistica-Bayesiana/"}],"blocks":[{"type":"rich_text","block_id":"9QOIV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I will be adding later this semester portuguese bayesian materials using Turing. I have a opensource creative-commons content for Bayesian Stats using R and Stan (mostly rstanarm and brms for PhD candidates in Social Sciences). All the materials are being reformulated now in a branch (2021-02 -- i.e next semester) that will be taught next semester but they are in Portuguese. You can access them here ("},{"type":"link","url":"https://storopoli.io/Estatistica-Bayesiana/"},{"type":"text","text":") and the github repo ("},{"type":"link","url":"https://github.com/storopoli/Estatistica-Bayesiana"},{"type":"text","text":"). My ideia is to update the R and Stan materials and create also a Julia version using Turing."}]}]}]},{"client_msg_id":"47f0d9ca-2d67-4e56-b186-e048125ee57c","type":"message","text":"I am defining a custom distribution to define parameter constraints of one parameter by a second parameter. The new custom distribution version compared to my initial `Turing` model which worked by rejection of parameter combination, which don't satisfy these conditions, doesn't seem to take the prior into account.\n\nSo I wanted to ask if that is to be expected. The MWE below hopefully illustrates the scenario, if parameter falls outside the condition/range of support you get `-Inf` but for the a supported parameter set you get different `logp`. The `reject` function is akin to the scenario where one uses `@addlogp -Inf` to reject a sample. Am I missing something here?\n```julia&gt; function reject(y)\n           x = similar(y)\n           x[1] = logpdf(Uniform(0.0,2.0), y[1])\n           x[2] = ifelse(y[2] &lt; y[1], -Inf, logpdf(Normal(0.6,.3), y[2]))\n           return x\n       end\n\njulia&gt; function cust_dist(y)\n           x = similar(y)\n           x[1] = logpdf(Uniform(0.0,2.0), y[1])\n           x[2] = logpdf(truncated(Normal(0.6,.3), y[2]), y[1], Inf)\n           return x\n       end\n\njulia&gt; rv = [0.1,.05];\n\njulia&gt; cust_dist(rv)\n2-element Array{Float64,1}:\n  -0.6931471805599453\n -Inf\n\njulia&gt; reject(rv)\n2-element Array{Float64,1}:\n  -0.6931471805599453\n -Inf\n\njulia&gt; rv = [0.1,.3]\n\njulia&gt; reject(rv)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -0.21496572887873655\n\njulia&gt; cust_dist(rv)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -0.16599567864432105```\n","user":"UGFMDAMC3","ts":"1615643956.064600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ozeta","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am defining a custom distribution to define parameter constraints of one parameter by a second parameter. The new custom distribution version compared to my initial "},{"type":"text","text":"Turing","style":{"code":true}},{"type":"text","text":" model which worked by rejection of parameter combination, which don't satisfy these conditions, doesn't seem to take the prior into account.\n\nSo I wanted to ask if that is to be expected. The MWE below hopefully illustrates the scenario, if parameter falls outside the condition/range of support you get "},{"type":"text","text":"-Inf","style":{"code":true}},{"type":"text","text":" but for the a supported parameter set you get different "},{"type":"text","text":"logp","style":{"code":true}},{"type":"text","text":". The "},{"type":"text","text":"reject","style":{"code":true}},{"type":"text","text":" function is akin to the scenario where one uses "},{"type":"text","text":"@addlogp -Inf","style":{"code":true}},{"type":"text","text":" to reject a sample. Am I missing something here?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> function reject(y)\n           x = similar(y)\n           x[1] = logpdf(Uniform(0.0,2.0), y[1])\n           x[2] = ifelse(y[2] < y[1], -Inf, logpdf(Normal(0.6,.3), y[2]))\n           return x\n       end\n\njulia> function cust_dist(y)\n           x = similar(y)\n           x[1] = logpdf(Uniform(0.0,2.0), y[1])\n           x[2] = logpdf(truncated(Normal(0.6,.3), y[2]), y[1], Inf)\n           return x\n       end\n\njulia> rv = [0.1,.05];\n\njulia> cust_dist(rv)\n2-element Array{Float64,1}:\n  -0.6931471805599453\n -Inf\n\njulia> reject(rv)\n2-element Array{Float64,1}:\n  -0.6931471805599453\n -Inf\n\njulia> rv = [0.1,.3]\n\njulia> reject(rv)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -0.21496572887873655\n\njulia> cust_dist(rv)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -0.16599567864432105"}]},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"6ba62d7a-cee0-4b5a-9347-85712ce0cdb2","type":"message","text":"The “rejection” approach doesn’t include log of the normalisation constant when you “truncate”. The logpdf of `truncated` includes `log` the normalisation constant.","user":"U85JBUGGP","ts":"1615648152.067100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jzQm5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The “rejection” approach doesn’t include log of the normalisation constant when you “truncate”. The logpdf of "},{"type":"text","text":"truncated","style":{"code":true}},{"type":"text","text":" includes "},{"type":"text","text":"log","style":{"code":true}},{"type":"text","text":" the normalisation constant."}]}]}]},{"client_msg_id":"81bf003e-c84a-42b4-831d-b806ab43f592","type":"message","text":"if you try different `rv` values, you will see that the difference in the second number is always constant up to machine precision.","user":"U85JBUGGP","ts":"1615648201.068000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4Eu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"if you try different "},{"type":"text","text":"rv","style":{"code":true}},{"type":"text","text":" values, you will see that the difference in the second number is always constant up to machine precision."}]}]}]},{"client_msg_id":"d9b17ee2-74bd-4ca2-adcc-ab4a4ea9bd13","type":"message","text":"I see, yeah you are right with varying rv there is a constant difference. I guess I am surprised by the following scenario where the prior of the second parameter is a `Normal(0.6, 0.3)` and prior probability of second parameter depends on the value of the first and increases the closer `rv[1]` gets to `rv[2]`  . It seems a bit un-intuitive to me but I am also somewhat out of my depth here.","user":"UGFMDAMC3","ts":"1615649311.074900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HB7j","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I see, yeah you are right with varying rv there is a constant difference. I guess I am surprised by the following scenario where the prior of the second parameter is a "},{"type":"text","text":"Normal(0.6, 0.3)","style":{"code":true}},{"type":"text","text":" and prior probability of second parameter depends on the value of the first and increases the closer "},{"type":"text","text":"rv[1]","style":{"code":true}},{"type":"text","text":" gets to "},{"type":"text","text":"rv[2]","style":{"code":true}},{"type":"text","text":"  . It seems a bit un-intuitive to me but I am also somewhat out of my depth here."}]}]}]},{"client_msg_id":"e3968cdb-053c-4344-9aca-06ced6eab388","type":"message","text":"plot the pdf surface to visualise what’s happening better","user":"U85JBUGGP","ts":"1615649480.075200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QZf08","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"plot the pdf surface to visualise what’s happening better"}]}]}]},{"type":"message","text":"Yeah, I had plotted the pdf surface before and I can see how the density increases near the truncation.","files":[{"id":"F01RVKGQBU0","created":1615654034,"timestamp":1615654034,"name":"pdf_plots.svg","title":"pdf_plots.svg","mimetype":"image/svg+xml","filetype":"svg","pretty_type":"SVG","user":"UGFMDAMC3","editable":false,"size":35902,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01RVKGQBU0/pdf_plots.svg","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01RVKGQBU0/download/pdf_plots.svg","permalink":"https://julialang.slack.com/files/UGFMDAMC3/F01RVKGQBU0/pdf_plots.svg","permalink_public":"https://slack-files.com/T68168MUP-F01RVKGQBU0-4d245ddc14","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"VAG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah, I had plotted the pdf surface before and I can see how the density increases near the truncation."}]}]}],"user":"UGFMDAMC3","display_as_bot":false,"ts":"1615654039.077800"},{"client_msg_id":"d132e30c-4d2f-4948-b653-9563df1d554e","type":"message","text":"The above the scenario outlined gives the below logprobs. which is counter-intuitve in the sense that prior probability of the second parameter depends on the first even though the prior was defined independent of the first parameter. That means that `rv2` has a higher prior probability then `rv3` , which pretty much lies near the mean of the prior. Whereas, for the \"rejection\" approach `rv3` has the highest prior probability. Btw, thanks for taking the time to discuss this here.\n\n```julia&gt; rv1 = [0.1,1.5]\n2-element Array{Float64,1}:\n 0.1\n 1.5\n\njulia&gt; rv2 = [1.4,1.5]\n2-element Array{Float64,1}:\n 1.4\n 1.5\n\njulia&gt; reject(rv1)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -4.214965728878737\n\njulia&gt; reject(rv2)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -4.214965728878737\n\njulia&gt; cust_dist(rv2)\n2-element Array{Float64,1}:\n -0.6931471805599453\n  1.349825386941264\n\njulia&gt; cust_dist(rv1)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -4.165995678644322\n\njulia&gt; rv3 = [0.1, 0.6]\n2-element Array{Float64,1}:\n 0.1\n 0.6\n\njulia&gt; cust_dist(rv3)\n2-element Array{Float64,1}:\n -0.6931471805599453\n  0.33400432135567887\n\njulia&gt; reject(rv3)\n2-element Array{Float64,1}:\n -0.6931471805599453\n  0.28503427112126334```\n","user":"UGFMDAMC3","ts":"1615655045.085200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"81H8h","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The above the scenario outlined gives the below logprobs. which is counter-intuitve in the sense that prior probability of the second parameter depends on the first even though the prior was defined independent of the first parameter. That means that "},{"type":"text","text":"rv2","style":{"code":true}},{"type":"text","text":" has a higher prior probability then "},{"type":"text","text":"rv3","style":{"code":true}},{"type":"text","text":" , which pretty much lies near the mean of the prior. Whereas, for the \"rejection\" approach "},{"type":"text","text":"rv3","style":{"code":true}},{"type":"text","text":" has the highest prior probability. Btw, thanks for taking the time to discuss this here.\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> rv1 = [0.1,1.5]\n2-element Array{Float64,1}:\n 0.1\n 1.5\n\njulia> rv2 = [1.4,1.5]\n2-element Array{Float64,1}:\n 1.4\n 1.5\n\njulia> reject(rv1)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -4.214965728878737\n\njulia> reject(rv2)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -4.214965728878737\n\njulia> cust_dist(rv2)\n2-element Array{Float64,1}:\n -0.6931471805599453\n  1.349825386941264\n\njulia> cust_dist(rv1)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -4.165995678644322\n\njulia> rv3 = [0.1, 0.6]\n2-element Array{Float64,1}:\n 0.1\n 0.6\n\njulia> cust_dist(rv3)\n2-element Array{Float64,1}:\n -0.6931471805599453\n  0.33400432135567887\n\njulia> reject(rv3)\n2-element Array{Float64,1}:\n -0.6931471805599453\n  0.28503427112126334"}]},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"02ffdfe6-90db-4fde-8afb-fd8f4069e32a","type":"message","text":"Note that the normalisation constant is a function of `y[1]`. So it’s only constant if you fix `y[1]`. The rejection method is not correct because it ignores this term.","user":"U85JBUGGP","ts":"1615659267.086200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FYQu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Note that the normalisation constant is a function of "},{"type":"text","text":"y[1]","style":{"code":true}},{"type":"text","text":". So it’s only constant if you fix "},{"type":"text","text":"y[1]","style":{"code":true}},{"type":"text","text":". The rejection method is not correct because it ignores this term."}]}]}]},{"client_msg_id":"db1ce0e2-75b2-4bf5-9113-cf2a89c9368e","type":"message","text":"I see. So the enforcing parameter constraints by rejection in a `Turing.jl` is nonsensical? So that `rv2 = [1.4,1.5]` has a higher prior probability then `rv3 = [0.1, 0.6]` is correct if the priors such that `p1 ~ Uniform(0.0,2.0)` and `p2 ~ truncated(Normal(0.6,0.3))` if the constraint is such that `p2 &gt; p1` . What I want to achieve with the prior on `p2`  is that p2 is unlikely to take bigger values such as `1.5` and beyond. The prior was derived from data where the relationship between `p1` and `p2` was not existing. Do I have to define the prior on `p2` differently?","user":"UGFMDAMC3","ts":"1615667108.095700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iD0f","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I see. So the enforcing parameter constraints by rejection in a "},{"type":"text","text":"Turing.jl","style":{"code":true}},{"type":"text","text":" is nonsensical? So that "},{"type":"text","text":"rv2 = [1.4,1.5]","style":{"code":true}},{"type":"text","text":" has a higher prior probability then "},{"type":"text","text":"rv3 = [0.1, 0.6]","style":{"code":true}},{"type":"text","text":" is correct if the priors such that "},{"type":"text","text":"p1 ~ Uniform(0.0,2.0)","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"p2 ~ truncated(Normal(0.6,0.3))","style":{"code":true}},{"type":"text","text":" if the constraint is such that "},{"type":"text","text":"p2 > p1","style":{"code":true}},{"type":"text","text":" . What I want to achieve with the prior on "},{"type":"text","text":"p2","style":{"code":true}},{"type":"text","text":"  is that p2 is unlikely to take bigger values such as "},{"type":"text","text":"1.5","style":{"code":true}},{"type":"text","text":" and beyond. The prior was derived from data where the relationship between "},{"type":"text","text":"p1","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"p2","style":{"code":true}},{"type":"text","text":" was not existing. Do I have to define the prior on "},{"type":"text","text":"p2","style":{"code":true}},{"type":"text","text":" differently?"}]}]}]},{"client_msg_id":"41c88383-5653-4d9f-918f-a3c235ffa02a","type":"message","text":"&gt; I see. So the enforcing parameter constraints by rejection in a `Turing.jl` is nonsensical?\nIn case of stochastic support, I think so yes. I don’t understand the rest of your comment.","user":"U85JBUGGP","ts":"1615669809.098500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"T/MwG","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"I see. So the enforcing parameter constraints by rejection in a "},{"type":"text","text":"Turing.jl","style":{"code":true}},{"type":"text","text":" is nonsensical?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"In case of stochastic support, I think so yes. I don’t understand the rest of your comment."}]}]}]},{"client_msg_id":"f22b7b47-3e4e-42b2-8c85-fda1ae988c38","type":"message","text":"Maybe it is just counter-intuitive to me but let consider the following parameter sets: If `parset=[p1,p2]` and  `parset4 = [1.999, 1.999111]` and `parset5 = [0.00001, 0.6]` and with priors such that `p1 ~ Uniform(0.0,1.0)` and `p2~ Normal(0.6,0.3)` you get:\n\n```julia&gt; function cust_dist(y)\n           x = similar(y)\n           x[1] = logpdf(Uniform(0.0,2.0), y[1])\n           x[2] = logpdf(truncated(Normal(0.6,.3), y[2]), y[1], Inf)\n           return x\n       end\n\njulia&gt; parset4 = [1.999, 1.999111]\n2-element Array{Float64,1}:\n 1.999\n 1.999111\n\njulia&gt; cust_dist(parset4)\n2-element Array{Float64,1}:\n -0.6931471805599453\n  2.7835928275731323\n\njulia&gt; parset5 = [0.00001, 0.6]\n2-element Array{Float64,1}:\n 1.0e-5\n 0.6\n\njulia&gt; cust_dist(parset5)\n2-element Array{Float64,1}:\n -0.6931471805599453\n  0.3080490221087328```\nAnd thus merely from a prior probability perspective `parset4` has a higher probability then `parset5` , which would mean in a situation where the prior dominates and little information coming from the likelihood the posterior estimates would lie near the neighbourhood of `parset4`, right? That is even though prior of `p2` would suggest that the highest probability density is near `mu=0.6` . I hope this is a a bit clearer. Let me know if not","user":"UGFMDAMC3","ts":"1615675026.113300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"63+I","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Maybe it is just counter-intuitive to me but let consider the following parameter sets: If "},{"type":"text","text":"parset=[p1,p2]","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":" parset4 = [1.999, 1.999111]","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"parset5 = [0.00001, 0.6]","style":{"code":true}},{"type":"text","text":" and with priors such that "},{"type":"text","text":"p1 ~ Uniform(0.0,1.0)","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"p2~ Normal(0.6,0.3)","style":{"code":true}},{"type":"text","text":" you get:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> function cust_dist(y)\n           x = similar(y)\n           x[1] = logpdf(Uniform(0.0,2.0), y[1])\n           x[2] = logpdf(truncated(Normal(0.6,.3), y[2]), y[1], Inf)\n           return x\n       end\n\njulia> parset4 = [1.999, 1.999111]\n2-element Array{Float64,1}:\n 1.999\n 1.999111\n\njulia> cust_dist(parset4)\n2-element Array{Float64,1}:\n -0.6931471805599453\n  2.7835928275731323\n\njulia> parset5 = [0.00001, 0.6]\n2-element Array{Float64,1}:\n 1.0e-5\n 0.6\n\njulia> cust_dist(parset5)\n2-element Array{Float64,1}:\n -0.6931471805599453\n  0.3080490221087328"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"And thus merely from a prior probability perspective "},{"type":"text","text":"parset4","style":{"code":true}},{"type":"text","text":" has a higher probability then "},{"type":"text","text":"parset5","style":{"code":true}},{"type":"text","text":" , which would mean in a situation where the prior dominates and little information coming from the likelihood the posterior estimates would lie near the neighbourhood of "},{"type":"text","text":"parset4","style":{"code":true}},{"type":"text","text":", right? That is even though prior of "},{"type":"text","text":"p2","style":{"code":true}},{"type":"text","text":" would suggest that the highest probability density is near "},{"type":"text","text":"mu=0.6","style":{"code":true}},{"type":"text","text":" . I hope this is a a bit clearer. Let me know if not"}]}]}]},{"client_msg_id":"592cc44d-be2a-4a05-8959-44fcd3a96056","type":"message","text":"Note that the prior probability of `p2 = 0.6` is not as high as you may think it is. This is a multivariate distribution after all so you need to look at the probability of both variables simultaneously. If you analyse the effect of `p1`, you see that `p2` can only be 0.6 if `p1 &lt; 0.6` which has a probability of `0.3`. `P(p2 = 0.6 | p1)` is itself a function of `p1` because of the normalisation constant. Perhaps, the semantics you are going for are not compatible with a truncated normal distribution for `p2` with stochastic support. Maybe you want to have the mean of `p2` offset from `p1` by a constant to maintain a constant shape in the truncated distribution? Or you may not even want normal distribution but exponential.","user":"U85JBUGGP","ts":"1615676100.118900","team":"T68168MUP","edited":{"user":"U85JBUGGP","ts":"1615676177.000000"},"blocks":[{"type":"rich_text","block_id":"TjW9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Note that the prior probability of "},{"type":"text","text":"p2 = 0.6","style":{"code":true}},{"type":"text","text":" is not as high as you may think it is. This is a multivariate distribution after all so you need to look at the probability of both variables simultaneously. If you analyse the effect of "},{"type":"text","text":"p1","style":{"code":true}},{"type":"text","text":", you see that "},{"type":"text","text":"p2","style":{"code":true}},{"type":"text","text":" can only be 0.6 if "},{"type":"text","text":"p1 < 0.6","style":{"code":true}},{"type":"text","text":" which has a probability of "},{"type":"text","text":"0.3","style":{"code":true}},{"type":"text","text":". "},{"type":"text","text":"P(p2 = 0.6 | p1)","style":{"code":true}},{"type":"text","text":" is itself a function of "},{"type":"text","text":"p1","style":{"code":true}},{"type":"text","text":" because of the normalisation constant. Perhaps, the semantics you are going for are not compatible with a truncated normal distribution for "},{"type":"text","text":"p2","style":{"code":true}},{"type":"text","text":" with stochastic support. Maybe you want to have the mean of "},{"type":"text","text":"p2","style":{"code":true}},{"type":"text","text":" offset from "},{"type":"text","text":"p1","style":{"code":true}},{"type":"text","text":" by a constant to maintain a constant shape in the truncated distribution? Or you may not even want normal distribution but exponential."}]}]}],"thread_ts":"1615676100.118900","reply_count":4,"reply_users_count":2,"latest_reply":"1615677521.122100","reply_users":["UGFMDAMC3","U85JBUGGP"],"subscribed":false},{"client_msg_id":"402fe937-6ce8-459e-981f-359b92d630af","type":"message","text":"So if the correct pdf doesn’t look like what you want, then change the distribution to make it look like what you want. But rejection sampling assumes that the “normalisation constant” is constant wrt `p1` which is not always the case with stochastic support.","user":"U85JBUGGP","ts":"1615676758.120900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tCH7t","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So if the correct pdf doesn’t look like what you want, then change the distribution to make it look like what you want. But rejection sampling assumes that the “normalisation constant” is constant wrt "},{"type":"text","text":"p1","style":{"code":true}},{"type":"text","text":" which is not always the case with stochastic support."}]}]}]},{"client_msg_id":"49b42c15-4ee7-46aa-a23b-0aefe04cbe0d","type":"message","text":"Hello Turing community, how do I compute predictions for `Turing.addlogprob!` ? MWE\n\n```@model function gdemo(y)\n    mu ~ Normal(0,2)\n    for i in eachindex(y)\n        y[i] ~ Normal(mu,1)\n    end\nend\n\n@model function gdemoFail(y)\n    mu ~ Normal(0,2)\n    for i in eachindex(y)\n        Turing.@addlogprob! logpdf(Normal(mu, 1), y[i])\n    end\nend\n\ny = randn(50)\nchn = sample(gdemo(y), NUTS(), 100)\nyTest = Vector{Union{Missing,Float64}}(undef, length(y))\n\ntestModel = gdemo(yTest)\npredictions = predict(testModel, chn)\n\ntestModelFail = gdemoFail(yTest)\npredictionsFail = predict(testModelFail, chn)\njulia&gt; ERROR: MethodError: no method matching logpdf(::Normal{Float64}, ::Missing).....................```","user":"U011PPW7K53","ts":"1615717913.125700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/Ec+Q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello Turing community, how do I compute predictions for "},{"type":"text","text":"Turing.addlogprob!","style":{"code":true}},{"type":"text","text":" ? MWE\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@model function gdemo(y)\n    mu ~ Normal(0,2)\n    for i in eachindex(y)\n        y[i] ~ Normal(mu,1)\n    end\nend\n\n@model function gdemoFail(y)\n    mu ~ Normal(0,2)\n    for i in eachindex(y)\n        Turing.@addlogprob! logpdf(Normal(mu, 1), y[i])\n    end\nend\n\ny = randn(50)\nchn = sample(gdemo(y), NUTS(), 100)\nyTest = Vector{Union{Missing,Float64}}(undef, length(y))\n\ntestModel = gdemo(yTest)\npredictions = predict(testModel, chn)\n\ntestModelFail = gdemoFail(yTest)\npredictionsFail = predict(testModelFail, chn)\njulia> ERROR: MethodError: no method matching logpdf(::Normal{Float64}, ::Missing)....................."}]}]}]},{"client_msg_id":"0913ebd4-edeb-492a-81f3-31eb69f72101","type":"message","text":"Some help with this type stability / AD issue would be greatly appreciated (details in thread)","user":"U017YGFQTE3","ts":"1615732094.126800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uFVsZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Some help with this type stability / AD issue would be greatly appreciated (details in thread)"}]}]}],"thread_ts":"1615732094.126800","reply_count":2,"reply_users_count":1,"latest_reply":"1615732504.127300","reply_users":["U017YGFQTE3"],"subscribed":false},{"client_msg_id":"fc6dcc54-34c2-4728-a794-3415120e58e8","type":"message","text":"A question to Turing Specialists.\n\nI performed a QR decomposition on my model matrix `X`.\n\n```#### QR Decomposition ####\nQ, R = qr(X)```\nThen i fitted a Turing Model to my Q Matrix, casting it to a `Matrix`:\n```model_qr = varying_intercept(Matrix(Q), idx, float(y));\nchn_qr = sample(model_qr, NUTS(1_000, 0.65), MCMCThreads(), 2_000, 4);```\nThis is the output:\n```julia&gt; chn_qr\nChains MCMC chain (2000×24×4 Array{Float64,3}):\n\nIterations        = 1:2000\nThinning interval = 1\nChains            = 1, 2, 3, 4\nSamples per chain = 2000\nparameters        = β[1], β[2], μ, μⱼ[1], μⱼ[2], μⱼ[3], μⱼ[4], μⱼ[5], μⱼ[6], μⱼ[7], σ, σⱼ\ninternals         = acceptance_rate, hamiltonian_energy, hamiltonian_energy_error, is_accept, log_density, lp, max_hamiltonian_energy_error, n_steps, nom_step_size, numerical_error, step_size, tree_depth\n\nSummary Statistics\n  parameters       mean       std   naive_se      mcse         ess      rhat \n      Symbol    Float64   Float64    Float64   Float64     Float64   Float64 \n\n        β[1]     0.0615    1.6131     0.0180    0.0242   5229.9895    0.9997\n        β[2]   -46.6065    4.4243     0.0495    0.0556   6779.8184    1.0009\n           μ    23.2964    1.7568     0.0196    0.0354   1548.2673    1.0012\n       μⱼ[1]     1.3817    1.8077     0.0202    0.0364   1599.0513    1.0011\n       μⱼ[2]     1.6890    1.7905     0.0200    0.0362   1604.2076    1.0009\n       μⱼ[3]    -4.0131    1.7790     0.0199    0.0355   1565.3781    1.0010\n       μⱼ[4]     5.8102    2.0615     0.0230    0.0377   2110.1449    1.0009\n       μⱼ[5]    -2.0791    1.8817     0.0210    0.0356   1732.1206    1.0011\n       μⱼ[6]    -5.2884    1.7996     0.0201    0.0362   1600.3742    1.0012\n       μⱼ[7]     1.9578    1.8095     0.0202    0.0365   1607.9793    1.0011\n           σ     2.6639    0.1189     0.0013    0.0014   7292.1689    1.0006\n          σⱼ     4.2218    1.4394     0.0161    0.0266   3021.6447    1.0011\n\nQuantiles\n  parameters       2.5%      25.0%      50.0%      75.0%      97.5% \n      Symbol    Float64    Float64    Float64    Float64    Float64 \n\n        β[1]    -2.9792    -0.7044     0.0580     0.8240     3.1977\n        β[2]   -55.0442   -49.6002   -46.6371   -43.6904   -37.9659\n           μ    19.9347    22.2450    23.2709    24.3002    26.9628\n       μⱼ[1]    -2.3245     0.3304     1.3992     2.4828     4.9164\n       μⱼ[2]    -2.0222     0.6622     1.6990     2.7723     5.1757\n       μⱼ[3]    -7.7450    -5.0484    -3.9772    -2.9457    -0.6303\n       μⱼ[4]     1.7884     4.5075     5.7806     7.0934     9.8737\n       μⱼ[5]    -6.0375    -3.1619    -2.0521    -0.9130     1.5270\n       μⱼ[6]    -9.0518    -6.3368    -5.2189    -4.1916    -1.8506\n       μⱼ[7]    -1.8358     0.9237     1.9936     3.0302     5.4514\n           σ     2.4414     2.5788     2.6598     2.7444     2.9033\n          σⱼ     2.4002     3.2597     3.9161     4.8519     7.7719```\nI've managed to reconstruct the quantiles of β by multiplying R^-1 * β (in a very ugly non-elegant way):\n```quantiles_beta = select(DataFrame(quantile(group(chn_qr, :β))), r\"%\");\nmapcols(x -&gt; R^-1 * x, quantiles_beta)\n2×5 DataFrame\n Row │ 2.5%         25.0%        50.0%        75.0%        97.5%       \n     │ Float64      Float64      Float64      Float64      Float64     \n─────┼─────────────────────────────────────────────────────────────────\n   1 │ -2.56661     -2.34772     -2.22018     -2.09349     -1.86299\n   2 │  0.00516392   0.00465319   0.00437521   0.00409877   0.00356173```\n*How would I apply R^-1 * β to all the β in the `chn_qr` object so that I can create a new \"`chn_qr_reconstructed`\" object and perform all the methods that uses `Chain` to it?*\n\nBTW: QR reparameterization is 19.6s sampling and using X is 135.6s sampling (with a way better ESS overall)","user":"U01QBF4PHKP","ts":"1615799946.141000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WmSC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"A question to Turing Specialists.\n\nI performed a QR decomposition on my model matrix "},{"type":"text","text":"X","style":{"code":true}},{"type":"text","text":".\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"#### QR Decomposition ####\nQ, R = qr(X)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nThen i fitted a Turing Model to my Q Matrix, casting it to a "},{"type":"text","text":"Matrix","style":{"code":true}},{"type":"text","text":":\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"model_qr = varying_intercept(Matrix(Q), idx, float(y));\nchn_qr = sample(model_qr, NUTS(1_000, 0.65), MCMCThreads(), 2_000, 4);"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nThis is the output:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> chn_qr\nChains MCMC chain (2000×24×4 Array{Float64,3}):\n\nIterations        = 1:2000\nThinning interval = 1\nChains            = 1, 2, 3, 4\nSamples per chain = 2000\nparameters        = β[1], β[2], μ, μⱼ[1], μⱼ[2], μⱼ[3], μⱼ[4], μⱼ[5], μⱼ[6], μⱼ[7], σ, σⱼ\ninternals         = acceptance_rate, hamiltonian_energy, hamiltonian_energy_error, is_accept, log_density, lp, max_hamiltonian_energy_error, n_steps, nom_step_size, numerical_error, step_size, tree_depth\n\nSummary Statistics\n  parameters       mean       std   naive_se      mcse         ess      rhat \n      Symbol    Float64   Float64    Float64   Float64     Float64   Float64 \n\n        β[1]     0.0615    1.6131     0.0180    0.0242   5229.9895    0.9997\n        β[2]   -46.6065    4.4243     0.0495    0.0556   6779.8184    1.0009\n           μ    23.2964    1.7568     0.0196    0.0354   1548.2673    1.0012\n       μⱼ[1]     1.3817    1.8077     0.0202    0.0364   1599.0513    1.0011\n       μⱼ[2]     1.6890    1.7905     0.0200    0.0362   1604.2076    1.0009\n       μⱼ[3]    -4.0131    1.7790     0.0199    0.0355   1565.3781    1.0010\n       μⱼ[4]     5.8102    2.0615     0.0230    0.0377   2110.1449    1.0009\n       μⱼ[5]    -2.0791    1.8817     0.0210    0.0356   1732.1206    1.0011\n       μⱼ[6]    -5.2884    1.7996     0.0201    0.0362   1600.3742    1.0012\n       μⱼ[7]     1.9578    1.8095     0.0202    0.0365   1607.9793    1.0011\n           σ     2.6639    0.1189     0.0013    0.0014   7292.1689    1.0006\n          σⱼ     4.2218    1.4394     0.0161    0.0266   3021.6447    1.0011\n\nQuantiles\n  parameters       2.5%      25.0%      50.0%      75.0%      97.5% \n      Symbol    Float64    Float64    Float64    Float64    Float64 \n\n        β[1]    -2.9792    -0.7044     0.0580     0.8240     3.1977\n        β[2]   -55.0442   -49.6002   -46.6371   -43.6904   -37.9659\n           μ    19.9347    22.2450    23.2709    24.3002    26.9628\n       μⱼ[1]    -2.3245     0.3304     1.3992     2.4828     4.9164\n       μⱼ[2]    -2.0222     0.6622     1.6990     2.7723     5.1757\n       μⱼ[3]    -7.7450    -5.0484    -3.9772    -2.9457    -0.6303\n       μⱼ[4]     1.7884     4.5075     5.7806     7.0934     9.8737\n       μⱼ[5]    -6.0375    -3.1619    -2.0521    -0.9130     1.5270\n       μⱼ[6]    -9.0518    -6.3368    -5.2189    -4.1916    -1.8506\n       μⱼ[7]    -1.8358     0.9237     1.9936     3.0302     5.4514\n           σ     2.4414     2.5788     2.6598     2.7444     2.9033\n          σⱼ     2.4002     3.2597     3.9161     4.8519     7.7719"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nI've managed to reconstruct the quantiles of β by multiplying R^-1 * β (in a very ugly non-elegant way):\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"quantiles_beta = select(DataFrame(quantile(group(chn_qr, :β))), r\"%\");\nmapcols(x -> R^-1 * x, quantiles_beta)\n2×5 DataFrame\n Row │ 2.5%         25.0%        50.0%        75.0%        97.5%       \n     │ Float64      Float64      Float64      Float64      Float64     \n─────┼─────────────────────────────────────────────────────────────────\n   1 │ -2.56661     -2.34772     -2.22018     -2.09349     -1.86299\n   2 │  0.00516392   0.00465319   0.00437521   0.00409877   0.00356173"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\n"},{"type":"text","text":"How would I apply R^-1 * β to all the β in the ","style":{"bold":true}},{"type":"text","text":"chn_qr","style":{"bold":true,"code":true}},{"type":"text","text":" object so that I can create a new \"","style":{"bold":true}},{"type":"text","text":"chn_qr_reconstructed","style":{"bold":true,"code":true}},{"type":"text","text":"\" object and perform all the methods that uses ","style":{"bold":true}},{"type":"text","text":"Chain","style":{"bold":true,"code":true}},{"type":"text","text":" to it?","style":{"bold":true}},{"type":"text","text":"\n\nBTW: QR reparameterization is 19.6s sampling and using X is 135.6s sampling (with a way better ESS overall)"}]}]}]},{"client_msg_id":"597ce232-6a21-493d-aa06-4f87e9043153","type":"message","text":"Small question. I'm trying to stick as close to the mathematical definition as possible [here](<https://statisticalrethinkingjulia.github.io/TuringModels.jl/models/varying-intercepts-reedfrogs/>) and wonder if it is possible to use a function for the link. Thanks!","user":"U01BTNDCUBX","ts":"1615823316.144800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UnD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Small question. I'm trying to stick as close to the mathematical definition as possible [here]("},{"type":"link","url":"https://statisticalrethinkingjulia.github.io/TuringModels.jl/models/varying-intercepts-reedfrogs/"},{"type":"text","text":") and wonder if it is possible to use a function for the link. Thanks!"}]}]}],"thread_ts":"1615823316.144800","reply_count":5,"reply_users_count":3,"latest_reply":"1615824042.145900","reply_users":["U7QLM6E2E","U01C2AJ9F63","U01QBF4PHKP"],"subscribed":false},{"client_msg_id":"056c0b0e-0d3d-4ee5-bb25-0b7ea3452043","type":"message","text":"I thought the Bayesian logistic regression example didn't look very \"Turian\" so I made some updates to it. I also reparameterized it for even *more* speed and efficiency\n\n```train_F = qr([ones(size(train,1)) train]);\ntrain_Q_ast = Matrix(train_F.Q) * sqrt(size(train,1)-1)\ntrain_R_ast = train_F.R / sqrt(size(train,1)-1)\ntrain_R_ast_inv = inv(train_R_ast)\n\n# Bayesian logistic regression (LR)\n@model logistic_regression_new(Q_ast, y, p, σ) = begin\n    theta ~ MvNormal(zeros(p), σ)\n    y ~ Distributions.Product(Bernoulli.(logistic.(Q_ast * theta)))\nend;\n# Retrieve the number of observations.\nn, p = size([ones(size(train,1)) train])\n\n# Sample using HMC.\n@time chain = mapreduce(c -&gt; sample(logistic_regression_new(train_Q_ast, train_label, p, 1), HMC(0.05, 10), 1500),\n    chainscat,\n    1:3\n)\n\ndescribe(chain)\n\nbeta = mapslices(x-&gt; train_R_ast_inv * x, chain[:,namesingroup(chain, :theta),:].value.data, dims=[2])\ntransformed_chain = Chains(beta, [\"Intercept\", \"student\", \"balance\", \"income\"]```\nOn my machine the original code gives\n```  28.732436 seconds (304.93 M allocations: 10.426 GiB, 9.61% gc time)```\nWhile the new code gives\n```  4.446898 seconds (8.36 M allocations: 2.351 GiB, 10.44% gc time)```\nPlus almost 6X increase in ESS :slightly_smiling_face:","user":"U6CFMFM2R","ts":"1615827429.147800","team":"T68168MUP","edited":{"user":"U6CFMFM2R","ts":"1615827586.000000"},"blocks":[{"type":"rich_text","block_id":"xGm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I thought the Bayesian logistic regression example didn't look very \"Turian\" so I made some updates to it. I also reparameterized it for even "},{"type":"text","text":"more","style":{"bold":true}},{"type":"text","text":" speed and efficiency\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"train_F = qr([ones(size(train,1)) train]);\ntrain_Q_ast = Matrix(train_F.Q) * sqrt(size(train,1)-1)\ntrain_R_ast = train_F.R / sqrt(size(train,1)-1)\ntrain_R_ast_inv = inv(train_R_ast)\n\n# Bayesian logistic regression (LR)\n@model logistic_regression_new(Q_ast, y, p, σ) = begin\n    theta ~ MvNormal(zeros(p), σ)\n    y ~ Distributions.Product(Bernoulli.(logistic.(Q_ast * theta)))\nend;\n# Retrieve the number of observations.\nn, p = size([ones(size(train,1)) train])\n\n# Sample using HMC.\n@time chain = mapreduce(c -> sample(logistic_regression_new(train_Q_ast, train_label, p, 1), HMC(0.05, 10), 1500),\n    chainscat,\n    1:3\n)\n\ndescribe(chain)\n\nbeta = mapslices(x-> train_R_ast_inv * x, chain[:,namesingroup(chain, :theta),:].value.data, dims=[2])\ntransformed_chain = Chains(beta, [\"Intercept\", \"student\", \"balance\", \"income\"]"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"On my machine the original code gives\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"  28.732436 seconds (304.93 M allocations: 10.426 GiB, 9.61% gc time)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"While the new code gives\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"  4.446898 seconds (8.36 M allocations: 2.351 GiB, 10.44% gc time)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Plus almost 6X increase in ESS "},{"type":"emoji","name":"slightly_smiling_face"}]}]}],"thread_ts":"1615827429.147800","reply_count":1,"reply_users_count":1,"latest_reply":"1615829809.148100","reply_users":["U01QBF4PHKP"],"subscribed":false,"reactions":[{"name":"tada","users":["UC0SY9JFP","U01QBF4PHKP"],"count":2},{"name":"fast_parrot","users":["U01QBF4PHKP"],"count":1}]},{"client_msg_id":"d0e26400-7390-4850-bcd5-0fabb6ee3d61","type":"message","text":"Are there any examples floating around of doing prior updating with Turing? i.e. it's not clear to me how I would use my previous posterior samples to specify the priors of a subsequent fit.","user":"U01H36BUDJB","ts":"1615884517.003400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2rZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Are there any examples floating around of doing prior updating with Turing? i.e. it's not clear to me how I would use my previous posterior samples to specify the priors of a subsequent fit."}]}]}]},{"client_msg_id":"8d746320-140f-42e8-b68c-990185b938d7","type":"message","text":"I don't see an implementation of the p-lag autoregressive likelihood in Turing or Distributions.jl. Is there another package that has it, perhaps?","user":"U01H36BUDJB","ts":"1615973192.009500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hXdt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I don't see an implementation of the p-lag autoregressive likelihood in Turing or Distributions.jl. Is there another package that has it, perhaps?"}]}]}]},{"client_msg_id":"50ff929b-cbab-46fa-bbf9-418b8fe11ab6","type":"message","text":"Any state space modelling package allows you to do that such as <https://lampspuc.github.io/StateSpaceModels.jl/latest/manual/#StateSpaceModels.SARIMA> can’t find the likelihood function in that package though","user":"U6C937ENB","ts":"1615974793.010700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jLH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Any state space modelling package allows you to do that such as "},{"type":"link","url":"https://lampspuc.github.io/StateSpaceModels.jl/latest/manual/#StateSpaceModels.SARIMA"},{"type":"text","text":" can’t find the likelihood function in that package though"}]}]}],"reactions":[{"name":"+1","users":["U01H36BUDJB"],"count":1}]},{"client_msg_id":"c504b259-2026-471c-b270-7f6e7b0e6079","type":"message","text":"Kalman.jl if you set up your state space model yourself","user":"U6C937ENB","ts":"1615974804.011000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vv82","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Kalman.jl if you set up your state space model yourself"}]}]}]},{"client_msg_id":"21530b6b-1b09-42c9-87c9-ff88217702d9","type":"message","text":"Hi. I was trying to make a custom distribution for Turing to help avoid loops, and to make certain common operations easier. I have to define a bijector for the distribution. My distribution is a Continuous matrix-variate, so it should be an identity bijector, but I see things like Identity{0}, Identity{1}, etc. What does that number refer to? The dimensionality of the transformation? Something else?","user":"UGFD16K6D","ts":"1615986916.014600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ZPf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi. I was trying to make a custom distribution for Turing to help avoid loops, and to make certain common operations easier. I have to define a bijector for the distribution. My distribution is a Continuous matrix-variate, so it should be an identity bijector, but I see things like Identity{0}, Identity{1}, etc. What does that number refer to? The dimensionality of the transformation? Something else?"}]}]}],"thread_ts":"1615986916.014600","reply_count":1,"reply_users_count":1,"latest_reply":"1615986968.014700","reply_users":["UGFD16K6D"],"subscribed":false},{"client_msg_id":"d3c99072-9c1a-4570-80f4-55094fcc1c94","type":"message","text":"Hi! Could you send me a tutorial of customized drawing from MCMC for Turing inference?","user":"U01Q398M3QB","ts":"1616016461.016900","team":"T68168MUP","edited":{"user":"U01Q398M3QB","ts":"1616017564.000000"},"blocks":[{"type":"rich_text","block_id":"5W/U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi! Could you send me a tutorial of customized drawing from MCMC for Turing inference?"}]}]}]},{"client_msg_id":"dff1a8bc-0661-4171-a9a0-e65d8728c7b3","type":"message","text":"What kind of customized plotting are you looking for? Suppose you already know how to extract parameters from the chain (see <https://github.com/TuringLang/MCMCChains.jl>), you could do any plots on the results using Plots.jl; or if you want to define custom plots on the chain directly, see <https://github.com/TuringLang/MCMCChains.jl/blob/master/src/plot.jl>.","user":"U6H9SJKCH","ts":"1616017550.018700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ed6aN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What kind of customized plotting are you looking for? Suppose you already know how to extract parameters from the chain (see "},{"type":"link","url":"https://github.com/TuringLang/MCMCChains.jl"},{"type":"text","text":"), you could do any plots on the results using Plots.jl; or if you want to define custom plots on the chain directly, see "},{"type":"link","url":"https://github.com/TuringLang/MCMCChains.jl/blob/master/src/plot.jl"},{"type":"text","text":"."}]}]}]},{"client_msg_id":"f026a53a-f7f1-4c95-bcac-2f07663e479c","type":"message","text":"<@U6H9SJKCH> Sorry for the unclear description. I met some obstacles that MvNormal() distribution doesn't accept non-positive definite covariance matrix. I tried some other method to force it to be symmetry and positive definite. However, those methods could not apply to the dual type variables in Turing process. Thus, I want to define a customMvNormal distribtuion which use svd decomposition instead of Cholesky() in the original MvNormal()","user":"U01Q398M3QB","ts":"1616017805.019600","team":"T68168MUP","edited":{"user":"U01Q398M3QB","ts":"1616017963.000000"},"blocks":[{"type":"rich_text","block_id":"Rp1US","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U6H9SJKCH"},{"type":"text","text":" Sorry for the unclear description. I met some obstacles that MvNormal() distribution doesn't accept non-positive definite covariance matrix. I tried some other method to force it to be symmetry and positive definite. However, those methods could not apply to the dual type variables in Turing process. Thus, I want to define a customMvNormal distribtuion which use svd decomposition instead of Cholesky() in the original MvNormal()"}]}]}]},{"client_msg_id":"5174db21-4045-4548-8a4d-7990d1c9577d","type":"message","text":"<https://turing.ml/dev/docs/using-turing/advanced#how-to-define-a-customized-distribution>","user":"U6H9SJKCH","ts":"1616020342.020100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3wWuF","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://turing.ml/dev/docs/using-turing/advanced#how-to-define-a-customized-distribution"}]}]}]},{"client_msg_id":"db063ca1-f7c1-4317-8a1f-a3f964dfb50a","type":"message","text":"Also checkout examples in <https://github.com/TuringLang/DistributionsAD.jl>","user":"U6H9SJKCH","ts":"1616020400.020300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PsV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Also checkout examples in "},{"type":"link","url":"https://github.com/TuringLang/DistributionsAD.jl"}]}]}],"thread_ts":"1616020400.020300","reply_count":1,"reply_users_count":1,"latest_reply":"1616020572.020400","reply_users":["U01Q398M3QB"],"subscribed":false},{"client_msg_id":"47A9B395-85EE-431A-A4AC-691B83473644","type":"message","text":"We could try to get <https://github.com/mschauer/GaussianDistributions.jl|https://github.com/mschauer/GaussianDistributions.jl> working for Turing, that would also be a good exercise for the work on <https://github.com/cscherrer/MeasureTheory.jl|https://github.com/cscherrer/MeasureTheory.jl>","user":"U6C937ENB","ts":"1616052406.022000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jRh8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We could try to get "},{"type":"link","url":"https://github.com/mschauer/GaussianDistributions.jl","text":"https://github.com/mschauer/GaussianDistributions.jl"},{"type":"text","text":" working for Turing, that would also be a good exercise for the work on "},{"type":"link","url":"https://github.com/cscherrer/MeasureTheory.jl","text":"https://github.com/cscherrer/MeasureTheory.jl"}]}]}],"thread_ts":"1616052406.022000","reply_count":1,"reply_users_count":1,"latest_reply":"1616053032.022300","reply_users":["U8T9JUA5R"],"subscribed":false,"reactions":[{"name":"+1","users":["UC0SY9JFP","UE98VNG4U"],"count":2}]}]}