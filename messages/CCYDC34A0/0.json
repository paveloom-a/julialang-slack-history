{"cursor": 1, "messages": [{"client_msg_id":"25d32f7d-905a-4dc4-8afd-71aef844ce6c","type":"message","text":"I know they are sometimes called various things so not sure if I am just searching for the wrong thing in Distributions docs etc.","user":"UMS7H0ASG","ts":"1611753424.002900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"i0cP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I know they are sometimes called various things so not sure if I am just searching for the wrong thing in Distributions docs etc."}]}]}]},{"client_msg_id":"db0b43dc-75d9-4f27-8e2c-730d4ac7d1f6","type":"message","text":"What's the problem here?\n","user":"U01ARRMLM7E","ts":"1611785736.004000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j4f","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What's the problem here?\n"}]}]}]},{"client_msg_id":"db0b43dc-75d9-4f27-8e2c-730d4ac7d1f6","type":"message","text":"```using Distributions\nusing Turing\nusing Random\nusing MCMCChains\n\n\n\nrng = Random.MersenneTwister(1)\n\n\n@model function modelfn(y)\n    y .~ Normal(0,1)\nend\n\n\nxs = rand(rng, Normal(5, 3), 1000)\n\nmod = modelfn(xs)\n\nsample(mod, NUTS(), 1000)\n\n \nERROR: LoadError: MethodError: no method matching phasepoint(::Random._GLOBAL_RNG, ::Array{Any,1}, ::AdvancedHMC.Hamiltonian{AdvancedHMC.DiagEuclideanMetric{Float64,Array{Float64,1}},Turing.Inference.var\"#logπ#52\"{DynamicPPL.VarInfo{NamedTuple{(),Tuple{}},Float64},DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}},Turing.Inference.var\"#∂logπ∂θ#51\"{DynamicPPL.VarInfo{NamedTuple{(),Tuple{}},Float64},DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}}})\nClosest candidates are:\n  phasepoint(::Union{AbstractRNG, AbstractArray{var\"#s46\",1} where var\"#s46\"&lt;:AbstractRNG}, ::Union{AbstractArray{T,1}, AbstractArray{T,2}}, ::AdvancedHMC.Hamiltonian) where T&lt;:Real at \n  phasepoint(::AdvancedHMC.Hamiltonian, ::T, ::T; ℓπ, ℓκ) where T&lt;:(Union{AbstractArray{T,1}, AbstractArray{T,2}} where T) at \n  phasepoint(::AdvancedHMC.Hamiltonian, ::T1, ::T2; r, ℓπ, ℓκ) where {T1&lt;:(Union{AbstractArray{T,1}, AbstractArray{T,2}} where T), T2&lt;:(Union{AbstractArray{T,1}, AbstractArray{T,2}} where T)} at \nStacktrace:\n [1] initialstep(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}, ::DynamicPPL.VarInfo{NamedTuple{(),Tuple{}},Float64}; init_params::Nothing, nadapts::Int64, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:nchains,),Tuple{Int64}}}) at \n [2] step(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}; resume_from::Nothing, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:nadapts, :nchains),Tuple{Int64,Int64}}}) at \n [3] macro expansion at AbstractMCMC/Nw3Wn/src/sample.jl:78 [inlined]\n [4] macro expansion at ProgressLogging/BBN0b/src/ProgressLogging.jl:328 [inlined]\n [5] macro expansion at AbstractMCMC/Nw3Wn/src/logging.jl:8 [inlined]\n [6] mcmcsample(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}, ::Int64; progress::Bool, progressname::String, callback::Nothing, discard_initial::Int64, thinning::Int64, chain_type::Type{T} where T, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:nadapts, :nchains),Tuple{Int64,Int64}}}) at /AbstractMCMC/Nw3Wn/src/sample.jl:76\n [7] sample(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}, ::Int64; chain_type::Type{T} where T, resume_from::Nothing, progress::Bool, nadapts::Int64, discard_adapt::Bool, discard_initial::Int64, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:nchains,),Tuple{Int64}}}) at /home/user/src/project/julia/packages/Turing/O1Pn0/src/inference/hmc.jl:140\n [8] #sample#2 at /home/user/src/project/julia/packages/Turing/O1Pn0/src/inference/Inference.jl:143 [inlined]\n [9] #sample#1 at /home/user/src/project/julia/packages/Turing/O1Pn0/src/inference/Inference.jl:133 [inlined]\n [10] top-level scope at /home/user/src/project/src/tools/DistributionFinder.jl:20\n [11] include_string(::Function, ::Module, ::String, ::String) at /nix/store/l899vbb7z6w01xl2mkn7xzc9s8bx2msp-julia-1.5.2/lib/julia/sys.so:?\n [12] invokelatest(::Any, ::Any, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at ./essentials.jl:710\n [13] invokelatest(::Any, ::Any, ::Vararg{Any,N} where N) at ./essentials.jl:709\n [14] inlineeval(::Module, ::String, ::Int64, ::Int64, ::String; softscope::Bool) at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:185\n [15] (::VSCodeServer.var\"#61#65\"{String,Int64,Int64,String,Module,Bool,VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:144\n [16] withpath(::VSCodeServer.var\"#61#65\"{String,Int64,Int64,String,Module,Bool,VSCodeServer.ReplRunCodeRequestParams}, ::String) at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/repl.jl:124\n [17] (::VSCodeServer.var\"#60#64\"{String,Int64,Int64,String,Module,Bool,Bool,VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:142\n [18] hideprompt(::VSCodeServer.var\"#60#64\"{String,Int64,Int64,String,Module,Bool,Bool,VSCodeServer.ReplRunCodeRequestParams}) at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/repl.jl:36\n [19] (::VSCodeServer.var\"#59#63\"{String,Int64,Int64,String,Module,Bool,Bool,VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:110\n [20] with_logstate(::Function, ::Any) at /nix/store/l899vbb7z6w01xl2mkn7xzc9s8bx2msp-julia-1.5.2/lib/julia/sys.so:?\n [21] with_logger at ./logging.jl:514 [inlined]\n [22] (::VSCodeServer.var\"#58#62\"{VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:109\n [23] #invokelatest#1 at ./essentials.jl:710 [inlined]\n [24] invokelatest(::Any) at ./essentials.jl:709\n [25] macro expansion at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:27 [inlined]\n [26] (::VSCodeServer.var\"#56#57\")() at ./task.jl:356\nin expression starting at /home/user/src/project/src/tools/```","user":"U01ARRMLM7E","ts":"1611785736.004100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j4f-I=/G","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Distributions\nusing Turing\nusing Random\nusing MCMCChains\n\n\n\nrng = Random.MersenneTwister(1)\n\n\n@model function modelfn(y)\n    y .~ Normal(0,1)\nend\n\n\nxs = rand(rng, Normal(5, 3), 1000)\n\nmod = modelfn(xs)\n\nsample(mod, NUTS(), 1000)\n\n \nERROR: LoadError: MethodError: no method matching phasepoint(::Random._GLOBAL_RNG, ::Array{Any,1}, ::AdvancedHMC.Hamiltonian{AdvancedHMC.DiagEuclideanMetric{Float64,Array{Float64,1}},Turing.Inference.var\"#logπ#52\"{DynamicPPL.VarInfo{NamedTuple{(),Tuple{}},Float64},DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}},Turing.Inference.var\"#∂logπ∂θ#51\"{DynamicPPL.VarInfo{NamedTuple{(),Tuple{}},Float64},DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}}})\nClosest candidates are:\n  phasepoint(::Union{AbstractRNG, AbstractArray{var\"#s46\",1} where var\"#s46\"<:AbstractRNG}, ::Union{AbstractArray{T,1}, AbstractArray{T,2}}, ::AdvancedHMC.Hamiltonian) where T<:Real at \n  phasepoint(::AdvancedHMC.Hamiltonian, ::T, ::T; ℓπ, ℓκ) where T<:(Union{AbstractArray{T,1}, AbstractArray{T,2}} where T) at \n  phasepoint(::AdvancedHMC.Hamiltonian, ::T1, ::T2; r, ℓπ, ℓκ) where {T1<:(Union{AbstractArray{T,1}, AbstractArray{T,2}} where T), T2<:(Union{AbstractArray{T,1}, AbstractArray{T,2}} where T)} at \nStacktrace:\n [1] initialstep(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}, ::DynamicPPL.VarInfo{NamedTuple{(),Tuple{}},Float64}; init_params::Nothing, nadapts::Int64, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:nchains,),Tuple{Int64}}}) at \n [2] step(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}; resume_from::Nothing, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:nadapts, :nchains),Tuple{Int64,Int64}}}) at \n [3] macro expansion at AbstractMCMC/Nw3Wn/src/sample.jl:78 [inlined]\n [4] macro expansion at ProgressLogging/BBN0b/src/ProgressLogging.jl:328 [inlined]\n [5] macro expansion at AbstractMCMC/Nw3Wn/src/logging.jl:8 [inlined]\n [6] mcmcsample(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}, ::Int64; progress::Bool, progressname::String, callback::Nothing, discard_initial::Int64, thinning::Int64, chain_type::Type{T} where T, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:nadapts, :nchains),Tuple{Int64,Int64}}}) at /AbstractMCMC/Nw3Wn/src/sample.jl:76\n [7] sample(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}, ::Int64; chain_type::Type{T} where T, resume_from::Nothing, progress::Bool, nadapts::Int64, discard_adapt::Bool, discard_initial::Int64, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:nchains,),Tuple{Int64}}}) at /home/user/src/project/julia/packages/Turing/O1Pn0/src/inference/hmc.jl:140\n [8] #sample#2 at /home/user/src/project/julia/packages/Turing/O1Pn0/src/inference/Inference.jl:143 [inlined]\n [9] #sample#1 at /home/user/src/project/julia/packages/Turing/O1Pn0/src/inference/Inference.jl:133 [inlined]\n [10] top-level scope at /home/user/src/project/src/tools/DistributionFinder.jl:20\n [11] include_string(::Function, ::Module, ::String, ::String) at /nix/store/l899vbb7z6w01xl2mkn7xzc9s8bx2msp-julia-1.5.2/lib/julia/sys.so:?\n [12] invokelatest(::Any, ::Any, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at ./essentials.jl:710\n [13] invokelatest(::Any, ::Any, ::Vararg{Any,N} where N) at ./essentials.jl:709\n [14] inlineeval(::Module, ::String, ::Int64, ::Int64, ::String; softscope::Bool) at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:185\n [15] (::VSCodeServer.var\"#61#65\"{String,Int64,Int64,String,Module,Bool,VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:144\n [16] withpath(::VSCodeServer.var\"#61#65\"{String,Int64,Int64,String,Module,Bool,VSCodeServer.ReplRunCodeRequestParams}, ::String) at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/repl.jl:124\n [17] (::VSCodeServer.var\"#60#64\"{String,Int64,Int64,String,Module,Bool,Bool,VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:142\n [18] hideprompt(::VSCodeServer.var\"#60#64\"{String,Int64,Int64,String,Module,Bool,Bool,VSCodeServer.ReplRunCodeRequestParams}) at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/repl.jl:36\n [19] (::VSCodeServer.var\"#59#63\"{String,Int64,Int64,String,Module,Bool,Bool,VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:110\n [20] with_logstate(::Function, ::Any) at /nix/store/l899vbb7z6w01xl2mkn7xzc9s8bx2msp-julia-1.5.2/lib/julia/sys.so:?\n [21] with_logger at ./logging.jl:514 [inlined]\n [22] (::VSCodeServer.var\"#58#62\"{VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:109\n [23] #invokelatest#1 at ./essentials.jl:710 [inlined]\n [24] invokelatest(::Any) at ./essentials.jl:709\n [25] macro expansion at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:27 [inlined]\n [26] (::VSCodeServer.var\"#56#57\")() at ./task.jl:356\nin expression starting at /home/user/src/project/src/tools/"}]}]}]},{"client_msg_id":"a830c95b-6de3-49a1-bf77-3c3b06b5d9b5","type":"message","text":"Hey all I am having really slow sampling and I am not sure what it is, seems like a simple model. I am using VScode and the sampling is stuck at 0.0% for long time and then after several minutes (~10) the sampling suddently shoots to 100%, but maybe thats just a vscode problem with showing the progress. I am trying ReverseDiff now but it seems the same (if not slower). I tried `chain2 = sample(model2, NUTS(0.65), 10000)` but even if I try to just sample 100 samples it takes a long time. Here is model:\n\n```@model function m3(damage, plate, species, trt, n1, n2, n3, ::Type{T} = Float64) where {T}\n    mu0 ~ Normal(0,10)\n    trt_effects ~ filldist(Normal(0,10), n1)\n    species_effects ~ filldist(Normal(0,10), n2)\n    interaction_effects ~ filldist(Normal(0,10), n1*n2) \n    plate_effects ~ filldist(Normal(0,10), n3)\n    sigma ~ Gamma(1,1)\n    mu = Array{T,3}(undef, n1+1,n2+1,n3+1)\n    mu .= 0.0\n    for i in 1:(n1+1)\n        for j in 1:(n2+1)\n            for k in 1:(n3+1)\n                mu[i,j,k] += mu0\n                if i != 1\n                    mu[i,j,k] += trt_effects[i-1]\n                end\n                if j != 1\n                    mu[i,j,k] += species_effects[j-1]\n                end\n                if k != 1\n                    mu[i,j,k] += plate_effects[k-1]\n                end\n                if (i != 1) &amp;&amp; (j != 1)\n                    mu[i,j,k] += interaction_effects[i-1 + n1*(j-2)]\n                end\n            end\n        end\n    end\n    residuals = Array{T,1}(undef, length(damage))\n    residuals .= 0.0\n\n    for i in 1:length(damage)\n        residuals[i] = damage[i] - mu[trt[i],species[i],plate[i]]\n        damage[i] ~ Normal(mu[trt[i],species[i],plate[i]],sigma)\n    end\n    return mu, residuals\nend```","user":"U6CFMFM2R","ts":"1611940926.006300","team":"T68168MUP","edited":{"user":"U6CFMFM2R","ts":"1611941634.000000"},"blocks":[{"type":"rich_text","block_id":"G4Yc9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey all I am having really slow sampling and I am not sure what it is, seems like a simple model. I am using VScode and the sampling is stuck at 0.0% for long time and then after several minutes (~10) the sampling suddently shoots to 100%, but maybe thats just a vscode problem with showing the progress. I am trying ReverseDiff now but it seems the same (if not slower). I tried "},{"type":"text","text":"chain2 = sample(model2, NUTS(0.65), 10000)","style":{"code":true}},{"type":"text","text":" but even if I try to just sample 100 samples it takes a long time. Here is model:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@model function m3(damage, plate, species, trt, n1, n2, n3, ::Type{T} = Float64) where {T}\n    mu0 ~ Normal(0,10)\n    trt_effects ~ filldist(Normal(0,10), n1)\n    species_effects ~ filldist(Normal(0,10), n2)\n    interaction_effects ~ filldist(Normal(0,10), n1*n2) \n    plate_effects ~ filldist(Normal(0,10), n3)\n    sigma ~ Gamma(1,1)\n    mu = Array{T,3}(undef, n1+1,n2+1,n3+1)\n    mu .= 0.0\n    for i in 1:(n1+1)\n        for j in 1:(n2+1)\n            for k in 1:(n3+1)\n                mu[i,j,k] += mu0\n                if i != 1\n                    mu[i,j,k] += trt_effects[i-1]\n                end\n                if j != 1\n                    mu[i,j,k] += species_effects[j-1]\n                end\n                if k != 1\n                    mu[i,j,k] += plate_effects[k-1]\n                end\n                if (i != 1) && (j != 1)\n                    mu[i,j,k] += interaction_effects[i-1 + n1*(j-2)]\n                end\n            end\n        end\n    end\n    residuals = Array{T,1}(undef, length(damage))\n    residuals .= 0.0\n\n    for i in 1:length(damage)\n        residuals[i] = damage[i] - mu[trt[i],species[i],plate[i]]\n        damage[i] ~ Normal(mu[trt[i],species[i],plate[i]],sigma)\n    end\n    return mu, residuals\nend"}]}]}],"thread_ts":"1611940926.006300","reply_count":3,"reply_users_count":2,"latest_reply":"1611952475.018100","reply_users":["U8T9JUA5R","U6CFMFM2R"],"subscribed":false},{"client_msg_id":"c8082a4f-2c23-4ef6-a19e-2e1b1be65829","type":"message","text":"with tape compilation?","user":"U69BL50BF","ts":"1611941089.006500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+mLa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"with tape compilation?"}]}]}],"thread_ts":"1611941089.006500","reply_count":1,"reply_users_count":1,"latest_reply":"1611941452.007200","reply_users":["U6CFMFM2R"],"subscribed":false},{"client_msg_id":"d4672b15-01be-4925-ae05-4025ab09dfe6","type":"message","text":"I also can't seem to kill the sampling with `CTRL+C` or `ESC`","user":"U6CFMFM2R","ts":"1611941934.008000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"C1he","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I also can't seem to kill the sampling with "},{"type":"text","text":"CTRL+C","style":{"code":true}},{"type":"text","text":" or "},{"type":"text","text":"ESC","style":{"code":true}}]}]}]},{"client_msg_id":"1b72a136-0eab-470e-b5a8-7f8ff6e188b7","type":"message","text":"what's the scale of n1,n2,n3?","user":"U01H36BUDJB","ts":"1611943191.008200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lIBB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"what's the scale of n1,n2,n3?"}]}]}],"thread_ts":"1611943191.008200","reply_count":5,"reply_users_count":2,"latest_reply":"1611953539.024500","reply_users":["U6CFMFM2R","U01H36BUDJB"],"subscribed":false},{"client_msg_id":"0835c6dc-2742-4e73-a5e4-f0fced3cdcae","type":"message","text":"maybe also do `@macroexpand` before `@model` and post the output as a reply here","user":"U01H36BUDJB","ts":"1611943529.010400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LTpzi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"maybe also do "},{"type":"text","text":"@macroexpand","style":{"code":true}},{"type":"text","text":" before "},{"type":"text","text":"@model","style":{"code":true}},{"type":"text","text":" and post the output as a reply here"}]}]}],"thread_ts":"1611943529.010400","reply_count":1,"reply_users_count":1,"latest_reply":"1611947119.017100","reply_users":["U6CFMFM2R"],"subscribed":false},{"client_msg_id":"8fbfb201-73a1-488c-ad62-2602437406b3","type":"message","text":"might also be worth trying with Metropolis-Hastings just to check whether or not its the gradient computations that are the problem","user":"U01H36BUDJB","ts":"1611943635.011200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"R2IX6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"might also be worth trying with Metropolis-Hastings just to check whether or not its the gradient computations that are the problem"}]}]}]},{"client_msg_id":"9dec6fe7-ec23-480c-9a9a-a783c6066295","type":"message","text":"maybe the conditionals are the problem? <@U69BL50BF> I thought that the DiffEqFlux docs said tape compilation didn't work in this case.","user":"U01H36BUDJB","ts":"1611943862.012500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"01B2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"maybe the conditionals are the problem? "},{"type":"user","user_id":"U69BL50BF"},{"type":"text","text":" I thought that the DiffEqFlux docs said tape compilation didn't work in this case."}]}]}]},{"client_msg_id":"c0955daa-7a73-4c4e-afcf-5d7d5ccd3425","type":"message","text":"I know it will break your model definition, but maybe try stripping out the conditionals and see if it improves the speed.","user":"U01H36BUDJB","ts":"1611943942.013300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5W2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I know it will break your model definition, but maybe try stripping out the conditionals and see if it improves the speed."}]}]}]},{"client_msg_id":"59d3d938-a7c8-4338-be5f-71b54f8275d7","type":"message","text":"The most frustrating thing is I can't kill the sampling. So when I want to change something and try again to see if it \"fixes it\" I have to kill julia, rerun all my code. This is making the try-test-change loop very difficult","user":"U6CFMFM2R","ts":"1611945382.014100","team":"T68168MUP","edited":{"user":"U6CFMFM2R","ts":"1611945397.000000"},"blocks":[{"type":"rich_text","block_id":"s+rsr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The most frustrating thing is I can't kill the sampling. So when I want to change something and try again to see if it \"fixes it\" I have to kill julia, rerun all my code. This is making the try-test-change loop very difficult"}]}]}]},{"client_msg_id":"25ce0c51-e944-48d7-a4d1-59070471a47a","type":"message","text":"It seems to kill just 1 iteration, and it goes on to the next one","user":"U6CFMFM2R","ts":"1611945423.014500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TYt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It seems to kill just 1 iteration, and it goes on to the next one"}]}]}]},{"client_msg_id":"9794CD9A-9CC9-4FBA-9C05-E9BF8A431FEB","type":"message","text":"Could you set the max iterations to 2 or something like that?","user":"U7THT3TM3","ts":"1611945541.015100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"CS31","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Could you set the max iterations to 2 or something like that?"}]}]}]},{"client_msg_id":"21deb547-b065-43db-8f98-3cd13bf02b7b","type":"message","text":"&gt; maybe the conditionals are the problem? <@U69BL50BF> I thought that the DiffEqFlux docs said tape compilation didn't work in this case.\nThese conditions are not value dependent","user":"U69BL50BF","ts":"1611946047.015500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/L6Se","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"maybe the conditionals are the problem? "},{"type":"user","user_id":"U69BL50BF"},{"type":"text","text":" I thought that the DiffEqFlux docs said tape compilation didn't work in this case."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"These conditions are not value dependent"}]}]}]},{"client_msg_id":"fd941454-da63-4a28-820c-1e9724609423","type":"message","text":"it still represents a static graph","user":"U69BL50BF","ts":"1611946053.015800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"NDJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it still represents a static graph"}]}]}]},{"client_msg_id":"6689b9c0-0e2f-4c5a-b294-a3c71bc0994b","type":"message","text":"Another way of saying it is, for these conditions, the order of true and false is independent of what values you give to the function, so their truth values can all be determined at compile time","user":"U69BL50BF","ts":"1611946104.016500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"I3Bm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Another way of saying it is, for these conditions, the order of true and false is independent of what values you give to the function, so their truth values can all be determined at compile time"}]}]}],"thread_ts":"1611946104.016500","reply_count":1,"reply_users_count":1,"latest_reply":"1611952688.018800","reply_users":["U01H36BUDJB"],"subscribed":false},{"client_msg_id":"c079921d-d777-43e7-a682-a724ea56dd6a","type":"message","text":"that would work with tape compilation.","user":"U69BL50BF","ts":"1611946109.016700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OSlu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"that would work with tape compilation."}]}]}]},{"client_msg_id":"3e41759b-ff7d-4005-9614-f71a0d5a6859","type":"message","text":".35 secs for 10 iterations, 130sec for 100 iterations","user":"U6CFMFM2R","ts":"1611946673.017000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"T18KZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":".35 secs for 10 iterations, 130sec for 100 iterations"}]}]}],"thread_ts":"1611946673.017000","reply_count":4,"reply_users_count":2,"latest_reply":"1611953255.024300","reply_users":["U01H36BUDJB","U6CFMFM2R"],"subscribed":false},{"client_msg_id":"9248f9b1-da21-4338-ac2e-c21edba74831","type":"message","text":"12 secs for 10_000 iterations with MH()","user":"U6CFMFM2R","ts":"1611952311.018000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lRXq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"12 secs for 10_000 iterations with MH()"}]}]}],"thread_ts":"1611952311.018000","reply_count":1,"reply_users_count":1,"latest_reply":"1611952788.019000","reply_users":["U01H36BUDJB"],"subscribed":false},{"client_msg_id":"6e61aac2-cddd-4a9e-a0f1-016404f13884","type":"message","text":"I guess Zygote wouldn't work with this model as written, but you could try Tracker. Beyond that, sounds like an issue needs to be created. The question is whether it's a Turing or ReverseDiff problem.","user":"U01H36BUDJB","ts":"1611952883.021800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7Bi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I guess Zygote wouldn't work with this model as written, but you could try Tracker. Beyond that, sounds like an issue needs to be created. The question is whether it's a Turing or ReverseDiff problem."}]}]}]},{"client_msg_id":"68491fd4-9d9a-4e19-9f99-d6b4fc4a7760","type":"message","text":"whats an easy way to make a MWE for people? The data has like ~800 rows","user":"U6CFMFM2R","ts":"1611952940.022500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MYj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"whats an easy way to make a MWE for people? The data has like ~800 rows"}]}]}],"thread_ts":"1611952940.022500","reply_count":1,"reply_users_count":1,"latest_reply":"1611954811.025300","reply_users":["U01H36BUDJB"],"subscribed":false},{"client_msg_id":"b4a633b3-c6cc-4d70-b01e-0d6820589b9b","type":"message","text":"Did you try removing the conditionals yet? I know it *shouldn't* matter... but still.","user":"U01H36BUDJB","ts":"1611953197.024200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hhO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Did you try removing the conditionals yet? I know it "},{"type":"text","text":"shouldn't","style":{"bold":true}},{"type":"text","text":" matter... but still."}]}]}],"thread_ts":"1611953197.024200","reply_count":12,"reply_users_count":3,"latest_reply":"1611958695.030700","reply_users":["U6CFMFM2R","U01H36BUDJB","U69BL50BF"],"subscribed":false},{"client_msg_id":"2930b2a8-4006-460e-8b77-f972be51342e","type":"message","text":"The loop variable order also should be reversed for column-major, but this doesn't seem to affect anything in my quick experiments. I suppose Julia's compiler is probably smart enough to do this automatically.","user":"U01H36BUDJB","ts":"1611957921.027700","team":"T68168MUP","edited":{"user":"U01H36BUDJB","ts":"1611957941.000000"},"blocks":[{"type":"rich_text","block_id":"4Gq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The loop variable order also should be reversed for column-major, but this doesn't seem to affect anything in my quick experiments. I suppose Julia's compiler is probably smart enough to do this automatically."}]}]}]},{"client_msg_id":"005c85a9-e567-4f02-b7df-6ed8217d805a","type":"message","text":"the cost of AD is probably just more than that.","user":"U69BL50BF","ts":"1611957944.028100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Kuk7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"the cost of AD is probably just more than that."}]}]}]},{"client_msg_id":"708116ee-342b-4c05-9e05-407a925dfa94","type":"message","text":"yeah but I tested it on MH just to see if it did anything","user":"U01H36BUDJB","ts":"1611957957.028500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"chHL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yeah but I tested it on MH just to see if it did anything"}]}]}]},{"client_msg_id":"684f2ea5-b270-4833-89f9-df57dc2884aa","type":"message","text":"reverse mode AD doesn't play by the rules.","user":"U69BL50BF","ts":"1611957958.028700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GL1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"reverse mode AD doesn't play by the rules."}]}]}]},{"client_msg_id":"5d88fa16-18d7-4746-9ee4-5a052b0fd5bc","type":"message","text":"oh okay","user":"U69BL50BF","ts":"1611957963.028900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hXvKk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"oh okay"}]}]}]},{"client_msg_id":"65c96dab-c6e3-4ddb-b553-04811c7f8579","type":"message","text":"it doesn't reverse it, but sometimes you can still SIMD with gaps as of LLVM X (I forget the version but it's quite recent and impacted the ForwardDiff2.jl design)","user":"U69BL50BF","ts":"1611957995.029600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j97Yy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it doesn't reverse it, but sometimes you can still SIMD with gaps as of LLVM X (I forget the version but it's quite recent and impacted the ForwardDiff2.jl design)"}]}]}]},{"client_msg_id":"652dbe54-06b2-405d-b910-cefe0a44e05b","type":"message","text":"oh interesting","user":"U01H36BUDJB","ts":"1611958022.029800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"y4K1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"oh interesting"}]}]}]},{"client_msg_id":"2197c8ea-0447-4978-846c-0c16c166a618","type":"message","text":"Hello, I want to transform prior distribution for downstream use.\n\n```@model function gdemo()\n    sigma ~ Exponential(2)\n    sigmaTransformed = sigma + 1\nend\n\nchn = sample(gdemo(), HMC(0.05, 10), 10)```\n1. What is the Turing way of transforming prior `sigma` to `sigmaTransformed`? Is my code correct or should I be looking into Bijectors.jl?\n2. What is the Turing way of returning `sigmaTransformed` for example for inspecting the posterior? Should i be looking into `generated_quantities` ?\nThank you","user":"U011PPW7K53","ts":"1612171908.036000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iLbM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello, I want to transform prior distribution for downstream use.\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@model function gdemo()\n    sigma ~ Exponential(2)\n    sigmaTransformed = sigma + 1\nend\n\nchn = sample(gdemo(), HMC(0.05, 10), 10)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What is the Turing way of transforming prior "},{"type":"text","text":"sigma","style":{"code":true}},{"type":"text","text":" to "},{"type":"text","text":"sigmaTransformed","style":{"code":true}},{"type":"text","text":"? Is my code correct or should I be looking into Bijectors.jl?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"What is the Turing way of returning "},{"type":"text","text":"sigmaTransformed","style":{"code":true}},{"type":"text","text":" for example for inspecting the posterior? Should i be looking into "},{"type":"text","text":"generated_quantities","style":{"code":true}},{"type":"text","text":" ?"}]}],"style":"ordered","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"\nThank you"}]}]}]},{"client_msg_id":"a581ab5d-0aed-4ccf-9ae5-5e24f4469f40","type":"message","text":"`generated_quantities` is your friend. The model implementation looks correct.","user":"UC0SY9JFP","ts":"1612173376.036600","team":"T68168MUP","edited":{"user":"UC0SY9JFP","ts":"1612269340.000000"},"blocks":[{"type":"rich_text","block_id":"sqe","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"generated_quantities","style":{"code":true}},{"type":"text","text":" is your friend. The model implementation looks correct."}]}]}],"thread_ts":"1612173376.036600","reply_count":1,"reply_users_count":1,"latest_reply":"1612269316.037000","reply_users":["U011PPW7K53"],"subscribed":false,"reactions":[{"name":"+1","users":["U011PPW7K53"],"count":1}]},{"client_msg_id":"64311dc9-4a0a-4a94-9294-23be3950d7e4","type":"message","text":"Hey folks. I was trying the tutorial on Bayesian Differential equations in a Pluto notebook. I am getting an MCMCChains method error -- Failed to show value. Here is the code and the message. I am not sure how I should read the error, like what is the cause of the issue. Any suggestions are appreciated.\n```Turing.setadbackend(:forwarddiff)\n\n@model function fitlv(data, prob1)\n    σ ~ InverseGamma(2, 3) # ~ is the tilde character\n    α ~ truncated(Normal(1.5,0.5),0.5,2.5)\n    β ~ truncated(Normal(1.2,0.5),0,2)\n    γ ~ truncated(Normal(3.0,0.5),1,4)\n    δ ~ truncated(Normal(1.0,0.5),0,2)\n\n    p = [α,β,γ,δ]\n    prob = remake(prob1, p=p)\n    predicted = solve(prob,Tsit5(),saveat=0.1)\n\n    for i = 1:length(predicted)\n        data[:,i] ~ MvNormal(predicted[i], σ)\n    end\nend\n\nmodel = fitlv(odedata, prob1)\n\n# This next command runs 3 independent chains without using multithreading. \nchain = mapreduce(c -&gt; sample(model, NUTS(.65),1000), chainscat, 1:3)```\nHere is the message.\n```Failed to show value:\n\nMethodError: no method matching iterate(::MCMCChains.Chains{Float64,AxisArrays.AxisArray{Float64,3,Array{Float64,3},Tuple{AxisArrays.Axis{:iter,StepRange{Int64,Int64}},AxisArrays.Axis{:var,Array{Symbol,1}},AxisArrays.Axis{:chain,UnitRange{Int64}}}},Missing,NamedTuple{(:parameters, :internals),Tuple{Array{Symbol,1},Array{Symbol,1}}},NamedTuple{(),Tuple{}}})\n\nClosest candidates are:\n\niterate(!Matched::Base.RegexMatchIterator) at regex.jl:552\n\niterate(!Matched::Base.RegexMatchIterator, !Matched::Any) at regex.jl:552\n\niterate(!Matched::Libtask.TRef, !Matched::Any...) at /home/krishnab/.julia/packages/Libtask/00Il9/src/tref.jl:86\n\n...```","user":"UDDSTBX19","ts":"1612292883.038900","team":"T68168MUP","edited":{"user":"UDDSTBX19","ts":"1612293101.000000"},"blocks":[{"type":"rich_text","block_id":"niA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey folks. I was trying the tutorial on Bayesian Differential equations in a Pluto notebook. I am getting an MCMCChains method error -- Failed to show value. Here is the code and the message. I am not sure how I should read the error, like what is the cause of the issue. Any suggestions are appreciated.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Turing.setadbackend(:forwarddiff)\n\n@model function fitlv(data, prob1)\n    σ ~ InverseGamma(2, 3) # ~ is the tilde character\n    α ~ truncated(Normal(1.5,0.5),0.5,2.5)\n    β ~ truncated(Normal(1.2,0.5),0,2)\n    γ ~ truncated(Normal(3.0,0.5),1,4)\n    δ ~ truncated(Normal(1.0,0.5),0,2)\n\n    p = [α,β,γ,δ]\n    prob = remake(prob1, p=p)\n    predicted = solve(prob,Tsit5(),saveat=0.1)\n\n    for i = 1:length(predicted)\n        data[:,i] ~ MvNormal(predicted[i], σ)\n    end\nend\n\nmodel = fitlv(odedata, prob1)\n\n# This next command runs 3 independent chains without using multithreading. \nchain = mapreduce(c -> sample(model, NUTS(.65),1000), chainscat, 1:3)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nHere is the message.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Failed to show value:\n\nMethodError: no method matching iterate(::MCMCChains.Chains{Float64,AxisArrays.AxisArray{Float64,3,Array{Float64,3},Tuple{AxisArrays.Axis{:iter,StepRange{Int64,Int64}},AxisArrays.Axis{:var,Array{Symbol,1}},AxisArrays.Axis{:chain,UnitRange{Int64}}}},Missing,NamedTuple{(:parameters, :internals),Tuple{Array{Symbol,1},Array{Symbol,1}}},NamedTuple{(),Tuple{}}})\n\nClosest candidates are:\n\niterate(!Matched::Base.RegexMatchIterator) at regex.jl:552\n\niterate(!Matched::Base.RegexMatchIterator, !Matched::Any) at regex.jl:552\n\niterate(!Matched::Libtask.TRef, !Matched::Any...) at /home/krishnab/.julia/packages/Libtask/00Il9/src/tref.jl:86\n\n..."}]}]}],"thread_ts":"1612292883.038900","reply_count":15,"reply_users_count":2,"latest_reply":"1612369526.042200","reply_users":["UDDSTBX19","U01H36BUDJB"],"subscribed":false,"reactions":[{"name":"face_with_monocle","users":["UC0SY9JFP"],"count":1}]},{"client_msg_id":"10605262-adea-4cf8-a838-525ba435ab49","type":"message","text":"Hi all-\n\nI noticed that the adaption steps are no longer removed from the chains. Where can I submit an issue?","user":"UH08DT0JU","ts":"1612372127.043400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IdCD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi all-\n\nI noticed that the adaption steps are no longer removed from the chains. Where can I submit an issue?"}]}]}],"thread_ts":"1612372127.043400","reply_count":6,"reply_users_count":2,"latest_reply":"1612430927.044600","reply_users":["UH08DT0JU","U8T9JUA5R"],"subscribed":false},{"client_msg_id":"2a980d06-c441-4c52-b733-30498db59d4e","type":"message","text":"Hi All,\nJust a quick question about syntax. From what I understand, using filldist is more performant than for loops. I am unsure if/how to translate this to use filldist:\n\n`for i ∈ 1:length(predicted)`\n        `data[:,i] ~ MvNormal(predicted[i], σ)` \n`end` \nwhere `predicted` is the solution to an ODEProblem and has size `(5,101)` .\n\nAny suggestions appreciated! :slightly_smiling_face:","user":"U01M641BZEY","ts":"1612464277.049800","team":"T68168MUP","edited":{"user":"U01M641BZEY","ts":"1612464333.000000"},"blocks":[{"type":"rich_text","block_id":"nFDJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi All,\nJust a quick question about syntax. From what I understand, using filldist is more performant than for loops. I am unsure if/how to translate this to use filldist:\n\n"},{"type":"text","text":"for i ∈ 1:length(predicted)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"        data[:,i] ~ MvNormal(predicted[i], σ) ","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"end ","style":{"code":true}},{"type":"text","text":"\nwhere "},{"type":"text","text":"predicted","style":{"code":true}},{"type":"text","text":" is the solution to an ODEProblem and has size "},{"type":"text","text":"(5,101)","style":{"code":true}},{"type":"text","text":" .\n\nAny suggestions appreciated! "},{"type":"emoji","name":"slightly_smiling_face"}]}]}],"thread_ts":"1612464277.049800","reply_count":4,"reply_users_count":2,"latest_reply":"1612466366.050800","reply_users":["U01H36BUDJB","U01M641BZEY"],"subscribed":false},{"client_msg_id":"d3e56c16-9fd2-442c-b044-caf2096fc853","type":"message","text":"Say, I find that whenever I use the command `Turing.turnprogress(false)`, I get an error message\n```ERROR: UndefVarError: turnprogress not defined\nStacktrace:\n [1] getproperty(::Module, ::Symbol) at ./Base.jl:26\n [2] top-level scope at REPL[27]:1```\nDoes this make sense to anyone. I could not find a reference to `turnprogress` in the documentation, and no issues are posted. So perhaps I am missing something simple.","user":"UDDSTBX19","ts":"1612471590.052100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bACP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Say, I find that whenever I use the command "},{"type":"text","text":"Turing.turnprogress(false)","style":{"code":true}},{"type":"text","text":", I get an error message\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: UndefVarError: turnprogress not defined\nStacktrace:\n [1] getproperty(::Module, ::Symbol) at ./Base.jl:26\n [2] top-level scope at REPL[27]:1"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Does this make sense to anyone. I could not find a reference to "},{"type":"text","text":"turnprogress","style":{"code":true}},{"type":"text","text":" in the documentation, and no issues are posted. So perhaps I am missing something simple."}]}]}],"thread_ts":"1612471590.052100","reply_count":2,"reply_users_count":2,"latest_reply":"1612473084.052900","reply_users":["UC0SY9JFP","UDDSTBX19"],"subscribed":false},{"client_msg_id":"7b7c4906-02b1-4001-962c-e6a1bfeac2b1","type":"message","text":"Hi! New user question: I'm having a really hard time navigating the documentation for Turing. I'm learning Julia from scratch, coming from R and taking a Bayesian class using RStan at the moment. (I'm a grad student, and I'm looking to learn Julia on the side.) I'm noticing the API is sort of spread out across different packages, and I was hoping for something a bit more comprehensive if it exists, similar to Stan's manuals. Any pointers would be highly appreciated! :)\n\n(One example: it took me a bit to discover Distributions.jl was a thing, since I kept trying to locate different distributions on the <http://turing.ml|turing.ml> website, with no luck! Another example is figuring out how to initialize my parameters: I see one brief mention of \"init_theta\" on the official guide at <http://turing.ml|turing.ml>, and some GitHub issues here and there, but as far as I can tell, there's no API documentation that tells me what I actually pass in)","user":"U01M951C4JX","ts":"1612680426.059800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ZMk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi! New user question: I'm having a really hard time navigating the documentation for Turing. I'm learning Julia from scratch, coming from R and taking a Bayesian class using RStan at the moment. (I'm a grad student, and I'm looking to learn Julia on the side.) I'm noticing the API is sort of spread out across different packages, and I was hoping for something a bit more comprehensive if it exists, similar to Stan's manuals. Any pointers would be highly appreciated! :)\n\n(One example: it took me a bit to discover Distributions.jl was a thing, since I kept trying to locate different distributions on the "},{"type":"link","url":"http://turing.ml","text":"turing.ml"},{"type":"text","text":" website, with no luck! Another example is figuring out how to initialize my parameters: I see one brief mention of \"init_theta\" on the official guide at "},{"type":"link","url":"http://turing.ml","text":"turing.ml"},{"type":"text","text":", and some GitHub issues here and there, but as far as I can tell, there's no API documentation that tells me what I actually pass in)"}]}]}]},{"client_msg_id":"e787a32e-3e94-479e-99d3-c25631b73eb2","type":"message","text":"Hey Folks. I was trying to understand how to use the `describe()` function on a `ChainDataFrame`. So I used\n`chain = mapreduce(c -&gt; sample(model, NUTS(.65),1000), chainscat, 1:3);`\nto generate multiple chains. But I can't figure out how to use the `describe` function on the chains. When I try `describe(chain)` that just shows `MCMCChains.ChainDataFrame[,]` . When I try `describe[chain[:,:,1]` that does not work either. So I am missing something in the syntax somewhere.","user":"UDDSTBX19","ts":"1612716147.063700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yEA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey Folks. I was trying to understand how to use the "},{"type":"text","text":"describe()","style":{"code":true}},{"type":"text","text":" function on a "},{"type":"text","text":"ChainDataFrame","style":{"code":true}},{"type":"text","text":". So I used\n"},{"type":"text","text":"chain = mapreduce(c -> sample(model, NUTS(.65),1000), chainscat, 1:3);","style":{"code":true}},{"type":"text","text":"\nto generate multiple chains. But I can't figure out how to use the "},{"type":"text","text":"describe","style":{"code":true}},{"type":"text","text":" function on the chains. When I try "},{"type":"text","text":"describe(chain)","style":{"code":true}},{"type":"text","text":" that just shows "},{"type":"text","text":"MCMCChains.ChainDataFrame[,]","style":{"code":true}},{"type":"text","text":" . When I try "},{"type":"text","text":"describe[chain[:,:,1]","style":{"code":true}},{"type":"text","text":" that does not work either. So I am missing something in the syntax somewhere."}]}]}],"thread_ts":"1612716147.063700","reply_count":11,"reply_users_count":2,"latest_reply":"1612717666.067100","reply_users":["U9JNHB83X","UDDSTBX19"],"subscribed":false},{"client_msg_id":"fe21c14c-7992-40ea-a430-ad3126a2ca25","type":"message","text":"Note I am using Pluto. So that might be a little difference between the Pluto output and the command line output?","user":"UDDSTBX19","ts":"1612716184.064300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jURM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Note I am using Pluto. So that might be a little difference between the Pluto output and the command line output?"}]}]}]},{"client_msg_id":"6b3872d7-8865-4f1b-bfd0-d834a8ccca02","type":"message","text":"The full model is:\n```Turing.setadbackend(:forwarddiff)\nfunction lotka_volterra(du,u,p,t)\n  x, y = u\n  α, β, γ, δ  = p\n  du[1] = (α - β*y)x # dx =\n  du[2] = (δ*x - γ)y # dy = \nend\np = [1.5, 1.0, 3.0, 1.0]\nu0 = [1.0,1.0]\n@model function fitlv(data, prob1)\n    σ ~ InverseGamma(2, 3) # ~ is the tilde character\n    α ~ truncated(Normal(1.5,0.5),0.5,2.5)\n    β ~ truncated(Normal(1.2,0.5),0,2)\n    γ ~ truncated(Normal(3.0,0.5),1,4)\n    δ ~ truncated(Normal(1.0,0.5),0,2)\n\n    p = [α,β,γ,δ]\n    prob = remake(prob1, p=p)\n    predicted = solve(prob,Tsit5(),saveat=0.1)\n\n    for i = 1:length(predicted)\n        data[:,i] ~ MvNormal(predicted[i], σ)\n    end\nend\n\nmodel = fitlv(odedata, prob1)\n\n# This next command runs 3 independent chains without using multithreading. \nchain = mapreduce(c -&gt; sample(model, NUTS(.65),1000), chainscat, 1:3)```","user":"UDDSTBX19","ts":"1612716510.065400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dNx+8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The full model is:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Turing.setadbackend(:forwarddiff)\nfunction lotka_volterra(du,u,p,t)\n  x, y = u\n  α, β, γ, δ  = p\n  du[1] = (α - β*y)x # dx =\n  du[2] = (δ*x - γ)y # dy = \nend\np = [1.5, 1.0, 3.0, 1.0]\nu0 = [1.0,1.0]\n@model function fitlv(data, prob1)\n    σ ~ InverseGamma(2, 3) # ~ is the tilde character\n    α ~ truncated(Normal(1.5,0.5),0.5,2.5)\n    β ~ truncated(Normal(1.2,0.5),0,2)\n    γ ~ truncated(Normal(3.0,0.5),1,4)\n    δ ~ truncated(Normal(1.0,0.5),0,2)\n\n    p = [α,β,γ,δ]\n    prob = remake(prob1, p=p)\n    predicted = solve(prob,Tsit5(),saveat=0.1)\n\n    for i = 1:length(predicted)\n        data[:,i] ~ MvNormal(predicted[i], σ)\n    end\nend\n\nmodel = fitlv(odedata, prob1)\n\n# This next command runs 3 independent chains without using multithreading. \nchain = mapreduce(c -> sample(model, NUTS(.65),1000), chainscat, 1:3)"}]}]}]},{"client_msg_id":"15c166f2-e967-46bb-990f-fc766cfe9570","type":"message","text":"Hi,\n\n I’m trying to get von Mises distributions to work with Turing and would like some advise on how best to proceed. I’ve got a simple FowardDiff implementation of SpecialFunctions besselix to enable HMC/NUTS to work with the distribution, but I’ve now got a more fundamental problem with the definition in Distributions.jl.\n\n If I test a distribution with a low κ (i.e. large variance) almost all the test proposals fail the isinfinite check. Looking through the code, the problem appears to be the definition of the range of the von Mises function:\n\nWith low κ the observation values (y) cover the whole range from -π to π, so when a test point has a non zero mean (μ) there’s always some observations where |y-μ|&gt;π. This violates the definition given in Distributions.jl\n\n`@distr_support VonMises d.μ - π d.μ + π`\n\nIf I remove the value check in the Distributions.jl logpdf definition, sampling works well. And, at least for my use case, this is perfectly valid definition. von Mises is then a continuous, repeating, distribution, and any limitations on the range of μ are defined in the prior.\n\nI’ve been trying to understand why Distributions.jl has this range definition, but haven’t been able to find much. It was introduced in #273 with this <https://github.com/JuliaStats/Distributions.jl/issues/273#issuecomment-61982908|comment> but without further discussion, or explanation in the quoted source (wikipedia).\n\nSo, does anyone know why von Mises has this range? And what would be the best way to get it working with Turing? A fix for Distributions.jl or an import of the logpdf function within Turing?\n\nThanks.","user":"U01JL6RGKU7","ts":"1612784378.068100","team":"T68168MUP","edited":{"user":"U01JL6RGKU7","ts":"1612784870.000000"},"blocks":[{"type":"rich_text","block_id":"ZG5Py","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi,\n\n I’m trying to get von Mises distributions to work with Turing and would like some advise on how best to proceed. I’ve got a simple FowardDiff implementation of SpecialFunctions besselix to enable HMC/NUTS to work with the distribution, but I’ve now got a more fundamental problem with the definition in Distributions.jl.\n\n If I test a distribution with a low κ (i.e. large variance) almost all the test proposals fail the isinfinite check. Looking through the code, the problem appears to be the definition of the range of the von Mises function:\n\nWith low κ the observation values (y) cover the whole range from -π to π, so when a test point has a non zero mean (μ) there’s always some observations where |y-μ|>π. This violates the definition given in Distributions.jl\n\n"},{"type":"text","text":"@distr_support VonMises d.μ - π d.μ + π","style":{"code":true}},{"type":"text","text":"\n\nIf I remove the value check in the Distributions.jl logpdf definition, sampling works well. And, at least for my use case, this is perfectly valid definition. von Mises is then a continuous, repeating, distribution, and any limitations on the range of μ are defined in the prior.\n\nI’ve been trying to understand why Distributions.jl has this range definition, but haven’t been able to find much. It was introduced in #273 with this "},{"type":"link","url":"https://github.com/JuliaStats/Distributions.jl/issues/273#issuecomment-61982908","text":"comment"},{"type":"text","text":" but without further discussion, or explanation in the quoted source (wikipedia).\n\nSo, does anyone know why von Mises has this range? And what would be the best way to get it working with Turing? A fix for Distributions.jl or an import of the logpdf function within Turing?\n\nThanks."}]}]}],"thread_ts":"1612784378.068100","reply_count":1,"reply_users_count":1,"latest_reply":"1612786123.068300","reply_users":["UC0SY9JFP"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"Another issue with circular distributions parameterized by an angle with NUTS is that you either are supported on the entire real line, in which case the distribution has infinite modes (and if you can transition between nodes in NUTS, then this will cause poor adaptation), or you are supported only on the the range [0, pi], in which case you could bifurcate a single mode and only explore one of them. The same is true whether we're talking about the random variable or the mean parameter of von Mises. The way Stan handles this is to internally represent circular variables in terms of two normally distributed cartesian coordinates x and y such that theta = atan(y, x). theta is then uniformly distributed on the circle. To apply a von Mises distribution, to theta, you'd want to increment the logpdf manually. Does Turing give us access to this? In brief something like\n```# equivalent to `μ ~ UniformOnCircle(); θ ~ VonMises(μ)` if we had such a thing\n@model function vMmodel(θ)\n    μx ~ Normal()\n    μy ~ Normal()\n    μ = atan(μy, μx)\n    lp += logpdf(VonMises(μ), θ) # is there a syntax for this?\nend```\n","user":"UHDQQ4GN6","ts":"1612825842.070100","thread_ts":"1612784378.068100","root":{"client_msg_id":"15c166f2-e967-46bb-990f-fc766cfe9570","type":"message","text":"Hi,\n\n I’m trying to get von Mises distributions to work with Turing and would like some advise on how best to proceed. I’ve got a simple FowardDiff implementation of SpecialFunctions besselix to enable HMC/NUTS to work with the distribution, but I’ve now got a more fundamental problem with the definition in Distributions.jl.\n\n If I test a distribution with a low κ (i.e. large variance) almost all the test proposals fail the isinfinite check. Looking through the code, the problem appears to be the definition of the range of the von Mises function:\n\nWith low κ the observation values (y) cover the whole range from -π to π, so when a test point has a non zero mean (μ) there’s always some observations where |y-μ|&gt;π. This violates the definition given in Distributions.jl\n\n`@distr_support VonMises d.μ - π d.μ + π`\n\nIf I remove the value check in the Distributions.jl logpdf definition, sampling works well. And, at least for my use case, this is perfectly valid definition. von Mises is then a continuous, repeating, distribution, and any limitations on the range of μ are defined in the prior.\n\nI’ve been trying to understand why Distributions.jl has this range definition, but haven’t been able to find much. It was introduced in #273 with this <https://github.com/JuliaStats/Distributions.jl/issues/273#issuecomment-61982908|comment> but without further discussion, or explanation in the quoted source (wikipedia).\n\nSo, does anyone know why von Mises has this range? And what would be the best way to get it working with Turing? A fix for Distributions.jl or an import of the logpdf function within Turing?\n\nThanks.","user":"U01JL6RGKU7","ts":"1612784378.068100","team":"T68168MUP","edited":{"user":"U01JL6RGKU7","ts":"1612784870.000000"},"blocks":[{"type":"rich_text","block_id":"ZG5Py","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi,\n\n I’m trying to get von Mises distributions to work with Turing and would like some advise on how best to proceed. I’ve got a simple FowardDiff implementation of SpecialFunctions besselix to enable HMC/NUTS to work with the distribution, but I’ve now got a more fundamental problem with the definition in Distributions.jl.\n\n If I test a distribution with a low κ (i.e. large variance) almost all the test proposals fail the isinfinite check. Looking through the code, the problem appears to be the definition of the range of the von Mises function:\n\nWith low κ the observation values (y) cover the whole range from -π to π, so when a test point has a non zero mean (μ) there’s always some observations where |y-μ|>π. This violates the definition given in Distributions.jl\n\n"},{"type":"text","text":"@distr_support VonMises d.μ - π d.μ + π","style":{"code":true}},{"type":"text","text":"\n\nIf I remove the value check in the Distributions.jl logpdf definition, sampling works well. And, at least for my use case, this is perfectly valid definition. von Mises is then a continuous, repeating, distribution, and any limitations on the range of μ are defined in the prior.\n\nI’ve been trying to understand why Distributions.jl has this range definition, but haven’t been able to find much. It was introduced in #273 with this "},{"type":"link","url":"https://github.com/JuliaStats/Distributions.jl/issues/273#issuecomment-61982908","text":"comment"},{"type":"text","text":" but without further discussion, or explanation in the quoted source (wikipedia).\n\nSo, does anyone know why von Mises has this range? And what would be the best way to get it working with Turing? A fix for Distributions.jl or an import of the logpdf function within Turing?\n\nThanks."}]}]}],"thread_ts":"1612784378.068100","reply_count":4,"reply_users_count":4,"latest_reply":"1612825842.070100","reply_users":["UC0SY9JFP","U01H36BUDJB","U01JL6RGKU7","UHDQQ4GN6"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"sjC9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Another issue with circular distributions parameterized by an angle with NUTS is that you either are supported on the entire real line, in which case the distribution has infinite modes (and if you can transition between nodes in NUTS, then this will cause poor adaptation), or you are supported only on the the range [0, pi], in which case you could bifurcate a single mode and only explore one of them. The same is true whether we're talking about the random variable or the mean parameter of von Mises. The way Stan handles this is to internally represent circular variables in terms of two normally distributed cartesian coordinates x and y such that theta = atan(y, x). theta is then uniformly distributed on the circle. To apply a von Mises distribution, to theta, you'd want to increment the logpdf manually. Does Turing give us access to this? In brief something like\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"# equivalent to `μ ~ UniformOnCircle(); θ ~ VonMises(μ)` if we had such a thing\n@model function vMmodel(θ)\n    μx ~ Normal()\n    μy ~ Normal()\n    μ = atan(μy, μx)\n    lp += logpdf(VonMises(μ), θ) # is there a syntax for this?\nend"}]},{"type":"rich_text_section","elements":[]}]}],"client_msg_id":"eaebf69d-e77b-4b42-9a18-927892bb3470"},{"client_msg_id":"5fdcced9-aa47-46ff-bcd2-e2e5c8fbe98e","type":"message","text":"Could someone help with : <https://discourse.julialang.org/t/sampling-from-power-posterior/54924>","user":"U7QLM6E2E","ts":"1612881181.074600","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"Sampling from power posterior","title_link":"https://discourse.julialang.org/t/sampling-from-power-posterior/54924","text":"Hey, I was wondering if someone could guide me (or direct me to the relevant code) on how to sample from the power posterior given a Turing model. By power posterior, I mean the posterior to modified joint : p(x|y; b) \\propto p(y|x)^b p(x), where b \\in [0,1].","fallback":"JuliaLang: Sampling from power posterior","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","fields":[{"title":"Reading time","value":"1 mins :clock2:","short":true},{"title":"Likes","value":"3 :heart:","short":true}],"ts":1612876144,"from_url":"https://discourse.julialang.org/t/sampling-from-power-posterior/54924","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/sampling-from-power-posterior/54924"}],"blocks":[{"type":"rich_text","block_id":"MNZsE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Could someone help with : "},{"type":"link","url":"https://discourse.julialang.org/t/sampling-from-power-posterior/54924"}]}]}],"thread_ts":"1612881181.074600","reply_count":4,"reply_users_count":2,"latest_reply":"1612882233.075600","reply_users":["UC0SY9JFP","U7QLM6E2E"],"subscribed":false},{"client_msg_id":"2ed1f7fd-5b7e-4550-a5fd-b159646976cb","type":"message","text":"I think the searchbar on <http://turing.ml|turing.ml> is broken btw.","user":"U01C2AJ9F63","ts":"1612963752.082700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6qbz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think the searchbar on "},{"type":"link","url":"http://turing.ml","text":"turing.ml"},{"type":"text","text":" is broken btw."}]}]}],"reactions":[{"name":"confused","users":["UC0SY9JFP"],"count":1}]},{"client_msg_id":"a532a28b-272a-4f9d-8d89-4f61f7e48edf","type":"message","text":"multiple hierarchical models with different sample sizes","user":"U01HSP1E1NW","ts":"1613004206.084100","team":"T68168MUP","edited":{"user":"U01HSP1E1NW","ts":"1613004670.000000"},"blocks":[{"type":"rich_text","block_id":"y7F","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"multiple hierarchical models with different sample sizes"}]}]}],"thread_ts":"1613004206.084100","reply_count":1,"reply_users_count":1,"latest_reply":"1613004561.084900","reply_users":["U01HSP1E1NW"],"subscribed":false},{"client_msg_id":"177374a5-6705-43ac-a9f5-6a51e9a79639","type":"message","text":"At the suggestion of someone very helpful at “helpdesk” I’m writing to ask about using Turing for Bayesian linear regression.  I’ve found the great documentation example: <https://turing.ml/dev/tutorials/5-linearregression/> and am working to use that a starter for the problem I’m looking to solve.\n\nThat problem is a Bayesian linear regression case where the with priors on the parameters are drawn from an underlying power-law distribution.  But I haven’t been able to find a power law distribution in Distributions.jl so I’m not sure how to go about trying implement this.\n\nThoughts and guidance much appreciated!","user":"UHRBR18HH","ts":"1613016923.085400","team":"T68168MUP","attachments":[{"title":"Linear Regression","title_link":"https://turing.ml/dev/tutorials/5-linearregression/","text":"Linear Regression","fallback":"Linear Regression","from_url":"https://turing.ml/dev/tutorials/5-linearregression/","service_icon":"https://turing.ml/dev/assets/img/favicon.ico","service_name":"turing.ml","id":1,"original_url":"https://turing.ml/dev/tutorials/5-linearregression/"}],"blocks":[{"type":"rich_text","block_id":"0+4yd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"At the suggestion of someone very helpful at “helpdesk” I’m writing to ask about using Turing for Bayesian linear regression.  I’ve found the great documentation example: "},{"type":"link","url":"https://turing.ml/dev/tutorials/5-linearregression/"},{"type":"text","text":" and am working to use that a starter for the problem I’m looking to solve.\n\nThat problem is a Bayesian linear regression case where the with priors on the parameters are drawn from an underlying power-law distribution.  But I haven’t been able to find a power law distribution in Distributions.jl so I’m not sure how to go about trying implement this.\n\nThoughts and guidance much appreciated!"}]}]}]},{"client_msg_id":"e103a083-9064-40e0-872e-3b8bd7befdee","type":"message","text":"quick question: can variational inference benefit from multithreading?","user":"U01M641BZEY","ts":"1613070473.090000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"W+YL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"quick question: can variational inference benefit from multithreading?"}]}]}]},{"client_msg_id":"869be028-f8de-486b-bd75-e21fc35b0711","type":"message","text":"Is there a way to infer from a Turing model if the likelihood is factorizable, i.e.  : `p(y|x) = \\prod_i p(y|x_i)` ?","user":"U7QLM6E2E","ts":"1613125563.093900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OFy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to infer from a Turing model if the likelihood is factorizable, i.e.  : "},{"type":"text","text":"p(y|x) = \\prod_i p(y|x_i)","style":{"code":true}},{"type":"text","text":" ?"}]}]}],"thread_ts":"1613125563.093900","reply_count":3,"reply_users_count":2,"latest_reply":"1613125764.094400","reply_users":["UC0SY9JFP","U7QLM6E2E"],"subscribed":false},{"client_msg_id":"255dcfc8-baf8-4c14-adf5-0ebb1174849a","type":"message","text":"Is there currently an predefined dense MvNormal variational posterior that I can set using something like vi(model, advi, q=DenseMvNormal) ?","user":"U01M641BZEY","ts":"1613149586.096000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jC8L","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there currently an predefined dense MvNormal variational posterior that I can set using something like vi(model, advi, q=DenseMvNormal) ?"}]}]}],"thread_ts":"1613149586.096000","reply_count":1,"reply_users_count":1,"latest_reply":"1613149665.096100","reply_users":["U01M641BZEY"],"subscribed":false},{"client_msg_id":"cf4e740c-d1d7-4c3a-b89b-cca8113d802d","type":"message","text":"Hi guys, trying to extend the GMM example into something a bit more general and running into problems, roughly:\n1. `filldist` doesn’t work on `InverseWishart` for learning cov matrices of the mixture components, have bumped an issue on DistributionsAD I found regarding this, is this an easy fix, not familiar with the underlying code, in the mean time what is the best way to jerry-rig something?\n2. Is doing a for loop as in the example actually efficient, I am not sure what formulation is preferable, is there some way to do a filldist on the `MvNormal` ’s for `x` to cycle over the drawn `k` ?\n3. If I want to say add some multiplier parameter to the `MvNormal` distribution, is it preferable to define a custom distribution or to use `Turing.@addlogprob!` as I am doing now to just add the log pdf multiplied with something I pass in? Mainly bothered about performance implications but also what comprises “good practice” here?\n4. Slight aside as I am sure I am missing something dumb but for some reason there is no progress bar in the Julia prompt when I run this model, I have tried `Turing.setprogress!(true)` etc to no avail\nMore generally I would appreciate any suggested improvements, as I plan on using this with quite a big dataset so any modest gains would be nice :slightly_smiling_face:\n\n``` @model LikelihoodWeightedGMM(x, K, ws) = begin\n    \n    D, N = size(x)\n\n    # Draw the parameters for the clusters.\n    # μ ~ filldist(MvNormal([0.5, 0.5], 0.5 * I(2)), K)\n    μ1 ~ MvNormal(repeat([0.5], K), 0.5)\n    μ2 ~ MvNormal(repeat([0.5], K), 0.5)\n    μ = [[μ1[i], μ2[i]] for i in 1:length(μ1)]\n\n    # Σ ~ filldist(InverseWishart(3, Matrix{Float64}([[2,0.5] [0.5,2]])), K)\n\n    # Draw the weights for the K clusters from a Dirichlet distribution.\n    α = 1.0\n    w ~ Dirichlet(K, α)\n    \n    # Comment out this line if you instead want to have uniform weights.\n    # w = repeat([1 / K], K)\n    \n    # Draw assignments for each datum and generate it from a multivariate normal.\n    k ~ filldist(Categorical(w), N)\n\n    # x .~ MvNormal.(μ[k], repeat([0.05], length(k)))\n    for i in 1:N\n        Turing.@addlogprob! logpdf(MvNormal([μ1[k[i]], μ2[k[i]]], 0.05), x[:, i]) * ws[i]\n        # Turing.@addlogprob! logpdf(MvNormal([μ1[k[i]], μ2[k[i]]], Σ[k[i]]), x[:, i]) * ws[i]\n    end\n    return k\nend```","user":"UMS7H0ASG","ts":"1613211463.103000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WXc1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi guys, trying to extend the GMM example into something a bit more general and running into problems, roughly:\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"filldist","style":{"code":true}},{"type":"text","text":" doesn’t work on "},{"type":"text","text":"InverseWishart","style":{"code":true}},{"type":"text","text":" for learning cov matrices of the mixture components, have bumped an issue on DistributionsAD I found regarding this, is this an easy fix, not familiar with the underlying code, in the mean time what is the best way to jerry-rig something?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Is doing a for loop as in the example actually efficient, I am not sure what formulation is preferable, is there some way to do a filldist on the "},{"type":"text","text":"MvNormal","style":{"code":true}},{"type":"text","text":" ’s for "},{"type":"text","text":"x","style":{"code":true}},{"type":"text","text":" to cycle over the drawn "},{"type":"text","text":"k","style":{"code":true}},{"type":"text","text":" ?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"If I want to say add some multiplier parameter to the "},{"type":"text","text":"MvNormal","style":{"code":true}},{"type":"text","text":" distribution, is it preferable to define a custom distribution or to use "},{"type":"text","text":"Turing.@addlogprob!","style":{"code":true}},{"type":"text","text":" as I am doing now to just add the log pdf multiplied with something I pass in? Mainly bothered about performance implications but also what comprises “good practice” here?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Slight aside as I am sure I am missing something dumb but for some reason there is no progress bar in the Julia prompt when I run this model, I have tried "},{"type":"text","text":"Turing.setprogress!(true)","style":{"code":true}},{"type":"text","text":" etc to no avail"}]}],"style":"ordered","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"More generally I would appreciate any suggested improvements, as I plan on using this with quite a big dataset so any modest gains would be nice "},{"type":"emoji","name":"slightly_smiling_face"},{"type":"text","text":"\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":" @model LikelihoodWeightedGMM(x, K, ws) = begin\n    \n    D, N = size(x)\n\n    # Draw the parameters for the clusters.\n    # μ ~ filldist(MvNormal([0.5, 0.5], 0.5 * I(2)), K)\n    μ1 ~ MvNormal(repeat([0.5], K), 0.5)\n    μ2 ~ MvNormal(repeat([0.5], K), 0.5)\n    μ = [[μ1[i], μ2[i]] for i in 1:length(μ1)]\n\n    # Σ ~ filldist(InverseWishart(3, Matrix{Float64}([[2,0.5] [0.5,2]])), K)\n\n    # Draw the weights for the K clusters from a Dirichlet distribution.\n    α = 1.0\n    w ~ Dirichlet(K, α)\n    \n    # Comment out this line if you instead want to have uniform weights.\n    # w = repeat([1 / K], K)\n    \n    # Draw assignments for each datum and generate it from a multivariate normal.\n    k ~ filldist(Categorical(w), N)\n\n    # x .~ MvNormal.(μ[k], repeat([0.05], length(k)))\n    for i in 1:N\n        Turing.@addlogprob! logpdf(MvNormal([μ1[k[i]], μ2[k[i]]], 0.05), x[:, i]) * ws[i]\n        # Turing.@addlogprob! logpdf(MvNormal([μ1[k[i]], μ2[k[i]]], Σ[k[i]]), x[:, i]) * ws[i]\n    end\n    return k\nend"}]}]}]},{"client_msg_id":"655266ff-507a-4dcd-8fde-439d05d18c66","type":"message","text":"I would use a loop until it’s too slow. No need to over-complicate a model if it’s fast enough. Similarly, use `ForwardDiff` until it’s too slow. Then try `ReverseDiff` or `Zygote`. If nothing else works, analyse bottlenecks. Which parameter that when removed or fixed speeds up your inference? Then look into speeding this one up. You can even write your own distribution for it with un-normalised logpdf. `filldist` essentially builds a custom distribution. The scaling of the logpdf can also be handled using a custom distribution. CC: <@U7QLM6E2E> who was trying to solve a similar problem not too long ago but I didn’t think of this solution back then.","user":"U85JBUGGP","ts":"1613216679.107100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QucID","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I would use a loop until it’s too slow. No need to over-complicate a model if it’s fast enough. Similarly, use "},{"type":"text","text":"ForwardDiff","style":{"code":true}},{"type":"text","text":" until it’s too slow. Then try "},{"type":"text","text":"ReverseDiff","style":{"code":true}},{"type":"text","text":" or "},{"type":"text","text":"Zygote","style":{"code":true}},{"type":"text","text":". If nothing else works, analyse bottlenecks. Which parameter that when removed or fixed speeds up your inference? Then look into speeding this one up. You can even write your own distribution for it with un-normalised logpdf. "},{"type":"text","text":"filldist","style":{"code":true}},{"type":"text","text":" essentially builds a custom distribution. The scaling of the logpdf can also be handled using a custom distribution. CC: "},{"type":"user","user_id":"U7QLM6E2E"},{"type":"text","text":" who was trying to solve a similar problem not too long ago but I didn’t think of this solution back then."}]}]}],"thread_ts":"1613216679.107100","reply_count":4,"reply_users_count":2,"latest_reply":"1613217749.107900","reply_users":["UMS7H0ASG","U85JBUGGP"],"subscribed":false},{"client_msg_id":"fd371119-693e-4a23-b731-a3913943636e","type":"message","text":"Hello, how do I make my prior ordered? MWE\n\n```@model function gdemo(y)\n    theta ~ filldist(Normal(1, 2),6)\n    y .~ OrderedLogistic(0, theta) \nend```\nTo be specific, I want to order my `theta` priors for `OrderedLogistic` so `theta[6] &gt; theta[5] &gt; theta[4] &gt; theta[3] &gt; theta[2] &gt; theta[1]` ?\n\nIf this isn't supported yet, is there any dirty workaround? I am stucked with this for a long time so I don't mind some workaround at the moment :eyes:","user":"U011PPW7K53","ts":"1613219560.111200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ZfIG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello, how do I make my prior ordered? MWE\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@model function gdemo(y)\n    theta ~ filldist(Normal(1, 2),6)\n    y .~ OrderedLogistic(0, theta) \nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nTo be specific, I want to order my "},{"type":"text","text":"theta","style":{"code":true}},{"type":"text","text":" priors for "},{"type":"text","text":"OrderedLogistic","style":{"code":true}},{"type":"text","text":" so "},{"type":"text","text":"theta[6] > theta[5] > theta[4] > theta[3] > theta[2] > theta[1]","style":{"code":true}},{"type":"text","text":" ?\n\nIf this isn't supported yet, is there any dirty workaround? I am stucked with this for a long time so I don't mind some workaround at the moment "},{"type":"emoji","name":"eyes"}]}]}],"thread_ts":"1613219560.111200","reply_count":1,"reply_users_count":1,"latest_reply":"1613220383.111300","reply_users":["U011PPW7K53"],"subscribed":false},{"client_msg_id":"e996e0fb-e9ce-41e7-8d5b-92ec5227d6d6","type":"message","text":"I remember a while ago talking to a couple of people about initialising parameters in Turing, has that been properly implemented yet? It is mentioned in the docs but cannot see a way to actually do it looking at the codebase","user":"UMS7H0ASG","ts":"1613250657.113200","team":"T68168MUP","edited":{"user":"UMS7H0ASG","ts":"1613250666.000000"},"blocks":[{"type":"rich_text","block_id":"wQCn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I remember a while ago talking to a couple of people about initialising parameters in Turing, has that been properly implemented yet? It is mentioned in the docs but cannot see a way to actually do it looking at the codebase"}]}]}]},{"type":"message","subtype":"thread_broadcast","text":"So I have noticed that when I try to fit a GMM where the components have low variance the sampling fails in that I guess perhaps nothing is meeting acceptance criteria for HMC? Does anyone see a fix for the problem I have outlined properly here with a comparison of graphs at different component sigmas: <https://discourse.julialang.org/t/strange-issues-fitting-gmms-in-turing-jl-by-extending-a-fairly-simple-example/55232> ? :slightly_smiling_face:","user":"UMS7H0ASG","ts":"1613255469.119800","thread_ts":"1613250657.113200","root":{"client_msg_id":"e996e0fb-e9ce-41e7-8d5b-92ec5227d6d6","type":"message","text":"I remember a while ago talking to a couple of people about initialising parameters in Turing, has that been properly implemented yet? It is mentioned in the docs but cannot see a way to actually do it looking at the codebase","user":"UMS7H0ASG","ts":"1613250657.113200","team":"T68168MUP","edited":{"user":"UMS7H0ASG","ts":"1613250666.000000"},"blocks":[{"type":"rich_text","block_id":"wQCn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I remember a while ago talking to a couple of people about initialising parameters in Turing, has that been properly implemented yet? It is mentioned in the docs but cannot see a way to actually do it looking at the codebase"}]}]}],"thread_ts":"1613250657.113200","reply_count":26,"reply_users_count":2,"latest_reply":"1613255469.119800","reply_users":["U9JNHB83X","UMS7H0ASG"],"subscribed":false},"attachments":[{"service_name":"JuliaLang","title":"Strange issues fitting GMMs in Turing.jl by extending a fairly simple example","title_link":"https://discourse.julialang.org/t/strange-issues-fitting-gmms-in-turing-jl-by-extending-a-fairly-simple-example/55232","text":"I have been extending the Turing example on Bayesian GMMs today in a number of ways, trying it out with non-symmetric 2D Gaussian locations and more clusters than the two in the example I have linked, and namely with some more tightly distributed components. This is where I have run into a problem I am struggling to diagnose and wondering if it is something looking into further. I first pick my parameters and generate some data: using Distributions, StatsPlots, Random, KernelDensity # Set a ra...","fallback":"JuliaLang: Strange issues fitting GMMs in Turing.jl by extending a fairly simple example","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/3X/d/0/d0f10d047cf6628e840b3f6498595ebaaa87ae50_2_1024x358.jpeg","ts":1613255325,"from_url":"https://discourse.julialang.org/t/strange-issues-fitting-gmms-in-turing-jl-by-extending-a-fairly-simple-example/55232","thumb_width":1024,"thumb_height":358,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/strange-issues-fitting-gmms-in-turing-jl-by-extending-a-fairly-simple-example/55232"}],"blocks":[{"type":"rich_text","block_id":"1dWa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So I have noticed that when I try to fit a GMM where the components have low variance the sampling fails in that I guess perhaps nothing is meeting acceptance criteria for HMC? Does anyone see a fix for the problem I have outlined properly here with a comparison of graphs at different component sigmas: "},{"type":"link","url":"https://discourse.julialang.org/t/strange-issues-fitting-gmms-in-turing-jl-by-extending-a-fairly-simple-example/55232"},{"type":"text","text":" ? "},{"type":"emoji","name":"slightly_smiling_face"}]}]}],"client_msg_id":"8cbcc685-b692-469a-b3f8-621ec21c76c9"},{"client_msg_id":"2959cd82-0d89-4ec0-b938-04fcea6589f1","type":"message","text":"Hey folks, does anyone know of an Julia example of running a dynamical system on a graph/network and then using Turing the estimate the parameters of that dynamical system. Seems like I could just use the Julia DifferentialEquations `DiscreteProblem` to define the problem and then run this inside of Turing as I normally would. Am I oversimplifying, or would something like this work. I imagine that running something like this might take a while, depending on the size of the graph. Right now this is just a proof of concept--so I can keep the graph pretty small.","user":"UDDSTBX19","ts":"1613333943.125600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jGI2e","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey folks, does anyone know of an Julia example of running a dynamical system on a graph/network and then using Turing the estimate the parameters of that dynamical system. Seems like I could just use the Julia DifferentialEquations "},{"type":"text","text":"DiscreteProblem","style":{"code":true}},{"type":"text","text":" to define the problem and then run this inside of Turing as I normally would. Am I oversimplifying, or would something like this work. I imagine that running something like this might take a while, depending on the size of the graph. Right now this is just a proof of concept--so I can keep the graph pretty small."}]}]}]},{"client_msg_id":"58529025-fb07-4446-b289-39c381c802a9","type":"message","text":"I know people doing NetworkDynamics.jl and the like are doing it with differentiation, not necessarily Turing, but that's the connection.","user":"U69BL50BF","ts":"1613334541.126800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/cQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I know people doing NetworkDynamics.jl and the like are doing it with differentiation, not necessarily Turing, but that's the connection."}]}]}]},{"client_msg_id":"4de86fb5-b451-4956-a812-ed9ca2119b4d","type":"message","text":"<@U69BL50BF> Oh okay. by differentiation do you mean the Global Sensitivity Analysis idea. That does make sense.","user":"UDDSTBX19","ts":"1613334870.130800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DTlR/","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U69BL50BF"},{"type":"text","text":" Oh okay. by differentiation do you mean the Global Sensitivity Analysis idea. That does make sense."}]}]}]},{"client_msg_id":"a7f6d5e4-63de-4356-89a6-96a1dddc2b99","type":"message","text":"no, I mean using adjoints and forward sensitivities of the solution of the dynamics on the graph","user":"U69BL50BF","ts":"1613335160.131500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8PcH3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"no, I mean using adjoints and forward sensitivities of the solution of the dynamics on the graph"}]}]}]},{"client_msg_id":"b3293579-f3b1-4dfe-9ce6-d762af6827d8","type":"message","text":"Ahh okay. I understand what you mean now. Thanks so much, I appreciate it.","user":"UDDSTBX19","ts":"1613335255.132200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jb7bv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ahh okay. I understand what you mean now. Thanks so much, I appreciate it."}]}]}]},{"client_msg_id":"E894B677-F63A-4A8D-B27B-8CEFCA84C8AA","type":"message","text":"What is your understanding of a dynamical system on a graph?","user":"U6C937ENB","ts":"1613413143.133600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2bl8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What is your understanding of a dynamical system on a graph?"}]}]}]},{"client_msg_id":"2be8db4e-5c6b-484a-8e17-cc94f1941db6","type":"message","text":"<@U6C937ENB> If we take something like diffusion on a graph, then I would have a parameter for the probability of transmitting heat or whatever to the adjacent nodes. Now as I vary the parameter value, I would essentially have a different time series for the fraction of the graph infected or the locations of infection over time. If I have some time series data, I was hoping to identify the best parameter estimate for the transmission (and corresponding uncertainty), given the data. So something like that. Of course I have to make some strong assumptions about the fixed size of the graph, etc. But I was just wondering if I could estimate something like that using Turing. Does that answer your question.","user":"UDDSTBX19","ts":"1613413862.137500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"euLU","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U6C937ENB"},{"type":"text","text":" If we take something like diffusion on a graph, then I would have a parameter for the probability of transmitting heat or whatever to the adjacent nodes. Now as I vary the parameter value, I would essentially have a different time series for the fraction of the graph infected or the locations of infection over time. If I have some time series data, I was hoping to identify the best parameter estimate for the transmission (and corresponding uncertainty), given the data. So something like that. Of course I have to make some strong assumptions about the fixed size of the graph, etc. But I was just wondering if I could estimate something like that using Turing. Does that answer your question."}]}]}]},{"client_msg_id":"d43941fd-0566-40d6-9c8b-505b10bb4b54","type":"message","text":"Sounds like it could be formulated as estimating the transition matrix of a suitable markov chain","user":"U019K6Q9N15","ts":"1613414836.138000","team":"T68168MUP","edited":{"user":"U019K6Q9N15","ts":"1613414865.000000"},"blocks":[{"type":"rich_text","block_id":"SNWCV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Sounds like it could be formulated as estimating the transition matrix of a suitable markov chain"}]}]}],"reactions":[{"name":"heavy_check_mark","users":["U6C937ENB"],"count":1}]},{"client_msg_id":"C9559384-2FD5-4CC9-BF04-CA96F1B276C8","type":"message","text":"We did pretty much that assuming a lattice (by hand without pp) <https://github.com/fmeulen/GuidedDAGExamples.jl/tree/master/src/sir|https://github.com/fmeulen/GuidedDAGExamples.jl/tree/master/src/sir>","user":"U6C937ENB","ts":"1613415047.139300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=N5/y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We did pretty much that assuming a lattice (by hand without pp) "},{"type":"link","url":"https://github.com/fmeulen/GuidedDAGExamples.jl/tree/master/src/sir","text":"https://github.com/fmeulen/GuidedDAGExamples.jl/tree/master/src/sir"}]}]}]},{"client_msg_id":"0E69F9AE-254D-41A9-9127-904E4D274340","type":"message","text":"<https://raw.githubusercontent.com/fmeulen/GuidedDAGExamples.jl/master/src/sir/figs/all.png|https://raw.githubusercontent.com/fmeulen/GuidedDAGExamples.jl/master/src/sir/figs/all.png>","user":"U6C937ENB","ts":"1613415102.139500","team":"T68168MUP","attachments":[{"fallback":"480x480px image","image_url":"https://raw.githubusercontent.com/fmeulen/GuidedDAGExamples.jl/master/src/sir/figs/all.png","image_width":480,"image_height":480,"image_bytes":59499,"from_url":"https://raw.githubusercontent.com/fmeulen/GuidedDAGExamples.jl/master/src/sir/figs/all.png","id":1,"original_url":"https://raw.githubusercontent.com/fmeulen/GuidedDAGExamples.jl/master/src/sir/figs/all.png"}],"blocks":[{"type":"rich_text","block_id":"cHM","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://raw.githubusercontent.com/fmeulen/GuidedDAGExamples.jl/master/src/sir/figs/all.png","text":"https://raw.githubusercontent.com/fmeulen/GuidedDAGExamples.jl/master/src/sir/figs/all.png"}]}]}]},{"client_msg_id":"6e4e1694-e15a-4ca4-8129-741530bba37c","type":"message","text":"<@U6C937ENB> oh interesting. Let me take a look at that. Seems cool. I am actually working on a problem where there is a mixture of discrete dynamics. So I have a decision process that is modeled on a graph/network. In that case, the parameter values correspond to voting for one candidate or another. But then I also have another process in parallel where the decision from the network affects the accumulation of people from one group versus another group. So that is why I wanted to integrate these models into a `DiscreteProblem` , where I can model all of the dynamics together. Of course I can have Continuous dynamics as well, or a mixture of continuous and discrete dynamics. So that is the fun.","user":"UDDSTBX19","ts":"1613415969.143900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DmZT","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U6C937ENB"},{"type":"text","text":" oh interesting. Let me take a look at that. Seems cool. I am actually working on a problem where there is a mixture of discrete dynamics. So I have a decision process that is modeled on a graph/network. In that case, the parameter values correspond to voting for one candidate or another. But then I also have another process in parallel where the decision from the network affects the accumulation of people from one group versus another group. So that is why I wanted to integrate these models into a "},{"type":"text","text":"DiscreteProblem","style":{"code":true}},{"type":"text","text":" , where I can model all of the dynamics together. Of course I can have Continuous dynamics as well, or a mixture of continuous and discrete dynamics. So that is the fun."}]}]}]},{"client_msg_id":"561089e5-8fc4-4bdd-98fe-25979209083e","type":"message","text":"<@U019K6Q9N15> I thought about modelling this as a markov process, and indeed it is perhaps semi-markov. The reason that was not working was that I have new people entering the system and other existing the system. So that kinda makes it hard to use a markov process. At some level, the model I am working with resembles a queuing process from stochastic processes.","user":"UDDSTBX19","ts":"1613416082.145800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"S4J","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U019K6Q9N15"},{"type":"text","text":" I thought about modelling this as a markov process, and indeed it is perhaps semi-markov. The reason that was not working was that I have new people entering the system and other existing the system. So that kinda makes it hard to use a markov process. At some level, the model I am working with resembles a queuing process from stochastic processes."}]}]}]},{"client_msg_id":"1B6CF2D0-174A-47B6-81BD-CD8BB498F0F7","type":"message","text":"Yeah, we didn’t handle POMDP there","user":"U6C937ENB","ts":"1613416370.146600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qZt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah, we didn’t handle POMDP there"}]}]}]},{"client_msg_id":"d6b409c1-5a0b-4978-99c3-2c96ffd074b7","type":"message","text":"<@U6C937ENB> yeah, I am definitely going to read the paper. Seems very interesting. I am finally getting comfortable with using Turing now, which is nice.","user":"UDDSTBX19","ts":"1613416526.147800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3uL","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U6C937ENB"},{"type":"text","text":" yeah, I am definitely going to read the paper. Seems very interesting. I am finally getting comfortable with using Turing now, which is nice."}]}]}]},{"client_msg_id":"24ed41a6-1f07-4813-8a3c-76fb64ee17f6","type":"message","text":"Hey, in the bijector packages, on what basis is the constrained-to-unconstrained bijector for a given distribution chosen? And also, why would one want to have just \"any\" bijector (I get it if the bijectors are parametrized, but if I just get a a fixed one: why?)","user":"U7PD3M3L5","ts":"1613644219.151700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6Pc2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey, in the bijector packages, on what basis is the constrained-to-unconstrained bijector for a given distribution chosen? And also, why would one want to have just \"any\" bijector (I get it if the bijectors are parametrized, but if I just get a a fixed one: why?)"}]}]}]},{"client_msg_id":"0495aca5-0d55-430a-a27a-f2a14667a989","type":"message","text":"For anyone who wants to plot Turing chains with Gadfly or wants to show credible intervals, take a look at <https://github.com/rikhuijzer/TuringPlots.jl>. I plan to register this package at some point, depending on how useful the package is for me and also on whether the Turing group approves of the current name.","user":"U01BTNDCUBX","ts":"1613651165.153400","team":"T68168MUP","edited":{"user":"U01BTNDCUBX","ts":"1613651198.000000"},"blocks":[{"type":"rich_text","block_id":"ARqh6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"For anyone who wants to plot Turing chains with Gadfly or wants to show credible intervals, take a look at "},{"type":"link","url":"https://github.com/rikhuijzer/TuringPlots.jl"},{"type":"text","text":". I plan to register this package at some point, depending on how useful the package is for me and also on whether the Turing group approves of the current name."}]}]}]},{"client_msg_id":"05D56180-4900-48CA-AAB5-8754E2781AF1","type":"message","text":"I‘m fitting a multilevel model on multiple nominal classes (say string a, b, c). Is there a way to propagate these classes to MCMCChains? Would that be via CategoricalArrays maybe?","user":"U01BTNDCUBX","ts":"1613746603.158900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xgR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I‘m fitting a multilevel model on multiple nominal classes (say string a, b, c). Is there a way to propagate these classes to MCMCChains? Would that be via CategoricalArrays maybe?"}]}]}],"thread_ts":"1613746603.158900","reply_count":1,"reply_users_count":1,"latest_reply":"1613747726.159000","reply_users":["U01C2AJ9F63"],"subscribed":false},{"client_msg_id":"4da6a3bc-3709-472e-b994-8cf5579034ce","type":"message","text":"Hello everyone, is there any docs for saving and loading Turing model?","user":"U011PPW7K53","ts":"1613801898.000700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+zEy7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello everyone, is there any docs for saving and loading Turing model?"}]}]}]},{"client_msg_id":"5c9cf348-7d00-4225-ac9a-9be60dbfc9f1","type":"message","text":"Hello, how do I set the constraints for `Dirichlet` as simplex (sum to one in this case). For example, in Stan, I can simply declare `simplex[4] delta`. As an example, Stan code\n\n\n```data{\n    # something else...\n    vector[4] alpha;  # where alpha is vector of 4 supplied as data\n}\nparameters{\n    # something else...\n    simplex[4] delta;\n}\nmodel{\n    # something else...\n    delta ~ dirichlet( alpha );\n}```\nWhat is the equivalent of that `simplex[4] delta` in Turing?\n\n```@model function gdemo(y) = begin\n    # something else...\n    delta ~ Dirichlet([2, 2, 2, 2])\nend```","user":"U011PPW7K53","ts":"1613807757.005600","team":"T68168MUP","edited":{"user":"U011PPW7K53","ts":"1613808209.000000"},"blocks":[{"type":"rich_text","block_id":"PZl=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello, how do I set the constraints for "},{"type":"text","text":"Dirichlet ","style":{"code":true}},{"type":"text","text":"as simplex (sum to one in this case). For example, in Stan, I can simply declare "},{"type":"text","text":"simplex[4] delta","style":{"code":true}},{"type":"text","text":". As an example, Stan code\n\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"data{\n    # something else...\n    vector[4] alpha;  # where alpha is vector of 4 supplied as data\n}\nparameters{\n    # something else...\n    simplex[4] delta;\n}\nmodel{\n    # something else...\n    delta ~ dirichlet( alpha );\n}"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nWhat is the equivalent of that "},{"type":"text","text":"simplex[4] delta","style":{"code":true}},{"type":"text","text":" in Turing?\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@model function gdemo(y) = begin\n    # something else...\n    delta ~ Dirichlet([2, 2, 2, 2])\nend"}]}]}],"thread_ts":"1613807757.005600","reply_count":1,"reply_users_count":1,"latest_reply":"1613809466.007200","reply_users":["U8T9JUA5R"],"subscribed":false},{"client_msg_id":"3f5a4d5a-b518-4a8a-abc9-eb6062f4d27d","type":"message","text":"Hi,\nI’m just running through the Bayesian Diff Eq tutorial and am getting an error with the reverse diff part:\n\n`ERROR: LoadError: UndefVarError: SensitivityInterpolation not defined`\n\nAnybody else come across this before and know how to fix? I’ll leave more detail in a comment","user":"U01M641BZEY","ts":"1614003892.011200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IfA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi,\nI’m just running through the Bayesian Diff Eq tutorial and am getting an error with the reverse diff part:\n\n"},{"type":"text","text":"ERROR: LoadError: UndefVarError: SensitivityInterpolation not defined","style":{"code":true}},{"type":"text","text":"\n\nAnybody else come across this before and know how to fix? I’ll leave more detail in a comment"}]}]}],"thread_ts":"1614003892.011200","reply_count":17,"reply_users_count":3,"latest_reply":"1614006823.014600","reply_users":["U01M641BZEY","U69BL50BF","U01H36BUDJB"],"subscribed":false},{"client_msg_id":"c93bf3fe-614b-4503-8aa7-f1157ecfbbf2","type":"message","text":"Hello, team!\nWith this year’s <#C67L30Z3L|jsoc> projects being announced, I’d like to inquire about prospective cooperation – _in a way_, I’d also be glad to commence a kind of <#C767M3VB3|academia> networking.\nOut of the JSoC topics under the Turing project, I’ve found the iterative methods for inference in GP especially interesting (not limited to, though) - and having thumbed through some of the related articles, I may just add it is also a :blossom: subject.\nOn the more personal side, I’ve been searching for a kind of a summer research project and eventually arrived at the idea it would be just amazing to include an element beneficial to the Julia community - particularly as I’ve found Julia incredibly useful for my private projects recently. I’m happy to free the JSoC-funded slots to others right away, yet I’d be glad to couple some folks experienced in the field so that we could aim at a high-quality contribution (potentially turning into a research paper?). I’m a graduate student in pure mathematics (pursuing a master degree) with a keen interest in the applications thereof (also with thorough exposure to numerics, probability theory, computer science, …) with some applied research experience in a big tech company’s lab.\nIf you think this is not the right place for the post or there are places more appropriate, I’d be sincerely grateful for your suggestions - thank you!","user":"U01C8179LB0","ts":"1614045102.018300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3vNh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello, team!\nWith this year’s "},{"type":"channel","channel_id":"C67L30Z3L"},{"type":"text","text":" projects being announced, I’d like to inquire about prospective cooperation – "},{"type":"text","text":"in a way","style":{"italic":true}},{"type":"text","text":", I’d also be glad to commence a kind of "},{"type":"channel","channel_id":"C767M3VB3"},{"type":"text","text":" networking.\nOut of the JSoC topics under the Turing project, I’ve found the iterative methods for inference in GP especially interesting (not limited to, though) - and having thumbed through some of the related articles, I may just add it is also a "},{"type":"emoji","name":"blossom"},{"type":"text","text":" subject.\nOn the more personal side, I’ve been searching for a kind of a summer research project and eventually arrived at the idea it would be just amazing to include an element beneficial to the Julia community - particularly as I’ve found Julia incredibly useful for my private projects recently. I’m happy to free the JSoC-funded slots to others right away, yet I’d be glad to couple some folks experienced in the field so that we could aim at a high-quality contribution (potentially turning into a research paper?). I’m a graduate student in pure mathematics (pursuing a master degree) with a keen interest in the applications thereof (also with thorough exposure to numerics, probability theory, computer science, …) with some applied research experience in a big tech company’s lab.\nIf you think this is not the right place for the post or there are places more appropriate, I’d be sincerely grateful for your suggestions - thank you!"}]}]}]},{"client_msg_id":"99588f58-a579-4c19-b292-78b030b347f7","type":"message","text":"Hello guys! I don't know if I should post here or in <#C017Z6Q3WS3|pluto>, but I am having a problem sampling from the posterior of a model in Pluto. I am using the example provided in the Turing documentation,\n\n```using Turing\n\n# Define a simple Normal model with unknown mean and variance.\n@model function gdemo(x, y)\n  s ~ InverseGamma(2, 3)\n  m ~ Normal(0, sqrt(s))\n  x ~ Normal(m, sqrt(s))\n  y ~ Normal(m, sqrt(s))\nend\n\n#  Run sampler, collect results\nchn = sample(gdemo(1.5, 2), HMC(0.1, 5), 1000)```\n\nThe `sample` line throws the following error:\n```Failed to show value:\n\nMethodError: no method matching iterate(::MCMCChains.Chains{Float64,AxisArrays.AxisArray{Float64,3,Array{Float64,3},Tuple{AxisArrays.Axis{:iter,StepRange{Int64,Int64}},AxisArrays.Axis{:var,Array{Symbol,1}},AxisArrays.Axis{:chain,UnitRange{Int64}}}},Missing,NamedTuple{(:parameters, :internals),Tuple{Array{Symbol,1},Array{Symbol,1}}},NamedTuple{(),Tuple{}}})\n\nClosest candidates are:\n\niterate(!Matched::Pkg.Resolve.NodePerm, !Matched::Any...) at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Pkg/src/Resolve/maxsum.jl:228\n\niterate(!Matched::Test.GenericString) at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1589\n\niterate(!Matched::Test.GenericString, !Matched::Integer) at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1589\n\n...\n\n1)isempty(::MCMCChains.Chains{Float64,AxisArrays.AxisArray{Float64,3,Array{Float64,3},Tuple{AxisArrays.Axis{:iter,StepRange{Int64,Int64}},AxisArrays.Axis{:var,Array{Symbol,1}},AxisArrays.Axis{:chain,UnitRange{Int64}}}},Missing,NamedTuple{(:parameters, :internals),Tuple{Array{Symbol,1},Array{Symbol,1}}},NamedTuple{(),Tuple{}}})@essentials.jl:737\n    2)table_data(::MCMCChains.Chains{Float64,AxisArrays.AxisArray{Float64,3,Array{Float64,3},Tuple{AxisArrays.Axis{:iter,StepRange{Int64,Int64}},AxisArrays.Axis{:var,Array{Symbol,1}},AxisArrays.Axis{:chain,UnitRange{Int64}}}},Missing,NamedTuple{(:parameters, :internals),Tuple{Array{Symbol,1},Array{Symbol,1}}},NamedTuple{(),Tuple{}}}, ::IOContext{Base.GenericIOBuffer{Array{UInt8,1}}})@PlutoRunner.jl:838\n3)show_richest(::IOContext{Base.GenericIOBuffer{Array{UInt8,1}}}, ::Any)@PlutoRunner.jl:607\n4)#sprint_withreturned#28(::IOContext{Base.DevNull}, ::Int64, ::typeof(Main.PlutoRunner.sprint_withreturned), ::Function, ::MCMCChains.Chains{Float64,AxisArrays.AxisArray{Float64,3,Array{Float64,3},Tuple{AxisArrays.Axis{:iter,StepRange{Int64,Int64}},AxisArrays.Axis{:var,Array{Symbol,1}},AxisArrays.Axis{:chain,UnitRange{Int64}}}},Missing,NamedTuple{(:parameters, :internals),Tuple{Array{Symbol,1},Array{Symbol,1}}},NamedTuple{(),Tuple{}}})@PlutoRunner.jl:557\n5)format_output_default(::Any, ::Any)@PlutoRunner.jl:482\n6)#format_output#17@PlutoRunner.jl:499[inlined]\n7)formatted_result_of(::Base.UUID, ::Bool, ::Nothing)@PlutoRunner.jl:424\n8)top-level scope@none:1```\nHowever, I can use the `chn` variable in other cells, which makes me think the error appears when showing the output of the Chains object\nMoreover, trying the same code in the REPL shows no problem at all. Am I doing something wrong? I am working in an enviroment with these dependencies,\n```  [336ed68f] CSV v0.8.3\n  [a93c6f00] DataFrames v0.22.5\n  [916415d5] Images v0.23.3\n  [c3e4b0f8] Pluto v0.12.21\n  [4c63d2b9] StatsFuns v0.9.6\n  [f3b207a7] StatsPlots v0.14.19\n  [fce5fe82] Turing v0.15.10```\nThank you very much for your time","user":"U01B4UR50MN","ts":"1614197058.033300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fOfy0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello guys! I don't know if I should post here or in "},{"type":"channel","channel_id":"C017Z6Q3WS3"},{"type":"text","text":", but I am having a problem sampling from the posterior of a model in Pluto. I am using the example provided in the Turing documentation,\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Turing\n\n# Define a simple Normal model with unknown mean and variance.\n@model function gdemo(x, y)\n  s ~ InverseGamma(2, 3)\n  m ~ Normal(0, sqrt(s))\n  x ~ Normal(m, sqrt(s))\n  y ~ Normal(m, sqrt(s))\nend\n\n#  Run sampler, collect results\nchn = sample(gdemo(1.5, 2), HMC(0.1, 5), 1000)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\n\nThe "},{"type":"text","text":"sample","style":{"code":true}},{"type":"text","text":" line throws the following error:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Failed to show value:\n\nMethodError: no method matching iterate(::MCMCChains.Chains{Float64,AxisArrays.AxisArray{Float64,3,Array{Float64,3},Tuple{AxisArrays.Axis{:iter,StepRange{Int64,Int64}},AxisArrays.Axis{:var,Array{Symbol,1}},AxisArrays.Axis{:chain,UnitRange{Int64}}}},Missing,NamedTuple{(:parameters, :internals),Tuple{Array{Symbol,1},Array{Symbol,1}}},NamedTuple{(),Tuple{}}})\n\nClosest candidates are:\n\niterate(!Matched::Pkg.Resolve.NodePerm, !Matched::Any...) at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Pkg/src/Resolve/maxsum.jl:228\n\niterate(!Matched::Test.GenericString) at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1589\n\niterate(!Matched::Test.GenericString, !Matched::Integer) at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Test/src/Test.jl:1589\n\n...\n\n1)isempty(::MCMCChains.Chains{Float64,AxisArrays.AxisArray{Float64,3,Array{Float64,3},Tuple{AxisArrays.Axis{:iter,StepRange{Int64,Int64}},AxisArrays.Axis{:var,Array{Symbol,1}},AxisArrays.Axis{:chain,UnitRange{Int64}}}},Missing,NamedTuple{(:parameters, :internals),Tuple{Array{Symbol,1},Array{Symbol,1}}},NamedTuple{(),Tuple{}}})@essentials.jl:737\n    2)table_data(::MCMCChains.Chains{Float64,AxisArrays.AxisArray{Float64,3,Array{Float64,3},Tuple{AxisArrays.Axis{:iter,StepRange{Int64,Int64}},AxisArrays.Axis{:var,Array{Symbol,1}},AxisArrays.Axis{:chain,UnitRange{Int64}}}},Missing,NamedTuple{(:parameters, :internals),Tuple{Array{Symbol,1},Array{Symbol,1}}},NamedTuple{(),Tuple{}}}, ::IOContext{Base.GenericIOBuffer{Array{UInt8,1}}})@PlutoRunner.jl:838\n3)show_richest(::IOContext{Base.GenericIOBuffer{Array{UInt8,1}}}, ::Any)@PlutoRunner.jl:607\n4)#sprint_withreturned#28(::IOContext{Base.DevNull}, ::Int64, ::typeof(Main.PlutoRunner.sprint_withreturned), ::Function, ::MCMCChains.Chains{Float64,AxisArrays.AxisArray{Float64,3,Array{Float64,3},Tuple{AxisArrays.Axis{:iter,StepRange{Int64,Int64}},AxisArrays.Axis{:var,Array{Symbol,1}},AxisArrays.Axis{:chain,UnitRange{Int64}}}},Missing,NamedTuple{(:parameters, :internals),Tuple{Array{Symbol,1},Array{Symbol,1}}},NamedTuple{(),Tuple{}}})@PlutoRunner.jl:557\n5)format_output_default(::Any, ::Any)@PlutoRunner.jl:482\n6)#format_output#17@PlutoRunner.jl:499[inlined]\n7)formatted_result_of(::Base.UUID, ::Bool, ::Nothing)@PlutoRunner.jl:424\n8)top-level scope@none:1"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"However, I can use the "},{"type":"text","text":"chn","style":{"code":true}},{"type":"text","text":" variable in other cells, which makes me think the error appears when showing the output of the Chains object\nMoreover, trying the same code in the REPL shows no problem at all. Am I doing something wrong? I am working in an enviroment with these dependencies,\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"  [336ed68f] CSV v0.8.3\n  [a93c6f00] DataFrames v0.22.5\n  [916415d5] Images v0.23.3\n  [c3e4b0f8] Pluto v0.12.21\n  [4c63d2b9] StatsFuns v0.9.6\n  [f3b207a7] StatsPlots v0.14.19\n  [fce5fe82] Turing v0.15.10"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Thank you very much for your time"}]}]}]},{"client_msg_id":"396a7eb9-03e4-4eb8-b182-5f3cfa8518e3","type":"message","text":"Yeah I think Pluto has some weird MIME type that screws this up. Try adding a semicolon to the end of the chain creation line so it doesn't automatically display. I think that `summarystats(chn)`should work for you instead","user":"U9JNHB83X","ts":"1614198853.035400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vAz/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah I think Pluto has some weird MIME type that screws this up. Try adding a semicolon to the end of the chain creation line so it doesn't automatically display. I think that "},{"type":"text","text":"summarystats(chn)","style":{"code":true}},{"type":"text","text":"should work for you instead"}]}]}],"thread_ts":"1614198853.035400","reply_count":2,"reply_users_count":2,"latest_reply":"1614199600.035900","reply_users":["U01B4UR50MN","U9JNHB83X"],"subscribed":false,"reactions":[{"name":"+1","users":["U7THT3TM3","U01B4UR50MN"],"count":2}]},{"client_msg_id":"b4bbad86-6c86-4bbb-b08e-350edd62a50f","type":"message","text":"My university internet have blocked <http://turing.ml|turing.ml> - is that a common occurrence? Not really ideal for Turing if that’s the case","user":"U73KENNG4","ts":"1614345638.036700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"XhWjW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"My university internet have blocked "},{"type":"link","url":"http://turing.ml","text":"turing.ml"},{"type":"text","text":" - is that a common occurrence? Not really ideal for Turing if that’s the case"}]}]}]},{"client_msg_id":"5d020a83-1de4-49bb-83a8-3b664364d065","type":"message","text":"Hi everyone! probably a basic question. Is there any concise syntax for declaring multiple parameters that share the same (but not joint) prior distribution? Something like: [par1; par2; par3; par4; par5;] ~ Normal(0, simga2), instead of par1 ~ N(0, sigma2), par2 ~ N(0, sigma2), and so on. Thank you!","user":"U01H6G07MST","ts":"1614382713.038200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Hvq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi everyone! probably a basic question. Is there any concise syntax for declaring multiple parameters that share the same (but not joint) prior distribution? Something like: [par1; par2; par3; par4; par5;] ~ Normal(0, simga2), instead of par1 ~ N(0, sigma2), par2 ~ N(0, sigma2), and so on. Thank you!"}]}]}],"thread_ts":"1614382713.038200","reply_count":2,"reply_users_count":2,"latest_reply":"1614386306.041700","reply_users":["U7QLM6E2E","ULG5V164A"],"subscribed":false},{"client_msg_id":"757950f1-545d-4d0f-bba0-693cfbf53f0a","type":"message","text":"Hi, after running `sample` with the progress bar I can see the time it took, but is it stored anywhere? I'm trying to calculate ESS/minute","user":"U01BD0TJ2EN","ts":"1614386171.041600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ms/o","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, after running "},{"type":"text","text":"sample","style":{"code":true}},{"type":"text","text":" with the progress bar I can see the time it took, but is it stored anywhere? I'm trying to calculate ESS/minute"}]}]}]},{"client_msg_id":"f9b953c2-cd74-43ec-9ce4-35bdb0b77916","type":"message","text":"Hi, I am trying to run tutorial notebooks from TuringTutorials. I am not particularly familiar with Julia tool rig.  On three different machines and operating systems, trying to run the notebooks results in failure, with this message spit to the console, in an infinite loop:\n\n```[ Info: Precompiling IJulia [7073ff75-c697-5162-941a-fcdaad2a7d2a]\nERROR: LoadError: ArgumentError: Package MbedTLS_jll does not have JLLWrappers in its dependencies:\n- If you have MbedTLS_jll checked out for development and have\n  added JLLWrappers as a dependency but haven't updated your primary\n  environment's manifest file, try `Pkg.resolve()`.\n- Otherwise you may need to report an issue with MbedTLS_jll\nStacktrace:\n [1] require(::Module, ::Symbol) at ./loading.jl:906\n [2] include(::Function, ::Module, ::String) at ./Base.jl:380\n [3] include(::Module, ::String) at ./Base.jl:368\n [4] top-level scope at none:2\n [5] eval at ./boot.jl:331 [inlined]\n [6] eval(::Expr) at ./client.jl:467\n [7] top-level scope at ./none:3\nin expression starting at /Users/dtolpin/.julia/packages/MbedTLS_jll/qMb7d/src/MbedTLS_jll.jl:5```","user":"U01JA0D7L56","ts":"1614603563.054900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"so3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I am trying to run tutorial notebooks from TuringTutorials. I am not particularly familiar with Julia tool rig.  On three different machines and operating systems, trying to run the notebooks results in failure, with this message spit to the console, in an infinite loop:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"[ Info: Precompiling IJulia [7073ff75-c697-5162-941a-fcdaad2a7d2a]\nERROR: LoadError: ArgumentError: Package MbedTLS_jll does not have JLLWrappers in its dependencies:\n- If you have MbedTLS_jll checked out for development and have\n  added JLLWrappers as a dependency but haven't updated your primary\n  environment's manifest file, try `Pkg.resolve()`.\n- Otherwise you may need to report an issue with MbedTLS_jll\nStacktrace:\n [1] require(::Module, ::Symbol) at ./loading.jl:906\n [2] include(::Function, ::Module, ::String) at ./Base.jl:380\n [3] include(::Module, ::String) at ./Base.jl:368\n [4] top-level scope at none:2\n [5] eval at ./boot.jl:331 [inlined]\n [6] eval(::Expr) at ./client.jl:467\n [7] top-level scope at ./none:3\nin expression starting at /Users/dtolpin/.julia/packages/MbedTLS_jll/qMb7d/src/MbedTLS_jll.jl:5"}]}]}],"thread_ts":"1614603563.054900","reply_count":1,"reply_users_count":1,"latest_reply":"1614603737.055000","reply_users":["U01JA0D7L56"],"subscribed":false},{"client_msg_id":"fbb216f1-3eba-43bc-93ab-1167eb3fccf7","type":"message","text":"Hi folks, I had a question about a line in the docs:\n• `prob\"s = 1.0, m = 1.0 | model = gdemo, x = nothing, y = nothing\"` calculates the joint probability of `s = 1` and `m = 1` ignoring `x` and `y`. `x` and `y` are ignored so they can be optionally dropped from the RHS of `|`, but it is recommended to define them.\nWhen the above says \"x and y are ignored\", does this mean that those variables are marginalized out? Could this process be used to estimate normalizing constants by \"ignoring\" other (all) variables?\nAny help would be appreciated!","user":"U9NH09E58","ts":"1614634206.057900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TTZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi folks, I had a question about a line in the docs:\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"prob\"s = 1.0, m = 1.0 | model = gdemo, x = nothing, y = nothing\"","style":{"code":true}},{"type":"text","text":" calculates the joint probability of "},{"type":"text","text":"s = 1","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"m = 1","style":{"code":true}},{"type":"text","text":" ignoring "},{"type":"text","text":"x","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":". "},{"type":"text","text":"x","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" are ignored so they can be optionally dropped from the RHS of "},{"type":"text","text":"|","style":{"code":true}},{"type":"text","text":", but it is recommended to define them."}]}],"style":"bullet","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"When the above says \"x and y are ignored\", does this mean that those variables are marginalized out? Could this process be used to estimate normalizing constants by \"ignoring\" other (all) variables?\nAny help would be appreciated!"}]}]}]},{"client_msg_id":"180d1b06-0b95-4659-877d-fc8e63a0384c","type":"message","text":"the above computes the prior probability of s and m","user":"U85JBUGGP","ts":"1614640224.058200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ACCR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"the above computes the prior probability of s and m"}]}]}]},{"client_msg_id":"fa76b162-e540-437a-bf2e-1b58814bad18","type":"message","text":"I am trying to run Gaussian mixture tutorial. My Turing version is 0.15.10 (Julia installed it).\n\nThings need fixing, I can fix some but not others. It seems like chains do not have 'value' field anymore and the data is in the chain. Fixable by removing `.value`. In addition, when a vector is observed (assignment vector `k`) there is no key `:k` , instead, there are keys `:k[1]`, `:k[2]`, etc. Is it a feature or a bug? If it is a feature, how should I re-write visualization of assignment?","user":"U01JA0D7L56","ts":"1614793235.065600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hzn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am trying to run Gaussian mixture tutorial. My Turing version is 0.15.10 (Julia installed it).\n\nThings need fixing, I can fix some but not others. It seems like chains do not have 'value' field anymore and the data is in the chain. Fixable by removing "},{"type":"text","text":".value","style":{"code":true}},{"type":"text","text":". In addition, when a vector is observed (assignment vector "},{"type":"text","text":"k","style":{"code":true}},{"type":"text","text":") there is no key "},{"type":"text","text":":k","style":{"code":true}},{"type":"text","text":" , instead, there are keys "},{"type":"text","text":":k[1]","style":{"code":true}},{"type":"text","text":", "},{"type":"text","text":":k[2]","style":{"code":true}},{"type":"text","text":", etc. Is it a feature or a bug? If it is a feature, how should I re-write visualization of assignment?"}]}]}],"thread_ts":"1614793235.065600","reply_count":1,"reply_users_count":1,"latest_reply":"1614793362.065700","reply_users":["U01JA0D7L56"],"subscribed":false},{"client_msg_id":"6fbceaae-04bd-4802-b6d4-284c0fe69111","type":"message","text":"Hi! I'm using Turing package for an inference which assumes that the data follows a MvNormal distribution.  The covariance matrix for MvNormal distribution is calculated by an ODE. The code runs into MethodError while forcing the covariance matrix (last ODE solution) to be positive semi-definite because that Turing AD automatically changes the ODE solution into ForwardDiff.dual type. And dual type matrix could not be the input of function Symmetric() as well as eigen() to find the decomposition. Could you help me with this? Thanks a lot in advance.\n```using DifferentialEquations\nusing StatsPlots\nusing Plots\nusing DiffEqSensitivity\nusing LinearAlgebra\nusing Statistics\nusing Distributions\nusing Turing\nusing PDMats\nusing ProximalOperators\n\n# data\nu_account = [953.0; 1075.0; 1223.0];\n\n# Turing function\n@model bayes_ode(y) = begin\n    \n    # priors of parameter v1 follow lognormal distributions\n    logv1 ~ Normal(0.0,1.0) \n    \n    p = [exp(logv1)]\n    \n    # priors of three states A,B,C follows nomal distributions\n    A_mean ~ Normal(7.0,1.0)\n    B_mean ~ Normal(7.0,1.0)\n    C_mean ~ Normal(7.0,1.0)\n    \n    u_mean = [exp(A_mean),exp(B_mean),exp(C_mean)]\n    \n    u_initial = eltype(u_mean).(zeros(3))\n    \n    # time period two days \n    tspan = (0.0,2.0)\n    ita0 = u_initial\n    \n    # covarince from an ODE solution \n    d_phi(u,p,t) = [-p[1]*ita0[3]     0     -ita0[1];\n                  p[1]*ita0[3]   -p[1]      ita0[1];\n                           0         p[1]        -p[1] ]*u \n    \n    # input of covarince ODE\n    phi_0 = zeros(3,3)\n    phi_0[1,1] = 1e-16\n    phi_0[2,2] = 1e-16\n    phi_0[3,3] = 1e-16\n    \n    # take the last solution of the ODE (fowarddiff.dual type)\n    sol  = solve(ODEProblem(d_phi,eltype(p).(phi_0),tspan,p))\n    phi_new = sol[end] \n    \n    # trying to force the output matrix to be semi-positive diffinite\n    phi_new, _ = prox(IndPSD(), phi_new)\n    \n    # force semi-positive diffinite matrix to be positive definite\n    phi_new = phi_new + 1e-12*Matrix(I, 3, 3)\n    y[:] ~ MvNormal(ita0,phi_new)\nend\n\nRandom.seed!(87654)\niterations = 20\nchain = sample(bayes_ode(u_account), NUTS(0.65),iterations)```\n```MethodError: no method matching eigen!(::Symmetric{ForwardDiff.Dual{ForwardDiff.Tag{Turing.Core.var\"#f#1\"{DynamicPPL.VarInfo{NamedTuple{(:logv1, :logv2, :A_mean, :B_mean, :C_mean),Tuple{DynamicPPL.Metadata{Dict{DynamicPPL.VarName....```","user":"U01Q398M3QB","ts":"1614815510.066500","team":"T68168MUP","edited":{"user":"U01Q398M3QB","ts":"1614816797.000000"},"blocks":[{"type":"rich_text","block_id":"GIbb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi! I'm using Turing package for an inference which assumes that the data follows a MvNormal distribution.  The covariance matrix for MvNormal distribution is calculated by an ODE. The code runs into MethodError while forcing the covariance matrix (last ODE solution) to be positive semi-definite because that Turing AD automatically changes the ODE solution into ForwardDiff.dual type. And dual type matrix could not be the input of function Symmetric() as well as eigen() to find the decomposition. Could you help me with this? Thanks a lot in advance.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using DifferentialEquations\nusing StatsPlots\nusing Plots\nusing DiffEqSensitivity\nusing LinearAlgebra\nusing Statistics\nusing Distributions\nusing Turing\nusing PDMats\nusing ProximalOperators\n\n# data\nu_account = [953.0; 1075.0; 1223.0];\n\n# Turing function\n@model bayes_ode(y) = begin\n    \n    # priors of parameter v1 follow lognormal distributions\n    logv1 ~ Normal(0.0,1.0) \n    \n    p = [exp(logv1)]\n    \n    # priors of three states A,B,C follows nomal distributions\n    A_mean ~ Normal(7.0,1.0)\n    B_mean ~ Normal(7.0,1.0)\n    C_mean ~ Normal(7.0,1.0)\n    \n    u_mean = [exp(A_mean),exp(B_mean),exp(C_mean)]\n    \n    u_initial = eltype(u_mean).(zeros(3))\n    \n    # time period two days \n    tspan = (0.0,2.0)\n    ita0 = u_initial\n    \n    # covarince from an ODE solution \n    d_phi(u,p,t) = [-p[1]*ita0[3]     0     -ita0[1];\n                  p[1]*ita0[3]   -p[1]      ita0[1];\n                           0         p[1]        -p[1] ]*u \n    \n    # input of covarince ODE\n    phi_0 = zeros(3,3)\n    phi_0[1,1] = 1e-16\n    phi_0[2,2] = 1e-16\n    phi_0[3,3] = 1e-16\n    \n    # take the last solution of the ODE (fowarddiff.dual type)\n    sol  = solve(ODEProblem(d_phi,eltype(p).(phi_0),tspan,p))\n    phi_new = sol[end] \n    \n    # trying to force the output matrix to be semi-positive diffinite\n    phi_new, _ = prox(IndPSD(), phi_new)\n    \n    # force semi-positive diffinite matrix to be positive definite\n    phi_new = phi_new + 1e-12*Matrix(I, 3, 3)\n    y[:] ~ MvNormal(ita0,phi_new)\nend\n\nRandom.seed!(87654)\niterations = 20\nchain = sample(bayes_ode(u_account), NUTS(0.65),iterations)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"MethodError: no method matching eigen!(::Symmetric{ForwardDiff.Dual{ForwardDiff.Tag{Turing.Core.var\"#f#1\"{DynamicPPL.VarInfo{NamedTuple{(:logv1, :logv2, :A_mean, :B_mean, :C_mean),Tuple{DynamicPPL.Metadata{Dict{DynamicPPL.VarName...."}]}]}]},{"client_msg_id":"56a062ee-6295-4123-b76b-58093b1cf83d","type":"message","text":"Is there an updated tutorial on VI for BNNs anywhere on the interwebs? From what I understand the Turing tutorials are largely being rewritten and the current one seems to be outdated. Thanks!","user":"USFR23ZHQ","ts":"1614841257.068700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1uKYz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there an updated tutorial on VI for BNNs anywhere on the interwebs? From what I understand the Turing tutorials are largely being rewritten and the current one seems to be outdated. Thanks!"}]}]}]},{"client_msg_id":"422a66f7-a1af-40ff-b02d-7a2722c20171","type":"message","text":"What is the best way to encode parameter interdependendence in a Turing. ? E.g. paramerter bounds are dependent on a second parameter. The parameter part of my models looks like this at the moment, basically just rejecting any sample which doesn't satisfy the conditions by add `-Inf` to `logprob` but that doesn't seem a particular efficient nor elegant way of doing it.\n\n```@model function _turing_model(data::NamedTuple, ode_prob::ODEProblem, solver, priors::NamedTuple, sim::Bool=false, logp::Bool=false; ode_args = (;))\n\t### priors\n    p_preDCbm ~ priors.p_preDCbm\n    p_cDC1bm ~ priors.p_cDC1bm\n    p_cDC2bm ~ priors.p_cDC2bm\n    δ_preDCb ~ Uniform(0.0,2.0)\n    λ_cDC1 ~ Uniform(0.0,2.0)\n    λ_cDC2 ~ Uniform(0.0,2.0)\n    Δ_cDC2bm ~ Uniform(0.0,2.0)\n    Δ_cDC2b ~ Uniform(0.0,2.0)\n    σ1 ~ TruncatedNormal(0.0, 1.0, 0.0,Inf)\n    σ2 ~ TruncatedNormal(0.0, 1.0, 0.0,Inf)\n    σ3 ~ TruncatedNormal(0.0, 1.0, 0.0,Inf)\n\n    λ_preDC = (Δ_cDC2b + δ_preDCb) / data.R.RpreDC\n\n\t### parameter constraints\n\tif p_preDCbm ≤ (λ_preDC + Δ_cDC2bm)\n\t\tTuring.@addlogprob! -Inf\n\t\treturn\n\telseif λ_cDC1 ≥ p_cDC1bm\n\t\tTuring.@addlogprob! -Inf\n\t\treturn\n\telseif λ_cDC2 ≥ p_cDC2bm + Δ_cDC2bm * data.R.RpreDC_cDC2_bm\n\t \tTuring.@addlogprob! -Inf\n\t \treturn\n\tend\n\n    ### compound parameter\n    δ_preDCbm = p_preDCbm - λ_preDC - Δ_cDC2bm\n    δ_cDC1bm = p_cDC1bm - λ_cDC1\n    δ_cDC2bm = p_cDC2bm + Δ_cDC2bm * data.R.RpreDC_cDC2_bm - λ_cDC2\n    δ_cDC1b = λ_cDC1 * data.R.RcDC1\n    δ_cDC2b = λ_cDC2 * data.R.RcDC2 + Δ_cDC2b * data.R.RpreDC_cDC2_blood\n\n    ### parameter vector\n    theta = [p_preDCbm, δ_preDCbm, p_cDC1bm, δ_cDC1bm, p_cDC2bm, δ_cDC2bm, δ_preDCb, δ_cDC1b, δ_cDC2b, λ_preDC, λ_cDC1, λ_cDC2, Δ_cDC2bm, Δ_cDC2b]\n\n...\nend```","user":"UGFMDAMC3","ts":"1614967762.071300","team":"T68168MUP","edited":{"user":"UGFMDAMC3","ts":"1614968023.000000"},"blocks":[{"type":"rich_text","block_id":"A+BE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What is the best way to encode parameter interdependendence in a Turing. ? E.g. paramerter bounds are dependent on a second parameter. The parameter part of my models looks like this at the moment, basically just rejecting any sample which doesn't satisfy the conditions by add "},{"type":"text","text":"-Inf","style":{"code":true}},{"type":"text","text":" to "},{"type":"text","text":"logprob","style":{"code":true}},{"type":"text","text":" but that doesn't seem a particular efficient nor elegant way of doing it.\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@model function _turing_model(data::NamedTuple, ode_prob::ODEProblem, solver, priors::NamedTuple, sim::Bool=false, logp::Bool=false; ode_args = (;))\n\t### priors\n    p_preDCbm ~ priors.p_preDCbm\n    p_cDC1bm ~ priors.p_cDC1bm\n    p_cDC2bm ~ priors.p_cDC2bm\n    δ_preDCb ~ Uniform(0.0,2.0)\n    λ_cDC1 ~ Uniform(0.0,2.0)\n    λ_cDC2 ~ Uniform(0.0,2.0)\n    Δ_cDC2bm ~ Uniform(0.0,2.0)\n    Δ_cDC2b ~ Uniform(0.0,2.0)\n    σ1 ~ TruncatedNormal(0.0, 1.0, 0.0,Inf)\n    σ2 ~ TruncatedNormal(0.0, 1.0, 0.0,Inf)\n    σ3 ~ TruncatedNormal(0.0, 1.0, 0.0,Inf)\n\n    λ_preDC = (Δ_cDC2b + δ_preDCb) / data.R.RpreDC\n\n\t### parameter constraints\n\tif p_preDCbm ≤ (λ_preDC + Δ_cDC2bm)\n\t\tTuring.@addlogprob! -Inf\n\t\treturn\n\telseif λ_cDC1 ≥ p_cDC1bm\n\t\tTuring.@addlogprob! -Inf\n\t\treturn\n\telseif λ_cDC2 ≥ p_cDC2bm + Δ_cDC2bm * data.R.RpreDC_cDC2_bm\n\t \tTuring.@addlogprob! -Inf\n\t \treturn\n\tend\n\n    ### compound parameter\n    δ_preDCbm = p_preDCbm - λ_preDC - Δ_cDC2bm\n    δ_cDC1bm = p_cDC1bm - λ_cDC1\n    δ_cDC2bm = p_cDC2bm + Δ_cDC2bm * data.R.RpreDC_cDC2_bm - λ_cDC2\n    δ_cDC1b = λ_cDC1 * data.R.RcDC1\n    δ_cDC2b = λ_cDC2 * data.R.RcDC2 + Δ_cDC2b * data.R.RpreDC_cDC2_blood\n\n    ### parameter vector\n    theta = [p_preDCbm, δ_preDCbm, p_cDC1bm, δ_cDC1bm, p_cDC2bm, δ_cDC2bm, δ_preDCb, δ_cDC1b, δ_cDC2b, λ_preDC, λ_cDC1, λ_cDC2, Δ_cDC2bm, Δ_cDC2b]\n\n...\nend"}]}]}]},{"client_msg_id":"d9140b52-cafe-451a-a0b8-260510a39e08","type":"message","text":"Anyone know if there's a package out there for Bayesian structural time series? I see this package does state space models, <https://github.com/LAMPSPUC/StateSpaceModels.jl>\nSo theoretically, it should be relatively easily to wrap this in Turing to do BSTS, right?","user":"U01H36BUDJB","ts":"1615210132.076400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WpBqS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Anyone know if there's a package out there for Bayesian structural time series? I see this package does state space models, "},{"type":"link","url":"https://github.com/LAMPSPUC/StateSpaceModels.jl"},{"type":"text","text":"\nSo theoretically, it should be relatively easily to wrap this in Turing to do BSTS, right?"}]}]}]},{"client_msg_id":"046ee409-0f2e-49f5-9154-ba733c88f10d","type":"message","text":"Perhaps the easiest to write your state space model directly in Turing?","user":"U6C937ENB","ts":"1615211441.077200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"43F","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Perhaps the easiest to write your state space model directly in Turing?"}]}]}]},{"client_msg_id":"22beef15-57d5-458e-9d0a-efbb58079b81","type":"message","text":"I'm not sure I'd call that easy. Anything beyond a trivial univariate Kalman filter requires a fair bit of work, in my experience.","user":"U01H36BUDJB","ts":"1615211737.078100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8UgI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm not sure I'd call that easy. Anything beyond a trivial univariate Kalman filter requires a fair bit of work, in my experience."}]}]}]},{"client_msg_id":"17e16901-e573-4cbb-a961-70c5d812a9b9","type":"message","text":"<@U7Z4938R3> has something we're going to be incorporating into SciML \"soon\" which is differentiable and compatible with Turing.jl","user":"U69BL50BF","ts":"1615211782.078700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QZSAu","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U7Z4938R3"},{"type":"text","text":" has something we're going to be incorporating into SciML \"soon\" which is differentiable and compatible with Turing.jl"}]}]}],"reactions":[{"name":"+1","users":["U01H36BUDJB","UC0SY9JFP"],"count":2}]},{"client_msg_id":"9dc2a87e-0f3c-49c5-b2ae-11d9933a941a","type":"message","text":"Cool!","user":"U01H36BUDJB","ts":"1615211803.079000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"BUZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Cool!"}]}]}]},{"client_msg_id":"928e527a-fd32-4385-9169-a9fb8e12474e","type":"message","text":"If we talk about plans, Mitosis already supports non-linear state-space systems and is differentiable to <https://github.com/mschauer/Mitosis.jl/blob/master/example/example.jl>","user":"U6C937ENB","ts":"1615212032.080300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jHQW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If we talk about plans, Mitosis already supports non-linear state-space systems and is differentiable to "},{"type":"link","url":"https://github.com/mschauer/Mitosis.jl/blob/master/example/example.jl"}]}]}]},{"client_msg_id":"77c99ada-226e-40f8-83fe-8fe7b9869841","type":"message","text":"Some words on what Jesse and the team on this are likely to achieve related to BSTS models with what we're currently planning to call DifferentiableStateSpaceModels.jl. It will have differentiable modules for the kind of linear gaussian state space models at the core of BSTS, including Kalman filters and marginal likelihoods for observables of linear state space models, meaning that you can plug it into Turing and apply differentiable inference to do things like put non-conjugate priors on the coefficients or make them a function of other things in a nonlinear way, and estimate jointly with whatever else you want in a probabilistic program. We will also have modules for the joint likelihood of observables and unobservables, which means inference will not be limited to the Kalman filter or linear-Gaussian models, though if used with generic methods this means you are treating the latent state variables just like any other parameter/latent variable in Bayes. This is great if your generic algorithm can handle that (some tests with HMC suggest it's not bad, in many cases), but isn't specialized to state space structure like the custom (mostly Gibbs-based) samplers in something like Google's BSTS or its successor in Tensorflow Probability, or the StateSpaceModels.jl package you linked. (or the kind of custom particle filters, as opposed to generic SMC, that other Julia PPLs focus on). Wrapping parts of existing state space code in Turing may be possible, though you run into the issue of composing inference algorithms, and also differentiable algorithms may face issues without dedicated implicit rules like we are implementing. One thing we could add but have not yet, that is a real strength of BSTS type methods, is dedicated modules for the kinds of state space structures that go into BSTS, like an ARIMA component, a dynamic linear model component, stochastic or nonstochastic trends, etc. These are constructed by composing matrices that go into the state space likelihood, and a special syntax is very helpful to make this easy (if you've ever looked at how you need to code a moving average model into a Kalman filter you know this is something you don't want to do by hand every time).  We are building a separate library, using MTK, for building time series model structures that can be put into state space model form, but because the team is focused on economics applications, the focus is more on tools for incorporating things economists do (like solve for equilibrium) than on generic time series tools.  Whether the path forward involves extending our structure software to include some of that kind of thing or whether it's better to separate it out is a decision path we haven't gone down yet.","user":"UJDHH8CG4","ts":"1615214248.099500","team":"T68168MUP","edited":{"user":"UJDHH8CG4","ts":"1615216273.000000"},"blocks":[{"type":"rich_text","block_id":"25KEC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Some words on what Jesse and the team on this are likely to achieve related to BSTS models with what we're currently planning to call DifferentiableStateSpaceModels.jl. It will have differentiable modules for the kind of linear gaussian state space models at the core of BSTS, including Kalman filters and marginal likelihoods for observables of linear state space models, meaning that you can plug it into Turing and apply differentiable inference to do things like put non-conjugate priors on the coefficients or make them a function of other things in a nonlinear way, and estimate jointly with whatever else you want in a probabilistic program. We will also have modules for the joint likelihood of observables and unobservables, which means inference will not be limited to the Kalman filter or linear-Gaussian models, though if used with generic methods this means you are treating the latent state variables just like any other parameter/latent variable in Bayes. This is great if your generic algorithm can handle that (some tests with HMC suggest it's not bad, in many cases), but isn't specialized to state space structure like the custom (mostly Gibbs-based) samplers in something like Google's BSTS or its successor in Tensorflow Probability, or the StateSpaceModels.jl package you linked. (or the kind of custom particle filters, as opposed to generic SMC, that other Julia PPLs focus on). Wrapping parts of existing state space code in Turing may be possible, though you run into the issue of composing inference algorithms, and also differentiable algorithms may face issues without dedicated implicit rules like we are implementing. One thing we could add but have not yet, that is a real strength of BSTS type methods, is dedicated modules for the kinds of state space structures that go into BSTS, like an ARIMA component, a dynamic linear model component, stochastic or nonstochastic trends, etc. These are constructed by composing matrices that go into the state space likelihood, and a special syntax is very helpful to make this easy (if you've ever looked at how you need to code a moving average model into a Kalman filter you know this is something you don't want to do by hand every time).  We are building a separate library, using MTK, for building time series model structures that can be put into state space model form, but because the team is focused on economics applications, the focus is more on tools for incorporating things economists do (like solve for equilibrium) than on generic time series tools.  Whether the path forward involves extending our structure software to include some of that kind of thing or whether it's better to separate it out is a decision path we haven't gone down yet."}]}]}]},{"client_msg_id":"001105ad-bf83-471e-b9d0-8c1ea9ec5352","type":"message","text":"which sampler do you guys recommend for non-trivial Bayesian differential equation models? i.e. if each diffeq solve takes several seconds or more, is NUTS/HMC still the way to go?","user":"U01H36BUDJB","ts":"1615300034.001300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"d1gU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"which sampler do you guys recommend for non-trivial Bayesian differential equation models? i.e. if each diffeq solve takes several seconds or more, is NUTS/HMC still the way to go?"}]}]}],"thread_ts":"1615300034.001300","reply_count":4,"reply_users_count":2,"latest_reply":"1615301181.006800","reply_users":["U6QF223TN","U01H36BUDJB"],"subscribed":false},{"client_msg_id":"633ee8a7-e47f-4e5e-b358-a20df723dc5a","type":"message","text":"<@U01H36BUDJB> we just started a research group around that (<@U85JBUGGP>)","user":"U69BL50BF","ts":"1615300070.001800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Na0+","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U01H36BUDJB"},{"type":"text","text":" we just started a research group around that ("},{"type":"user","user_id":"U85JBUGGP"},{"type":"text","text":")"}]}]}]},{"client_msg_id":"a4c75966-2fd5-4751-87e2-aa6610b76d1b","type":"message","text":"It's hard to tell.","user":"U69BL50BF","ts":"1615300080.002000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TC2B8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It's hard to tell."}]}]}]},{"client_msg_id":"a2ade181-8fc1-4738-824d-c9a939db55cb","type":"message","text":"You might also want to try ABC.","user":"U69BL50BF","ts":"1615300085.002200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SMV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You might also want to try ABC."}]}]}]},{"client_msg_id":"b19f95fd-b15b-4933-9e26-b90bdb74c2a0","type":"message","text":"I'm not familiar with ABC?","user":"U01H36BUDJB","ts":"1615300219.002400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"T8ZNO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm not familiar with ABC?"}]}]}]},{"client_msg_id":"bc48994d-c7d9-4914-b7b9-aa8df6610493","type":"message","text":"what kind of sampler is that?","user":"U01H36BUDJB","ts":"1615300222.002600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3oy1p","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"what kind of sampler is that?"}]}]}]},{"client_msg_id":"9a88c7ef-ffb0-4920-87ff-eca0f0810633","type":"message","text":"Approximate Bayesian Computation.","user":"U69BL50BF","ts":"1615300364.002800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4Iqd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Approximate Bayesian Computation."}]}]}]},{"client_msg_id":"a231f94d-56ec-41cc-a0f1-fb2499101798","type":"message","text":"ah, right","user":"U01H36BUDJB","ts":"1615300607.003000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8HF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ah, right"}]}]}]},{"client_msg_id":"98579e6d-1ba2-466c-a0e7-a8dd02f683c4","type":"message","text":"approximate bayes... never really worked much with that before","user":"U01H36BUDJB","ts":"1615300629.003300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dSb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"approximate bayes... never really worked much with that before"}]}]}]},{"client_msg_id":"45fcb3ae-01ef-47b6-bcc2-ba476543aef3","type":"message","text":"Is there an effort atm to include likelihood-free/simulation based inference in Turing (or elsewhere?)","user":"U01M641BZEY","ts":"1615300873.005100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RCx07","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there an effort atm to include likelihood-free/simulation based inference in Turing (or elsewhere?)"}]}]}]},{"client_msg_id":"b7d828eb-89e6-47cb-b819-c66d8c0302a5","type":"message","text":"as for e.g. is described nicely here by Miles Cranmer: <https://astroautomata.com/blog/simulation-based-inference/>","user":"U01M641BZEY","ts":"1615301140.006600","team":"T68168MUP","attachments":[{"service_name":"astro automata","title":"A tutorial on simulation-based inference","title_link":"https://astroautomata.com/blog/simulation-based-inference/","text":"Personal website of Miles Cranmer","fallback":"astro automata: A tutorial on simulation-based inference","image_url":"http://astroautomata.com//assets/images/nflow.png","ts":1595808000,"from_url":"https://astroautomata.com/blog/simulation-based-inference/","image_width":425,"image_height":250,"image_bytes":60223,"service_icon":"https://astroautomata.com/apple-icon-57x57.png","id":1,"original_url":"https://astroautomata.com/blog/simulation-based-inference/"}],"blocks":[{"type":"rich_text","block_id":"+kLa0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"as for e.g. is described nicely here by Miles Cranmer: "},{"type":"link","url":"https://astroautomata.com/blog/simulation-based-inference/"}]}]}]},{"type":"message","subtype":"channel_join","ts":"1615364009.010700","user":"U01QXPZ8SR2","text":"<@U01QXPZ8SR2> has joined the channel","inviter":"U01JA0D7L56"},{"type":"message","subtype":"channel_join","ts":"1615367004.011000","user":"U01QB1S7C0P","text":"<@U01QB1S7C0P> has joined the channel","inviter":"U01JA0D7L56"},{"client_msg_id":"d7c4394a-6d19-4ea7-a320-8dc0fdd8b53c","type":"message","text":"how is reversediff so much faster?! argh spent like an hour optimizing a Turing model and then realized that I had forgot to change the ad backend and sampling went from 2hrs for 1000 samples to 2 min","user":"U6CFMFM2R","ts":"1615488171.019400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"CRR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"how is reversediff so much faster?! argh spent like an hour optimizing a Turing model and then realized that I had forgot to change the ad backend and sampling went from 2hrs for 1000 samples to 2 min"}]}]}]},{"client_msg_id":"06f3535a-7525-4c78-b59d-c2e51c13eee1","type":"message","text":"Reverse-mode AD vs forward-mode AD. If you have N parameters and your model is properly vectorised, you can expect reverse-mode to be ~N times faster. Less in practice but ideally not by much.","user":"U85JBUGGP","ts":"1615494112.021200","team":"T68168MUP","edited":{"user":"U85JBUGGP","ts":"1615494122.000000"},"blocks":[{"type":"rich_text","block_id":"aGP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Reverse-mode AD vs forward-mode AD. If you have N parameters and your model is properly vectorised, you can expect reverse-mode to be ~N times faster. Less in practice but ideally not by much."}]}]}]},{"client_msg_id":"eafb4e2c-c5a2-4862-a196-59727bed2b4d","type":"message","text":"`N` needs to be large enough though","user":"U85JBUGGP","ts":"1615494133.021500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bMh6v","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"N","style":{"code":true}},{"type":"text","text":" needs to be large enough though"}]}]}]},{"client_msg_id":"3c9d0a4e-3833-4e12-b83f-8c584db8ad39","type":"message","text":"When Diffractor is out and Turing uses it, you can expect AD to be even faster! But that won’t be for a while.","user":"U85JBUGGP","ts":"1615494210.022100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2N+q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"When Diffractor is out and Turing uses it, you can expect AD to be even faster! But that won’t be for a while."}]}]}]},{"client_msg_id":"6e63c40a-f166-43e2-b44c-e15bfd498a5b","type":"message","text":"its really incredible","user":"U6CFMFM2R","ts":"1615497774.023000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"M+t","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"its really incredible"}]}]}]},{"client_msg_id":"e04f1568-3a34-4297-896e-f1d2d6fa2e47","type":"message","text":"I’m trying to implement this simple hierarchical model in Turing:\ny_j | θ_j, σ_j ~ N(θ_j, σ_{j}^2$\n θ_j | μ, τ ~ N(μ, τ^2)$\nwith priors p(μ, τ)\n\nWould\n```@model meta(y) = begin\n    μ ~ Uniform(0, 1)\n    τ ~ Uniform(0, 1)\n    θ ~ Normal(μ, τ^2)\n\n    for j in eachindex(y) \n      y[j] ~  Normal(θ, τ)\n    end\nend```\nbe correct? Do I have to do anything to specify that the theta is specific to / potentially different for each y_j?","user":"US8V7JSKB","ts":"1615541250.026300","team":"T68168MUP","edited":{"user":"US8V7JSKB","ts":"1615541347.000000"},"blocks":[{"type":"rich_text","block_id":"JPOp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’m trying to implement this simple hierarchical model in Turing:\ny_j | θ_j, σ_j ~ N(θ_j, σ_{j}^2$\n θ_j | μ, τ ~ N(μ, τ^2)$\nwith priors p(μ, τ)\n\nWould\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@model meta(y) = begin\n    μ ~ Uniform(0, 1)\n    τ ~ Uniform(0, 1)\n    θ ~ Normal(μ, τ^2)\n\n    for j in eachindex(y) \n      y[j] ~  Normal(θ, τ)\n    end\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"be correct? Do I have to do anything to specify that the theta is specific to / potentially different for each y_j?"}]}]}],"thread_ts":"1615541250.026300","reply_count":4,"reply_users_count":2,"latest_reply":"1615541586.027200","reply_users":["U01C2AJ9F63","US8V7JSKB"],"subscribed":false},{"client_msg_id":"022f8bc2-934c-4e01-935a-83eaa47fe49d","type":"message","text":"Is there a trick to working with parameter vectors and callbacks? For example, say I have a vector of parameters `κ ~ MvNormal(Fill(0.0,5),1.0)` . I've tried TuringCallbacks (without setting a variable_filter) and a few different things with Turkie but no luck so far.","user":"U017YGFQTE3","ts":"1615544384.035400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Bt97","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a trick to working with parameter vectors and callbacks? For example, say I have a vector of parameters "},{"type":"text","text":"κ ~ MvNormal(Fill(0.0,5),1.0)","style":{"code":true}},{"type":"text","text":" . I've tried TuringCallbacks (without setting a variable_filter) and a few different things with Turkie but no luck so far."}]}]}]},{"type":"message","text":"No signs of slowing down!","files":[{"id":"F01RTKCCRJL","created":1615571882,"timestamp":1615571882,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U85JBUGGP","editable":false,"size":462351,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01RTKCCRJL/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01RTKCCRJL/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_360.png","thumb_360_w":360,"thumb_360_h":230,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_480.png","thumb_480_w":480,"thumb_480_h":307,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_720.png","thumb_720_w":720,"thumb_720_h":460,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_800.png","thumb_800_w":800,"thumb_800_h":511,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_960.png","thumb_960_w":960,"thumb_960_h":614,"thumb_1024":"https://files.slack.com/files-tmb/T68168MUP-F01RTKCCRJL-3281bcde99/image_1024.png","thumb_1024_w":1024,"thumb_1024_h":655,"original_w":1658,"original_h":1060,"thumb_tiny":"AwAeADDSJOeKPm9qOppaADrR0oooAKOv0pOv0paAE/ipaQg9jijB9aAFzSYz1o5x1o59aAFopOfX9KXn1oA//9k=","permalink":"https://julialang.slack.com/files/U85JBUGGP/F01RTKCCRJL/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01RTKCCRJL-a5dde4754b","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"4RB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"No signs of slowing down!"}]}]}],"user":"U85JBUGGP","display_as_bot":false,"ts":"1615571913.037400","reactions":[{"name":"fire","users":["UC0SY9JFP","U85JBUGGP"],"count":2}]},{"client_msg_id":"c8f3f275-b1ae-46c1-98f6-4d8f0c64ffbd","type":"message","text":"Hey I was one of those stars!","user":"U01H36BUDJB","ts":"1615590217.044200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"cxlDp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey I was one of those stars!"}]}]}]},{"client_msg_id":"e2d45524-80bf-41b6-b072-77b304ba67e1","type":"message","text":"I was really excited when I found Turing. You guys have done awesome work :)","user":"U01H36BUDJB","ts":"1615590280.044600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pEjw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I was really excited when I found Turing. You guys have done awesome work :)"}]}]}]},{"client_msg_id":"9cadb051-c86e-4ed0-b465-4aad3451e810","type":"message","text":"Would any of you happen to have any teaching materials for Turing? I’m trying to learn it for a Bayesian Stats class (that is unfortunately R-based), and I’m trying to find as many Turing examples as I can. I’m already aware of the <http://Turing.ml|Turing.ml> tutorials and Rob Goedman’s Stat Rethinking code.","user":"US8V7JSKB","ts":"1615599144.047400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wrc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Would any of you happen to have any teaching materials for Turing? I’m trying to learn it for a Bayesian Stats class (that is unfortunately R-based), and I’m trying to find as many Turing examples as I can. I’m already aware of the "},{"type":"link","url":"http://Turing.ml","text":"Turing.ml"},{"type":"text","text":" tutorials and Rob Goedman’s Stat Rethinking code."}]}]}]},{"client_msg_id":"6f680ca7-37b2-4ab5-b365-3d26f7b4138c","type":"message","text":"I can't think of anything outside those materials","user":"U9JNHB83X","ts":"1615608332.048200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HiCk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I can't think of anything outside those materials"}]}]}]},{"client_msg_id":"60475152-19c8-47e6-a071-6bdf0f5748e9","type":"message","text":"They tend to be mostly comprehensive","user":"U9JNHB83X","ts":"1615608341.048600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xFi1Z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"They tend to be mostly comprehensive"}]}]}]},{"client_msg_id":"0f786972-ef30-40b8-8462-9b03fea01a25","type":"message","text":"I will be adding later this semester portuguese bayesian materials using Turing. I have a opensource creative-commons content for Bayesian Stats using R and Stan (mostly rstanarm and brms for PhD candidates in Social Sciences). All the materials are being reformulated now in a branch (2021-02 -- i.e next semester) that will be taught next semester but they are in Portuguese. You can access them here (<https://storopoli.io/Estatistica-Bayesiana/>) and the github repo (<https://github.com/storopoli/Estatistica-Bayesiana>). My ideia is to update the R and Stan materials and create also a Julia version using Turing.","user":"U01QBF4PHKP","ts":"1615634458.052600","team":"T68168MUP","attachments":[{"service_name":"Estatística Bayesiana com R e RStan","title":"Estatística Bayesiana com R e RStan: Estatística Bayesiana com R e Stan","title_link":"https://storopoli.io/Estatistica-Bayesiana/","text":"Companion para a disciplina de Estatística Bayesiana para alunos de Mestrado e Doutorado da UNINOVE","fallback":"Estatística Bayesiana com R e RStan: Estatística Bayesiana com R e RStan: Estatística Bayesiana com R e Stan","from_url":"https://storopoli.io/Estatistica-Bayesiana/","id":1,"original_url":"https://storopoli.io/Estatistica-Bayesiana/"}],"blocks":[{"type":"rich_text","block_id":"9QOIV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I will be adding later this semester portuguese bayesian materials using Turing. I have a opensource creative-commons content for Bayesian Stats using R and Stan (mostly rstanarm and brms for PhD candidates in Social Sciences). All the materials are being reformulated now in a branch (2021-02 -- i.e next semester) that will be taught next semester but they are in Portuguese. You can access them here ("},{"type":"link","url":"https://storopoli.io/Estatistica-Bayesiana/"},{"type":"text","text":") and the github repo ("},{"type":"link","url":"https://github.com/storopoli/Estatistica-Bayesiana"},{"type":"text","text":"). My ideia is to update the R and Stan materials and create also a Julia version using Turing."}]}]}]},{"client_msg_id":"47f0d9ca-2d67-4e56-b186-e048125ee57c","type":"message","text":"I am defining a custom distribution to define parameter constraints of one parameter by a second parameter. The new custom distribution version compared to my initial `Turing` model which worked by rejection of parameter combination, which don't satisfy these conditions, doesn't seem to take the prior into account.\n\nSo I wanted to ask if that is to be expected. The MWE below hopefully illustrates the scenario, if parameter falls outside the condition/range of support you get `-Inf` but for the a supported parameter set you get different `logp`. The `reject` function is akin to the scenario where one uses `@addlogp -Inf` to reject a sample. Am I missing something here?\n```julia&gt; function reject(y)\n           x = similar(y)\n           x[1] = logpdf(Uniform(0.0,2.0), y[1])\n           x[2] = ifelse(y[2] &lt; y[1], -Inf, logpdf(Normal(0.6,.3), y[2]))\n           return x\n       end\n\njulia&gt; function cust_dist(y)\n           x = similar(y)\n           x[1] = logpdf(Uniform(0.0,2.0), y[1])\n           x[2] = logpdf(truncated(Normal(0.6,.3), y[2]), y[1], Inf)\n           return x\n       end\n\njulia&gt; rv = [0.1,.05];\n\njulia&gt; cust_dist(rv)\n2-element Array{Float64,1}:\n  -0.6931471805599453\n -Inf\n\njulia&gt; reject(rv)\n2-element Array{Float64,1}:\n  -0.6931471805599453\n -Inf\n\njulia&gt; rv = [0.1,.3]\n\njulia&gt; reject(rv)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -0.21496572887873655\n\njulia&gt; cust_dist(rv)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -0.16599567864432105```\n","user":"UGFMDAMC3","ts":"1615643956.064600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ozeta","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am defining a custom distribution to define parameter constraints of one parameter by a second parameter. The new custom distribution version compared to my initial "},{"type":"text","text":"Turing","style":{"code":true}},{"type":"text","text":" model which worked by rejection of parameter combination, which don't satisfy these conditions, doesn't seem to take the prior into account.\n\nSo I wanted to ask if that is to be expected. The MWE below hopefully illustrates the scenario, if parameter falls outside the condition/range of support you get "},{"type":"text","text":"-Inf","style":{"code":true}},{"type":"text","text":" but for the a supported parameter set you get different "},{"type":"text","text":"logp","style":{"code":true}},{"type":"text","text":". The "},{"type":"text","text":"reject","style":{"code":true}},{"type":"text","text":" function is akin to the scenario where one uses "},{"type":"text","text":"@addlogp -Inf","style":{"code":true}},{"type":"text","text":" to reject a sample. Am I missing something here?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> function reject(y)\n           x = similar(y)\n           x[1] = logpdf(Uniform(0.0,2.0), y[1])\n           x[2] = ifelse(y[2] < y[1], -Inf, logpdf(Normal(0.6,.3), y[2]))\n           return x\n       end\n\njulia> function cust_dist(y)\n           x = similar(y)\n           x[1] = logpdf(Uniform(0.0,2.0), y[1])\n           x[2] = logpdf(truncated(Normal(0.6,.3), y[2]), y[1], Inf)\n           return x\n       end\n\njulia> rv = [0.1,.05];\n\njulia> cust_dist(rv)\n2-element Array{Float64,1}:\n  -0.6931471805599453\n -Inf\n\njulia> reject(rv)\n2-element Array{Float64,1}:\n  -0.6931471805599453\n -Inf\n\njulia> rv = [0.1,.3]\n\njulia> reject(rv)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -0.21496572887873655\n\njulia> cust_dist(rv)\n2-element Array{Float64,1}:\n -0.6931471805599453\n -0.16599567864432105"}]},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"6ba62d7a-cee0-4b5a-9347-85712ce0cdb2","type":"message","text":"The “rejection” approach doesn’t include log of the normalisation constant when you “truncate”. The logpdf of `truncated` includes `log` the normalisation constant.","user":"U85JBUGGP","ts":"1615648152.067100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jzQm5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The “rejection” approach doesn’t include log of the normalisation constant when you “truncate”. The logpdf of "},{"type":"text","text":"truncated","style":{"code":true}},{"type":"text","text":" includes "},{"type":"text","text":"log","style":{"code":true}},{"type":"text","text":" the normalisation constant."}]}]}]},{"client_msg_id":"81bf003e-c84a-42b4-831d-b806ab43f592","type":"message","text":"if you try different `rv` values, you will see that the difference in the second number is always constant up to machine precision.","user":"U85JBUGGP","ts":"1615648201.068000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4Eu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"if you try different "},{"type":"text","text":"rv","style":{"code":true}},{"type":"text","text":" values, you will see that the difference in the second number is always constant up to machine precision."}]}]}]}]}