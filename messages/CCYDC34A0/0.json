{"cursor": 0, "messages": [{"client_msg_id":"756f0666-93a0-43d0-9d68-f8f86a345e3f","type":"message","text":"To everyone,\nWe will have an open meeting on *18.12.2020 at 3pm CET*. If you have anything in mind that you like to discuss/ask (things that bother you, feature requests, …) or just want to hang out then fell free to join the video call. I’ll post the invite link in time in this channel.","user":"UC0SY9JFP","ts":"1607931638.077900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qEeDO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"To everyone,\nWe will have an open meeting on "},{"type":"text","text":"18.12.2020 at 3pm CET","style":{"bold":true}},{"type":"text","text":". If you have anything in mind that you like to discuss/ask (things that bother you, feature requests, …) or just want to hang out then fell free to join the video call. I’ll post the invite link in time in this channel."}]}]}],"thread_ts":"1607931638.077900","reply_count":2,"reply_users_count":2,"latest_reply":"1608023698.082900","reply_users":["U011PPW7K53","UC0SY9JFP"],"subscribed":false,"reactions":[{"name":"+1","users":["UGB3MK8MC","UHDQQ4GN6","UH08DT0JU","UTVH86LJ0","U01C2AJ9F63","U01A08JMUKT","U9JNHB83X","UAZHEDVBR","UCRDHV7PB","U011PPW7K53"],"count":10},{"name":"turing","users":["U011PPW7K53"],"count":1}]},{"type":"message","subtype":"thread_broadcast","text":"Some ongoing progress:\n\n1. Refactor and improve particle MCMC and particle filtering samplers into a light-weight package `AdvancedPS` \n2. Unify and modularise some core tracing data structure in `DynamicPPL` and separate them into `AbstractPPL` <@UN45LV5K6> <@UCRDHV7PB> \n3. Implement an experimental sub-DSL that’s compatible with the BUGS modelling language, and add support for efficient generalised Gibbs sampling.  \n4. Improve documentation and tutorials. \n5. Initial GPU accelerated HMC and RWMH support. \n6. Initial Annealed Important Sampling support and other model assessment methods. \n7. Initial Nested Sampling support. \n8. Revisit all Github existing issues, improve stability of public APIs and plan for Turing 1.0 release. \n9. any other issues raised by the Turing.jl and probabilistic programming community during the Turing.jl general public meeting and later feedbacks. \nWe aim to keep this progress as transparent as possible. We’ll also try to minimise disruption to existing Turing.jl users by maximising backward compatibility.\nWe anticipate it might still be a long way ahead before we can release Turing 1.0. Any help, comments feedbacks on improving the development and release of Turing are welcome! We also welcome community developers to join the Turing.jl team. Together, we aim to make Julia the go-to language for Bayesian inference and computation.","user":"UCRDHV7PB","ts":"1607986152.079200","thread_ts":"1607864180.072700","root":{"client_msg_id":"087cca24-8b16-476f-afc5-91096c1fe149","type":"message","text":"Out of curiosity, what is the development plan for Turing over the next 6-12 months?","user":"UH08DT0JU","ts":"1607864180.072700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Q6W","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Out of curiosity, what is the development plan for Turing over the next 6-12 months?"}]}]}],"thread_ts":"1607864180.072700","reply_count":10,"reply_users_count":5,"latest_reply":"1608002405.080600","reply_users":["U9JNHB83X","UH08DT0JU","U011PPW7K53","U85JBUGGP","UCRDHV7PB"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"oiGby","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Some ongoing progress:\n\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Refactor and improve particle MCMC and particle filtering samplers into a light-weight package "},{"type":"text","text":"AdvancedPS","style":{"code":true}},{"type":"text","text":" "}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Unify and modularise some core tracing data structure in "},{"type":"text","text":"DynamicPPL","style":{"code":true}},{"type":"text","text":" and separate them into "},{"type":"text","text":"AbstractPPL","style":{"code":true}},{"type":"text","text":" "},{"type":"user","user_id":"UN45LV5K6"},{"type":"text","text":" "},{"type":"user","user_id":"UCRDHV7PB"},{"type":"text","text":" "}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Implement an experimental sub-DSL that’s compatible with the BUGS modelling language, and add support for efficient generalised Gibbs sampling.  "}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Improve documentation and tutorials. "}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Initial GPU accelerated HMC and RWMH support. "}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Initial Annealed Important Sampling support and other model assessment methods. "}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Initial Nested Sampling support. "}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Revisit all Github existing issues, improve stability of public APIs and plan for Turing 1.0 release. "}]},{"type":"rich_text_section","elements":[{"type":"text","text":"any other issues raised by the Turing.jl and probabilistic programming community during the Turing.jl general public meeting and later feedbacks. "}]}],"style":"ordered","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"\nWe aim to keep this progress as transparent as possible. We’ll also try to minimise disruption to existing Turing.jl users by maximising backward compatibility.\nWe anticipate it might still be a long way ahead before we can release Turing 1.0. Any help, comments feedbacks on improving the development and release of Turing are welcome! We also welcome community developers to join the Turing.jl team. Together, we aim to make Julia the go-to language for Bayesian inference and computation."}]}]}],"client_msg_id":"936126fd-fb92-4b66-a35b-b9e8ee041c28","edited":{"user":"UCRDHV7PB","ts":"1607986264.000000"},"reactions":[{"name":"heart","users":["UGD4K0Z25","U011PPW7K53","UTVH86LJ0","U7QLM6E2E","UF6T1632L","UC0SY9JFP","UH08DT0JU","U01A08JMUKT","U01BTNDCUBX","U011SGGQJCA","U018NKR9X70","UE98VNG4U","U680T6770","U6H9SJKCH"],"count":14},{"name":"juliaspinner","users":["U011PPW7K53","UGU761DU2","U018NKR9X70","U01A4SJJQDA","U680T6770","U6H9SJKCH"],"count":6},{"name":"turing","users":["U019X0KMT6Y","U01BTNDCUBX","U011PPW7K53","U018NKR9X70","U680T6770","U6H9SJKCH"],"count":6}]},{"client_msg_id":"332233e2-11f5-43ac-911c-0659cfd1b3fb","type":"message","text":"Hi, is there a way to stop sampling based on some criterion, like the callbacks in Tensorflow?","user":"U018NKR9X70","ts":"1608102775.084800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wpw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, is there a way to stop sampling based on some criterion, like the callbacks in Tensorflow?"}]}]}],"thread_ts":"1608102775.084800","reply_count":8,"reply_users_count":4,"latest_reply":"1608218495.111000","reply_users":["UH08DT0JU","UHDNY2YMA","U8T9JUA5R","U018NKR9X70"],"subscribed":false},{"client_msg_id":"78c2f7ac-491f-4979-b149-63c6c121531d","type":"message","text":"How come <https://turing.ml/dev/tutorials/10-bayesiandiffeq/> never got updated?","user":"U69BL50BF","ts":"1608118498.085900","team":"T68168MUP","attachments":[{"title":"Bayesian Estimation of Differential Equations","title_link":"https://turing.ml/dev/tutorials/10-bayesiandiffeq/","text":"Bayesian Estimation of Differential Equations","fallback":"Bayesian Estimation of Differential Equations","from_url":"https://turing.ml/dev/tutorials/10-bayesiandiffeq/","service_icon":"https://turing.ml/dev/assets/img/favicon.ico","service_name":"turing.ml","id":1,"original_url":"https://turing.ml/dev/tutorials/10-bayesiandiffeq/"}],"blocks":[{"type":"rich_text","block_id":"mNDx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How come "},{"type":"link","url":"https://turing.ml/dev/tutorials/10-bayesiandiffeq/"},{"type":"text","text":" never got updated?"}]}]}]},{"client_msg_id":"1847f756-f6ff-48c1-b271-2e6435c2e906","type":"message","text":"<@U9JNHB83X>","user":"U69BL50BF","ts":"1608118502.086200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Eyv","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U9JNHB83X"}]}]}],"thread_ts":"1608118502.086200","reply_count":1,"reply_users_count":1,"latest_reply":"1608126797.088400","reply_users":["U6QF223TN"],"subscribed":false},{"type":"message","text":"I was doing some (unrelated) dark-mode things today and thought you might enjoy this take of your graphic:","files":[{"id":"F01H1DGV70S","created":1608123938,"timestamp":1608123938,"name":"Screenshot 2020-12-16 at 13.04.19.png","title":"Screenshot 2020-12-16 at 13.04.19.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U69EQD0CD","editable":false,"size":252867,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01H1DGV70S/screenshot_2020-12-16_at_13.04.19.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01H1DGV70S/download/screenshot_2020-12-16_at_13.04.19.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01H1DGV70S-bb9c5e5386/screenshot_2020-12-16_at_13.04.19_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01H1DGV70S-bb9c5e5386/screenshot_2020-12-16_at_13.04.19_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01H1DGV70S-bb9c5e5386/screenshot_2020-12-16_at_13.04.19_360.png","thumb_360_w":331,"thumb_360_h":360,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01H1DGV70S-bb9c5e5386/screenshot_2020-12-16_at_13.04.19_480.png","thumb_480_w":441,"thumb_480_h":480,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01H1DGV70S-bb9c5e5386/screenshot_2020-12-16_at_13.04.19_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01H1DGV70S-bb9c5e5386/screenshot_2020-12-16_at_13.04.19_720.png","thumb_720_w":662,"thumb_720_h":720,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01H1DGV70S-bb9c5e5386/screenshot_2020-12-16_at_13.04.19_800.png","thumb_800_w":800,"thumb_800_h":871,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01H1DGV70S-bb9c5e5386/screenshot_2020-12-16_at_13.04.19_960.png","thumb_960_w":882,"thumb_960_h":960,"thumb_1024":"https://files.slack.com/files-tmb/T68168MUP-F01H1DGV70S-bb9c5e5386/screenshot_2020-12-16_at_13.04.19_1024.png","thumb_1024_w":941,"thumb_1024_h":1024,"original_w":1042,"original_h":1134,"thumb_tiny":"AwAwACynkAY/pQo3HAyT7CkVQSSThR1NI0hI2r8q+g/r60ASeUc88fUimuhQ8gge9MSMvnGAB3JxQGeJiOnqD0oAWinEBl3qMY6r6f8A1qZQA6U4VV9tx/H/AOtUYBPQE1LIBlWboVGAO9CEn52+4vbsT6UMBJflCx+nJ+tD/PGr9x8p/pTd+77/AD79xT4xtzuI2NxntSsFhkTYkGeh4P0NKRgkHqDilwTJ5b8NniiQ5kYjuTVAOU7kxtDMvIz3FMaXdjKjA6CjkHIpxKPy4Kt/eXv+FIBm4f3BSb2znNP8sf8APVfyP+FAWNepLn06CmFx6SME3Ng44TjnP/1qipzMWOT/APqptID/2Q==","permalink":"https://julialang.slack.com/files/U69EQD0CD/F01H1DGV70S/screenshot_2020-12-16_at_13.04.19.png","permalink_public":"https://slack-files.com/T68168MUP-F01H1DGV70S-bd95de3bfc","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"5+3y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I was doing some (unrelated) dark-mode things today and thought you might enjoy this take of your graphic:"}]}]}],"user":"U69EQD0CD","display_as_bot":false,"ts":"1608124133.087900","thread_ts":"1608124133.087900","reply_count":1,"reply_users_count":1,"latest_reply":"1608138856.088900","reply_users":["UN45LV5K6"],"subscribed":false,"reactions":[{"name":"juliaheartpulse-dark","users":["U85JBUGGP","U7QLM6E2E","UGU761DU2"],"count":3},{"name":"+1","users":["UC0SY9JFP","UH08DT0JU"],"count":2}]},{"type":"message","subtype":"thread_broadcast","text":"So very recently we actually introduced an iterator interface to Turing.jl. This means that you can now do stuff like: <https://gist.github.com/torfjelde/cc5c41e97eb4c97e22a19b8440f6d506>\n\nThis gives you waaay more control over the sampling process, if you want it:) This is a very recent change though, hence it's not yet in the documentation (soon!).","user":"UHDNY2YMA","ts":"1608164498.089400","thread_ts":"1608102775.084800","root":{"client_msg_id":"332233e2-11f5-43ac-911c-0659cfd1b3fb","type":"message","text":"Hi, is there a way to stop sampling based on some criterion, like the callbacks in Tensorflow?","user":"U018NKR9X70","ts":"1608102775.084800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wpw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, is there a way to stop sampling based on some criterion, like the callbacks in Tensorflow?"}]}]}],"thread_ts":"1608102775.084800","reply_count":8,"reply_users_count":4,"latest_reply":"1608218495.111000","reply_users":["UH08DT0JU","UHDNY2YMA","U8T9JUA5R","U018NKR9X70"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"mQ4I","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So very recently we actually introduced an iterator interface to Turing.jl. This means that you can now do stuff like: "},{"type":"link","url":"https://gist.github.com/torfjelde/cc5c41e97eb4c97e22a19b8440f6d506"},{"type":"text","text":"\n\nThis gives you waaay more control over the sampling process, if you want it:) This is a very recent change though, hence it's not yet in the documentation (soon!)."}]}]}],"client_msg_id":"abf89027-90e2-4945-81c3-586c7c18f1c8","edited":{"user":"UHDNY2YMA","ts":"1608164533.000000"},"reactions":[{"name":"+1","users":["U018NKR9X70","U017YGFQTE3","UH08DT0JU","U7QLM6E2E"],"count":4},{"name":"100","users":["U018NKR9X70"],"count":1}]},{"client_msg_id":"7df3ff22-5b9b-4adb-89cd-a6754239e5d5","type":"message","text":"<@UCRDHV7PB> <@UHDNY2YMA> what does it take to get the Bayesian differential equations tutorial updated?","user":"U69BL50BF","ts":"1608199288.092000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=xG","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UCRDHV7PB"},{"type":"text","text":" "},{"type":"user","user_id":"UHDNY2YMA"},{"type":"text","text":" what does it take to get the Bayesian differential equations tutorial updated?"}]}]}]},{"client_msg_id":"05696569-026d-4432-bd5b-a48524179683","type":"message","text":"we've had <https://github.com/TuringLang/TuringTutorials/blob/master/10_diffeq.ipynb> sitting in the tutorials repo since September, and want to know what the process is to update the tutorial on the website","user":"U69BL50BF","ts":"1608199323.092600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hM3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"we've had "},{"type":"link","url":"https://github.com/TuringLang/TuringTutorials/blob/master/10_diffeq.ipynb"},{"type":"text","text":" sitting in the tutorials repo since September, and want to know what the process is to update the tutorial on the website"}]}]}],"thread_ts":"1608199323.092600","reply_count":1,"reply_users_count":1,"latest_reply":"1608200120.093400","reply_users":["U017YGFQTE3"],"subscribed":false},{"client_msg_id":"b95dceca-24eb-421d-b005-9d78c4579e7a","type":"message","text":"We'll be tweeting out everything around the Bayesian neural ODE paper today but hopefully this can get fixed up","user":"U69BL50BF","ts":"1608199785.093200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qGnSJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We'll be tweeting out everything around the Bayesian neural ODE paper today but hopefully this can get fixed up"}]}]}],"reactions":[{"name":"+1","users":["UC0SY9JFP"],"count":1}]},{"client_msg_id":"544f38e2-4fc0-4dd1-a7eb-b625f6e73d82","type":"message","text":"It is online: <https://turing.ml/dev/tutorials/10-diffeq/>","user":"U8T9JUA5R","ts":"1608200695.093800","team":"T68168MUP","attachments":[{"title":"Bayesian Estimation of Differential Equations","title_link":"https://turing.ml/dev/tutorials/10-diffeq/","text":"Bayesian Estimation of Differential Equations","fallback":"Bayesian Estimation of Differential Equations","from_url":"https://turing.ml/dev/tutorials/10-diffeq/","service_icon":"https://turing.ml/dev/assets/img/favicon.ico","service_name":"turing.ml","id":1,"original_url":"https://turing.ml/dev/tutorials/10-diffeq/"}],"blocks":[{"type":"rich_text","block_id":"lGks","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It is online: "},{"type":"link","url":"https://turing.ml/dev/tutorials/10-diffeq/"}]}]}]},{"client_msg_id":"94e1d86e-80f6-499f-95a9-ebafb1a9c219","type":"message","text":"As soon as Turing's documentation is updated (e.g., for a new release) the changes in TuringTutorials end up on the webpage automatically","user":"U8T9JUA5R","ts":"1608200732.094700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ufH60","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"As soon as Turing's documentation is updated (e.g., for a new release) the changes in TuringTutorials end up on the webpage automatically"}]}]}]},{"client_msg_id":"1378d287-9cd8-4fb6-a11e-3844f5dc45a4","type":"message","text":"It just seems the new tutorial was added but did not replace the old one","user":"U8T9JUA5R","ts":"1608200766.095200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VV9yT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It just seems the new tutorial was added but did not replace the old one"}]}]}]},{"client_msg_id":"ebd32ec3-37c6-4a9e-8ebc-e71b517d5c4e","type":"message","text":"In TuringTutorials","user":"U8T9JUA5R","ts":"1608200776.095400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Eq+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In TuringTutorials"}]}]}]},{"client_msg_id":"16ac8c4c-e0f3-4c4e-8551-f9df2a6e50cc","type":"message","text":"Hmm although seems correct in the repo. Maybe it's problematic that it was renamed","user":"U8T9JUA5R","ts":"1608200825.096500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sOLeS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hmm although seems correct in the repo. Maybe it's problematic that it was renamed"}]}]}],"thread_ts":"1608200825.096500","reply_count":16,"reply_users_count":2,"latest_reply":"1608243771.111800","reply_users":["U6QF223TN","U8T9JUA5R"],"subscribed":false},{"client_msg_id":"1d68c237-ef3a-4efe-b756-aa3853914cb6","type":"message","text":"It seems the current setup in Turing does not actually delete stuff that was removed: <https://github.com/TuringLang/Turing.jl/tree/gh-pages/dev/tutorials> Both versions are there","user":"U8T9JUA5R","ts":"1608200995.099300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ZR/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It seems the current setup in Turing does not actually delete stuff that was removed: "},{"type":"link","url":"https://github.com/TuringLang/Turing.jl/tree/gh-pages/dev/tutorials"},{"type":"text","text":" Both versions are there"}]}]}],"reactions":[{"name":"man-facepalming","users":["UC0SY9JFP"],"count":1}]},{"client_msg_id":"599b1b23-e110-4ca8-84cb-b00137956c9d","type":"message","text":"So you can link to the tutorial but to fix the webpage one would have to remove all duplicates in the gh-pages branch manually (and I don't know if this would break any internal links) and remove the markdown files in TuringTutorials (to avoid the problem when the docs are rebuilt)","user":"U8T9JUA5R","ts":"1608201258.102900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gCrF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So you can link to the tutorial but to fix the webpage one would have to remove all duplicates in the gh-pages branch manually (and I don't know if this would break any internal links) and remove the markdown files in TuringTutorials (to avoid the problem when the docs are rebuilt)"}]}]}]},{"client_msg_id":"5965271b-c764-4805-bdb1-23ece2ab7d25","type":"message","text":"Maybe the new markdown-based setup of the tutorials that <@UHDNY2YMA> is working on would fix the problem for future iterations of the webpage automatically (<https://github.com/TuringLang/TuringTutorials/issues/86>)","user":"U8T9JUA5R","ts":"1608201487.106200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9Cs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Maybe the new markdown-based setup of the tutorials that "},{"type":"user","user_id":"UHDNY2YMA"},{"type":"text","text":" is working on would fix the problem for future iterations of the webpage automatically ("},{"type":"link","url":"https://github.com/TuringLang/TuringTutorials/issues/86"},{"type":"text","text":")"}]}]}]},{"client_msg_id":"5d790a7d-f398-4560-abc4-74fb5df5624a","type":"message","text":"But maybe the better approach would be to rename everything to the old name 10-bayesiandiffeq such that links that might float around somewhere would still be valid?","user":"U8T9JUA5R","ts":"1608201836.108300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"T54Vs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But maybe the better approach would be to rename everything to the old name 10-bayesiandiffeq such that links that might float around somewhere would still be valid?"}]}]}],"thread_ts":"1608201836.108300","reply_count":3,"reply_users_count":2,"latest_reply":"1608218691.111400","reply_users":["UHDNY2YMA","U8T9JUA5R"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"For reference: <https://github.com/TuringLang/TuringTutorials/pull/83#issuecomment-747361811|https://github.com/TuringLang/TuringTutorials/pull/83#issuecomment-747361811>","user":"U8T9JUA5R","ts":"1608202618.109600","thread_ts":"1608200825.096500","root":{"client_msg_id":"16ac8c4c-e0f3-4c4e-8551-f9df2a6e50cc","type":"message","text":"Hmm although seems correct in the repo. Maybe it's problematic that it was renamed","user":"U8T9JUA5R","ts":"1608200825.096500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sOLeS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hmm although seems correct in the repo. Maybe it's problematic that it was renamed"}]}]}],"thread_ts":"1608200825.096500","reply_count":16,"reply_users_count":2,"latest_reply":"1608243771.111800","reply_users":["U6QF223TN","U8T9JUA5R"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"cY42","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"For reference: "},{"type":"link","url":"https://github.com/TuringLang/TuringTutorials/pull/83#issuecomment-747361811","text":"https://github.com/TuringLang/TuringTutorials/pull/83#issuecomment-747361811"}]}]}],"client_msg_id":"1b9b17c0-d838-45de-af11-302a1fa4e726","reactions":[{"name":"+1","users":["U6QF223TN"],"count":1}]},{"type":"message","subtype":"thread_broadcast","text":"The updated version is renamed now and available at <https://turing.ml/dev/tutorials/10-bayesiandiffeq/|https://turing.ml/dev/tutorials/10-bayesiandiffeq/> (same address as the old DE tutorial).","user":"U8T9JUA5R","ts":"1608243771.111800","thread_ts":"1608200825.096500","root":{"client_msg_id":"16ac8c4c-e0f3-4c4e-8551-f9df2a6e50cc","type":"message","text":"Hmm although seems correct in the repo. Maybe it's problematic that it was renamed","user":"U8T9JUA5R","ts":"1608200825.096500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sOLeS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hmm although seems correct in the repo. Maybe it's problematic that it was renamed"}]}]}],"thread_ts":"1608200825.096500","reply_count":16,"reply_users_count":2,"latest_reply":"1608243771.111800","reply_users":["U6QF223TN","U8T9JUA5R"],"subscribed":false},"attachments":[{"title":"Bayesian Estimation of Differential Equations","title_link":"https://turing.ml/dev/tutorials/10-bayesiandiffeq/","text":"Bayesian Estimation of Differential Equations","fallback":"Bayesian Estimation of Differential Equations","from_url":"https://turing.ml/dev/tutorials/10-bayesiandiffeq/","service_icon":"https://turing.ml/dev/assets/img/favicon.ico","service_name":"turing.ml","id":1,"original_url":"https://turing.ml/dev/tutorials/10-bayesiandiffeq/"}],"blocks":[{"type":"rich_text","block_id":"tc8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The updated version is renamed now and available at "},{"type":"link","url":"https://turing.ml/dev/tutorials/10-bayesiandiffeq/","text":"https://turing.ml/dev/tutorials/10-bayesiandiffeq/"},{"type":"text","text":" (same address as the old DE tutorial)."}]}]}],"client_msg_id":"1a9c9ded-4664-402e-83d1-81d6d814fe81","reactions":[{"name":"+1","users":["UC0SY9JFP"],"count":1}]},{"client_msg_id":"4d03ad6c-811c-4efe-9c54-595447faf389","type":"message","text":"<@U8T9JUA5R> the image links are broken","user":"U69BL50BF","ts":"1608245348.112500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Xld5k","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U8T9JUA5R"},{"type":"text","text":" the image links are broken"}]}]}],"thread_ts":"1608245348.112500","reply_count":12,"reply_users_count":3,"latest_reply":"1608260318.115400","reply_users":["U8T9JUA5R","U69BL50BF","UHDNY2YMA"],"subscribed":false},{"client_msg_id":"4ae4dcc5-f024-48f4-bfdd-9342b468d73c","type":"message","text":"<@U8T9JUA5R> could you un-404 this while the paper is still going around? <https://github.com/TuringLang/TuringTutorials/blob/master/10_diffeq.ipynb>","user":"U69BL50BF","ts":"1608281279.116200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=QV","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U8T9JUA5R"},{"type":"text","text":" could you un-404 this while the paper is still going around? "},{"type":"link","url":"https://github.com/TuringLang/TuringTutorials/blob/master/10_diffeq.ipynb"}]}]}]},{"client_msg_id":"7b7b30c7-efbd-45a5-9365-26685ec399ba","type":"message","text":"I just got about 100 messages about that, so...","user":"U69BL50BF","ts":"1608281304.116600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"E7jht","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I just got about 100 messages about that, so..."}]}]}],"thread_ts":"1608281304.116600","reply_count":2,"reply_users_count":2,"latest_reply":"1608292726.119100","reply_users":["UCRDHV7PB","U69BL50BF"],"subscribed":false},{"client_msg_id":"f1508143-e474-4b54-81c5-9186f615ae20","type":"message","text":"To everyone, join us *today* at *3 pm* CET on gather town!\n\nLink: <https://gather.town/app/gzvyzGbOmGlfScYJ/turing>\nPasswd: turingisawesome","user":"UC0SY9JFP","ts":"1608290652.118600","team":"T68168MUP","attachments":[{"title":"Gather","title_link":"https://gather.town/app/gzvyzGbOmGlfScYJ/turing","text":"Gather is a video-calling space that lets multiple people hold separate conversations in parallel, walking in and out of those conversations just as easily as they would in real life.","fallback":"Gather","image_url":"https://gather.town/images/site/site_preview.png","from_url":"https://gather.town/app/gzvyzGbOmGlfScYJ/turing","image_width":418,"image_height":250,"image_bytes":168628,"service_icon":"https://gather.town/favicon.ico","service_name":"gather.town","id":1,"original_url":"https://gather.town/app/gzvyzGbOmGlfScYJ/turing"}],"blocks":[{"type":"rich_text","block_id":"/Hm7Z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"To everyone, join us "},{"type":"text","text":"today","style":{"bold":true}},{"type":"text","text":" at "},{"type":"text","text":"3 pm","style":{"bold":true}},{"type":"text","text":" CET on gather town!\n\nLink: "},{"type":"link","url":"https://gather.town/app/gzvyzGbOmGlfScYJ/turing"},{"type":"text","text":"\nPasswd: turingisawesome"}]}]}]},{"client_msg_id":"7e2cbc64-cbc8-4819-b71d-9cc7a31a9883","type":"message","text":"If my Turing model has a positive variable `τ`, then `log(τ)` is what's actually being sampled. Is there a way given a `Chains` object and a `Model` to get all of the transformed samples (i.e. `log(τ)`)?","user":"UHDQQ4GN6","ts":"1608317946.121400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Zt6em","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If my Turing model has a positive variable "},{"type":"text","text":"τ","style":{"code":true}},{"type":"text","text":", then "},{"type":"text","text":"log(τ)","style":{"code":true}},{"type":"text","text":" is what's actually being sampled. Is there a way given a "},{"type":"text","text":"Chains","style":{"code":true}},{"type":"text","text":" object and a "},{"type":"text","text":"Model","style":{"code":true}},{"type":"text","text":" to get all of the transformed samples (i.e. "},{"type":"text","text":"log(τ)","style":{"code":true}},{"type":"text","text":")?"}]}]}],"thread_ts":"1608317946.121400","reply_count":7,"reply_users_count":2,"latest_reply":"1608326456.122800","reply_users":["U9JNHB83X","UHDQQ4GN6"],"subscribed":false},{"client_msg_id":"dee24eb2-4cde-40cf-a807-e4e3a74f68e3","type":"message","text":"Somehow when I run the Turing sampler with `progress = true`  in VSCode, there's no progress bar any more. Does anyone encounter the same issue?","user":"UMRTL1HKP","ts":"1608332106.123900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"akIq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Somehow when I run the Turing sampler with "},{"type":"text","text":"progress = true","style":{"code":true}},{"type":"text","text":"  in VSCode, there's no progress bar any more. Does anyone encounter the same issue?"}]}]}],"thread_ts":"1608332106.123900","reply_count":21,"reply_users_count":4,"latest_reply":"1608541096.159100","reply_users":["UN97XTLCV","UMRTL1HKP","U8T9JUA5R","U01C2AJ9F63"],"subscribed":false},{"type":"message","text":"Hi folks, possibly non-Turing question here. I'm a newbie to Bayesian modeling and am working on learning the rudiments using a dataset of dogs in New York City.\n\nI want to model the joint distribution of name and breed, and my raw data is (name, breed, count).\n\nSo far I've gotten as far as \"learning\" a Dirichlet posterior over dog breeds using a uniform Dirichlet prior and a multinomial likelihood (the observed counts).\n\nI've been using a funny strategy of learning from e.g. all of the dogs NOT named Pugsley, then additionally learning from the Pugsleys and ranking breeds by the relative entropy between the prior and posterior, which surfaces those breeds for which learning that a name is Pugsley is most informative.\n\nThis feels like a bit of a hack, and what I'd really like is a model where I can ask about `p(name|pug)` and `p(breed|pugsley)`, which sounds like it would involve learning something about the joint distribution rather than going one variable at a time.\n\nNot sure this is the best place to ask, but any suggestions or pointers to resources would be appreciated!\n\nFor fun, here are the breeds that are most likely when the dog name includes \"snow\":","files":[{"id":"F01HNLL81L1","created":1608352084,"timestamp":1608352084,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U68M6ERG8","editable":false,"size":203186,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01HNLL81L1/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01HNLL81L1/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01HNLL81L1-f22cbda5d0/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01HNLL81L1-f22cbda5d0/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01HNLL81L1-f22cbda5d0/image_360.png","thumb_360_w":360,"thumb_360_h":221,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01HNLL81L1-f22cbda5d0/image_480.png","thumb_480_w":480,"thumb_480_h":295,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01HNLL81L1-f22cbda5d0/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01HNLL81L1-f22cbda5d0/image_720.png","thumb_720_w":720,"thumb_720_h":443,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01HNLL81L1-f22cbda5d0/image_800.png","thumb_800_w":800,"thumb_800_h":492,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01HNLL81L1-f22cbda5d0/image_960.png","thumb_960_w":960,"thumb_960_h":590,"thumb_1024":"https://files.slack.com/files-tmb/T68168MUP-F01HNLL81L1-f22cbda5d0/image_1024.png","thumb_1024_w":1024,"thumb_1024_h":630,"original_w":2234,"original_h":1374,"thumb_tiny":"AwAdADDSIzRt96WigBuPel/GkJ9cYpW6dPzoAX8aKB0ooAKD0ooPSgBvelbpRjmg5NACjpRQOlFAH//Z","permalink":"https://julialang.slack.com/files/U68M6ERG8/F01HNLL81L1/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01HNLL81L1-1307c53b4b","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"4AH8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi folks, possibly non-Turing question here. I'm a newbie to Bayesian modeling and am working on learning the rudiments using a dataset of dogs in New York City.\n\nI want to model the joint distribution of name and breed, and my raw data is (name, breed, count).\n\nSo far I've gotten as far as \"learning\" a Dirichlet posterior over dog breeds using a uniform Dirichlet prior and a multinomial likelihood (the observed counts).\n\nI've been using a funny strategy of learning from e.g. all of the dogs NOT named Pugsley, then additionally learning from the Pugsleys and ranking breeds by the relative entropy between the prior and posterior, which surfaces those breeds for which learning that a name is Pugsley is most informative.\n\nThis feels like a bit of a hack, and what I'd really like is a model where I can ask about "},{"type":"text","text":"p(name|pug)","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"p(breed|pugsley)","style":{"code":true}},{"type":"text","text":", which sounds like it would involve learning something about the joint distribution rather than going one variable at a time.\n\nNot sure this is the best place to ask, but any suggestions or pointers to resources would be appreciated!\n\nFor fun, here are the breeds that are most likely when the dog name includes \"snow\":"}]}]}],"user":"U68M6ERG8","display_as_bot":false,"ts":"1608352096.137600","thread_ts":"1608352096.137600","reply_count":4,"reply_users_count":2,"latest_reply":"1608389923.147900","reply_users":["U01C2AJ9F63","U68M6ERG8"],"subscribed":false},{"client_msg_id":"3ee970dd-e17f-4985-afe0-ba0c72aa15eb","type":"message","text":"So we talked yesterday during the meeting about how my relatively simple model takes quite some time to run already (40+ minutes), and that I should post a snippet here to see if maybe there is a problem in the code, because I thought it might be normal given the 1800 samples and the model. There is quite a lot of missing data as well, but even when I just reran it with complete cases only, I still got above 40 minutes somehow anyway. I'll post a code snippet in the thread/comments.","user":"U01C2AJ9F63","ts":"1608379428.141500","team":"T68168MUP","edited":{"user":"U01C2AJ9F63","ts":"1608379706.000000"},"blocks":[{"type":"rich_text","block_id":"yLl0f","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So we talked yesterday during the meeting about how my relatively simple model takes quite some time to run already (40+ minutes), and that I should post a snippet here to see if maybe there is a problem in the code, because I thought it might be normal given the 1800 samples and the model. There is quite a lot of missing data as well, but even when I just reran it with complete cases only, I still got above 40 minutes somehow anyway. I'll post a code snippet in the thread/comments."}]}]}],"thread_ts":"1608379428.141500","reply_count":29,"reply_users_count":5,"latest_reply":"1608649623.167500","reply_users":["U01C2AJ9F63","U017YGFQTE3","UH08DT0JU","UC0SY9JFP","U85JBUGGP"],"subscribed":false},{"client_msg_id":"f4131c24-621c-4d43-bd00-e2f75be9b20a","type":"message","text":"Hi, if I have a model like (from tutorial)\n\n@model function coinFlip(Y)\n\tp ~ Beta(1, 1)\n\tfor n in 1:length(Y)\n\t\tY[n] ~ Bernoulli(p)\n\tend\nend\n\nand want to compute the posterior likelihood given p=0.5 with coinFlip([true, true, true, false]). Is there a way to do this within Turing?\n\nThank you!","user":"UCM61ND7F","ts":"1608466243.153300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MZOyY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, if I have a model like (from tutorial)\n\n@model function coinFlip(Y)\n\tp ~ Beta(1, 1)\n\tfor n in 1:length(Y)\n\t\tY[n] ~ Bernoulli(p)\n\tend\nend\n\nand want to compute the posterior likelihood given p=0.5 with coinFlip([true, true, true, false]). Is there a way to do this within Turing?\n\nThank you!"}]}]}],"thread_ts":"1608466243.153300","reply_count":6,"reply_users_count":3,"latest_reply":"1608475134.158300","reply_users":["UH08DT0JU","UC0SY9JFP","UCM61ND7F"],"subscribed":false},{"client_msg_id":"0f45f689-10e3-4971-bee5-cadd0593d85e","type":"message","text":"Hello Turing friends! The <https://turing.ml/dev/tutorials/10-bayesiandiffeq/#inference-of-a-stochastic-differential-equation|Bayesian DiffEq tutorial> mentions SGHMC as a possibly better alternative to NUTS for SDE inference. Is this currently implemented? I don't see it in the docs. I implemented SGHMC in python/Tensorflow Probability a few months ago, so I would happy to help contribute to an implementation.","user":"U01H36BUDJB","ts":"1608733275.169800","team":"T68168MUP","attachments":[{"title":"Bayesian Estimation of Differential Equations","title_link":"https://turing.ml/dev/tutorials/10-bayesiandiffeq/#inference-of-a-stochastic-differential-equation","text":"Bayesian Estimation of Differential Equations","fallback":"Bayesian Estimation of Differential Equations","from_url":"https://turing.ml/dev/tutorials/10-bayesiandiffeq/#inference-of-a-stochastic-differential-equation","service_icon":"https://turing.ml/dev/assets/img/favicon.ico","service_name":"turing.ml","id":1,"original_url":"https://turing.ml/dev/tutorials/10-bayesiandiffeq/#inference-of-a-stochastic-differential-equation"}],"blocks":[{"type":"rich_text","block_id":"xeVKj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello Turing friends! The "},{"type":"link","url":"https://turing.ml/dev/tutorials/10-bayesiandiffeq/#inference-of-a-stochastic-differential-equation","text":"Bayesian DiffEq tutorial"},{"type":"text","text":" mentions SGHMC as a possibly better alternative to NUTS for SDE inference. Is this currently implemented? I don't see it in the docs. I implemented SGHMC in python/Tensorflow Probability a few months ago, so I would happy to help contribute to an implementation."}]}]}],"thread_ts":"1608733275.169800","reply_count":10,"reply_users_count":3,"latest_reply":"1608734644.172800","reply_users":["U8T9JUA5R","U01H36BUDJB","U6C937ENB"],"subscribed":false},{"client_msg_id":"d9cc74f8-f1d7-4cdb-9ce2-55fbaabede23","type":"message","text":"Hello everyone\nI was trying to write a custom gradient in `reversediff` for lambertw function from <https://github.com/jlapeyre/LambertW.jl|this package>. Here is my code\n```using ReverseDiff, LambertW\n\nlambert_ext(x::Float64) = lambertw(x)\nlambert_ext(x::Array{Float64, 1}) = lambertw.(x)\n\nlambert_ext(x::ReverseDiff.TrackedVector) = ReverseDiff.track(lambert_ext, x)\n\nReverseDiff.@grad function lambert_ext(x)\n    xv = ReverseDiff.value(x)\n    return lambert_ext(xv), function (Δ)\n        w = lambert_ext(xv)\n        return (Δ.*((xv.*w)./(w.+1)), )\n    end\nend\n\ninputs = [1.0]\nw = lambert_ext(inputs)\nprintln(\"actual derivative: $((inputs.*w)./(w.+1))\")\nprintln(\"derivative reversediff: $(ReverseDiff.gradient(lambert_ext, inputs))\")\n\ninputs = [1.0, 2.0]\nw = lambert_ext(inputs)\nprintln(\"actual derivative: $((inputs.*w)./(w.+1))\")\nprintln(\"derivative reversediff: $(ReverseDiff.gradient(lambert_ext, inputs))\")```\nThe method works fine with array of shape (1,). But it","user":"UR43MA5DY","ts":"1608748159.177000","team":"T68168MUP","edited":{"user":"UR43MA5DY","ts":"1608748217.000000"},"blocks":[{"type":"rich_text","block_id":"EvKZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello everyone\nI was trying to write a custom gradient in `reversediff` for lambertw function from "},{"type":"link","url":"https://github.com/jlapeyre/LambertW.jl","text":"this package"},{"type":"text","text":". Here is my code\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using ReverseDiff, LambertW\n\nlambert_ext(x::Float64) = lambertw(x)\nlambert_ext(x::Array{Float64, 1}) = lambertw.(x)\n\nlambert_ext(x::ReverseDiff.TrackedVector) = ReverseDiff.track(lambert_ext, x)\n\nReverseDiff.@grad function lambert_ext(x)\n    xv = ReverseDiff.value(x)\n    return lambert_ext(xv), function (Δ)\n        w = lambert_ext(xv)\n        return (Δ.*((xv.*w)./(w.+1)), )\n    end\nend\n\ninputs = [1.0]\nw = lambert_ext(inputs)\nprintln(\"actual derivative: $((inputs.*w)./(w.+1))\")\nprintln(\"derivative reversediff: $(ReverseDiff.gradient(lambert_ext, inputs))\")\n\ninputs = [1.0, 2.0]\nw = lambert_ext(inputs)\nprintln(\"actual derivative: $((inputs.*w)./(w.+1))\")\nprintln(\"derivative reversediff: $(ReverseDiff.gradient(lambert_ext, inputs))\")"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"The method works fine with array of shape (1,). But it"}]}]}]},{"client_msg_id":"a7d168fc-37c6-4cfc-9154-9ab1ebe64a60","type":"message","text":"Puzzling question here <https://discourse.julialang.org/t/why-does-shuffling-rows-change-the-estimates/52305/1>","user":"U01ARRMLM7E","ts":"1608780880.178700","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"Why does shuffling rows change the estimates?","title_link":"https://discourse.julialang.org/t/why-does-shuffling-rows-change-the-estimates/52305/1","text":"using DataFrames using Pipe: @pipe using StatsBase: StatsBase, mad, median, percentile, sample, shuffle using Distributions using CategoricalArrays using Turing ## df, parameters = let β = ( β_0 = 7, β_1 = 0.001, β_2 = 0.05, β_3 = 0.05, β_4 = 0.01, ) σ = 2 parameters = (β..., σ=σ) parameters_df = DataFrame( parameter=collect(keys(parameters)), value=collect(values(parameters)), ) N = 100_000 X = DataF...","fallback":"JuliaLang: Why does shuffling rows change the estimates?","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","ts":1608765251,"from_url":"https://discourse.julialang.org/t/why-does-shuffling-rows-change-the-estimates/52305/1","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/why-does-shuffling-rows-change-the-estimates/52305/1"}],"blocks":[{"type":"rich_text","block_id":"a/8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Puzzling question here "},{"type":"link","url":"https://discourse.julialang.org/t/why-does-shuffling-rows-change-the-estimates/52305/1"}]}]}]},{"client_msg_id":"24d42679-410c-4fdb-a559-25dfdff94129","type":"message","text":"Hi all-\n\nI have noticed that Turing takes about twice as long to load and precompile than Plots.jl. I was wondering whether there are any optimizations that could be used to speed this up?","user":"UH08DT0JU","ts":"1608816492.181200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"aOHe","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi all-\n\nI have noticed that Turing takes about twice as long to load and precompile than Plots.jl. I was wondering whether there are any optimizations that could be used to speed this up?"}]}]}]},{"type":"message","text":"Hi, I'm playing with Turing and I want to create a model for ridge regression solution via HMC. My resource is in the picture. I can't find a way to do this and compose `@model`. I posted my question here: <https://discourse.julialang.org/t/ridge-regression-using-turing-jl/52378>\n\nIs there anyone who could help me? Is it even possible to use HMC for this kind of problem? Thanks in advance for any suggestions :slightly_smiling_face:","files":[{"id":"F01HMDS6JHH","created":1608917268,"timestamp":1608917268,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"UR51BK15E","editable":false,"size":41601,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01HMDS6JHH/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01HMDS6JHH/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01HMDS6JHH-45ce8df9b2/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01HMDS6JHH-45ce8df9b2/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01HMDS6JHH-45ce8df9b2/image_360.png","thumb_360_w":360,"thumb_360_h":236,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01HMDS6JHH-45ce8df9b2/image_480.png","thumb_480_w":480,"thumb_480_h":314,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01HMDS6JHH-45ce8df9b2/image_160.png","original_w":561,"original_h":367,"thumb_tiny":"AwAfADDSIPqfwpM4HOaUn3x+FA57/pQAm7/OaUMD7UY96MH1/SgBaKTBz1pRQAhJzwP1o69ePxpTntSHJ7CgAx7n86Me5/OgD1UUuB6UAAooAx0ooA//2Q==","permalink":"https://julialang.slack.com/files/UR51BK15E/F01HMDS6JHH/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01HMDS6JHH-eccf6b9442","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"R7Bb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I'm playing with Turing and I want to create a model for ridge regression solution via HMC. My resource is in the picture. I can't find a way to do this and compose "},{"type":"text","text":"@model","style":{"code":true}},{"type":"text","text":". I posted my question here: "},{"type":"link","url":"https://discourse.julialang.org/t/ridge-regression-using-turing-jl/52378"},{"type":"text","text":"\n\nIs there anyone who could help me? Is it even possible to use HMC for this kind of problem? Thanks in advance for any suggestions "},{"type":"emoji","name":"slightly_smiling_face"}]}]}],"user":"UR51BK15E","display_as_bot":false,"ts":"1608917433.198700"},{"client_msg_id":"c0b375d6-be00-4d6e-bb77-c07d94393e4b","type":"message","text":"Is it possible to get a list of the parameters of a model object?","user":"U01ARRMLM7E","ts":"1609099471.206500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Va9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is it possible to get a list of the parameters of a model object?"}]}]}],"thread_ts":"1609099471.206500","reply_count":1,"reply_users_count":1,"latest_reply":"1609099921.206600","reply_users":["U01ARRMLM7E"],"subscribed":false},{"client_msg_id":"963f39b5-342e-4945-b1bb-68f86093aa8e","type":"message","text":"Hi all. Quick question. How can I quickly test out whether I've made a mistake in my model definition? Currently, I'm using the NUTS sampler with as few as 4 samples, but it takes ages (infinite maybe) since it keeps rejecting proposals.","user":"U01BTNDCUBX","ts":"1609242820.210900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MRP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi all. Quick question. How can I quickly test out whether I've made a mistake in my model definition? Currently, I'm using the NUTS sampler with as few as 4 samples, but it takes ages (infinite maybe) since it keeps rejecting proposals."}]}]}],"thread_ts":"1609242820.210900","reply_count":3,"reply_users_count":2,"latest_reply":"1609243877.211600","reply_users":["U01BTNDCUBX","U8T9JUA5R"],"subscribed":false},{"client_msg_id":"37973ed3-3138-4331-9b1e-b343ea154287","type":"message","text":"Hey folks. I was hoping someone could point me in the right direction with this error message. I am setting up a very simple vanilla discrete dynamical simulation (to proof of concept). So I define a simple growth model, generate some data, and then wrote a Turing model to see if I can recover the parameters used to generate the simulated data. The code is below. But I am getting an error when I run the `chain` function to execute the sample. The error is `TypeError: in typeassert, expected Float64, got a value of type ForwardDiff.Dual{Nothing,Float64,2}`. I am not really sure how to debug these types of issues in Turing, so if anyone could tell me where to look, I would appreciate it. Here is the simple code that I wrote. I use the Turing Diffeq tutorial as a model, but removed the Diffeq solver with my own solver. I can add more to the stacktrace if it is helpful.\n```\n# Create a simple discrete simulation\nfunction simple_growth!(du, u0, p)\n    r = p[1]\n    du[1] = r*u0[1]\n    du\nend\n\n# solve the dynamical system\nfunction solve_system(f,u0,p,n)\n    u = Vector{typeof(u0)}(undef,n)\n    du = similar(u0)\n    u[1] = u0\n    for i in 1:n-1\n        f(du,u[i],p)\n        u[i+1] = copy(du)\n    end\n    u\nend\n\n# simulate some data\ndata = solve_system(simple_growth!, [0.9], [2.0], 10)\ndata = [data[i][1] for i in 1:length(data)]\n\n# define the Turing model\nTuring.setadbackend(:forwarddiff)\n\n@model function fitlv(data, f, u0, n)\n    σ ~ InverseGamma(2, 3) # ~ is the tilde character\n    α ~ truncated(Normal(0.7,0.5),0.5,2.5)\n\n    p = [α]\n    #prob = remake(prob1, p=p)\n    predicted = solve_system(f, u0, p, n)\n\tpredicted = [predicted[i][1] for i in 1:length(predicted)]\n    for i = 1:length(predicted)\n        data[i] ~ Normal(predicted[i], σ)\n    end\nend\n\n# setup the Turing sampler\nu0 = [0.9]\nn = 10\nmodel = fitlv(data, simple_growth!, u0, n)\nchain = mapreduce(c -&gt; sample(model, NUTS(.65),1000), chainscat, 1:3)```\nHere is just the first part of the stacktrace for the error.\n\n```chain\n\nTypeError: in typeassert, expected Float64, got a value of type ForwardDiff.Dual{Nothing,Float64,2}\n\n    setindex!(::Array{Float64,1}, ::ForwardDiff.Dual{ForwardDiff.Tag{Turing.Core.var\"#f#1\"{DynamicPPL.VarInfo{NamedTuple{(:σ, :α),Tuple{DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:σ,Tuple{}},Int64},Array{Distributions.InverseGamma{Float64},1},Array{DynamicPPL.VarName{:σ,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}},DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:α,Tuple{}},Int64},Array{Distributions.Truncated{Distributions.Normal{Float64},Distributions.Continuous,Float64},1},Array{DynamicPPL.VarName{:α,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}}}},Float64},DynamicPPL.Model{Main.workspace150.var\"#1#3\",(:data, :f, :u0, :n),(),(),Tuple{Array{Float64,1},typeof(Main.workspace115.simple_growth!),Array{Float64,1},Int64},Tuple{}},DynamicPPL.Sampler{Turing.Inference.NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.DefaultContext},Float64},Float64,2}, ::Int64)@array.jl:847\n    simple_growth!(::Array{Float64,1}, ::Array{Float64,1}, ::Array{ForwardDiff.Dual{ForwardDiff.Tag{Turing.Core.var\"#f#1\"{DynamicPPL.VarInfo{NamedTuple{(:σ, :α),Tuple{DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:σ,Tuple{}},Int64},Array{Distributions.InverseGamma{Float64},1},Array{DynamicPPL.VarName{:σ,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}},DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:α,Tuple{}},Int64},Array{Distributions.Truncated{Distributions.Normal{Float64},Distributions.Continuous,Float64},1},Array{DynamicPPL.VarName{:α,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}}}},Float64},DynamicPPL.Model{Main.workspace150.var\"#1#3\",(:data, :f, :u0, :n),(),(),Tuple{Array{Float64,1},typeof(Main.workspace115.simple_growth!),Array{Float64,1},Int64},Tuple{}},DynamicPPL.Sampler{Turing.Inference.NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.DefaultContext},Float64},Float64,2},1})@Other: 3\n    solve_system(::typeof(Main.workspace115.simple_growth!), ::Array{Float64,1}, ::Array{ForwardDiff.Dual{ForwardDiff.Tag{Turing.Core.var\"#f#1\"{DynamicPPL.VarInfo{NamedTuple{(:σ, :α),Tuple{DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:σ,Tuple{}},Int64},Array{Distributions.InverseGamma{Float64},1},Array{DynamicPPL.VarName{:σ,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}},DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:α,Tuple{}},Int64},Array{Distributions.Truncated{Distributions.Normal{Float64},Distributions.Continuous,Float64},1},Array{DynamicPPL.VarName{:α,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}}}},Float64},DynamicPPL.Model{Main.workspace150.var\"#1#3\",(:data, :f, :u0, :n),(),(),Tuple{Array{Float64,1},typeof(Main.workspace115.simple_growth!),Array{Float64,1},Int64},Tuple{}},DynamicPPL.Sampler{Turing.Inference.NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.DefaultContext},Float64},Float64,2},1}, ::Int64)@Other: 6\n    #1@Other: 10[inlined]\n    (::Main.workspace150.var\"#1#3\")(::Random._GLOBAL_RNG, ::DynamicPPL.Model{Main.workspace150.var\"#1#3\",(:data, :f, :u0, :n),(),(),Tuple{Array{Float64,1},typeof(Main.workspace115.simple_growth!),Array{Float64,1},Int64},Tuple{}}, ::DynamicPPL.ThreadSafeVarInfo{DynamicPPL.VarInfo{NamedTuple{(:σ, :α),Tuple{DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:σ,Tuple{}},Int64},Array{Distributions.InverseGamma{Float64},1},Array{DynamicPPL.VarName{:σ,Tuple{}},1},Array{ForwardDiff.Dual{ForwardDiff.Tag{Turing.Core.var\"#f#1\"{DynamicPPL.VarInfo{NamedTuple{(:σ, :α),Tuple{DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:σ,Tuple{}},Int64},Array{Distributions.InverseGamma{Float64},1},Array{DynamicPPL.VarName{:σ,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}},DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:α,Tuple{}},Int64},Array{Distributions.Truncated{Distributions.Normal{Float64},Distributions.Continuous,Float64},1},Array{DynamicPPL.VarName{:α,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}}}},Float64},DynamicPPL.Model{Main.workspace150.var\"#1#3\",(:data, :f, :u0, :n),(),(),Tuple{Array{Float64,1},typeof(Main.workspace115.simple_growth!),Array{Float64,1},Int64},Tuple{}},DynamicPPL.Sampler{Turing.Inference.NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.DefaultContext},Float64},Float64,2},1},Array{Set{DynamicPPL.Selector},1}},DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:α,Tuple{}},Int64},Array{Distributions.Truncated{Distributions.Normal{Float64},Distributions.Continuous,Float64},1},Array{DynamicPPL.VarName{:α,Tuple{}},1},Array{ForwardDiff.Dual{ForwardDiff.Tag{Turing.Core.var\"#f#1\"{DynamicPPL.VarInfo{NamedTuple{(:σ, :α),Tuple{DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:σ,Tuple{}},Int64},Array{Distributions.InverseGamma{Float64},1},Array{DynamicPPL.VarName{:σ,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}},DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:α,Tuple{}},Int64},Array{Distributions.Truncated{Distributions.Normal{Float64},Distributions.Continuous,Float64},1},Array{DynamicPPL.VarName{:α,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}}}},Float64},DynamicPPL.Model{Main.workspace150.var\"#1#3\",(:data, :f, :u0, :n),(),(),Tuple{Array{Float64,1},typeof(Main.workspace115.simple_growth!),Array{Float64,1},Int64},Tuple{}},DynamicPPL.Sampler{Turing.Inference.NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.DefaultContext},Float64},Float64,2},1},Array{Set{DynamicPPL.Selector},1}}}},ForwardDiff.Dual{ForwardDiff.Tag{Turing.Core.var\"#f#1\"{DynamicPPL.VarInfo{NamedTuple{(:σ, :α),Tuple{DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:σ,Tuple{}},Int64},Array{Distributions.InverseGamma{Float64},1},Array{DynamicPPL.VarName{:σ,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}},DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:α,Tuple{}},Int64},Array{Distributions.Truncated{Distributions.Normal{Float64},Distributions.Continuous,Float64},1},Array{DynamicPPL.VarName{:α,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}}}},Float64},DynamicPPL.Model{Main.workspace150.var\"#1#3\",(:data, :f, :u0, :n),(),(),Tuple{Array{Float64,1},typeof(Main.workspace115.simple_growth!),Array{Float64,1},Int64},Tuple{}},DynamicPPL.Sampler{Turing.Inference.NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.DefaultContext},Float64},Float64,2}},Array{Base.RefValue{ForwardDiff.Dual{ForwardDiff.Tag{Turing.Core.var\"#f#1\"{DynamicPPL.VarInfo{NamedTuple{(:σ, :α),Tuple{DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:σ,Tuple{}},Int64},Array{Distributions.InverseGamma{Float64},1},Array{DynamicPPL.VarName{:σ,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}},DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:α,Tuple{}},Int64},Array{Distributions.Truncated{Distributions.Normal{Float64},Distributions.Continuous,Float64},1},Array{DynamicPPL.VarName{:α,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}}}},Float64},DynamicPPL.Model{Main.workspace150.var\"#1#3\",(:data, :f, :u0, :n),(),(),Tuple{Array{Float64,1},typeof(Main.workspace115.simple_growth!),Array{Float64,1},Int64},Tuple{}},DynamicPPL.Sampler{Turing.Inference.NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.DefaultContext},Float64},Float64,2}},1}}, ::DynamicPPL.Sampler{Turing.Inference.NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}, ::DynamicPPL.DefaultContext, ::Array{Float64,1}, ::Function, ::Array{Float64,1}, ::Int64)@none:0```","user":"UDDSTBX19","ts":"1609447055.218000","team":"T68168MUP","edited":{"user":"UDDSTBX19","ts":"1609447459.000000"},"blocks":[{"type":"rich_text","block_id":"ZZ9K","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey folks. I was hoping someone could point me in the right direction with this error message. I am setting up a very simple vanilla discrete dynamical simulation (to proof of concept). So I define a simple growth model, generate some data, and then wrote a Turing model to see if I can recover the parameters used to generate the simulated data. The code is below. But I am getting an error when I run the `chain` function to execute the sample. The error is "},{"type":"text","text":"TypeError: in typeassert, expected Float64, got a value of type ForwardDiff.Dual{Nothing,Float64,2}","style":{"code":true}},{"type":"text","text":". I am not really sure how to debug these types of issues in Turing, so if anyone could tell me where to look, I would appreciate it. Here is the simple code that I wrote. I use the Turing Diffeq tutorial as a model, but removed the Diffeq solver with my own solver. I can add more to the stacktrace if it is helpful.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"\n# Create a simple discrete simulation\nfunction simple_growth!(du, u0, p)\n    r = p[1]\n    du[1] = r*u0[1]\n    du\nend\n\n# solve the dynamical system\nfunction solve_system(f,u0,p,n)\n    u = Vector{typeof(u0)}(undef,n)\n    du = similar(u0)\n    u[1] = u0\n    for i in 1:n-1\n        f(du,u[i],p)\n        u[i+1] = copy(du)\n    end\n    u\nend\n\n# simulate some data\ndata = solve_system(simple_growth!, [0.9], [2.0], 10)\ndata = [data[i][1] for i in 1:length(data)]\n\n# define the Turing model\nTuring.setadbackend(:forwarddiff)\n\n@model function fitlv(data, f, u0, n)\n    σ ~ InverseGamma(2, 3) # ~ is the tilde character\n    α ~ truncated(Normal(0.7,0.5),0.5,2.5)\n\n    p = [α]\n    #prob = remake(prob1, p=p)\n    predicted = solve_system(f, u0, p, n)\n\tpredicted = [predicted[i][1] for i in 1:length(predicted)]\n    for i = 1:length(predicted)\n        data[i] ~ Normal(predicted[i], σ)\n    end\nend\n\n# setup the Turing sampler\nu0 = [0.9]\nn = 10\nmodel = fitlv(data, simple_growth!, u0, n)\nchain = mapreduce(c -> sample(model, NUTS(.65),1000), chainscat, 1:3)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Here is just the first part of the stacktrace for the error.\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"chain\n\nTypeError: in typeassert, expected Float64, got a value of type ForwardDiff.Dual{Nothing,Float64,2}\n\n    setindex!(::Array{Float64,1}, ::ForwardDiff.Dual{ForwardDiff.Tag{Turing.Core.var\"#f#1\"{DynamicPPL.VarInfo{NamedTuple{(:σ, :α),Tuple{DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:σ,Tuple{}},Int64},Array{Distributions.InverseGamma{Float64},1},Array{DynamicPPL.VarName{:σ,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}},DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:α,Tuple{}},Int64},Array{Distributions.Truncated{Distributions.Normal{Float64},Distributions.Continuous,Float64},1},Array{DynamicPPL.VarName{:α,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}}}},Float64},DynamicPPL.Model{Main.workspace150.var\"#1#3\",(:data, :f, :u0, :n),(),(),Tuple{Array{Float64,1},typeof(Main.workspace115.simple_growth!),Array{Float64,1},Int64},Tuple{}},DynamicPPL.Sampler{Turing.Inference.NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.DefaultContext},Float64},Float64,2}, ::Int64)@array.jl:847\n    simple_growth!(::Array{Float64,1}, ::Array{Float64,1}, ::Array{ForwardDiff.Dual{ForwardDiff.Tag{Turing.Core.var\"#f#1\"{DynamicPPL.VarInfo{NamedTuple{(:σ, :α),Tuple{DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:σ,Tuple{}},Int64},Array{Distributions.InverseGamma{Float64},1},Array{DynamicPPL.VarName{:σ,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}},DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:α,Tuple{}},Int64},Array{Distributions.Truncated{Distributions.Normal{Float64},Distributions.Continuous,Float64},1},Array{DynamicPPL.VarName{:α,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}}}},Float64},DynamicPPL.Model{Main.workspace150.var\"#1#3\",(:data, :f, :u0, :n),(),(),Tuple{Array{Float64,1},typeof(Main.workspace115.simple_growth!),Array{Float64,1},Int64},Tuple{}},DynamicPPL.Sampler{Turing.Inference.NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.DefaultContext},Float64},Float64,2},1})@Other: 3\n    solve_system(::typeof(Main.workspace115.simple_growth!), ::Array{Float64,1}, ::Array{ForwardDiff.Dual{ForwardDiff.Tag{Turing.Core.var\"#f#1\"{DynamicPPL.VarInfo{NamedTuple{(:σ, :α),Tuple{DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:σ,Tuple{}},Int64},Array{Distributions.InverseGamma{Float64},1},Array{DynamicPPL.VarName{:σ,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}},DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:α,Tuple{}},Int64},Array{Distributions.Truncated{Distributions.Normal{Float64},Distributions.Continuous,Float64},1},Array{DynamicPPL.VarName{:α,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}}}},Float64},DynamicPPL.Model{Main.workspace150.var\"#1#3\",(:data, :f, :u0, :n),(),(),Tuple{Array{Float64,1},typeof(Main.workspace115.simple_growth!),Array{Float64,1},Int64},Tuple{}},DynamicPPL.Sampler{Turing.Inference.NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.DefaultContext},Float64},Float64,2},1}, ::Int64)@Other: 6\n    #1@Other: 10[inlined]\n    (::Main.workspace150.var\"#1#3\")(::Random._GLOBAL_RNG, ::DynamicPPL.Model{Main.workspace150.var\"#1#3\",(:data, :f, :u0, :n),(),(),Tuple{Array{Float64,1},typeof(Main.workspace115.simple_growth!),Array{Float64,1},Int64},Tuple{}}, ::DynamicPPL.ThreadSafeVarInfo{DynamicPPL.VarInfo{NamedTuple{(:σ, :α),Tuple{DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:σ,Tuple{}},Int64},Array{Distributions.InverseGamma{Float64},1},Array{DynamicPPL.VarName{:σ,Tuple{}},1},Array{ForwardDiff.Dual{ForwardDiff.Tag{Turing.Core.var\"#f#1\"{DynamicPPL.VarInfo{NamedTuple{(:σ, :α),Tuple{DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:σ,Tuple{}},Int64},Array{Distributions.InverseGamma{Float64},1},Array{DynamicPPL.VarName{:σ,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}},DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:α,Tuple{}},Int64},Array{Distributions.Truncated{Distributions.Normal{Float64},Distributions.Continuous,Float64},1},Array{DynamicPPL.VarName{:α,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}}}},Float64},DynamicPPL.Model{Main.workspace150.var\"#1#3\",(:data, :f, :u0, :n),(),(),Tuple{Array{Float64,1},typeof(Main.workspace115.simple_growth!),Array{Float64,1},Int64},Tuple{}},DynamicPPL.Sampler{Turing.Inference.NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.DefaultContext},Float64},Float64,2},1},Array{Set{DynamicPPL.Selector},1}},DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:α,Tuple{}},Int64},Array{Distributions.Truncated{Distributions.Normal{Float64},Distributions.Continuous,Float64},1},Array{DynamicPPL.VarName{:α,Tuple{}},1},Array{ForwardDiff.Dual{ForwardDiff.Tag{Turing.Core.var\"#f#1\"{DynamicPPL.VarInfo{NamedTuple{(:σ, :α),Tuple{DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:σ,Tuple{}},Int64},Array{Distributions.InverseGamma{Float64},1},Array{DynamicPPL.VarName{:σ,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}},DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:α,Tuple{}},Int64},Array{Distributions.Truncated{Distributions.Normal{Float64},Distributions.Continuous,Float64},1},Array{DynamicPPL.VarName{:α,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}}}},Float64},DynamicPPL.Model{Main.workspace150.var\"#1#3\",(:data, :f, :u0, :n),(),(),Tuple{Array{Float64,1},typeof(Main.workspace115.simple_growth!),Array{Float64,1},Int64},Tuple{}},DynamicPPL.Sampler{Turing.Inference.NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.DefaultContext},Float64},Float64,2},1},Array{Set{DynamicPPL.Selector},1}}}},ForwardDiff.Dual{ForwardDiff.Tag{Turing.Core.var\"#f#1\"{DynamicPPL.VarInfo{NamedTuple{(:σ, :α),Tuple{DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:σ,Tuple{}},Int64},Array{Distributions.InverseGamma{Float64},1},Array{DynamicPPL.VarName{:σ,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}},DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:α,Tuple{}},Int64},Array{Distributions.Truncated{Distributions.Normal{Float64},Distributions.Continuous,Float64},1},Array{DynamicPPL.VarName{:α,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}}}},Float64},DynamicPPL.Model{Main.workspace150.var\"#1#3\",(:data, :f, :u0, :n),(),(),Tuple{Array{Float64,1},typeof(Main.workspace115.simple_growth!),Array{Float64,1},Int64},Tuple{}},DynamicPPL.Sampler{Turing.Inference.NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.DefaultContext},Float64},Float64,2}},Array{Base.RefValue{ForwardDiff.Dual{ForwardDiff.Tag{Turing.Core.var\"#f#1\"{DynamicPPL.VarInfo{NamedTuple{(:σ, :α),Tuple{DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:σ,Tuple{}},Int64},Array{Distributions.InverseGamma{Float64},1},Array{DynamicPPL.VarName{:σ,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}},DynamicPPL.Metadata{Dict{DynamicPPL.VarName{:α,Tuple{}},Int64},Array{Distributions.Truncated{Distributions.Normal{Float64},Distributions.Continuous,Float64},1},Array{DynamicPPL.VarName{:α,Tuple{}},1},Array{Float64,1},Array{Set{DynamicPPL.Selector},1}}}},Float64},DynamicPPL.Model{Main.workspace150.var\"#1#3\",(:data, :f, :u0, :n),(),(),Tuple{Array{Float64,1},typeof(Main.workspace115.simple_growth!),Array{Float64,1},Int64},Tuple{}},DynamicPPL.Sampler{Turing.Inference.NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.DefaultContext},Float64},Float64,2}},1}}, ::DynamicPPL.Sampler{Turing.Inference.NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}, ::DynamicPPL.DefaultContext, ::Array{Float64,1}, ::Function, ::Array{Float64,1}, ::Int64)@none:0"}]}]}],"thread_ts":"1609447055.218000","reply_count":10,"reply_users_count":2,"latest_reply":"1609448470.220100","reply_users":["U69BL50BF","UDDSTBX19"],"subscribed":false},{"client_msg_id":"50d55569-e8ca-428a-bc5d-65d21d4fc23c","type":"message","text":"I can correctly estimate the parameters with NUTS, but ADVI gives very wrong answers. How can I fix it?\n```\nimport Random\nusing DataFrames\n\nusing StatsBase: StatsBase, mad, median, percentile, sample, shuffle\nusing Distributions\nusing Turing\nusing Chain\n\nRandom.seed!(1)\n##\n\n\n\ndf, parameters = let\n    β = (\n        β_0 = 7,\n        β_1 = 0.05,\n        β_2 = 0.10,\n        β_3 = 0.15,\n        β_4 = 0.20,\n    )\n    σ = 2\n    parameters = (β..., σ=σ)\n    parameters_df = DataFrame(\n        parameter=collect(keys(parameters)),\n        value=collect(values(parameters)),\n    )\n\n    N = 100_000\n    X = DataFrame(\n        x_0=fill(1, N),\n        x_1=rand([1,2], N),\n        x_2=rand(1:1:10, N),\n        x_3=rand([0,1], N),\n        x_4=rand(0:1:10, N),\n    )\n    df = transform(X)\n    μ = Matrix(df) * collect(β)\n    ϵ = rand(Normal(0, σ), N)\n    y = μ .+ ϵ\n\n    (df = transform(\n        X,\n        [] =&gt; (() -&gt; μ) =&gt; :μ,\n        [] =&gt; (() -&gt; ϵ) =&gt; :ϵ,\n        [] =&gt; (() -&gt; y) =&gt; :y,\n        ),\n     parameters = parameters_df,    )\nend\n\n##\n@model function linear_outcome_model(X, y)\n    β_0 ~ Normal(mean(y), 2std(y))\n    β_1 ~ Normal(0, .5)\n    β_2 ~ Normal(0, .5)\n    β_3 ~ Normal(0, .5)\n    β_4 ~ Normal(0, .5)\n\n    μ = (\n            β_0 .* X.x_0\n            .+ β_1 .* X.x_1\n            .+ β_2 .* X.x_2\n            .+ β_3 .* X.x_3\n            .+ β_4 .* X.x_4\n\n    )\n    σ ~ truncated(Normal(0, 2std(y)), 0, Inf)\n    y ~ MvNormal(μ, σ)\nend\n\n\nfields = [    :x_0,    :x_1,    :x_2,    :x_3,    :x_4,]\n\n\nmodel = @chain df begin\n    linear_outcome_model(_[:, fields], _.y) \nend\n\n\nsamples_nuts =  sample(model, NUTS(0.65), 3000)\n\nsamples_vi = let\n    estimate_vi = vi(model, ADVI())\n    samplesarray_vi = rand(estimate_vi, 1000)\n    _, sym2range = bijector(model, Val(true))\n    parameters = keys(sym2range)\n\n    rearranged = DataFrame(Dict(\n        param =&gt; vec(samplesarray_vi[sym2range[param]..., :])\n        for param in parameters\n    ))\n    insertcols!(rearranged, 1, :index =&gt; 1:1:nrow(rearranged))\n    DataFrames.stack(rearranged, Not(:index), variable_name=:parameter)\nend\n\n\nsummary_nuts = @chain samples_nuts begin\n    DataFrame(summarize(_, mean, std)) \n    rename(_, :parameters =&gt; :parameter) \n    transform(:parameter =&gt; ByRow(Symbol) =&gt; :parameter)\n    innerjoin(_, parameters, on=:parameter)\nend\n\n\nsummary_vi = @chain samples_vi begin\n    groupby(:parameter)\n    combine(:value =&gt; mean =&gt; :mean, :value =&gt; std =&gt; :std)\n    transform(:parameter =&gt; ByRow(Symbol) =&gt; :parameter)\n    innerjoin(parameters, on=:parameter)\nend```","user":"U01ARRMLM7E","ts":"1609457022.221800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WEsN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I can correctly estimate the parameters with NUTS, but ADVI gives very wrong answers. How can I fix it?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"\nimport Random\nusing DataFrames\n\nusing StatsBase: StatsBase, mad, median, percentile, sample, shuffle\nusing Distributions\nusing Turing\nusing Chain\n\nRandom.seed!(1)\n##\n\n\n\ndf, parameters = let\n    β = (\n        β_0 = 7,\n        β_1 = 0.05,\n        β_2 = 0.10,\n        β_3 = 0.15,\n        β_4 = 0.20,\n    )\n    σ = 2\n    parameters = (β..., σ=σ)\n    parameters_df = DataFrame(\n        parameter=collect(keys(parameters)),\n        value=collect(values(parameters)),\n    )\n\n    N = 100_000\n    X = DataFrame(\n        x_0=fill(1, N),\n        x_1=rand([1,2], N),\n        x_2=rand(1:1:10, N),\n        x_3=rand([0,1], N),\n        x_4=rand(0:1:10, N),\n    )\n    df = transform(X)\n    μ = Matrix(df) * collect(β)\n    ϵ = rand(Normal(0, σ), N)\n    y = μ .+ ϵ\n\n    (df = transform(\n        X,\n        [] => (() -> μ) => :μ,\n        [] => (() -> ϵ) => :ϵ,\n        [] => (() -> y) => :y,\n        ),\n     parameters = parameters_df,    )\nend\n\n##\n@model function linear_outcome_model(X, y)\n    β_0 ~ Normal(mean(y), 2std(y))\n    β_1 ~ Normal(0, .5)\n    β_2 ~ Normal(0, .5)\n    β_3 ~ Normal(0, .5)\n    β_4 ~ Normal(0, .5)\n\n    μ = (\n            β_0 .* X.x_0\n            .+ β_1 .* X.x_1\n            .+ β_2 .* X.x_2\n            .+ β_3 .* X.x_3\n            .+ β_4 .* X.x_4\n\n    )\n    σ ~ truncated(Normal(0, 2std(y)), 0, Inf)\n    y ~ MvNormal(μ, σ)\nend\n\n\nfields = [    :x_0,    :x_1,    :x_2,    :x_3,    :x_4,]\n\n\nmodel = @chain df begin\n    linear_outcome_model(_[:, fields], _.y) \nend\n\n\nsamples_nuts =  sample(model, NUTS(0.65), 3000)\n\nsamples_vi = let\n    estimate_vi = vi(model, ADVI())\n    samplesarray_vi = rand(estimate_vi, 1000)\n    _, sym2range = bijector(model, Val(true))\n    parameters = keys(sym2range)\n\n    rearranged = DataFrame(Dict(\n        param => vec(samplesarray_vi[sym2range[param]..., :])\n        for param in parameters\n    ))\n    insertcols!(rearranged, 1, :index => 1:1:nrow(rearranged))\n    DataFrames.stack(rearranged, Not(:index), variable_name=:parameter)\nend\n\n\nsummary_nuts = @chain samples_nuts begin\n    DataFrame(summarize(_, mean, std)) \n    rename(_, :parameters => :parameter) \n    transform(:parameter => ByRow(Symbol) => :parameter)\n    innerjoin(_, parameters, on=:parameter)\nend\n\n\nsummary_vi = @chain samples_vi begin\n    groupby(:parameter)\n    combine(:value => mean => :mean, :value => std => :std)\n    transform(:parameter => ByRow(Symbol) => :parameter)\n    innerjoin(parameters, on=:parameter)\nend"}]}]}],"thread_ts":"1609457022.221800","reply_count":1,"reply_users_count":1,"latest_reply":"1609457035.221900","reply_users":["U01ARRMLM7E"],"subscribed":false},{"client_msg_id":"d94de493-fe5c-4ede-8bb0-415f80ec8012","type":"message","text":"Hi! I was messing around with Bijectors.jl and was really interested in the normalizing flows section and was wondering how to fit these flows. Below is some code that I am trying to use as an example. I keep running into a mutating arrays issue like `ERROR: LoadError: Mutating arrays is not supported`\n\nAny help would be appreciated!\n```@model function gen()\n    x ~ Exponential(0.5)\n    y ~ Normal(0, x)\nend\n\ns = sample(gen(), NUTS(), 500)\ntrain_data = hcat(s[:x].data, s[:y].data)' |&gt; Array\n\nb = PlanarLayer(2) ∘ LeakyReLU{Float64, 1}(0.05) ∘ RadialLayer(2) ∘ LeakyReLU{Float64, 1}(0.05) ∘ PlanarLayer(2)\nd = MvNormal(zeros(2), ones(2))\ntb = transformed(d, b);\nloss(tb :: Bijectors.TransformedDistribution, x :: Matrix{Float64}) = begin\n    loss = 0\n    for i ∈ 1:size(x, 2)\n        loss += -logpdf(tb, x[:, i])\n    end\n    return loss\nend\n\nfunction bnf_train!(tb, x, opt, ps, epochs)\n    @showprogress for i ∈ 1:epochs\n        train_loss, back = Zygote.pullback(() -&gt; loss(tb, x), ps)\n        gs = back(one(train_loss))\n        Flux.update!(opt, ps, gs)\n    end\nend\n\nbnf_train!(tb, train_data, ADAM(), Zygote.Params(Flux.params(b)), 10)```\n","user":"U01HWKB076G","ts":"1609460100.224400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"AwB8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi! I was messing around with Bijectors.jl and was really interested in the normalizing flows section and was wondering how to fit these flows. Below is some code that I am trying to use as an example. I keep running into a mutating arrays issue like "},{"type":"text","text":"ERROR: LoadError: Mutating arrays is not supported","style":{"code":true}},{"type":"text","text":"\n\nAny help would be appreciated!\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@model function gen()\n    x ~ Exponential(0.5)\n    y ~ Normal(0, x)\nend\n\ns = sample(gen(), NUTS(), 500)\ntrain_data = hcat(s[:x].data, s[:y].data)' |> Array\n\nb = PlanarLayer(2) ∘ LeakyReLU{Float64, 1}(0.05) ∘ RadialLayer(2) ∘ LeakyReLU{Float64, 1}(0.05) ∘ PlanarLayer(2)\nd = MvNormal(zeros(2), ones(2))\ntb = transformed(d, b);\nloss(tb :: Bijectors.TransformedDistribution, x :: Matrix{Float64}) = begin\n    loss = 0\n    for i ∈ 1:size(x, 2)\n        loss += -logpdf(tb, x[:, i])\n    end\n    return loss\nend\n\nfunction bnf_train!(tb, x, opt, ps, epochs)\n    @showprogress for i ∈ 1:epochs\n        train_loss, back = Zygote.pullback(() -> loss(tb, x), ps)\n        gs = back(one(train_loss))\n        Flux.update!(opt, ps, gs)\n    end\nend\n\nbnf_train!(tb, train_data, ADAM(), Zygote.Params(Flux.params(b)), 10)"}]},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"2E6D4F3F-7864-4831-9FC0-AF63DC882FC9","type":"message","text":"Happy new year everyone!","user":"UC0SY9JFP","ts":"1609460266.224900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3fs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Happy new year everyone!"}]}]}]},{"client_msg_id":"e7d1e22e-5be3-4e6d-8a30-e9dd38641d84","type":"message","text":"Happy New Year.","user":"UDDSTBX19","ts":"1609471604.225400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DmP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Happy New Year."}]}]}]},{"client_msg_id":"905C64D7-F88F-4885-A11D-A7D88F3F274E","type":"message","text":"Hi Turing developers. I think an compliment is in order. I’m now halfway moving the StatisticalRethinkingJulia models over to a static site and most models run in about 10 seconds (!!). Yes, “using Turing” takes ages, but I’m still impressed with the speed! Thanks for all the work","user":"U01BTNDCUBX","ts":"1609626254.230300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3v3+h","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi Turing developers. I think an compliment is in order. I’m now halfway moving the StatisticalRethinkingJulia models over to a static site and most models run in about 10 seconds (!!). Yes, “using Turing” takes ages, but I’m still impressed with the speed! Thanks for all the work"}]}]}]},{"client_msg_id":"6617e50b-f8ba-44dc-8470-c759b0bc3934","type":"message","text":"I'm doing linear regression with 5 data columns and 100k rows, and it takes about 15 minutes with ADVI(10, 10_000). Is that normal?","user":"U01ARRMLM7E","ts":"1609634954.233000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"CtCIh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm doing linear regression with 5 data columns and 100k rows, and it takes about 15 minutes with ADVI(10, 10_000). Is that normal?"}]}]}],"thread_ts":"1609634954.233000","reply_count":2,"reply_users_count":2,"latest_reply":"1609635314.233700","reply_users":["U9JNHB83X","U01ARRMLM7E"],"subscribed":false},{"client_msg_id":"6b9a5fee-be1e-4195-bac0-79612050ac3b","type":"message","text":"In a Turing model is there a distinction made between the \"prior\" and the \"likelihood\"?","user":"U7QLM6E2E","ts":"1609770503.245200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3mfW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In a Turing model is there a distinction made between the \"prior\" and the \"likelihood\"?"}]}]}]},{"client_msg_id":"c9971acf-8af8-4425-b538-5ceae7c4ec0d","type":"message","text":"Hi, I've been trying out the new SGHMC release but cannot get it to work. When I call `chain = sample(model, SGHMC(1000, 0.02, 0.5), progress=true)`  I get the MethodError: `ERROR: MethodError: no method matching sample(::DynamicPPL.Model{var\"#7#8\",(:data,),(),(),Tuple{Array{Float64,2}},Tuple{}}, ::SGHMC{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.UnitEuclideanMetric}; progress=true)`\n`Closest candidates are:`\n  `sample(::AbstractMCMC.AbstractModel, ::Turing.Inference.InferenceAlgorithm, ::Integer; kwargs...) at C:\\Users\\karen\\.julia\\packages\\Turing\\493By\\src\\inference\\Inference.jl:148`\n  `sample(::AbstractMCMC.AbstractModel, ::Turing.Inference.InferenceAlgorithm, ::AbstractMCMC.AbstractMCMCParallel, ::Integer, ::Integer; kwargs...) at C:\\Users\\karen\\.julia\\packages\\Turing\\493By\\src\\inference\\Inference.jl:203`\n  `sample(::AbstractMCMC.AbstractModel, ::AbstractMCMC.AbstractSampler, ::AbstractMCMC.AbstractMCMCParallel, ::Integer, ::Integer; kwargs...) at C:\\Users\\karen\\.julia\\packages\\AbstractMCMC\\iOkTf\\src\\sample.jl:22`","user":"U016Q7EHVPT","ts":"1609814655.250000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Zjl5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I've been trying out the new SGHMC release but cannot get it to work. When I call "},{"type":"text","text":"chain = sample(model, SGHMC(1000, 0.02, 0.5), progress=true) ","style":{"code":true}},{"type":"text","text":" I get the MethodError: "},{"type":"text","text":"ERROR: MethodError: no method matching sample(::DynamicPPL.Model{var\"#7#8\",(:data,),(),(),Tuple{Array{Float64,2}},Tuple{}}, ::SGHMC{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.UnitEuclideanMetric}; progress=true)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"Closest candidates are:","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"  sample(::AbstractMCMC.AbstractModel, ::Turing.Inference.InferenceAlgorithm, ::Integer; kwargs...) at C:\\Users\\karen\\.julia\\packages\\Turing\\493By\\src\\inference\\Inference.jl:148","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"  sample(::AbstractMCMC.AbstractModel, ::Turing.Inference.InferenceAlgorithm, ::AbstractMCMC.AbstractMCMCParallel, ::Integer, ::Integer; kwargs...) at C:\\Users\\karen\\.julia\\packages\\Turing\\493By\\src\\inference\\Inference.jl:203","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"  sample(::AbstractMCMC.AbstractModel, ::AbstractMCMC.AbstractSampler, ::AbstractMCMC.AbstractMCMCParallel, ::Integer, ::Integer; kwargs...) at C:\\Users\\karen\\.julia\\packages\\AbstractMCMC\\iOkTf\\src\\sample.jl:22","style":{"code":true}}]}]}]},{"client_msg_id":"df218e77-1ef2-4007-ab2a-86656b13b329","type":"message","text":"Alternatively, calling `chain = sample(model, SGHMC(1000, 0.02, 0.5), 1000,progress=true)` results in `ERROR: MethodError: no method matching getmetricT(::SGHMC{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.UnitEuclideanMetric})`\n`Closest candidates are:`\n  `getmetricT(::HMC{var\"#s141\",var\"#s132\",metricT} where var\"#s132\" where var\"#s141\") where metricT at C:\\Users\\karen\\.julia\\packages\\TuringBy\\src\\inference\\hmc.jl:363`\n  `getmetricT(::HMCDA{var\"#s141\",var\"#s132\",metricT} where var\"#s132\" where var\"#s141\") where metricT at C:\\Users\\karen\\.julia\\packages\\Turi93By\\src\\inference\\hmc.jl:363`\n  `getmetricT(::NUTS{var\"#s141\",var\"#s132\",metricT} where var\"#s132\" where var\"#s141\") where metricT at C:\\Users\\karen\\.julia\\packages\\Turin3By\\src\\inference\\hmc.jl:363`","user":"U016Q7EHVPT","ts":"1609814710.250700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Sqfl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Alternatively, calling "},{"type":"text","text":"chain = sample(model, SGHMC(1000, 0.02, 0.5), 1000,progress=true)","style":{"code":true}},{"type":"text","text":" results in "},{"type":"text","text":"ERROR: MethodError: no method matching getmetricT(::SGHMC{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.UnitEuclideanMetric})","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"Closest candidates are:","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"  getmetricT(::HMC{var\"#s141\",var\"#s132\",metricT} where var\"#s132\" where var\"#s141\") where metricT at C:\\Users\\karen\\.julia\\packages\\TuringBy\\src\\inference\\hmc.jl:363","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"  getmetricT(::HMCDA{var\"#s141\",var\"#s132\",metricT} where var\"#s132\" where var\"#s141\") where metricT at C:\\Users\\karen\\.julia\\packages\\Turi93By\\src\\inference\\hmc.jl:363","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"  getmetricT(::NUTS{var\"#s141\",var\"#s132\",metricT} where var\"#s132\" where var\"#s141\") where metricT at C:\\Users\\karen\\.julia\\packages\\Turin3By\\src\\inference\\hmc.jl:363","style":{"code":true}}]}]}]},{"client_msg_id":"21e61a15-7679-49b8-990c-32425b47ffdc","type":"message","text":"I'm confused because I don't think I'm missing anything in the syntax. Could you point me to how to fix this? :slightly_smiling_face: Thanks in advance","user":"U016Q7EHVPT","ts":"1609814753.251300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"aZ+MY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm confused because I don't think I'm missing anything in the syntax. Could you point me to how to fix this? "},{"type":"emoji","name":"slightly_smiling_face"},{"type":"text","text":" Thanks in advance"}]}]}]},{"client_msg_id":"ffaa2824-a71c-4167-b928-6034868c2474","type":"message","text":"Is there a way to turn off Warning messages during sampling?","user":"U6CFMFM2R","ts":"1609877297.252500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Z=1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to turn off Warning messages during sampling?"}]}]}]},{"client_msg_id":"48f993db-3b91-4e61-a831-983951c4e64b","type":"message","text":"I am getting this error message:\n`ERROR: TypeError: in typeassert, expected Float64, got a value of type ForwardDiff.Dual{Nothing,Float64,5}`\nwhen trying to run\n`chn = sample(model(a, b, c, d), HMC(0.1, 5), Nstep)`\nIt seems to be coming from\n`function weights!(a, b, w)`\n    `p1, p2 = a`\n    `for i in 1:20`\n        `w[i] = 0.5*(1 + ((b[i] - p1)/p2))`\n    `end`\n`end`\nthat gets called from inside the model. Could this be happening because something is nondifferentiable in this function? If so, which part is it exactly?","user":"UPK2KJ95Y","ts":"1609883760.258700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"NDH2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am getting this error message:\n"},{"type":"text","text":"ERROR: TypeError: in typeassert, expected Float64, got a value of type ForwardDiff.Dual{Nothing,Float64,5}","style":{"code":true}},{"type":"text","text":"\nwhen trying to run\n"},{"type":"text","text":"chn = sample(model(a, b, c, d), HMC(0.1, 5), Nstep)","style":{"code":true}},{"type":"text","text":"\nIt seems to be coming from\n"},{"type":"text","text":"function weights!(a, b, w)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    p1, p2 = a","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    for i in 1:20","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"        w[i] = 0.5*(1 + ((b[i] - p1)/p2))","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    end","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"end","style":{"code":true}},{"type":"text","text":"\nthat gets called from inside the model. Could this be happening because something is nondifferentiable in this function? If so, which part is it exactly?"}]}]}],"thread_ts":"1609883760.258700","reply_count":1,"reply_users_count":1,"latest_reply":"1609884224.258800","reply_users":["UH08DT0JU"],"subscribed":false},{"client_msg_id":"3bece1e4-479b-4bc1-b865-de80141eb35a","type":"message","text":"This is a simpler version of my question above. Why does the code below fail to sample with HMC\n```using Turing\n\nB = [5141181, 3795214, 2739591, 1782803, 1262850,  822732,  551119, 353990,  220015,  132850,   76394,   42184,   21126,    9761, 4016,    1318,     409,      83,      11,       3]\n\n\nfunction MM_theory(a)\n    w = zeros(Float64, 10)\n    for i in 1:10\n        w[i] = B[i] - a\n    end\n    return w\nend\n\nfunction run_mcmc(Nstep)\n\n    @model function MM_model(MM)\n        a ~ Uniform()\n        MM ~ MvNormal(MM_theory(a), ones(10))\n    end\n\n    chn = sample(MM_model(ones(10)), HMC(0.1, 5), Nstep)\n\nend```\nwhen I run `run_mcmc(1000)` ?","user":"UPK2KJ95Y","ts":"1609897872.263000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Poerw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This is a simpler version of my question above. Why does the code below fail to sample with HMC\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Turing\n\nB = [5141181, 3795214, 2739591, 1782803, 1262850,  822732,  551119, 353990,  220015,  132850,   76394,   42184,   21126,    9761, 4016,    1318,     409,      83,      11,       3]\n\n\nfunction MM_theory(a)\n    w = zeros(Float64, 10)\n    for i in 1:10\n        w[i] = B[i] - a\n    end\n    return w\nend\n\nfunction run_mcmc(Nstep)\n\n    @model function MM_model(MM)\n        a ~ Uniform()\n        MM ~ MvNormal(MM_theory(a), ones(10))\n    end\n\n    chn = sample(MM_model(ones(10)), HMC(0.1, 5), Nstep)\n\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"when I run "},{"type":"text","text":"run_mcmc(1000)","style":{"code":true}},{"type":"text","text":" ?"}]}]}],"thread_ts":"1609897872.263000","reply_count":3,"reply_users_count":2,"latest_reply":"1609899021.263500","reply_users":["UHDQQ4GN6","UPK2KJ95Y"],"subscribed":false},{"type":"message","text":"I have a more general MCMC question if I may. If my chains look like the ones below, with them getting stuck in certain points for long time, what is this indicative off and how can it be fixed? These chains were produced by Metropolis-Hastings sampler in Turing.","files":[{"id":"F01J8RJKE4U","created":1609899829,"timestamp":1609899829,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"UPK2KJ95Y","editable":false,"size":180796,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01J8RJKE4U/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01J8RJKE4U/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01J8RJKE4U-ebc06b2523/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01J8RJKE4U-ebc06b2523/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01J8RJKE4U-ebc06b2523/image_360.png","thumb_360_w":291,"thumb_360_h":360,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01J8RJKE4U-ebc06b2523/image_480.png","thumb_480_w":388,"thumb_480_h":480,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01J8RJKE4U-ebc06b2523/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01J8RJKE4U-ebc06b2523/image_720.png","thumb_720_w":582,"thumb_720_h":720,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01J8RJKE4U-ebc06b2523/image_800.png","thumb_800_w":647,"thumb_800_h":800,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01J8RJKE4U-ebc06b2523/image_960.png","thumb_960_w":777,"thumb_960_h":960,"original_w":779,"original_h":963,"thumb_tiny":"AwAwACbRDEdQaAw9CKQvjtSK/PTrTsK4/OfajI70gbIzilPJpDFzRRRQA3K0ArnqKXA7UgX2piFBB6UEUAAUHOaQxaKKKAGhcUD8ePekw3rRhs/epiHYxRkg/WkGe/NLmkMXmikzS0Af/9k=","permalink":"https://julialang.slack.com/files/UPK2KJ95Y/F01J8RJKE4U/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01J8RJKE4U-e3f93d48be","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"kUfj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a more general MCMC question if I may. If my chains look like the ones below, with them getting stuck in certain points for long time, what is this indicative off and how can it be fixed? These chains were produced by Metropolis-Hastings sampler in Turing."}]}]}],"user":"UPK2KJ95Y","display_as_bot":false,"ts":"1609899835.265200"},{"client_msg_id":"a15d9039-d8c9-4b17-9732-6edcfc08851e","type":"message","text":"Are there any examples in Turing where model conditions are defined as implicit functions? I am planning to do this, and was going to use NLsolve with a custom adjoint via ChainRules. But I was wondering, if I define the problem using ModelingToolkit (this example: <https://mtk.sciml.ai/dev/tutorials/nonlinear/> ) will ModelingToolkit generate the derivatives (and pass them on to Turing/Zygote/ForwardDiff) for me?","user":"U017YGFQTE3","ts":"1609927138.272300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+C2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Are there any examples in Turing where model conditions are defined as implicit functions? I am planning to do this, and was going to use NLsolve with a custom adjoint via ChainRules. But I was wondering, if I define the problem using ModelingToolkit (this example: "},{"type":"link","url":"https://mtk.sciml.ai/dev/tutorials/nonlinear/"},{"type":"text","text":" ) will ModelingToolkit generate the derivatives (and pass them on to Turing/Zygote/ForwardDiff) for me?"}]}]}]},{"client_msg_id":"1b5fb50e-5614-420f-b432-ca570a8bdf6e","type":"message","text":"it won't pass them onto Turing","user":"U69BL50BF","ts":"1609942474.273100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HEKH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it won't pass them onto Turing"}]}]}]},{"client_msg_id":"cb92db3f-ec0b-4483-ac81-5f11585f6357","type":"message","text":"but we could add the overloads to the generated function","user":"U69BL50BF","ts":"1609942482.273400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"aDSr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"but we could add the overloads to the generated function"}]}]}]},{"type":"message","subtype":"thread_broadcast","text":"Ok so once I have my model fixed (I passed  my observations), I am not sure how to use `logprior`/`loglikelihood`, the docs tell me to pass a `VarInfo` variable but I don't know how to create one","user":"U7QLM6E2E","ts":"1609974066.274900","thread_ts":"1609770503.245200","root":{"client_msg_id":"6b9a5fee-be1e-4195-bac0-79612050ac3b","type":"message","text":"In a Turing model is there a distinction made between the \"prior\" and the \"likelihood\"?","user":"U7QLM6E2E","ts":"1609770503.245200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3mfW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In a Turing model is there a distinction made between the \"prior\" and the \"likelihood\"?"}]}]}],"thread_ts":"1609770503.245200","reply_count":5,"reply_users_count":3,"latest_reply":"1609974066.274900","reply_users":["U01H36BUDJB","U7QLM6E2E","U8T9JUA5R"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"ihyj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ok so once I have my model fixed (I passed  my observations), I am not sure how to use "},{"type":"text","text":"logprior","style":{"code":true}},{"type":"text","text":"/"},{"type":"text","text":"loglikelihood","style":{"code":true}},{"type":"text","text":", the docs tell me to pass a "},{"type":"text","text":"VarInfo","style":{"code":true}},{"type":"text","text":" variable but I don't know how to create one"}]}]}],"client_msg_id":"ae1b4dad-01b1-42e6-8a1c-fa3e11b73712"},{"client_msg_id":"e2f36751-83a8-4a83-8422-c762a955ba44","type":"message","text":"Stan and pymc3 warn about divergent NUTS chains. Does Turing have anything like that?","user":"U01ARRMLM7E","ts":"1610008439.278500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"35vt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Stan and pymc3 warn about divergent NUTS chains. Does Turing have anything like that?"}]}]}]},{"client_msg_id":"a9081b4b-a079-4c75-a0cc-ce1f44108914","type":"message","text":"You should see numerical error warnings when sampling if divergent transitions are encountered. Samples returned when a divergence was encountered are marked with `numerical_error` as `true` in the internal variables in the returned `Chains` object.","user":"UHDQQ4GN6","ts":"1610008771.280300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jE4M","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You should see numerical error warnings when sampling if divergent transitions are encountered. Samples returned when a divergence was encountered are marked with "},{"type":"text","text":"numerical_error","style":{"code":true}},{"type":"text","text":" as "},{"type":"text","text":"true","style":{"code":true}},{"type":"text","text":" in the internal variables in the returned "},{"type":"text","text":"Chains","style":{"code":true}},{"type":"text","text":" object."}]}]}],"reactions":[{"name":"correct_answer","users":["UC0SY9JFP"],"count":1}]},{"client_msg_id":"73471a79-c061-4b81-b67a-e9770769f276","type":"message","text":"```┌ Warning: The current proposal will be rejected due to numerical error(s).\n│   isfinite.((θ, r, ℓπ, ℓκ)) = (true, false, false, false)```\nLike this?","user":"U01ARRMLM7E","ts":"1610009492.280900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8MG","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"┌ Warning: The current proposal will be rejected due to numerical error(s).\n│   isfinite.((θ, r, ℓπ, ℓκ)) = (true, false, false, false)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Like this?"}]}]}]},{"client_msg_id":"5f53902f-216e-48f4-8e27-33ba24a1296f","type":"message","text":"I'm getting that warning but also\n```julia&gt; sum(samples_nuts[:numerical_error])\n0.0```","user":"U01ARRMLM7E","ts":"1610010053.281800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pm+yn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm getting that warning but also\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> sum(samples_nuts[:numerical_error])\n0.0"}]}]}]},{"client_msg_id":"b7fd2a3c-53df-48f4-b2be-f0b17a0eeb00","type":"message","text":"The warning will be raised both during warm-up and while sampling. Divergent transitions during warm-up is common and normal. By default warm-up samples aren't returned, though IIRC there is a keyword argument for `sample` that will let you return them.","user":"UHDQQ4GN6","ts":"1610010650.284000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SqiTE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The warning will be raised both during warm-up and while sampling. Divergent transitions during warm-up is common and normal. By default warm-up samples aren't returned, though IIRC there is a keyword argument for "},{"type":"text","text":"sample","style":{"code":true}},{"type":"text","text":" that will let you return them."}]}]}]},{"client_msg_id":"143bcc68-03e2-4b27-9949-61f776fe14e3","type":"message","text":"How can I tell that warm-up samples aren't returned? <https://turing.ml/dev/docs/library/advancedhmc/#StatsBase.sample-Union{Tuple{T},%20Tuple{Union{Random.AbstractRNG,%20AbstractArray{var%22#s195%22,1}%20where%20var%22#s195%22%3C:Random.AbstractRNG},Hamiltonian,AdvancedHMC.AbstractProposal,T,Int64},%20Tuple{Union{Random.AbstractRNG,%20AbstractArray{var%22#s196%22,1}%20where%20var%22#s196%22%3C:Random.AbstractRNG},Hamiltonian,AdvancedHMC.AbstractProposal,T,Int64,AdvancedHMC.Adaptation.AbstractAdaptor},%20Tuple{Union{Random.AbstractRNG,%20AbstractArray{var%22#s197%22,1}%20where%20var%22#s197%22%3C:Random.AbstractRNG},Hamiltonian,AdvancedHMC.AbstractProposal,T,Int64,AdvancedHMC.Adaptation.AbstractAdaptor,Int64}}%20where%20T%3C:(Union{AbstractArray{var%22#s194%22,1},%20AbstractArray{var%22#s194%22,2}}%20where%20var%22#s194%22%3C:AbstractFloat)|https://turing.ml/dev/docs/library/advancedhmc/#StatsBase.sample-Union{Tuple{T},%20Tuple{Union{R[…]22,2}}%20where%20var%22#s194%22%3C:AbstractFloat)> has a default `drop_warmup=False`.","user":"U01ARRMLM7E","ts":"1610010746.284700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lRpDf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I tell that warm-up samples aren't returned? "},{"type":"link","url":"https://turing.ml/dev/docs/library/advancedhmc/#StatsBase.sample-Union{Tuple{T},%20Tuple{Union{Random.AbstractRNG,%20AbstractArray{var%22#s195%22,1}%20where%20var%22#s195%22%3C:Random.AbstractRNG},Hamiltonian,AdvancedHMC.AbstractProposal,T,Int64},%20Tuple{Union{Random.AbstractRNG,%20AbstractArray{var%22#s196%22,1}%20where%20var%22#s196%22%3C:Random.AbstractRNG},Hamiltonian,AdvancedHMC.AbstractProposal,T,Int64,AdvancedHMC.Adaptation.AbstractAdaptor},%20Tuple{Union{Random.AbstractRNG,%20AbstractArray{var%22#s197%22,1}%20where%20var%22#s197%22%3C:Random.AbstractRNG},Hamiltonian,AdvancedHMC.AbstractProposal,T,Int64,AdvancedHMC.Adaptation.AbstractAdaptor,Int64}}%20where%20T%3C:(Union{AbstractArray{var%22#s194%22,1},%20AbstractArray{var%22#s194%22,2}}%20where%20var%22#s194%22%3C:AbstractFloat)","text":"https://turing.ml/dev/docs/library/advancedhmc/#StatsBase.sample-Union{Tuple{T},%20Tuple{Union{R[…]22,2}}%20where%20var%22#s194%22%3C:AbstractFloat)"},{"type":"text","text":" has a default "},{"type":"text","text":"drop_warmup=False","style":{"code":true}},{"type":"text","text":"."}]}]}]},{"client_msg_id":"8db8f720-c3b4-4d14-bdfd-07b05ebba428","type":"message","text":"That's it. Set `drop_warmup=true` for warm-up to be returned. You should get more samples, and there's an internal variable designating warm-up. I believe it's `is_adapt=true` .","user":"UHDQQ4GN6","ts":"1610010863.285800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"aHXY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That's it. Set "},{"type":"text","text":"drop_warmup=true","style":{"code":true}},{"type":"text","text":" for warm-up to be returned. You should get more samples, and there's an internal variable designating warm-up. I believe it's "},{"type":"text","text":"is_adapt=true","style":{"code":true}},{"type":"text","text":" ."}]}]}]},{"client_msg_id":"3484ed83-03a3-4698-bf54-ef28f23dce3e","type":"message","text":"(with the caveat that warm-up samples are only useful for diagnosing problems and shouldn't be used for anything else)","user":"UHDQQ4GN6","ts":"1610010914.286300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2jJC=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(with the caveat that warm-up samples are only useful for diagnosing problems and shouldn't be used for anything else)"}]}]}]},{"client_msg_id":"44e69ef6-8e66-45f0-ab23-a917f0423310","type":"message","text":"I would have thought that \"dropping\" the samples means _not_ returning them. So `drop_warmup=false` would mean not dropping them, i.e. returning them. But it's the other way around?","user":"U01ARRMLM7E","ts":"1610011022.287400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xyBP+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I would have thought that \"dropping\" the samples means "},{"type":"text","text":"not","style":{"italic":true}},{"type":"text","text":" returning them. So "},{"type":"text","text":"drop_warmup=false","style":{"code":true}},{"type":"text","text":" would mean not dropping them, i.e. returning them. But it's the other way around?"}]}]}]},{"client_msg_id":"5b4b4a4b-930a-4174-8d87-1e4d6406f982","type":"message","text":"test/sampler.jl seems to support dropping as not returning\n```    samples, stats = sample(h, τ, θ_init, n_samples, adaptor, n_adapts; verbose=false, progress=false, drop_warmup=true)\n    @test length(samples) == n_samples - n_adapts\n    @test length(stats) == n_samples - n_adapts\n    samples, stats = sample(h, τ, θ_init, n_samples, adaptor, n_adapts; verbose=false, progress=false, drop_warmup=false)\n    @test length(samples) == n_samples\n    @test length(stats) == n_samples```\n","user":"U01ARRMLM7E","ts":"1610011204.288900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8m1y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"test/sampler.jl seems to support dropping as not returning\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"    samples, stats = sample(h, τ, θ_init, n_samples, adaptor, n_adapts; verbose=false, progress=false, drop_warmup=true)\n    @test length(samples) == n_samples - n_adapts\n    @test length(stats) == n_samples - n_adapts\n    samples, stats = sample(h, τ, θ_init, n_samples, adaptor, n_adapts; verbose=false, progress=false, drop_warmup=false)\n    @test length(samples) == n_samples\n    @test length(stats) == n_samples"}]},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"b5d1ea4a-5e88-4de5-88fe-4fa85d138d44","type":"message","text":"Oh, whoops, no, you're right. If you didn't set it, then it should return the warm-up as well. Perhaps you're encountering divergences during choosing of the initial step size, which is technically before adaptation happens (and returned by no PPL I'm aware of). It would be helpful if the error message recorded a step number for easier diagnosis. Perhaps open an issue on AdvancedHMC.jl?","user":"UHDQQ4GN6","ts":"1610011289.290500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OzZj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oh, whoops, no, you're right. If you didn't set it, then it should return the warm-up as well. Perhaps you're encountering divergences during choosing of the initial step size, which is technically before adaptation happens (and returned by no PPL I'm aware of). It would be helpful if the error message recorded a step number for easier diagnosis. Perhaps open an issue on AdvancedHMC.jl?"}]}]}]},{"client_msg_id":"f99fcc14-6a3a-4427-958b-d923a09ed0ea","type":"message","text":"(and can you confirm without setting `drop_warmup` that you're getting warm-up samples with `is_adapt=true`?)","user":"UHDQQ4GN6","ts":"1610011327.291100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GSXE7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(and can you confirm without setting "},{"type":"text","text":"drop_warmup","style":{"code":true}},{"type":"text","text":" that you're getting warm-up samples with "},{"type":"text","text":"is_adapt=true","style":{"code":true}},{"type":"text","text":"?)"}]}]}]},{"client_msg_id":"6aae7108-e2aa-4c40-a9f6-4faa2e01bc05","type":"message","text":"```julia&gt; samples_nuts.name_map.internals\n12-element Array{Symbol,1}:\n :acceptance_rate\n :hamiltonian_energy\n :hamiltonian_energy_error\n :is_accept\n :log_density\n :lp\n :max_hamiltonian_energy_error\n :n_steps\n :nom_step_size\n :numerical_error\n :step_size\n :tree_depth```\nI don't see `is_adapt`","user":"U01ARRMLM7E","ts":"1610011590.292000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eqt","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> samples_nuts.name_map.internals\n12-element Array{Symbol,1}:\n :acceptance_rate\n :hamiltonian_energy\n :hamiltonian_energy_error\n :is_accept\n :log_density\n :lp\n :max_hamiltonian_energy_error\n :n_steps\n :nom_step_size\n :numerical_error\n :step_size\n :tree_depth"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I don't see "},{"type":"text","text":"is_adapt","style":{"code":true}}]}]}]},{"client_msg_id":"02368383-9af0-418d-9d83-29288314eb17","type":"message","text":"Ah it's new in 4b9c12c1da4c0996c7ef09de1b6d65bdc752c451","user":"U01ARRMLM7E","ts":"1610011684.292600","team":"T68168MUP","edited":{"user":"U01ARRMLM7E","ts":"1610011708.000000"},"blocks":[{"type":"rich_text","block_id":"bmj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah it's new in 4b9c12c1da4c0996c7ef09de1b6d65bdc752c451"}]}]}]},{"client_msg_id":"3f8591bf-3b05-47c3-b66b-c921898cfb1f","type":"message","text":"Oh yeah, no that's over a year old. I guess the change to Turing to return that parameter was never made? I'll look into it.","user":"UHDQQ4GN6","ts":"1610011787.293500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"BkG5K","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oh yeah, no that's over a year old. I guess the change to Turing to return that parameter was never made? I'll look into it."}]}]}]},{"client_msg_id":"5f8e61d5-4d09-4d9c-b9fb-5ad34be79837","type":"message","text":"oh you're right, I didn't notice the year :slightly_smiling_face:","user":"U01ARRMLM7E","ts":"1610011841.294100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"riAJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"oh you're right, I didn't notice the year "},{"type":"emoji","name":"slightly_smiling_face"}]}]}]},{"client_msg_id":"23fc47f3-55d8-45ac-8e18-17cc223b3413","type":"message","text":"Hmm, not seeing it. Turing devs, I recall there being a place in Turing where names of statistics that might be returned were hardcoded. I can't find it now. Could you direct me to that?","user":"UHDQQ4GN6","ts":"1610012058.295100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ba6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hmm, not seeing it. Turing devs, I recall there being a place in Turing where names of statistics that might be returned were hardcoded. I can't find it now. Could you direct me to that?"}]}]}]},{"client_msg_id":"2d4302ff-7390-49e5-aded-edd0ad393005","type":"message","text":"Inference.jl","user":"U01ARRMLM7E","ts":"1610012110.295300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"78Sk5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Inference.jl"}]}]}]},{"client_msg_id":"90325a48-1787-42ae-99fb-a2f26805af3d","type":"message","text":"```\nconst TURING_INTERNAL_VARS = (internals = [\n    \"elapsed\",\n    \"eval_num\",\n    \"lf_eps\",\n    \"lp\",\n    \"weight\",\n    \"le\",\n    \"acceptance_rate\",\n    \"hamiltonian_energy\",\n    \"hamiltonian_energy_error\",\n    \"max_hamiltonian_energy_error\",\n    \"is_accept\",\n    \"log_density\",\n    \"n_steps\",\n    \"numerical_error\",\n    \"step_size\",\n    \"nom_step_size\",\n    \"tree_depth\",\n    \"is_adapt\"\n],)```","user":"U01ARRMLM7E","ts":"1610012148.295600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jB61","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"\nconst TURING_INTERNAL_VARS = (internals = [\n    \"elapsed\",\n    \"eval_num\",\n    \"lf_eps\",\n    \"lp\",\n    \"weight\",\n    \"le\",\n    \"acceptance_rate\",\n    \"hamiltonian_energy\",\n    \"hamiltonian_energy_error\",\n    \"max_hamiltonian_energy_error\",\n    \"is_accept\",\n    \"log_density\",\n    \"n_steps\",\n    \"numerical_error\",\n    \"step_size\",\n    \"nom_step_size\",\n    \"tree_depth\",\n    \"is_adapt\"\n],)"}]}]}]},{"client_msg_id":"e64b136c-0320-459c-8f8d-520ebba10d70","type":"message","text":"<https://github.com/TuringLang/Turing.jl/pull/1297/files#diff-9ba6d3bd298527f8219cd1964dff231902c5c81dce8afdcbc5dc80e948e3e01cR141>","user":"U01ARRMLM7E","ts":"1610012406.296000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6UX","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/TuringLang/Turing.jl/pull/1297/files#diff-9ba6d3bd298527f8219cd1964dff231902c5c81dce8afdcbc5dc80e948e3e01cR141"}]}]}]},{"client_msg_id":"5dca5116-bb99-4b6f-8ed7-2c7d53535a54","type":"message","text":"but I'm on 0.15.8","user":"U01ARRMLM7E","ts":"1610012454.296300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"c1OE1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"but I'm on 0.15.8"}]}]}]},{"client_msg_id":"eda9f978-9df4-4921-b141-8d7a3339257b","type":"message","text":"(or at least I am now. It's possible I upgraded after making these samples and forgot about it...)","user":"U01ARRMLM7E","ts":"1610012865.297300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"i/P","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(or at least I am now. It's possible I upgraded after making these samples and forgot about it...)"}]}]}]},{"client_msg_id":"41f16bef-c230-4883-a8a5-93ac8f615ffd","type":"message","text":"Turing does use different keyword arguments and a different function signature than AdvancedHMC, it's a bit unfortunate that the AdvancedHMC documentation is placed so prominently on the Turing webpage. The correct keyword arguments are listed in <https://github.com/TuringLang/Turing.jl/blob/339877206cb2f084cbd30eb6c36081d2f81565f9/src/inference/hmc.jl#L118-120|https://github.com/TuringLang/Turing.jl/blob/339877206cb2f084cbd30eb6c36081d2f81565f9/src/inference/hmc.jl#L118-120> If you want to keep the adaptation phase, you have to add `discard_adapt=false` (the default value is `true`)","user":"U8T9JUA5R","ts":"1610016025.303000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nib","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Turing does use different keyword arguments and a different function signature than AdvancedHMC, it's a bit unfortunate that the AdvancedHMC documentation is placed so prominently on the Turing webpage. The correct keyword arguments are listed in "},{"type":"link","url":"https://github.com/TuringLang/Turing.jl/blob/339877206cb2f084cbd30eb6c36081d2f81565f9/src/inference/hmc.jl#L118-120","text":"https://github.com/TuringLang/Turing.jl/blob/339877206cb2f084cbd30eb6c36081d2f81565f9/src/inference/hmc.jl#L118-120"},{"type":"text","text":" If you want to keep the adaptation phase, you have to add "},{"type":"text","text":"discard_adapt=false","style":{"code":true}},{"type":"text","text":" (the default value is "},{"type":"text","text":"true","style":{"code":true}},{"type":"text","text":")"}]}]}]},{"type":"message","text":"```samples_nuts = sample(model, NUTS(1000, 0.65), MCMCThreads(), 1000, 10)\njulia&gt; sum(samples_nuts[:numerical_error])\n0.0```\nAll rhat are close to 1.0. What's going on in these weird traces?","files":[{"id":"F01J4UEJ6F7","created":1610052542,"timestamp":1610052542,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01ARRMLM7E","editable":false,"size":876591,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01J4UEJ6F7/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01J4UEJ6F7/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01J4UEJ6F7-4acfc296d4/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01J4UEJ6F7-4acfc296d4/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01J4UEJ6F7-4acfc296d4/image_360.png","thumb_360_w":360,"thumb_360_h":349,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01J4UEJ6F7-4acfc296d4/image_480.png","thumb_480_w":480,"thumb_480_h":466,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01J4UEJ6F7-4acfc296d4/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01J4UEJ6F7-4acfc296d4/image_720.png","thumb_720_w":720,"thumb_720_h":699,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01J4UEJ6F7-4acfc296d4/image_800.png","thumb_800_w":800,"thumb_800_h":776,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01J4UEJ6F7-4acfc296d4/image_960.png","thumb_960_w":960,"thumb_960_h":931,"original_w":1009,"original_h":979,"thumb_tiny":"AwAuADDRZtoyeaj88f3TUhUMMHpTPJT3pO/QBPPX+6aPPH9007yU96PJT3pe8ITzl/umhZQzAbetL5KULEqsCM0agPoyPUUhAbgjIpPLT+6KoY7cPUUbh6j86b5af3RR5af3RS1AduHqKMj1FN8tP7gpQig5C80ANlJCnHXNQl37k/lVrFJj6UNXAr739T+VIHf1NWcUDB7UuXzEVt7+p/KnRsxcAk4z6VPijpRbzGf/2Q==","permalink":"https://julialang.slack.com/files/U01ARRMLM7E/F01J4UEJ6F7/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01J4UEJ6F7-6fbb2f21f9","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"4qMjH","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"samples_nuts = sample(model, NUTS(1000, 0.65), MCMCThreads(), 1000, 10)\njulia> sum(samples_nuts[:numerical_error])\n0.0"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"All rhat are close to 1.0. What's going on in these weird traces?"}]}]}],"user":"U01ARRMLM7E","display_as_bot":false,"ts":"1610052600.305000","edited":{"user":"U01ARRMLM7E","ts":"1610052658.000000"},"thread_ts":"1610052600.305000","reply_count":4,"reply_users_count":2,"latest_reply":"1610053509.306000","reply_users":["UHDQQ4GN6","U01ARRMLM7E"],"subscribed":false},{"client_msg_id":"8f7d3613-db92-4a31-9df2-8b23be41f410","type":"message","text":"Hi, I am new to Turing and also Bayesian in general (looking into it because compared to PyMC3 or Stan it seems easier ) and so I am trying the simple coinflip example in the tutorial. I was wondering, how does one extract the posterior (which may have no analytical form in general) and then use it as the prior in the next model with different data? Basically I mean running the coinflip example again with the prior as the posterior from the previous run. (I know in this particular example there is an analytical way, but I want the general method)","user":"U01EF0QVAB0","ts":"1610067192.308700","team":"T68168MUP","edited":{"user":"U01EF0QVAB0","ts":"1610067222.000000"},"blocks":[{"type":"rich_text","block_id":"2TE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I am new to Turing and also Bayesian in general (looking into it because compared to PyMC3 or Stan it seems easier ) and so I am trying the simple coinflip example in the tutorial. I was wondering, how does one extract the posterior (which may have no analytical form in general) and then use it as the prior in the next model with different data? Basically I mean running the coinflip example again with the prior as the posterior from the previous run. (I know in this particular example there is an analytical way, but I want the general method)"}]}]}],"thread_ts":"1610067192.308700","reply_count":1,"reply_users_count":1,"latest_reply":"1610067764.308900","reply_users":["UN97XTLCV"],"subscribed":false},{"type":"message","text":"By the way are there any tutorials on Bayesian (Generalized) Linear Mixed Models in Turing? I don't see any here <https://turing.ml/dev/tutorials/>, seems like it would be nice to have that considering GLMMs are a big thing people use Bayesian for. In my case, I need to do an LMM with a random slope/intercept but in 1D but at each x for each ID I need to estimate the variance. Basically I have a bunch of continuous x with replicates (designed experiment) for each ID and need to get a *good estimate for the variance at each x:ID combo*, with limited data like possibly only 4 or less replicates for each x:ID. Looks something like this, but with way more Trial IDs (100+) and multiple observations for each Trial at each x. Data is also heteroscedastic. Then in the end, I also need to average the x:ID variance estimates at each x and then do another average of that across the whole range. Its a multilevel error analysis madness","files":[{"id":"F01HU92SJ6T","created":1610091166,"timestamp":1610091166,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01EF0QVAB0","editable":false,"size":83313,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01HU92SJ6T/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01HU92SJ6T/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01HU92SJ6T-d6baace478/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01HU92SJ6T-d6baace478/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01HU92SJ6T-d6baace478/image_360.png","thumb_360_w":360,"thumb_360_h":254,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01HU92SJ6T-d6baace478/image_480.png","thumb_480_w":480,"thumb_480_h":338,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01HU92SJ6T-d6baace478/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01HU92SJ6T-d6baace478/image_720.png","thumb_720_w":720,"thumb_720_h":507,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01HU92SJ6T-d6baace478/image_800.png","thumb_800_w":800,"thumb_800_h":563,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01HU92SJ6T-d6baace478/image_960.png","thumb_960_w":960,"thumb_960_h":676,"thumb_1024":"https://files.slack.com/files-tmb/T68168MUP-F01HU92SJ6T-d6baace478/image_1024.png","thumb_1024_w":1024,"thumb_1024_h":721,"original_w":1346,"original_h":948,"thumb_tiny":"AwAhADDRbrSUrc0lABRRSUALTl6UynDtQAN1pKVgSaTaaAEpaMGjBoAKco4puDTl6UALRRRQAHpRQelFABRRRQB//9k=","permalink":"https://julialang.slack.com/files/U01EF0QVAB0/F01HU92SJ6T/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01HU92SJ6T-fa4f532d0b","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"8CcX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"By the way are there any tutorials on Bayesian (Generalized) Linear Mixed Models in Turing? I don't see any here "},{"type":"link","url":"https://turing.ml/dev/tutorials/"},{"type":"text","text":", seems like it would be nice to have that considering GLMMs are a big thing people use Bayesian for. In my case, I need to do an LMM with a random slope/intercept but in 1D but at each x for each ID I need to estimate the variance. Basically I have a bunch of continuous x with replicates (designed experiment) for each ID and need to get a "},{"type":"text","text":"good estimate for the variance at each x:ID combo","style":{"bold":true}},{"type":"text","text":", with limited data like possibly only 4 or less replicates for each x:ID. Looks something like this, but with way more Trial IDs (100+) and multiple observations for each Trial at each x. Data is also heteroscedastic. Then in the end, I also need to average the x:ID variance estimates at each x and then do another average of that across the whole range. Its a multilevel error analysis madness"}]}]}],"user":"U01EF0QVAB0","display_as_bot":false,"ts":"1610091236.315900","edited":{"user":"U01EF0QVAB0","ts":"1610091357.000000"},"thread_ts":"1610091236.315900","reply_count":1,"reply_users_count":1,"latest_reply":"1610091579.316500","reply_users":["U01EF0QVAB0"],"subscribed":false},{"client_msg_id":"a58de80e-9101-4a4f-ad36-3ad02e4848db","type":"message","text":"I am confused on the usage of the init_theta kwarg on sample().\n```trueodeprob = ODEProblem(lv, u0, tspan, p);\node_data = Array(solve(trueodeprob, Tsit5(), saveat = tsteps));\ny_train = ode_data[:, 1:35];\n\ndudt2 = FastChain(FastDense(2, 50, tanh), FastDense(50, 2));\nprob_node = NeuralODE(dudt2, (0., 4.5), Tsit5(), saveat = tsteps); #neural ode\ntrain_prob = NeuralODE(dudt2, (0., 3.5), Tsit5(), saveat = tsteps[1:35]);\n\n\nfunction predict_ode(p)  # predict with given params\n    Array(train_prob(u0, p))\nend\n\nfunction loss(p)  # loss function to minimize\n    sum(abs2, y_train .- predict_ode(p))\nend\n\n### Fit neural ode to the data\n@model function fit_node(data)\n    σ ~ InverseGamma(2, 3)\n    p ~ MvNormal(prob_node.p, 1.0)\n    # Calculate predictions for the inputs given the params.\n    predicted = predict_ode(p)\n    # observe each prediction.\n    for i = 1:length(predicted)\n        data[i] ~ Normal(predicted[i], σ)\n    end\nend\n\n####### Perform inference\nfunction perform_inference(lr, alpha, samplesize, pmin)\n    model = fit_node(ode_data) # fit model to average simulated data\n    alg = SGHMC(learning_rate=lr, momentum_decay=alpha)\n    chain = sample(model, alg, samplesize, init_theta=pmin, verbose=true, progress=true);\n    return chain\nend\n\nfunction map_loss(chain)\n    params = length(namesingroup(chain, \"p\"))\n    losses = loss.([chain.value.data[i,1:params,1] for i in 1:size(chain.value.data,1)])\n    return losses\nend```\n","user":"U016Q7EHVPT","ts":"1610163029.325000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"W22","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am confused on the usage of the init_theta kwarg on sample().\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"trueodeprob = ODEProblem(lv, u0, tspan, p);\node_data = Array(solve(trueodeprob, Tsit5(), saveat = tsteps));\ny_train = ode_data[:, 1:35];\n\ndudt2 = FastChain(FastDense(2, 50, tanh), FastDense(50, 2));\nprob_node = NeuralODE(dudt2, (0., 4.5), Tsit5(), saveat = tsteps); #neural ode\ntrain_prob = NeuralODE(dudt2, (0., 3.5), Tsit5(), saveat = tsteps[1:35]);\n\n\nfunction predict_ode(p)  # predict with given params\n    Array(train_prob(u0, p))\nend\n\nfunction loss(p)  # loss function to minimize\n    sum(abs2, y_train .- predict_ode(p))\nend\n\n### Fit neural ode to the data\n@model function fit_node(data)\n    σ ~ InverseGamma(2, 3)\n    p ~ MvNormal(prob_node.p, 1.0)\n    # Calculate predictions for the inputs given the params.\n    predicted = predict_ode(p)\n    # observe each prediction.\n    for i = 1:length(predicted)\n        data[i] ~ Normal(predicted[i], σ)\n    end\nend\n\n####### Perform inference\nfunction perform_inference(lr, alpha, samplesize, pmin)\n    model = fit_node(ode_data) # fit model to average simulated data\n    alg = SGHMC(learning_rate=lr, momentum_decay=alpha)\n    chain = sample(model, alg, samplesize, init_theta=pmin, verbose=true, progress=true);\n    return chain\nend\n\nfunction map_loss(chain)\n    params = length(namesingroup(chain, \"p\"))\n    losses = loss.([chain.value.data[i,1:params,1] for i in 1:size(chain.value.data,1)])\n    return losses\nend"}]},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"ca9e1a83-fd5a-48ab-9296-a9b2ed1d35bc","type":"message","text":"Given this setup, I would call:","user":"U016Q7EHVPT","ts":"1610163075.325700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"aRit","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Given this setup, I would call:"}]}]}]},{"client_msg_id":"17f0c50f-41f8-40cc-82f1-b7044a8b9498","type":"message","text":"```chain1 = perform_inference(unit, 0.1, samples, pmin);     losses = map_loss(chain1)```\n","user":"U016Q7EHVPT","ts":"1610163093.326000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kAiwY","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"chain1 = perform_inference(unit, 0.1, samples, pmin);     losses = map_loss(chain1)"}]},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"66160412-7a28-4646-bc5d-d146ec225ed9","type":"message","text":"But for the same values of unit, samples, and pmin, calling the same above lines twice leads to two different initial values of losses.","user":"U016Q7EHVPT","ts":"1610163162.327400","team":"T68168MUP","edited":{"user":"U016Q7EHVPT","ts":"1610163260.000000"},"blocks":[{"type":"rich_text","block_id":"w7YUC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But for the same values of unit, samples, and pmin, calling the same above lines twice leads to two different initial values of losses."}]}]}]},{"client_msg_id":"d575a733-23e0-4b8e-9004-59e0f2269de4","type":"message","text":"shouldn't each chain that starts with the same init_theta value have the same initial loss function value? NOt sure what im missing. Thanks in advance.","user":"U016Q7EHVPT","ts":"1610163240.328800","team":"T68168MUP","edited":{"user":"U016Q7EHVPT","ts":"1610163346.000000"},"blocks":[{"type":"rich_text","block_id":"pGQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"shouldn't each chain that starts with the same init_theta value have the same initial loss function value? NOt sure what im missing. Thanks in advance."}]}]}]},{"type":"message","subtype":"thread_broadcast","text":"I've got an example working in the gist below. Any suggestions are very welcome.\n<https://gist.github.com/alfredjmduncan/099e1493118489be6b632fdfd8213fd3>","user":"U017YGFQTE3","ts":"1610212749.329800","thread_ts":"1609927138.272300","root":{"client_msg_id":"a15d9039-d8c9-4b17-9732-6edcfc08851e","type":"message","text":"Are there any examples in Turing where model conditions are defined as implicit functions? I am planning to do this, and was going to use NLsolve with a custom adjoint via ChainRules. But I was wondering, if I define the problem using ModelingToolkit (this example: <https://mtk.sciml.ai/dev/tutorials/nonlinear/> ) will ModelingToolkit generate the derivatives (and pass them on to Turing/Zygote/ForwardDiff) for me?","user":"U017YGFQTE3","ts":"1609927138.272300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+C2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Are there any examples in Turing where model conditions are defined as implicit functions? I am planning to do this, and was going to use NLsolve with a custom adjoint via ChainRules. But I was wondering, if I define the problem using ModelingToolkit (this example: "},{"type":"link","url":"https://mtk.sciml.ai/dev/tutorials/nonlinear/"},{"type":"text","text":" ) will ModelingToolkit generate the derivatives (and pass them on to Turing/Zygote/ForwardDiff) for me?"}]}]}],"thread_ts":"1609927138.272300","reply_count":1,"reply_users_count":1,"latest_reply":"1610212749.329800","reply_users":["U017YGFQTE3"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"kwv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I've got an example working in the gist below. Any suggestions are very welcome.\n"},{"type":"link","url":"https://gist.github.com/alfredjmduncan/099e1493118489be6b632fdfd8213fd3"}]}]}],"client_msg_id":"71a9fb07-5dcf-41ad-8a0f-e443d9606e7d"},{"client_msg_id":"2cf737e8-128b-4b85-8f7f-71d72e449f0e","type":"message","text":"Is there a way to specify the quantiles that are computed/shown in the output?","user":"U6QF223TN","ts":"1610637120.000500","team":"T68168MUP","edited":{"user":"U6QF223TN","ts":"1610637144.000000"},"blocks":[{"type":"rich_text","block_id":"=6M","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to specify the quantiles that are computed/shown in the output?"}]}]}]},{"client_msg_id":"d9c428d6-2b3d-4f49-b714-c1d3cd65f8ef","type":"message","text":"No, for any fine-tuned statistical analysis you have to call the corresponding statistical functions. Such as `quantile` (<https://github.com/TuringLang/MCMCChains.jl/blob/29c24adc2c298c4415c680c36e4cb77016c6e4f3/src/stats.jl#L193>) in this case.","user":"U8T9JUA5R","ts":"1610637903.001800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"S6Nc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"No, for any fine-tuned statistical analysis you have to call the corresponding statistical functions. Such as "},{"type":"text","text":"quantile","style":{"code":true}},{"type":"text","text":" ("},{"type":"link","url":"https://github.com/TuringLang/MCMCChains.jl/blob/29c24adc2c298c4415c680c36e4cb77016c6e4f3/src/stats.jl#L193"},{"type":"text","text":") in this case."}]}]}]},{"client_msg_id":"6c28908c-7228-4ec2-a125-a97ffae0b751","type":"message","text":"Okay, thanks :+1::skin-tone-4:","user":"U6QF223TN","ts":"1610638091.002100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7UPH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Okay, thanks "},{"type":"emoji","name":"+1","skin_tone":4}]}]}]},{"client_msg_id":"69336a8d-3c04-45d8-8714-ee7bf1e6ac36","type":"message","text":"Short answer here is `describe(chain; q=[0.25, 0.5, 0.75])` for the full summary output or replace `describe with` quantile for just the quantile table.","user":"U9JNHB83X","ts":"1610645298.006300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Pz10","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Short answer here is "},{"type":"text","text":"describe(chain; q=[0.25, 0.5, 0.75])","style":{"code":true}},{"type":"text","text":" for the full summary output or replace "},{"type":"text","text":"describe with ","style":{"code":true}},{"type":"text","text":"quantile for just the quantile table."}]}]}],"thread_ts":"1610645298.006300","reply_count":1,"reply_users_count":1,"latest_reply":"1610647633.006400","reply_users":["U6QF223TN"],"subscribed":false},{"client_msg_id":"ddd04314-131a-475f-b2dd-fb18d1f23875","type":"message","text":"I noticed that more and more questions about Turing are asked on Slack rather than Discourse. A few days ago there was an interesting <https://discourse.julialang.org/t/ive-heard-a-number-of-people-say-julia-is-not-mainstream-because-it-doesnt-have-many-stackoverflow-questions-answered-but-they-dont-know-about-discourse/53168|discussion> about this practice reducing Julia's exposure because Slack discussions are not archived and searchable on Google (see the second post) .  This, of course, could be a problem for Turing by extension. This makes me wonder whether certain questions should be addressed in Discourse rather than Slack so that the solutions are searchable and archived, which could also reduce duplication.","user":"UH08DT0JU","ts":"1610706976.012200","team":"T68168MUP","edited":{"user":"UH08DT0JU","ts":"1610706985.000000"},"blocks":[{"type":"rich_text","block_id":"dZC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I noticed that more and more questions about Turing are asked on Slack rather than Discourse. A few days ago there was an interesting "},{"type":"link","url":"https://discourse.julialang.org/t/ive-heard-a-number-of-people-say-julia-is-not-mainstream-because-it-doesnt-have-many-stackoverflow-questions-answered-but-they-dont-know-about-discourse/53168","text":"discussion"},{"type":"text","text":" about this practice reducing Julia's exposure because Slack discussions are not archived and searchable on Google (see the second post) .  This, of course, could be a problem for Turing by extension. This makes me wonder whether certain questions should be addressed in Discourse rather than Slack so that the solutions are searchable and archived, which could also reduce duplication."}]}]}],"thread_ts":"1610706976.012200","reply_count":5,"reply_users_count":3,"latest_reply":"1610707546.016000","reply_users":["UC0SY9JFP","UH08DT0JU","U8T9JUA5R"],"subscribed":false},{"client_msg_id":"a5bc4233-e1ff-40d5-9864-e2d71fa43754","type":"message","text":"I've just installed the Julia 1.6 beta, and can't precompile Turing. Is there a lot of work to do to enable that? Is there an issue on GH (or other resource) that holds info on this?","user":"U01K8QN8RK6","ts":"1611014985.022900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eDv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I've just installed the Julia 1.6 beta, and can't precompile Turing. Is there a lot of work to do to enable that? Is there an issue on GH (or other resource) that holds info on this?"}]}]}],"thread_ts":"1611014985.022900","reply_count":1,"reply_users_count":1,"latest_reply":"1611046852.023000","reply_users":["U8T9JUA5R"],"subscribed":false},{"client_msg_id":"ce5ed491-0f47-48c4-8ca5-efebcbcd4244","type":"message","text":"I have some code where for some functions I've provided custom  `frule`s via ChainRules. I've tested these frules using Zygote's `forwarddiff` function, and they work well. But, if I use Zygote in Turing, it defaults to reverse mode, and I don't have the relevant rrules defined. Is there a way to use Turing with Zygote in forward mode?","user":"U017YGFQTE3","ts":"1611074037.027100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DFF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have some code where for some functions I've provided custom  "},{"type":"text","text":"frule","style":{"code":true}},{"type":"text","text":"s via ChainRules. I've tested these frules using Zygote's "},{"type":"text","text":"forwarddiff","style":{"code":true}},{"type":"text","text":" function, and they work well. But, if I use Zygote in Turing, it defaults to reverse mode, and I don't have the relevant rrules defined. Is there a way to use Turing with Zygote in forward mode?"}]}]}],"thread_ts":"1611074037.027100","reply_count":5,"reply_users_count":2,"latest_reply":"1611081162.028000","reply_users":["UHDQQ4GN6","U017YGFQTE3"],"subscribed":false},{"client_msg_id":"43c03480-f6c8-4992-ae2d-c93e3bda4487","type":"message","text":"Has anyone done Turing ports of any of the Stan code at <https://github.com/avehtari/BDA_R_demos>  and <https://github.com/avehtari/BDA_R_demos/tree/master/demos_rstan>?","user":"US8V7JSKB","ts":"1611126448.028600","team":"T68168MUP","edited":{"user":"US8V7JSKB","ts":"1611126453.000000"},"blocks":[{"type":"rich_text","block_id":"VnBr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Has anyone done Turing ports of any of the Stan code at "},{"type":"link","url":"https://github.com/avehtari/BDA_R_demos"},{"type":"text","text":"  and "},{"type":"link","url":"https://github.com/avehtari/BDA_R_demos/tree/master/demos_rstan"},{"type":"text","text":"?"}]}]}],"thread_ts":"1611126448.028600","reply_count":2,"reply_users_count":2,"latest_reply":"1611162548.032300","reply_users":["U01HSP1E1NW","US8V7JSKB"],"subscribed":false},{"client_msg_id":"51877594-209d-46f2-bb10-9549335b93f7","type":"message","text":"Does `AdvancedHMC` has some interface to just perform one step? Or should I just `sample` , 1 sample at a time?","user":"U7QLM6E2E","ts":"1611161036.029800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"AsC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does "},{"type":"text","text":"AdvancedHMC","style":{"code":true}},{"type":"text","text":" has some interface to just perform one step? Or should I just "},{"type":"text","text":"sample","style":{"code":true}},{"type":"text","text":" , 1 sample at a time?"}]}]}],"thread_ts":"1611161036.029800","reply_count":11,"reply_users_count":3,"latest_reply":"1611162019.032100","reply_users":["U018NKR9X70","U7QLM6E2E","U8T9JUA5R"],"subscribed":false},{"client_msg_id":"f4af6c6e-ced9-44d3-9fb6-e7335ccf6d65","type":"message","text":"Is there a way I can specify the starting location when sampling?","user":"U010ZCVMTT6","ts":"1611710158.001400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UG5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way I can specify the starting location when sampling?"}]}]}]},{"client_msg_id":"4ef269b2-bdc1-4c8f-b69b-8ddb8ac859ec","type":"message","text":"I tried init_theta and it doesn't seem to do anything","user":"U010ZCVMTT6","ts":"1611710174.001800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Hwh1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I tried init_theta and it doesn't seem to do anything"}]}]}],"thread_ts":"1611710174.001800","reply_count":3,"reply_users_count":2,"latest_reply":"1611764581.003200","reply_users":["U8T9JUA5R","U010ZCVMTT6"],"subscribed":false},{"client_msg_id":"76ec95aa-1d8a-49e1-a96d-194938a9dccc","type":"message","text":"Is there a shortcut in Julia for doing horseshoe priors, or do i have to specify manually?","user":"UMS7H0ASG","ts":"1611753378.002400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"EyUz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a shortcut in Julia for doing horseshoe priors, or do i have to specify manually?"}]}]}]},{"client_msg_id":"25d32f7d-905a-4dc4-8afd-71aef844ce6c","type":"message","text":"I know they are sometimes called various things so not sure if I am just searching for the wrong thing in Distributions docs etc.","user":"UMS7H0ASG","ts":"1611753424.002900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"i0cP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I know they are sometimes called various things so not sure if I am just searching for the wrong thing in Distributions docs etc."}]}]}]},{"client_msg_id":"db0b43dc-75d9-4f27-8e2c-730d4ac7d1f6","type":"message","text":"What's the problem here?\n","user":"U01ARRMLM7E","ts":"1611785736.004000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j4f","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What's the problem here?\n"}]}]}]},{"client_msg_id":"db0b43dc-75d9-4f27-8e2c-730d4ac7d1f6","type":"message","text":"```using Distributions\nusing Turing\nusing Random\nusing MCMCChains\n\n\n\nrng = Random.MersenneTwister(1)\n\n\n@model function modelfn(y)\n    y .~ Normal(0,1)\nend\n\n\nxs = rand(rng, Normal(5, 3), 1000)\n\nmod = modelfn(xs)\n\nsample(mod, NUTS(), 1000)\n\n \nERROR: LoadError: MethodError: no method matching phasepoint(::Random._GLOBAL_RNG, ::Array{Any,1}, ::AdvancedHMC.Hamiltonian{AdvancedHMC.DiagEuclideanMetric{Float64,Array{Float64,1}},Turing.Inference.var\"#logπ#52\"{DynamicPPL.VarInfo{NamedTuple{(),Tuple{}},Float64},DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}},Turing.Inference.var\"#∂logπ∂θ#51\"{DynamicPPL.VarInfo{NamedTuple{(),Tuple{}},Float64},DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}}})\nClosest candidates are:\n  phasepoint(::Union{AbstractRNG, AbstractArray{var\"#s46\",1} where var\"#s46\"&lt;:AbstractRNG}, ::Union{AbstractArray{T,1}, AbstractArray{T,2}}, ::AdvancedHMC.Hamiltonian) where T&lt;:Real at \n  phasepoint(::AdvancedHMC.Hamiltonian, ::T, ::T; ℓπ, ℓκ) where T&lt;:(Union{AbstractArray{T,1}, AbstractArray{T,2}} where T) at \n  phasepoint(::AdvancedHMC.Hamiltonian, ::T1, ::T2; r, ℓπ, ℓκ) where {T1&lt;:(Union{AbstractArray{T,1}, AbstractArray{T,2}} where T), T2&lt;:(Union{AbstractArray{T,1}, AbstractArray{T,2}} where T)} at \nStacktrace:\n [1] initialstep(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}, ::DynamicPPL.VarInfo{NamedTuple{(),Tuple{}},Float64}; init_params::Nothing, nadapts::Int64, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:nchains,),Tuple{Int64}}}) at \n [2] step(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}; resume_from::Nothing, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:nadapts, :nchains),Tuple{Int64,Int64}}}) at \n [3] macro expansion at AbstractMCMC/Nw3Wn/src/sample.jl:78 [inlined]\n [4] macro expansion at ProgressLogging/BBN0b/src/ProgressLogging.jl:328 [inlined]\n [5] macro expansion at AbstractMCMC/Nw3Wn/src/logging.jl:8 [inlined]\n [6] mcmcsample(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}, ::Int64; progress::Bool, progressname::String, callback::Nothing, discard_initial::Int64, thinning::Int64, chain_type::Type{T} where T, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:nadapts, :nchains),Tuple{Int64,Int64}}}) at /AbstractMCMC/Nw3Wn/src/sample.jl:76\n [7] sample(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}, ::Int64; chain_type::Type{T} where T, resume_from::Nothing, progress::Bool, nadapts::Int64, discard_adapt::Bool, discard_initial::Int64, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:nchains,),Tuple{Int64}}}) at /home/user/src/project/julia/packages/Turing/O1Pn0/src/inference/hmc.jl:140\n [8] #sample#2 at /home/user/src/project/julia/packages/Turing/O1Pn0/src/inference/Inference.jl:143 [inlined]\n [9] #sample#1 at /home/user/src/project/julia/packages/Turing/O1Pn0/src/inference/Inference.jl:133 [inlined]\n [10] top-level scope at /home/user/src/project/src/tools/DistributionFinder.jl:20\n [11] include_string(::Function, ::Module, ::String, ::String) at /nix/store/l899vbb7z6w01xl2mkn7xzc9s8bx2msp-julia-1.5.2/lib/julia/sys.so:?\n [12] invokelatest(::Any, ::Any, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at ./essentials.jl:710\n [13] invokelatest(::Any, ::Any, ::Vararg{Any,N} where N) at ./essentials.jl:709\n [14] inlineeval(::Module, ::String, ::Int64, ::Int64, ::String; softscope::Bool) at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:185\n [15] (::VSCodeServer.var\"#61#65\"{String,Int64,Int64,String,Module,Bool,VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:144\n [16] withpath(::VSCodeServer.var\"#61#65\"{String,Int64,Int64,String,Module,Bool,VSCodeServer.ReplRunCodeRequestParams}, ::String) at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/repl.jl:124\n [17] (::VSCodeServer.var\"#60#64\"{String,Int64,Int64,String,Module,Bool,Bool,VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:142\n [18] hideprompt(::VSCodeServer.var\"#60#64\"{String,Int64,Int64,String,Module,Bool,Bool,VSCodeServer.ReplRunCodeRequestParams}) at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/repl.jl:36\n [19] (::VSCodeServer.var\"#59#63\"{String,Int64,Int64,String,Module,Bool,Bool,VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:110\n [20] with_logstate(::Function, ::Any) at /nix/store/l899vbb7z6w01xl2mkn7xzc9s8bx2msp-julia-1.5.2/lib/julia/sys.so:?\n [21] with_logger at ./logging.jl:514 [inlined]\n [22] (::VSCodeServer.var\"#58#62\"{VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:109\n [23] #invokelatest#1 at ./essentials.jl:710 [inlined]\n [24] invokelatest(::Any) at ./essentials.jl:709\n [25] macro expansion at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:27 [inlined]\n [26] (::VSCodeServer.var\"#56#57\")() at ./task.jl:356\nin expression starting at /home/user/src/project/src/tools/```","user":"U01ARRMLM7E","ts":"1611785736.004100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j4f-I=/G","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Distributions\nusing Turing\nusing Random\nusing MCMCChains\n\n\n\nrng = Random.MersenneTwister(1)\n\n\n@model function modelfn(y)\n    y .~ Normal(0,1)\nend\n\n\nxs = rand(rng, Normal(5, 3), 1000)\n\nmod = modelfn(xs)\n\nsample(mod, NUTS(), 1000)\n\n \nERROR: LoadError: MethodError: no method matching phasepoint(::Random._GLOBAL_RNG, ::Array{Any,1}, ::AdvancedHMC.Hamiltonian{AdvancedHMC.DiagEuclideanMetric{Float64,Array{Float64,1}},Turing.Inference.var\"#logπ#52\"{DynamicPPL.VarInfo{NamedTuple{(),Tuple{}},Float64},DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}},Turing.Inference.var\"#∂logπ∂θ#51\"{DynamicPPL.VarInfo{NamedTuple{(),Tuple{}},Float64},DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}},DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}}})\nClosest candidates are:\n  phasepoint(::Union{AbstractRNG, AbstractArray{var\"#s46\",1} where var\"#s46\"<:AbstractRNG}, ::Union{AbstractArray{T,1}, AbstractArray{T,2}}, ::AdvancedHMC.Hamiltonian) where T<:Real at \n  phasepoint(::AdvancedHMC.Hamiltonian, ::T, ::T; ℓπ, ℓκ) where T<:(Union{AbstractArray{T,1}, AbstractArray{T,2}} where T) at \n  phasepoint(::AdvancedHMC.Hamiltonian, ::T1, ::T2; r, ℓπ, ℓκ) where {T1<:(Union{AbstractArray{T,1}, AbstractArray{T,2}} where T), T2<:(Union{AbstractArray{T,1}, AbstractArray{T,2}} where T)} at \nStacktrace:\n [1] initialstep(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}, ::DynamicPPL.VarInfo{NamedTuple{(),Tuple{}},Float64}; init_params::Nothing, nadapts::Int64, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:nchains,),Tuple{Int64}}}) at \n [2] step(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}; resume_from::Nothing, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:nadapts, :nchains),Tuple{Int64,Int64}}}) at \n [3] macro expansion at AbstractMCMC/Nw3Wn/src/sample.jl:78 [inlined]\n [4] macro expansion at ProgressLogging/BBN0b/src/ProgressLogging.jl:328 [inlined]\n [5] macro expansion at AbstractMCMC/Nw3Wn/src/logging.jl:8 [inlined]\n [6] mcmcsample(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}, ::Int64; progress::Bool, progressname::String, callback::Nothing, discard_initial::Int64, thinning::Int64, chain_type::Type{T} where T, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol,Symbol},NamedTuple{(:nadapts, :nchains),Tuple{Int64,Int64}}}) at /AbstractMCMC/Nw3Wn/src/sample.jl:76\n [7] sample(::Random._GLOBAL_RNG, ::DynamicPPL.Model{var\"#67#68\",(:y,),(),(),Tuple{Array{Float64,1}},Tuple{}}, ::DynamicPPL.Sampler{NUTS{Turing.Core.ForwardDiffAD{40},(),AdvancedHMC.DiagEuclideanMetric}}, ::Int64; chain_type::Type{T} where T, resume_from::Nothing, progress::Bool, nadapts::Int64, discard_adapt::Bool, discard_initial::Int64, kwargs::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:nchains,),Tuple{Int64}}}) at /home/user/src/project/julia/packages/Turing/O1Pn0/src/inference/hmc.jl:140\n [8] #sample#2 at /home/user/src/project/julia/packages/Turing/O1Pn0/src/inference/Inference.jl:143 [inlined]\n [9] #sample#1 at /home/user/src/project/julia/packages/Turing/O1Pn0/src/inference/Inference.jl:133 [inlined]\n [10] top-level scope at /home/user/src/project/src/tools/DistributionFinder.jl:20\n [11] include_string(::Function, ::Module, ::String, ::String) at /nix/store/l899vbb7z6w01xl2mkn7xzc9s8bx2msp-julia-1.5.2/lib/julia/sys.so:?\n [12] invokelatest(::Any, ::Any, ::Vararg{Any,N} where N; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at ./essentials.jl:710\n [13] invokelatest(::Any, ::Any, ::Vararg{Any,N} where N) at ./essentials.jl:709\n [14] inlineeval(::Module, ::String, ::Int64, ::Int64, ::String; softscope::Bool) at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:185\n [15] (::VSCodeServer.var\"#61#65\"{String,Int64,Int64,String,Module,Bool,VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:144\n [16] withpath(::VSCodeServer.var\"#61#65\"{String,Int64,Int64,String,Module,Bool,VSCodeServer.ReplRunCodeRequestParams}, ::String) at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/repl.jl:124\n [17] (::VSCodeServer.var\"#60#64\"{String,Int64,Int64,String,Module,Bool,Bool,VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:142\n [18] hideprompt(::VSCodeServer.var\"#60#64\"{String,Int64,Int64,String,Module,Bool,Bool,VSCodeServer.ReplRunCodeRequestParams}) at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/repl.jl:36\n [19] (::VSCodeServer.var\"#59#63\"{String,Int64,Int64,String,Module,Bool,Bool,VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:110\n [20] with_logstate(::Function, ::Any) at /nix/store/l899vbb7z6w01xl2mkn7xzc9s8bx2msp-julia-1.5.2/lib/julia/sys.so:?\n [21] with_logger at ./logging.jl:514 [inlined]\n [22] (::VSCodeServer.var\"#58#62\"{VSCodeServer.ReplRunCodeRequestParams})() at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:109\n [23] #invokelatest#1 at ./essentials.jl:710 [inlined]\n [24] invokelatest(::Any) at ./essentials.jl:709\n [25] macro expansion at /home/user/.vscode/extensions/julialang.language-julia-1.0.10/scripts/packages/VSCodeServer/src/eval.jl:27 [inlined]\n [26] (::VSCodeServer.var\"#56#57\")() at ./task.jl:356\nin expression starting at /home/user/src/project/src/tools/"}]}]}]},{"client_msg_id":"a830c95b-6de3-49a1-bf77-3c3b06b5d9b5","type":"message","text":"Hey all I am having really slow sampling and I am not sure what it is, seems like a simple model. I am using VScode and the sampling is stuck at 0.0% for long time and then after several minutes (~10) the sampling suddently shoots to 100%, but maybe thats just a vscode problem with showing the progress. I am trying ReverseDiff now but it seems the same (if not slower). I tried `chain2 = sample(model2, NUTS(0.65), 10000)` but even if I try to just sample 100 samples it takes a long time. Here is model:\n\n```@model function m3(damage, plate, species, trt, n1, n2, n3, ::Type{T} = Float64) where {T}\n    mu0 ~ Normal(0,10)\n    trt_effects ~ filldist(Normal(0,10), n1)\n    species_effects ~ filldist(Normal(0,10), n2)\n    interaction_effects ~ filldist(Normal(0,10), n1*n2) \n    plate_effects ~ filldist(Normal(0,10), n3)\n    sigma ~ Gamma(1,1)\n    mu = Array{T,3}(undef, n1+1,n2+1,n3+1)\n    mu .= 0.0\n    for i in 1:(n1+1)\n        for j in 1:(n2+1)\n            for k in 1:(n3+1)\n                mu[i,j,k] += mu0\n                if i != 1\n                    mu[i,j,k] += trt_effects[i-1]\n                end\n                if j != 1\n                    mu[i,j,k] += species_effects[j-1]\n                end\n                if k != 1\n                    mu[i,j,k] += plate_effects[k-1]\n                end\n                if (i != 1) &amp;&amp; (j != 1)\n                    mu[i,j,k] += interaction_effects[i-1 + n1*(j-2)]\n                end\n            end\n        end\n    end\n    residuals = Array{T,1}(undef, length(damage))\n    residuals .= 0.0\n\n    for i in 1:length(damage)\n        residuals[i] = damage[i] - mu[trt[i],species[i],plate[i]]\n        damage[i] ~ Normal(mu[trt[i],species[i],plate[i]],sigma)\n    end\n    return mu, residuals\nend```","user":"U6CFMFM2R","ts":"1611940926.006300","team":"T68168MUP","edited":{"user":"U6CFMFM2R","ts":"1611941634.000000"},"blocks":[{"type":"rich_text","block_id":"G4Yc9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey all I am having really slow sampling and I am not sure what it is, seems like a simple model. I am using VScode and the sampling is stuck at 0.0% for long time and then after several minutes (~10) the sampling suddently shoots to 100%, but maybe thats just a vscode problem with showing the progress. I am trying ReverseDiff now but it seems the same (if not slower). I tried "},{"type":"text","text":"chain2 = sample(model2, NUTS(0.65), 10000)","style":{"code":true}},{"type":"text","text":" but even if I try to just sample 100 samples it takes a long time. Here is model:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@model function m3(damage, plate, species, trt, n1, n2, n3, ::Type{T} = Float64) where {T}\n    mu0 ~ Normal(0,10)\n    trt_effects ~ filldist(Normal(0,10), n1)\n    species_effects ~ filldist(Normal(0,10), n2)\n    interaction_effects ~ filldist(Normal(0,10), n1*n2) \n    plate_effects ~ filldist(Normal(0,10), n3)\n    sigma ~ Gamma(1,1)\n    mu = Array{T,3}(undef, n1+1,n2+1,n3+1)\n    mu .= 0.0\n    for i in 1:(n1+1)\n        for j in 1:(n2+1)\n            for k in 1:(n3+1)\n                mu[i,j,k] += mu0\n                if i != 1\n                    mu[i,j,k] += trt_effects[i-1]\n                end\n                if j != 1\n                    mu[i,j,k] += species_effects[j-1]\n                end\n                if k != 1\n                    mu[i,j,k] += plate_effects[k-1]\n                end\n                if (i != 1) && (j != 1)\n                    mu[i,j,k] += interaction_effects[i-1 + n1*(j-2)]\n                end\n            end\n        end\n    end\n    residuals = Array{T,1}(undef, length(damage))\n    residuals .= 0.0\n\n    for i in 1:length(damage)\n        residuals[i] = damage[i] - mu[trt[i],species[i],plate[i]]\n        damage[i] ~ Normal(mu[trt[i],species[i],plate[i]],sigma)\n    end\n    return mu, residuals\nend"}]}]}],"thread_ts":"1611940926.006300","reply_count":3,"reply_users_count":2,"latest_reply":"1611952475.018100","reply_users":["U8T9JUA5R","U6CFMFM2R"],"subscribed":false},{"client_msg_id":"c8082a4f-2c23-4ef6-a19e-2e1b1be65829","type":"message","text":"with tape compilation?","user":"U69BL50BF","ts":"1611941089.006500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+mLa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"with tape compilation?"}]}]}],"thread_ts":"1611941089.006500","reply_count":1,"reply_users_count":1,"latest_reply":"1611941452.007200","reply_users":["U6CFMFM2R"],"subscribed":false},{"client_msg_id":"d4672b15-01be-4925-ae05-4025ab09dfe6","type":"message","text":"I also can't seem to kill the sampling with `CTRL+C` or `ESC`","user":"U6CFMFM2R","ts":"1611941934.008000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"C1he","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I also can't seem to kill the sampling with "},{"type":"text","text":"CTRL+C","style":{"code":true}},{"type":"text","text":" or "},{"type":"text","text":"ESC","style":{"code":true}}]}]}]},{"client_msg_id":"1b72a136-0eab-470e-b5a8-7f8ff6e188b7","type":"message","text":"what's the scale of n1,n2,n3?","user":"U01H36BUDJB","ts":"1611943191.008200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lIBB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"what's the scale of n1,n2,n3?"}]}]}],"thread_ts":"1611943191.008200","reply_count":5,"reply_users_count":2,"latest_reply":"1611953539.024500","reply_users":["U6CFMFM2R","U01H36BUDJB"],"subscribed":false},{"client_msg_id":"0835c6dc-2742-4e73-a5e4-f0fced3cdcae","type":"message","text":"maybe also do `@macroexpand` before `@model` and post the output as a reply here","user":"U01H36BUDJB","ts":"1611943529.010400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LTpzi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"maybe also do "},{"type":"text","text":"@macroexpand","style":{"code":true}},{"type":"text","text":" before "},{"type":"text","text":"@model","style":{"code":true}},{"type":"text","text":" and post the output as a reply here"}]}]}],"thread_ts":"1611943529.010400","reply_count":1,"reply_users_count":1,"latest_reply":"1611947119.017100","reply_users":["U6CFMFM2R"],"subscribed":false},{"client_msg_id":"8fbfb201-73a1-488c-ad62-2602437406b3","type":"message","text":"might also be worth trying with Metropolis-Hastings just to check whether or not its the gradient computations that are the problem","user":"U01H36BUDJB","ts":"1611943635.011200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"R2IX6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"might also be worth trying with Metropolis-Hastings just to check whether or not its the gradient computations that are the problem"}]}]}]},{"client_msg_id":"9dec6fe7-ec23-480c-9a9a-a783c6066295","type":"message","text":"maybe the conditionals are the problem? <@U69BL50BF> I thought that the DiffEqFlux docs said tape compilation didn't work in this case.","user":"U01H36BUDJB","ts":"1611943862.012500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"01B2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"maybe the conditionals are the problem? "},{"type":"user","user_id":"U69BL50BF"},{"type":"text","text":" I thought that the DiffEqFlux docs said tape compilation didn't work in this case."}]}]}]},{"client_msg_id":"c0955daa-7a73-4c4e-afcf-5d7d5ccd3425","type":"message","text":"I know it will break your model definition, but maybe try stripping out the conditionals and see if it improves the speed.","user":"U01H36BUDJB","ts":"1611943942.013300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5W2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I know it will break your model definition, but maybe try stripping out the conditionals and see if it improves the speed."}]}]}]},{"client_msg_id":"59d3d938-a7c8-4338-be5f-71b54f8275d7","type":"message","text":"The most frustrating thing is I can't kill the sampling. So when I want to change something and try again to see if it \"fixes it\" I have to kill julia, rerun all my code. This is making the try-test-change loop very difficult","user":"U6CFMFM2R","ts":"1611945382.014100","team":"T68168MUP","edited":{"user":"U6CFMFM2R","ts":"1611945397.000000"},"blocks":[{"type":"rich_text","block_id":"s+rsr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The most frustrating thing is I can't kill the sampling. So when I want to change something and try again to see if it \"fixes it\" I have to kill julia, rerun all my code. This is making the try-test-change loop very difficult"}]}]}]},{"client_msg_id":"25ce0c51-e944-48d7-a4d1-59070471a47a","type":"message","text":"It seems to kill just 1 iteration, and it goes on to the next one","user":"U6CFMFM2R","ts":"1611945423.014500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TYt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It seems to kill just 1 iteration, and it goes on to the next one"}]}]}]},{"client_msg_id":"9794CD9A-9CC9-4FBA-9C05-E9BF8A431FEB","type":"message","text":"Could you set the max iterations to 2 or something like that?","user":"U7THT3TM3","ts":"1611945541.015100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"CS31","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Could you set the max iterations to 2 or something like that?"}]}]}]},{"client_msg_id":"21deb547-b065-43db-8f98-3cd13bf02b7b","type":"message","text":"&gt; maybe the conditionals are the problem? <@U69BL50BF> I thought that the DiffEqFlux docs said tape compilation didn't work in this case.\nThese conditions are not value dependent","user":"U69BL50BF","ts":"1611946047.015500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/L6Se","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"maybe the conditionals are the problem? "},{"type":"user","user_id":"U69BL50BF"},{"type":"text","text":" I thought that the DiffEqFlux docs said tape compilation didn't work in this case."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"These conditions are not value dependent"}]}]}]},{"client_msg_id":"fd941454-da63-4a28-820c-1e9724609423","type":"message","text":"it still represents a static graph","user":"U69BL50BF","ts":"1611946053.015800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"NDJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it still represents a static graph"}]}]}]},{"client_msg_id":"6689b9c0-0e2f-4c5a-b294-a3c71bc0994b","type":"message","text":"Another way of saying it is, for these conditions, the order of true and false is independent of what values you give to the function, so their truth values can all be determined at compile time","user":"U69BL50BF","ts":"1611946104.016500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"I3Bm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Another way of saying it is, for these conditions, the order of true and false is independent of what values you give to the function, so their truth values can all be determined at compile time"}]}]}],"thread_ts":"1611946104.016500","reply_count":1,"reply_users_count":1,"latest_reply":"1611952688.018800","reply_users":["U01H36BUDJB"],"subscribed":false},{"client_msg_id":"c079921d-d777-43e7-a682-a724ea56dd6a","type":"message","text":"that would work with tape compilation.","user":"U69BL50BF","ts":"1611946109.016700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OSlu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"that would work with tape compilation."}]}]}]},{"client_msg_id":"3e41759b-ff7d-4005-9614-f71a0d5a6859","type":"message","text":".35 secs for 10 iterations, 130sec for 100 iterations","user":"U6CFMFM2R","ts":"1611946673.017000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"T18KZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":".35 secs for 10 iterations, 130sec for 100 iterations"}]}]}],"thread_ts":"1611946673.017000","reply_count":4,"reply_users_count":2,"latest_reply":"1611953255.024300","reply_users":["U01H36BUDJB","U6CFMFM2R"],"subscribed":false},{"client_msg_id":"9248f9b1-da21-4338-ac2e-c21edba74831","type":"message","text":"12 secs for 10_000 iterations with MH()","user":"U6CFMFM2R","ts":"1611952311.018000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lRXq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"12 secs for 10_000 iterations with MH()"}]}]}],"thread_ts":"1611952311.018000","reply_count":1,"reply_users_count":1,"latest_reply":"1611952788.019000","reply_users":["U01H36BUDJB"],"subscribed":false},{"client_msg_id":"6e61aac2-cddd-4a9e-a0f1-016404f13884","type":"message","text":"I guess Zygote wouldn't work with this model as written, but you could try Tracker. Beyond that, sounds like an issue needs to be created. The question is whether it's a Turing or ReverseDiff problem.","user":"U01H36BUDJB","ts":"1611952883.021800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7Bi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I guess Zygote wouldn't work with this model as written, but you could try Tracker. Beyond that, sounds like an issue needs to be created. The question is whether it's a Turing or ReverseDiff problem."}]}]}]},{"client_msg_id":"68491fd4-9d9a-4e19-9f99-d6b4fc4a7760","type":"message","text":"whats an easy way to make a MWE for people? The data has like ~800 rows","user":"U6CFMFM2R","ts":"1611952940.022500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MYj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"whats an easy way to make a MWE for people? The data has like ~800 rows"}]}]}],"thread_ts":"1611952940.022500","reply_count":1,"reply_users_count":1,"latest_reply":"1611954811.025300","reply_users":["U01H36BUDJB"],"subscribed":false},{"client_msg_id":"b4a633b3-c6cc-4d70-b01e-0d6820589b9b","type":"message","text":"Did you try removing the conditionals yet? I know it *shouldn't* matter... but still.","user":"U01H36BUDJB","ts":"1611953197.024200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hhO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Did you try removing the conditionals yet? I know it "},{"type":"text","text":"shouldn't","style":{"bold":true}},{"type":"text","text":" matter... but still."}]}]}],"thread_ts":"1611953197.024200","reply_count":12,"reply_users_count":3,"latest_reply":"1611958695.030700","reply_users":["U6CFMFM2R","U01H36BUDJB","U69BL50BF"],"subscribed":false},{"client_msg_id":"2930b2a8-4006-460e-8b77-f972be51342e","type":"message","text":"The loop variable order also should be reversed for column-major, but this doesn't seem to affect anything in my quick experiments. I suppose Julia's compiler is probably smart enough to do this automatically.","user":"U01H36BUDJB","ts":"1611957921.027700","team":"T68168MUP","edited":{"user":"U01H36BUDJB","ts":"1611957941.000000"},"blocks":[{"type":"rich_text","block_id":"4Gq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The loop variable order also should be reversed for column-major, but this doesn't seem to affect anything in my quick experiments. I suppose Julia's compiler is probably smart enough to do this automatically."}]}]}]},{"client_msg_id":"005c85a9-e567-4f02-b7df-6ed8217d805a","type":"message","text":"the cost of AD is probably just more than that.","user":"U69BL50BF","ts":"1611957944.028100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Kuk7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"the cost of AD is probably just more than that."}]}]}]},{"client_msg_id":"708116ee-342b-4c05-9e05-407a925dfa94","type":"message","text":"yeah but I tested it on MH just to see if it did anything","user":"U01H36BUDJB","ts":"1611957957.028500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"chHL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yeah but I tested it on MH just to see if it did anything"}]}]}]},{"client_msg_id":"684f2ea5-b270-4833-89f9-df57dc2884aa","type":"message","text":"reverse mode AD doesn't play by the rules.","user":"U69BL50BF","ts":"1611957958.028700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GL1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"reverse mode AD doesn't play by the rules."}]}]}]},{"client_msg_id":"5d88fa16-18d7-4746-9ee4-5a052b0fd5bc","type":"message","text":"oh okay","user":"U69BL50BF","ts":"1611957963.028900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hXvKk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"oh okay"}]}]}]},{"client_msg_id":"65c96dab-c6e3-4ddb-b553-04811c7f8579","type":"message","text":"it doesn't reverse it, but sometimes you can still SIMD with gaps as of LLVM X (I forget the version but it's quite recent and impacted the ForwardDiff2.jl design)","user":"U69BL50BF","ts":"1611957995.029600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j97Yy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it doesn't reverse it, but sometimes you can still SIMD with gaps as of LLVM X (I forget the version but it's quite recent and impacted the ForwardDiff2.jl design)"}]}]}]},{"client_msg_id":"652dbe54-06b2-405d-b910-cefe0a44e05b","type":"message","text":"oh interesting","user":"U01H36BUDJB","ts":"1611958022.029800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"y4K1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"oh interesting"}]}]}]},{"client_msg_id":"2197c8ea-0447-4978-846c-0c16c166a618","type":"message","text":"Hello, I want to transform prior distribution for downstream use.\n\n```@model function gdemo()\n    sigma ~ Exponential(2)\n    sigmaTransformed = sigma + 1\nend\n\nchn = sample(gdemo(), HMC(0.05, 10), 10)```\n1. What is the Turing way of transforming prior `sigma` to `sigmaTransformed`? Is my code correct or should I be looking into Bijectors.jl?\n2. What is the Turing way of returning `sigmaTransformed` for example for inspecting the posterior? Should i be looking into `generated_quantities` ?\nThank you","user":"U011PPW7K53","ts":"1612171908.036000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iLbM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello, I want to transform prior distribution for downstream use.\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@model function gdemo()\n    sigma ~ Exponential(2)\n    sigmaTransformed = sigma + 1\nend\n\nchn = sample(gdemo(), HMC(0.05, 10), 10)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What is the Turing way of transforming prior "},{"type":"text","text":"sigma","style":{"code":true}},{"type":"text","text":" to "},{"type":"text","text":"sigmaTransformed","style":{"code":true}},{"type":"text","text":"? Is my code correct or should I be looking into Bijectors.jl?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"What is the Turing way of returning "},{"type":"text","text":"sigmaTransformed","style":{"code":true}},{"type":"text","text":" for example for inspecting the posterior? Should i be looking into "},{"type":"text","text":"generated_quantities","style":{"code":true}},{"type":"text","text":" ?"}]}],"style":"ordered","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"\nThank you"}]}]}]},{"client_msg_id":"a581ab5d-0aed-4ccf-9ae5-5e24f4469f40","type":"message","text":"`generated_quantities` is your friend. The model implementation looks correct.","user":"UC0SY9JFP","ts":"1612173376.036600","team":"T68168MUP","edited":{"user":"UC0SY9JFP","ts":"1612269340.000000"},"blocks":[{"type":"rich_text","block_id":"sqe","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"generated_quantities","style":{"code":true}},{"type":"text","text":" is your friend. The model implementation looks correct."}]}]}],"thread_ts":"1612173376.036600","reply_count":1,"reply_users_count":1,"latest_reply":"1612269316.037000","reply_users":["U011PPW7K53"],"subscribed":false,"reactions":[{"name":"+1","users":["U011PPW7K53"],"count":1}]},{"client_msg_id":"64311dc9-4a0a-4a94-9294-23be3950d7e4","type":"message","text":"Hey folks. I was trying the tutorial on Bayesian Differential equations in a Pluto notebook. I am getting an MCMCChains method error -- Failed to show value. Here is the code and the message. I am not sure how I should read the error, like what is the cause of the issue. Any suggestions are appreciated.\n```Turing.setadbackend(:forwarddiff)\n\n@model function fitlv(data, prob1)\n    σ ~ InverseGamma(2, 3) # ~ is the tilde character\n    α ~ truncated(Normal(1.5,0.5),0.5,2.5)\n    β ~ truncated(Normal(1.2,0.5),0,2)\n    γ ~ truncated(Normal(3.0,0.5),1,4)\n    δ ~ truncated(Normal(1.0,0.5),0,2)\n\n    p = [α,β,γ,δ]\n    prob = remake(prob1, p=p)\n    predicted = solve(prob,Tsit5(),saveat=0.1)\n\n    for i = 1:length(predicted)\n        data[:,i] ~ MvNormal(predicted[i], σ)\n    end\nend\n\nmodel = fitlv(odedata, prob1)\n\n# This next command runs 3 independent chains without using multithreading. \nchain = mapreduce(c -&gt; sample(model, NUTS(.65),1000), chainscat, 1:3)```\nHere is the message.\n```Failed to show value:\n\nMethodError: no method matching iterate(::MCMCChains.Chains{Float64,AxisArrays.AxisArray{Float64,3,Array{Float64,3},Tuple{AxisArrays.Axis{:iter,StepRange{Int64,Int64}},AxisArrays.Axis{:var,Array{Symbol,1}},AxisArrays.Axis{:chain,UnitRange{Int64}}}},Missing,NamedTuple{(:parameters, :internals),Tuple{Array{Symbol,1},Array{Symbol,1}}},NamedTuple{(),Tuple{}}})\n\nClosest candidates are:\n\niterate(!Matched::Base.RegexMatchIterator) at regex.jl:552\n\niterate(!Matched::Base.RegexMatchIterator, !Matched::Any) at regex.jl:552\n\niterate(!Matched::Libtask.TRef, !Matched::Any...) at /home/krishnab/.julia/packages/Libtask/00Il9/src/tref.jl:86\n\n...```","user":"UDDSTBX19","ts":"1612292883.038900","team":"T68168MUP","edited":{"user":"UDDSTBX19","ts":"1612293101.000000"},"blocks":[{"type":"rich_text","block_id":"niA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey folks. I was trying the tutorial on Bayesian Differential equations in a Pluto notebook. I am getting an MCMCChains method error -- Failed to show value. Here is the code and the message. I am not sure how I should read the error, like what is the cause of the issue. Any suggestions are appreciated.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Turing.setadbackend(:forwarddiff)\n\n@model function fitlv(data, prob1)\n    σ ~ InverseGamma(2, 3) # ~ is the tilde character\n    α ~ truncated(Normal(1.5,0.5),0.5,2.5)\n    β ~ truncated(Normal(1.2,0.5),0,2)\n    γ ~ truncated(Normal(3.0,0.5),1,4)\n    δ ~ truncated(Normal(1.0,0.5),0,2)\n\n    p = [α,β,γ,δ]\n    prob = remake(prob1, p=p)\n    predicted = solve(prob,Tsit5(),saveat=0.1)\n\n    for i = 1:length(predicted)\n        data[:,i] ~ MvNormal(predicted[i], σ)\n    end\nend\n\nmodel = fitlv(odedata, prob1)\n\n# This next command runs 3 independent chains without using multithreading. \nchain = mapreduce(c -> sample(model, NUTS(.65),1000), chainscat, 1:3)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nHere is the message.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Failed to show value:\n\nMethodError: no method matching iterate(::MCMCChains.Chains{Float64,AxisArrays.AxisArray{Float64,3,Array{Float64,3},Tuple{AxisArrays.Axis{:iter,StepRange{Int64,Int64}},AxisArrays.Axis{:var,Array{Symbol,1}},AxisArrays.Axis{:chain,UnitRange{Int64}}}},Missing,NamedTuple{(:parameters, :internals),Tuple{Array{Symbol,1},Array{Symbol,1}}},NamedTuple{(),Tuple{}}})\n\nClosest candidates are:\n\niterate(!Matched::Base.RegexMatchIterator) at regex.jl:552\n\niterate(!Matched::Base.RegexMatchIterator, !Matched::Any) at regex.jl:552\n\niterate(!Matched::Libtask.TRef, !Matched::Any...) at /home/krishnab/.julia/packages/Libtask/00Il9/src/tref.jl:86\n\n..."}]}]}],"thread_ts":"1612292883.038900","reply_count":15,"reply_users_count":2,"latest_reply":"1612369526.042200","reply_users":["UDDSTBX19","U01H36BUDJB"],"subscribed":false,"reactions":[{"name":"face_with_monocle","users":["UC0SY9JFP"],"count":1}]},{"client_msg_id":"10605262-adea-4cf8-a838-525ba435ab49","type":"message","text":"Hi all-\n\nI noticed that the adaption steps are no longer removed from the chains. Where can I submit an issue?","user":"UH08DT0JU","ts":"1612372127.043400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IdCD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi all-\n\nI noticed that the adaption steps are no longer removed from the chains. Where can I submit an issue?"}]}]}],"thread_ts":"1612372127.043400","reply_count":6,"reply_users_count":2,"latest_reply":"1612430927.044600","reply_users":["UH08DT0JU","U8T9JUA5R"],"subscribed":false},{"client_msg_id":"2a980d06-c441-4c52-b733-30498db59d4e","type":"message","text":"Hi All,\nJust a quick question about syntax. From what I understand, using filldist is more performant than for loops. I am unsure if/how to translate this to use filldist:\n\n`for i ∈ 1:length(predicted)`\n        `data[:,i] ~ MvNormal(predicted[i], σ)` \n`end` \nwhere `predicted` is the solution to an ODEProblem and has size `(5,101)` .\n\nAny suggestions appreciated! :slightly_smiling_face:","user":"U01M641BZEY","ts":"1612464277.049800","team":"T68168MUP","edited":{"user":"U01M641BZEY","ts":"1612464333.000000"},"blocks":[{"type":"rich_text","block_id":"nFDJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi All,\nJust a quick question about syntax. From what I understand, using filldist is more performant than for loops. I am unsure if/how to translate this to use filldist:\n\n"},{"type":"text","text":"for i ∈ 1:length(predicted)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"        data[:,i] ~ MvNormal(predicted[i], σ) ","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"end ","style":{"code":true}},{"type":"text","text":"\nwhere "},{"type":"text","text":"predicted","style":{"code":true}},{"type":"text","text":" is the solution to an ODEProblem and has size "},{"type":"text","text":"(5,101)","style":{"code":true}},{"type":"text","text":" .\n\nAny suggestions appreciated! "},{"type":"emoji","name":"slightly_smiling_face"}]}]}],"thread_ts":"1612464277.049800","reply_count":4,"reply_users_count":2,"latest_reply":"1612466366.050800","reply_users":["U01H36BUDJB","U01M641BZEY"],"subscribed":false},{"client_msg_id":"d3e56c16-9fd2-442c-b044-caf2096fc853","type":"message","text":"Say, I find that whenever I use the command `Turing.turnprogress(false)`, I get an error message\n```ERROR: UndefVarError: turnprogress not defined\nStacktrace:\n [1] getproperty(::Module, ::Symbol) at ./Base.jl:26\n [2] top-level scope at REPL[27]:1```\nDoes this make sense to anyone. I could not find a reference to `turnprogress` in the documentation, and no issues are posted. So perhaps I am missing something simple.","user":"UDDSTBX19","ts":"1612471590.052100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bACP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Say, I find that whenever I use the command "},{"type":"text","text":"Turing.turnprogress(false)","style":{"code":true}},{"type":"text","text":", I get an error message\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: UndefVarError: turnprogress not defined\nStacktrace:\n [1] getproperty(::Module, ::Symbol) at ./Base.jl:26\n [2] top-level scope at REPL[27]:1"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Does this make sense to anyone. I could not find a reference to "},{"type":"text","text":"turnprogress","style":{"code":true}},{"type":"text","text":" in the documentation, and no issues are posted. So perhaps I am missing something simple."}]}]}],"thread_ts":"1612471590.052100","reply_count":2,"reply_users_count":2,"latest_reply":"1612473084.052900","reply_users":["UC0SY9JFP","UDDSTBX19"],"subscribed":false},{"client_msg_id":"7b7c4906-02b1-4001-962c-e6a1bfeac2b1","type":"message","text":"Hi! New user question: I'm having a really hard time navigating the documentation for Turing. I'm learning Julia from scratch, coming from R and taking a Bayesian class using RStan at the moment. (I'm a grad student, and I'm looking to learn Julia on the side.) I'm noticing the API is sort of spread out across different packages, and I was hoping for something a bit more comprehensive if it exists, similar to Stan's manuals. Any pointers would be highly appreciated! :)\n\n(One example: it took me a bit to discover Distributions.jl was a thing, since I kept trying to locate different distributions on the <http://turing.ml|turing.ml> website, with no luck! Another example is figuring out how to initialize my parameters: I see one brief mention of \"init_theta\" on the official guide at <http://turing.ml|turing.ml>, and some GitHub issues here and there, but as far as I can tell, there's no API documentation that tells me what I actually pass in)","user":"U01M951C4JX","ts":"1612680426.059800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ZMk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi! New user question: I'm having a really hard time navigating the documentation for Turing. I'm learning Julia from scratch, coming from R and taking a Bayesian class using RStan at the moment. (I'm a grad student, and I'm looking to learn Julia on the side.) I'm noticing the API is sort of spread out across different packages, and I was hoping for something a bit more comprehensive if it exists, similar to Stan's manuals. Any pointers would be highly appreciated! :)\n\n(One example: it took me a bit to discover Distributions.jl was a thing, since I kept trying to locate different distributions on the "},{"type":"link","url":"http://turing.ml","text":"turing.ml"},{"type":"text","text":" website, with no luck! Another example is figuring out how to initialize my parameters: I see one brief mention of \"init_theta\" on the official guide at "},{"type":"link","url":"http://turing.ml","text":"turing.ml"},{"type":"text","text":", and some GitHub issues here and there, but as far as I can tell, there's no API documentation that tells me what I actually pass in)"}]}]}]},{"client_msg_id":"e787a32e-3e94-479e-99d3-c25631b73eb2","type":"message","text":"Hey Folks. I was trying to understand how to use the `describe()` function on a `ChainDataFrame`. So I used\n`chain = mapreduce(c -&gt; sample(model, NUTS(.65),1000), chainscat, 1:3);`\nto generate multiple chains. But I can't figure out how to use the `describe` function on the chains. When I try `describe(chain)` that just shows `MCMCChains.ChainDataFrame[,]` . When I try `describe[chain[:,:,1]` that does not work either. So I am missing something in the syntax somewhere.","user":"UDDSTBX19","ts":"1612716147.063700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yEA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey Folks. I was trying to understand how to use the "},{"type":"text","text":"describe()","style":{"code":true}},{"type":"text","text":" function on a "},{"type":"text","text":"ChainDataFrame","style":{"code":true}},{"type":"text","text":". So I used\n"},{"type":"text","text":"chain = mapreduce(c -> sample(model, NUTS(.65),1000), chainscat, 1:3);","style":{"code":true}},{"type":"text","text":"\nto generate multiple chains. But I can't figure out how to use the "},{"type":"text","text":"describe","style":{"code":true}},{"type":"text","text":" function on the chains. When I try "},{"type":"text","text":"describe(chain)","style":{"code":true}},{"type":"text","text":" that just shows "},{"type":"text","text":"MCMCChains.ChainDataFrame[,]","style":{"code":true}},{"type":"text","text":" . When I try "},{"type":"text","text":"describe[chain[:,:,1]","style":{"code":true}},{"type":"text","text":" that does not work either. So I am missing something in the syntax somewhere."}]}]}],"thread_ts":"1612716147.063700","reply_count":11,"reply_users_count":2,"latest_reply":"1612717666.067100","reply_users":["U9JNHB83X","UDDSTBX19"],"subscribed":false},{"client_msg_id":"fe21c14c-7992-40ea-a430-ad3126a2ca25","type":"message","text":"Note I am using Pluto. So that might be a little difference between the Pluto output and the command line output?","user":"UDDSTBX19","ts":"1612716184.064300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jURM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Note I am using Pluto. So that might be a little difference between the Pluto output and the command line output?"}]}]}]},{"client_msg_id":"6b3872d7-8865-4f1b-bfd0-d834a8ccca02","type":"message","text":"The full model is:\n```Turing.setadbackend(:forwarddiff)\nfunction lotka_volterra(du,u,p,t)\n  x, y = u\n  α, β, γ, δ  = p\n  du[1] = (α - β*y)x # dx =\n  du[2] = (δ*x - γ)y # dy = \nend\np = [1.5, 1.0, 3.0, 1.0]\nu0 = [1.0,1.0]\n@model function fitlv(data, prob1)\n    σ ~ InverseGamma(2, 3) # ~ is the tilde character\n    α ~ truncated(Normal(1.5,0.5),0.5,2.5)\n    β ~ truncated(Normal(1.2,0.5),0,2)\n    γ ~ truncated(Normal(3.0,0.5),1,4)\n    δ ~ truncated(Normal(1.0,0.5),0,2)\n\n    p = [α,β,γ,δ]\n    prob = remake(prob1, p=p)\n    predicted = solve(prob,Tsit5(),saveat=0.1)\n\n    for i = 1:length(predicted)\n        data[:,i] ~ MvNormal(predicted[i], σ)\n    end\nend\n\nmodel = fitlv(odedata, prob1)\n\n# This next command runs 3 independent chains without using multithreading. \nchain = mapreduce(c -&gt; sample(model, NUTS(.65),1000), chainscat, 1:3)```","user":"UDDSTBX19","ts":"1612716510.065400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dNx+8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The full model is:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Turing.setadbackend(:forwarddiff)\nfunction lotka_volterra(du,u,p,t)\n  x, y = u\n  α, β, γ, δ  = p\n  du[1] = (α - β*y)x # dx =\n  du[2] = (δ*x - γ)y # dy = \nend\np = [1.5, 1.0, 3.0, 1.0]\nu0 = [1.0,1.0]\n@model function fitlv(data, prob1)\n    σ ~ InverseGamma(2, 3) # ~ is the tilde character\n    α ~ truncated(Normal(1.5,0.5),0.5,2.5)\n    β ~ truncated(Normal(1.2,0.5),0,2)\n    γ ~ truncated(Normal(3.0,0.5),1,4)\n    δ ~ truncated(Normal(1.0,0.5),0,2)\n\n    p = [α,β,γ,δ]\n    prob = remake(prob1, p=p)\n    predicted = solve(prob,Tsit5(),saveat=0.1)\n\n    for i = 1:length(predicted)\n        data[:,i] ~ MvNormal(predicted[i], σ)\n    end\nend\n\nmodel = fitlv(odedata, prob1)\n\n# This next command runs 3 independent chains without using multithreading. \nchain = mapreduce(c -> sample(model, NUTS(.65),1000), chainscat, 1:3)"}]}]}]},{"client_msg_id":"15c166f2-e967-46bb-990f-fc766cfe9570","type":"message","text":"Hi,\n\n I’m trying to get von Mises distributions to work with Turing and would like some advise on how best to proceed. I’ve got a simple FowardDiff implementation of SpecialFunctions besselix to enable HMC/NUTS to work with the distribution, but I’ve now got a more fundamental problem with the definition in Distributions.jl.\n\n If I test a distribution with a low κ (i.e. large variance) almost all the test proposals fail the isinfinite check. Looking through the code, the problem appears to be the definition of the range of the von Mises function:\n\nWith low κ the observation values (y) cover the whole range from -π to π, so when a test point has a non zero mean (μ) there’s always some observations where |y-μ|&gt;π. This violates the definition given in Distributions.jl\n\n`@distr_support VonMises d.μ - π d.μ + π`\n\nIf I remove the value check in the Distributions.jl logpdf definition, sampling works well. And, at least for my use case, this is perfectly valid definition. von Mises is then a continuous, repeating, distribution, and any limitations on the range of μ are defined in the prior.\n\nI’ve been trying to understand why Distributions.jl has this range definition, but haven’t been able to find much. It was introduced in #273 with this <https://github.com/JuliaStats/Distributions.jl/issues/273#issuecomment-61982908|comment> but without further discussion, or explanation in the quoted source (wikipedia).\n\nSo, does anyone know why von Mises has this range? And what would be the best way to get it working with Turing? A fix for Distributions.jl or an import of the logpdf function within Turing?\n\nThanks.","user":"U01JL6RGKU7","ts":"1612784378.068100","team":"T68168MUP","edited":{"user":"U01JL6RGKU7","ts":"1612784870.000000"},"blocks":[{"type":"rich_text","block_id":"ZG5Py","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi,\n\n I’m trying to get von Mises distributions to work with Turing and would like some advise on how best to proceed. I’ve got a simple FowardDiff implementation of SpecialFunctions besselix to enable HMC/NUTS to work with the distribution, but I’ve now got a more fundamental problem with the definition in Distributions.jl.\n\n If I test a distribution with a low κ (i.e. large variance) almost all the test proposals fail the isinfinite check. Looking through the code, the problem appears to be the definition of the range of the von Mises function:\n\nWith low κ the observation values (y) cover the whole range from -π to π, so when a test point has a non zero mean (μ) there’s always some observations where |y-μ|>π. This violates the definition given in Distributions.jl\n\n"},{"type":"text","text":"@distr_support VonMises d.μ - π d.μ + π","style":{"code":true}},{"type":"text","text":"\n\nIf I remove the value check in the Distributions.jl logpdf definition, sampling works well. And, at least for my use case, this is perfectly valid definition. von Mises is then a continuous, repeating, distribution, and any limitations on the range of μ are defined in the prior.\n\nI’ve been trying to understand why Distributions.jl has this range definition, but haven’t been able to find much. It was introduced in #273 with this "},{"type":"link","url":"https://github.com/JuliaStats/Distributions.jl/issues/273#issuecomment-61982908","text":"comment"},{"type":"text","text":" but without further discussion, or explanation in the quoted source (wikipedia).\n\nSo, does anyone know why von Mises has this range? And what would be the best way to get it working with Turing? A fix for Distributions.jl or an import of the logpdf function within Turing?\n\nThanks."}]}]}],"thread_ts":"1612784378.068100","reply_count":1,"reply_users_count":1,"latest_reply":"1612786123.068300","reply_users":["UC0SY9JFP"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"Another issue with circular distributions parameterized by an angle with NUTS is that you either are supported on the entire real line, in which case the distribution has infinite modes (and if you can transition between nodes in NUTS, then this will cause poor adaptation), or you are supported only on the the range [0, pi], in which case you could bifurcate a single mode and only explore one of them. The same is true whether we're talking about the random variable or the mean parameter of von Mises. The way Stan handles this is to internally represent circular variables in terms of two normally distributed cartesian coordinates x and y such that theta = atan(y, x). theta is then uniformly distributed on the circle. To apply a von Mises distribution, to theta, you'd want to increment the logpdf manually. Does Turing give us access to this? In brief something like\n```# equivalent to `μ ~ UniformOnCircle(); θ ~ VonMises(μ)` if we had such a thing\n@model function vMmodel(θ)\n    μx ~ Normal()\n    μy ~ Normal()\n    μ = atan(μy, μx)\n    lp += logpdf(VonMises(μ), θ) # is there a syntax for this?\nend```\n","user":"UHDQQ4GN6","ts":"1612825842.070100","thread_ts":"1612784378.068100","root":{"client_msg_id":"15c166f2-e967-46bb-990f-fc766cfe9570","type":"message","text":"Hi,\n\n I’m trying to get von Mises distributions to work with Turing and would like some advise on how best to proceed. I’ve got a simple FowardDiff implementation of SpecialFunctions besselix to enable HMC/NUTS to work with the distribution, but I’ve now got a more fundamental problem with the definition in Distributions.jl.\n\n If I test a distribution with a low κ (i.e. large variance) almost all the test proposals fail the isinfinite check. Looking through the code, the problem appears to be the definition of the range of the von Mises function:\n\nWith low κ the observation values (y) cover the whole range from -π to π, so when a test point has a non zero mean (μ) there’s always some observations where |y-μ|&gt;π. This violates the definition given in Distributions.jl\n\n`@distr_support VonMises d.μ - π d.μ + π`\n\nIf I remove the value check in the Distributions.jl logpdf definition, sampling works well. And, at least for my use case, this is perfectly valid definition. von Mises is then a continuous, repeating, distribution, and any limitations on the range of μ are defined in the prior.\n\nI’ve been trying to understand why Distributions.jl has this range definition, but haven’t been able to find much. It was introduced in #273 with this <https://github.com/JuliaStats/Distributions.jl/issues/273#issuecomment-61982908|comment> but without further discussion, or explanation in the quoted source (wikipedia).\n\nSo, does anyone know why von Mises has this range? And what would be the best way to get it working with Turing? A fix for Distributions.jl or an import of the logpdf function within Turing?\n\nThanks.","user":"U01JL6RGKU7","ts":"1612784378.068100","team":"T68168MUP","edited":{"user":"U01JL6RGKU7","ts":"1612784870.000000"},"blocks":[{"type":"rich_text","block_id":"ZG5Py","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi,\n\n I’m trying to get von Mises distributions to work with Turing and would like some advise on how best to proceed. I’ve got a simple FowardDiff implementation of SpecialFunctions besselix to enable HMC/NUTS to work with the distribution, but I’ve now got a more fundamental problem with the definition in Distributions.jl.\n\n If I test a distribution with a low κ (i.e. large variance) almost all the test proposals fail the isinfinite check. Looking through the code, the problem appears to be the definition of the range of the von Mises function:\n\nWith low κ the observation values (y) cover the whole range from -π to π, so when a test point has a non zero mean (μ) there’s always some observations where |y-μ|>π. This violates the definition given in Distributions.jl\n\n"},{"type":"text","text":"@distr_support VonMises d.μ - π d.μ + π","style":{"code":true}},{"type":"text","text":"\n\nIf I remove the value check in the Distributions.jl logpdf definition, sampling works well. And, at least for my use case, this is perfectly valid definition. von Mises is then a continuous, repeating, distribution, and any limitations on the range of μ are defined in the prior.\n\nI’ve been trying to understand why Distributions.jl has this range definition, but haven’t been able to find much. It was introduced in #273 with this "},{"type":"link","url":"https://github.com/JuliaStats/Distributions.jl/issues/273#issuecomment-61982908","text":"comment"},{"type":"text","text":" but without further discussion, or explanation in the quoted source (wikipedia).\n\nSo, does anyone know why von Mises has this range? And what would be the best way to get it working with Turing? A fix for Distributions.jl or an import of the logpdf function within Turing?\n\nThanks."}]}]}],"thread_ts":"1612784378.068100","reply_count":4,"reply_users_count":4,"latest_reply":"1612825842.070100","reply_users":["UC0SY9JFP","U01H36BUDJB","U01JL6RGKU7","UHDQQ4GN6"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"sjC9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Another issue with circular distributions parameterized by an angle with NUTS is that you either are supported on the entire real line, in which case the distribution has infinite modes (and if you can transition between nodes in NUTS, then this will cause poor adaptation), or you are supported only on the the range [0, pi], in which case you could bifurcate a single mode and only explore one of them. The same is true whether we're talking about the random variable or the mean parameter of von Mises. The way Stan handles this is to internally represent circular variables in terms of two normally distributed cartesian coordinates x and y such that theta = atan(y, x). theta is then uniformly distributed on the circle. To apply a von Mises distribution, to theta, you'd want to increment the logpdf manually. Does Turing give us access to this? In brief something like\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"# equivalent to `μ ~ UniformOnCircle(); θ ~ VonMises(μ)` if we had such a thing\n@model function vMmodel(θ)\n    μx ~ Normal()\n    μy ~ Normal()\n    μ = atan(μy, μx)\n    lp += logpdf(VonMises(μ), θ) # is there a syntax for this?\nend"}]},{"type":"rich_text_section","elements":[]}]}],"client_msg_id":"eaebf69d-e77b-4b42-9a18-927892bb3470"},{"client_msg_id":"5fdcced9-aa47-46ff-bcd2-e2e5c8fbe98e","type":"message","text":"Could someone help with : <https://discourse.julialang.org/t/sampling-from-power-posterior/54924>","user":"U7QLM6E2E","ts":"1612881181.074600","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"Sampling from power posterior","title_link":"https://discourse.julialang.org/t/sampling-from-power-posterior/54924","text":"Hey, I was wondering if someone could guide me (or direct me to the relevant code) on how to sample from the power posterior given a Turing model. By power posterior, I mean the posterior to modified joint : p(x|y; b) \\propto p(y|x)^b p(x), where b \\in [0,1].","fallback":"JuliaLang: Sampling from power posterior","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","fields":[{"title":"Reading time","value":"1 mins :clock2:","short":true},{"title":"Likes","value":"3 :heart:","short":true}],"ts":1612876144,"from_url":"https://discourse.julialang.org/t/sampling-from-power-posterior/54924","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/sampling-from-power-posterior/54924"}],"blocks":[{"type":"rich_text","block_id":"MNZsE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Could someone help with : "},{"type":"link","url":"https://discourse.julialang.org/t/sampling-from-power-posterior/54924"}]}]}],"thread_ts":"1612881181.074600","reply_count":4,"reply_users_count":2,"latest_reply":"1612882233.075600","reply_users":["UC0SY9JFP","U7QLM6E2E"],"subscribed":false},{"client_msg_id":"2ed1f7fd-5b7e-4550-a5fd-b159646976cb","type":"message","text":"I think the searchbar on <http://turing.ml|turing.ml> is broken btw.","user":"U01C2AJ9F63","ts":"1612963752.082700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6qbz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think the searchbar on "},{"type":"link","url":"http://turing.ml","text":"turing.ml"},{"type":"text","text":" is broken btw."}]}]}],"reactions":[{"name":"confused","users":["UC0SY9JFP"],"count":1}]},{"client_msg_id":"a532a28b-272a-4f9d-8d89-4f61f7e48edf","type":"message","text":"multiple hierarchical models with different sample sizes","user":"U01HSP1E1NW","ts":"1613004206.084100","team":"T68168MUP","edited":{"user":"U01HSP1E1NW","ts":"1613004670.000000"},"blocks":[{"type":"rich_text","block_id":"y7F","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"multiple hierarchical models with different sample sizes"}]}]}],"thread_ts":"1613004206.084100","reply_count":1,"reply_users_count":1,"latest_reply":"1613004561.084900","reply_users":["U01HSP1E1NW"],"subscribed":false},{"client_msg_id":"177374a5-6705-43ac-a9f5-6a51e9a79639","type":"message","text":"At the suggestion of someone very helpful at “helpdesk” I’m writing to ask about using Turing for Bayesian linear regression.  I’ve found the great documentation example: <https://turing.ml/dev/tutorials/5-linearregression/> and am working to use that a starter for the problem I’m looking to solve.\n\nThat problem is a Bayesian linear regression case where the with priors on the parameters are drawn from an underlying power-law distribution.  But I haven’t been able to find a power law distribution in Distributions.jl so I’m not sure how to go about trying implement this.\n\nThoughts and guidance much appreciated!","user":"UHRBR18HH","ts":"1613016923.085400","team":"T68168MUP","attachments":[{"title":"Linear Regression","title_link":"https://turing.ml/dev/tutorials/5-linearregression/","text":"Linear Regression","fallback":"Linear Regression","from_url":"https://turing.ml/dev/tutorials/5-linearregression/","service_icon":"https://turing.ml/dev/assets/img/favicon.ico","service_name":"turing.ml","id":1,"original_url":"https://turing.ml/dev/tutorials/5-linearregression/"}],"blocks":[{"type":"rich_text","block_id":"0+4yd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"At the suggestion of someone very helpful at “helpdesk” I’m writing to ask about using Turing for Bayesian linear regression.  I’ve found the great documentation example: "},{"type":"link","url":"https://turing.ml/dev/tutorials/5-linearregression/"},{"type":"text","text":" and am working to use that a starter for the problem I’m looking to solve.\n\nThat problem is a Bayesian linear regression case where the with priors on the parameters are drawn from an underlying power-law distribution.  But I haven’t been able to find a power law distribution in Distributions.jl so I’m not sure how to go about trying implement this.\n\nThoughts and guidance much appreciated!"}]}]}]},{"client_msg_id":"e103a083-9064-40e0-872e-3b8bd7befdee","type":"message","text":"quick question: can variational inference benefit from multithreading?","user":"U01M641BZEY","ts":"1613070473.090000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"W+YL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"quick question: can variational inference benefit from multithreading?"}]}]}]},{"client_msg_id":"869be028-f8de-486b-bd75-e21fc35b0711","type":"message","text":"Is there a way to infer from a Turing model if the likelihood is factorizable, i.e.  : `p(y|x) = \\prod_i p(y|x_i)` ?","user":"U7QLM6E2E","ts":"1613125563.093900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OFy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to infer from a Turing model if the likelihood is factorizable, i.e.  : "},{"type":"text","text":"p(y|x) = \\prod_i p(y|x_i)","style":{"code":true}},{"type":"text","text":" ?"}]}]}],"thread_ts":"1613125563.093900","reply_count":3,"reply_users_count":2,"latest_reply":"1613125764.094400","reply_users":["UC0SY9JFP","U7QLM6E2E"],"subscribed":false},{"client_msg_id":"255dcfc8-baf8-4c14-adf5-0ebb1174849a","type":"message","text":"Is there currently an predefined dense MvNormal variational posterior that I can set using something like vi(model, advi, q=DenseMvNormal) ?","user":"U01M641BZEY","ts":"1613149586.096000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jC8L","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there currently an predefined dense MvNormal variational posterior that I can set using something like vi(model, advi, q=DenseMvNormal) ?"}]}]}],"thread_ts":"1613149586.096000","reply_count":1,"reply_users_count":1,"latest_reply":"1613149665.096100","reply_users":["U01M641BZEY"],"subscribed":false}]}