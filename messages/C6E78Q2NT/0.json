{"cursor": 0, "messages": [{"client_msg_id":"8f5e66fa-9e14-47bf-8639-50a99b22bd38","type":"message","text":"<https://github.com/JuliaParallel/ClusterManagers.jl/pull/153>\nis there someone willing to review this?","user":"UH8A351DJ","ts":"1608713958.180000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zMXNb","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/JuliaParallel/ClusterManagers.jl/pull/153"},{"type":"text","text":"\nis there someone willing to review this?"}]}]}]},{"client_msg_id":"c7983177-ede9-4d6a-9cae-28c8f25b4801","type":"message","text":"I have a large amount of data which is needed by a dozen of different processes (running on the same machine) as a base for calculations. At the moment this is a set of arrays, each one ~500Mb .\nI am thinking of using SharedArrays so that I don't have to load a copy of the data in each process. Any better solution?","user":"UKNN7CJ86","ts":"1610557179.188400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xx2+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a large amount of data which is needed by a dozen of different processes (running on the same machine) as a base for calculations. At the moment this is a set of arrays, each one ~500Mb .\nI am thinking of using SharedArrays so that I don't have to load a copy of the data in each process. Any better solution?"}]}]}]},{"client_msg_id":"7acf43be-132e-445e-902a-9b4295f72068","type":"message","text":"Seems like a good way to go","user":"U67461GUB","ts":"1610557617.188700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bMFS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Seems like a good way to go"}]}]}]},{"client_msg_id":"e9762f36-f423-4b12-9ae0-aaf2291996c2","type":"message","text":"Thanks <@U67461GUB>!","user":"UKNN7CJ86","ts":"1610558472.188900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1Y0a","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks "},{"type":"user","user_id":"U67461GUB"},{"type":"text","text":"!"}]}]}]},{"client_msg_id":"51ab4246-cdf8-4b64-bb6f-272e1f79c7d9","type":"message","text":"Could someone explain to me how to use a custom reduction function for @distributed loops? I’m trying to do something like\n```using Distributed\nxs = [1]\nreduce_x(xs,x) = x in xs ? xs : push!(xs,x)\nxs = @distributed (reduce_x) for _ in 1:10\n    rand(1:20)   # some costly function\nend```\nSo that a costly functions is executed in parallel on some workers, and the main process reduces those with a custom function `reduce_x` that extends an array `xs` in case the costly function returned a new unique result.","user":"UHZUXBR2P","ts":"1611838549.004000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"krOVQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Could someone explain to me how to use a custom reduction function for @distributed loops? I’m trying to do something like\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Distributed\nxs = [1]\nreduce_x(xs,x) = x in xs ? xs : push!(xs,x)\nxs = @distributed (reduce_x) for _ in 1:10\n    rand(1:20)   # some costly function\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"So that a costly functions is executed in parallel on some workers, and the main process reduces those with a custom function "},{"type":"text","text":"reduce_x","style":{"code":true}},{"type":"text","text":" that extends an array "},{"type":"text","text":"xs","style":{"code":true}},{"type":"text","text":" in case the costly function returned a new unique result."}]}]}],"thread_ts":"1611838549.004000","reply_count":1,"reply_users_count":1,"latest_reply":"1611839711.004100","reply_users":["UHZUXBR2P"],"subscribed":false},{"client_msg_id":"5b7202f1-cb46-49f0-b3d7-749d76680a19","type":"message","text":"For people who test packages that use MPI.jl, how do you design rank-specific tests? This seems to work but feels a little clunky\n```if MPI.Comm_rank(MPI.COMM_WORLD) == 0\n    # @test rank 0 stuff...\nelseif MPI.Comm_rank(MPI.COMM_WORLD) == 1\n    # @test rank 1 stuff...\n...\nend```","user":"UEP056STX","ts":"1612533263.007300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VdM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"For people who test packages that use MPI.jl, how do you design rank-specific tests? This seems to work but feels a little clunky\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"if MPI.Comm_rank(MPI.COMM_WORLD) == 0\n    # @test rank 0 stuff...\nelseif MPI.Comm_rank(MPI.COMM_WORLD) == 1\n    # @test rank 1 stuff...\n...\nend"}]}]}],"thread_ts":"1612533263.007300","reply_count":1,"reply_users_count":1,"latest_reply":"1612534435.007400","reply_users":["UGU761DU2"],"subscribed":false},{"type":"message","subtype":"tombstone","text":"This message was deleted.","user":"USLACKBOT","hidden":true,"ts":"1612555862.010400","thread_ts":"1612555862.010400","reply_count":1,"reply_users_count":1,"latest_reply":"1612555935.010500","reply_users":["U6A0PD8CR"],"subscribed":false},{"client_msg_id":"3a304a06-931a-405f-885a-5e22666967ea","type":"message","text":"Hi, I am having trouble with some basic parallel programming in Julia, and I was hoping someone could point out my misunderstanding here. I have the following example\n`using Distributed`\n`addprocs(1)`\n`@everywhere using JLD2`\n`@everywhere include(\"file1.jl\"); # contains foo()`\n`@everywhere include(\"file2.jl\"); # contains bar`\n\n`# @spawnat 2 @load wasn't working`\n`@load \"data.jld2\" Z1 Z2`\n`put!(RemoteChannel(2), Z1)`\n`put!(RemoteChannel(2), Z2)`\n\n`# variables a..c created with \"a = @spawnat 2 ...\"`\n`@spawnat foo(bar, a, b, c, true, 'Z')`","user":"U01655DR0JZ","ts":"1612556057.012600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"x2KpO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I am having trouble with some basic parallel programming in Julia, and I was hoping someone could point out my misunderstanding here. I have the following example\n"},{"type":"text","text":"using Distributed","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"addprocs(1)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"@everywhere using JLD2","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"@everywhere include(\"file1.jl\"); # contains foo()","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"@everywhere include(\"file2.jl\"); # contains bar","style":{"code":true}},{"type":"text","text":"\n\n"},{"type":"text","text":"# @spawnat 2 @load wasn't working","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"@load \"data.jld2\" Z1 Z2","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"put!(RemoteChannel(2), Z1)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"put!(RemoteChannel(2), Z2)","style":{"code":true}},{"type":"text","text":"\n\n"},{"type":"text","text":"# variables a..c created with \"a = @spawnat 2 ...\"","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"@spawnat foo(bar, a, b, c, true, 'Z')","style":{"code":true}}]}]}]},{"client_msg_id":"4ebb214e-91c9-424c-9f92-8741db5ea314","type":"message","text":"If I have\n`wait(@spawnat 2 x2 = f())`\n`wait(@spawnat 2 x2 = g(x2))`\n`x1 = f(x)`\n`x1 = g(x1)`\n`x1 + fetch(x2)`\nare the commands on worker 1 running simultaneously with the commands on worker 2 or has `wait` “paused” it?","user":"U01655DR0JZ","ts":"1612718554.016300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Uz4L","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If I have\n"},{"type":"text","text":"wait(@spawnat 2 x2 = f())","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"wait(@spawnat 2 x2 = g(x2))","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"x1 = f(x)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"x1 = g(x1)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"x1 + fetch(x2)","style":{"code":true}},{"type":"text","text":"\nare the commands on worker 1 running simultaneously with the commands on worker 2 or has "},{"type":"text","text":"wait","style":{"code":true}},{"type":"text","text":" “paused” it?"}]}]}]},{"client_msg_id":"fa7ced5e-b2cb-4a4c-98f2-9451ad1e56a3","type":"message","text":"I’m having issues with some basic stuff:\n```(partest) pkg&gt; st\nProject partest v0.1.0\nStatus `~/repos/scratch/julia/partest/Project.toml`\n  [6e4b80f9] BenchmarkTools v0.5.0\n  [a93c6f00] DataFrames v0.22.5\n  [38e38edf] GLM v1.3.11\n  [ff71e718] MixedModels v3.1.5\n  [91a5bcdd] Plots v1.10.4\n  [f3b207a7] StatsPlots v0.14.19\n\njulia&gt; using GLM\n\njulia&gt; @everywhere using GLM\nERROR: On worker 2:\nArgumentError: Package GLM not found in current path:\n- Run `import Pkg; Pkg.add(\"GLM\")` to install the GLM package.\n\nrequire at ./loading.jl:893\neval at ./boot.jl:331\n#103 at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Distributed/src/process_messages.jl:290\nrun_work_thunk at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Distributed/src/process_messages.jl:79\nrun_work_thunk at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Distributed/src/process_messages.jl:88\n#96 at ./task.jl:356\n\n...and 2 more exception(s).\n\nStacktrace:\n [1] sync_end(::Channel{Any}) at ./task.jl:314\n [2] macro expansion at ./task.jl:333 [inlined]\n [3] remotecall_eval(::Module, ::Array{Int64,1}, ::Expr) at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Distributed/src/macros.jl:218\n [4] top-level scope at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Distributed/src/macros.jl:202\n\njulia&gt; versioninfo()\nJulia Version 1.5.2\nCommit 539f3ce943 (2020-09-23 23:17 UTC)\nPlatform Info:\n  OS: macOS (x86_64-apple-darwin18.7.0)\n  CPU: Intel(R) Core(TM) i7-8700B CPU @ 3.20GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)```","user":"U017JTQFNEQ","ts":"1612940309.021200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"50WEr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’m having issues with some basic stuff:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"(partest) pkg> st\nProject partest v0.1.0\nStatus `~/repos/scratch/julia/partest/Project.toml`\n  [6e4b80f9] BenchmarkTools v0.5.0\n  [a93c6f00] DataFrames v0.22.5\n  [38e38edf] GLM v1.3.11\n  [ff71e718] MixedModels v3.1.5\n  [91a5bcdd] Plots v1.10.4\n  [f3b207a7] StatsPlots v0.14.19\n\njulia> using GLM\n\njulia> @everywhere using GLM\nERROR: On worker 2:\nArgumentError: Package GLM not found in current path:\n- Run `import Pkg; Pkg.add(\"GLM\")` to install the GLM package.\n\nrequire at ./loading.jl:893\neval at ./boot.jl:331\n#103 at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Distributed/src/process_messages.jl:290\nrun_work_thunk at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Distributed/src/process_messages.jl:79\nrun_work_thunk at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Distributed/src/process_messages.jl:88\n#96 at ./task.jl:356\n\n...and 2 more exception(s).\n\nStacktrace:\n [1] sync_end(::Channel{Any}) at ./task.jl:314\n [2] macro expansion at ./task.jl:333 [inlined]\n [3] remotecall_eval(::Module, ::Array{Int64,1}, ::Expr) at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Distributed/src/macros.jl:218\n [4] top-level scope at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Distributed/src/macros.jl:202\n\njulia> versioninfo()\nJulia Version 1.5.2\nCommit 539f3ce943 (2020-09-23 23:17 UTC)\nPlatform Info:\n  OS: macOS (x86_64-apple-darwin18.7.0)\n  CPU: Intel(R) Core(TM) i7-8700B CPU @ 3.20GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)"}]}]}]},{"client_msg_id":"0b9cbdaa-14fb-4bce-8b40-8e66cc20b5ae","type":"message","text":"Not sure if this is the right/best place for this, but does anyone know of a web site that discusses some of the features/capabilities/nuances of Parallel HDF5 and use cases for when (and when not) to use it?  Some of the logistics around which/when operations need to be collective and which/when they can be independent would be very helpful.  I've also encountered some limitations of Parallel HDF5 and would like to know how others work within these limits (or if I'm just doing things improperly).  The two I've found so far are:\n1. Parallel HDF5 does not seem to support variable length strings, but fixed strings are OK.  I've worked around this by padding my strings to a fixed length and converting them to `SVector{N,UInt8}`.  That works, but maybe there's an easier way?\n2. Parallel HDF5 does not seem to support using filters (e.g. compression) when performing independent writes.  This seems to be true even when the writes are all in units of complete chunks.  I think I could use filters if I were doing collective writes, but in some cases the writes are very independent and forcing them to be collective seems rather awkward (which might be suggesting that parallel HDF5 is not the right tool for these cases).\nFWIW, I think these are both limitations of HDF5 itself (i.e. not specific to HDF5.jl).","user":"U01FKQQ7J0J","ts":"1612986144.036000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=+fQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not sure if this is the right/best place for this, but does anyone know of a web site that discusses some of the features/capabilities/nuances of Parallel HDF5 and use cases for when (and when not) to use it?  Some of the logistics around which/when operations need to be collective and which/when they can be independent would be very helpful.  I've also encountered some limitations of Parallel HDF5 and would like to know how others work within these limits (or if I'm just doing things improperly).  The two I've found so far are:\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Parallel HDF5 does not seem to support variable length strings, but fixed strings are OK.  I've worked around this by padding my strings to a fixed length and converting them to "},{"type":"text","text":"SVector{N,UInt8}","style":{"code":true}},{"type":"text","text":".  That works, but maybe there's an easier way?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Parallel HDF5 does not seem to support using filters (e.g. compression) when performing independent writes.  This seems to be true even when the writes are all in units of complete chunks.  I think I could use filters if I were doing collective writes, but in some cases the writes are very independent and forcing them to be collective seems rather awkward (which might be suggesting that parallel HDF5 is not the right tool for these cases)."}]}],"style":"ordered","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"FWIW, I think these are both limitations of HDF5 itself (i.e. not specific to HDF5.jl)."}]}]}]},{"client_msg_id":"9f8b68ce-7cba-46a3-9f57-a36c93bd77fe","type":"message","text":"1. That seems to be correct according to <https://support.hdfgroup.org/HDF5/doc/TechNotes/VLTypes.html>. You might be able to convert it to `FixedString` (a HDF5.jl type)","user":"U68P09RFZ","ts":"1613064474.037500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"v+9","elements":[{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That seems to be correct according to "},{"type":"link","url":"https://support.hdfgroup.org/HDF5/doc/TechNotes/VLTypes.html"},{"type":"text","text":". You might be able to convert it to "},{"type":"text","text":"FixedString","style":{"code":true}},{"type":"text","text":" (a HDF5.jl type)"}]}],"style":"ordered","indent":0}]}]},{"client_msg_id":"15ee84ec-f753-4c76-967f-b6b385f04f2f","type":"message","text":"2. It should be possible with the latest hdf5 release (<https://forum.hdfgroup.org/t/parallel-i-o-does-not-support-filters-yet/884>)","user":"U68P09RFZ","ts":"1613064559.038000","team":"T68168MUP","attachments":[{"service_name":"HDF Forum","title":"Parallel I/O does not support filters yet","title_link":"https://forum.hdfgroup.org/t/parallel-i-o-does-not-support-filters-yet/884","text":"Hi, I’ve got this error message (in the subject of this topic) when I try to setup compression on datasets in the MPI environment. Is there any workaround to compress datasets in parallel I/O or any plans to support this in C API? Regards, Rafal","fallback":"HDF Forum: Parallel I/O does not support filters yet","thumb_url":"https://dbkt.hdfgroup.org/original/1X/5fc85e124523a961f1933b7843c6d80f3d26b72a.png","from_url":"https://forum.hdfgroup.org/t/parallel-i-o-does-not-support-filters-yet/884","thumb_width":512,"thumb_height":512,"service_icon":"https://dbkt.hdfgroup.org/original/1X/5fc85e124523a961f1933b7843c6d80f3d26b72a.png","id":1,"original_url":"https://forum.hdfgroup.org/t/parallel-i-o-does-not-support-filters-yet/884"}],"blocks":[{"type":"rich_text","block_id":"qx2RQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"2. It should be possible with the latest hdf5 release ("},{"type":"link","url":"https://forum.hdfgroup.org/t/parallel-i-o-does-not-support-filters-yet/884"},{"type":"text","text":")"}]}]}]},{"client_msg_id":"a9d794c5-f139-48fb-a05f-59709acdad01","type":"message","text":"Another elementary question here. I am doing the following\n```@everywhere include(\"file.jl\") # loads f()\n@everywhere x = load(\"file.jld2\")\ny = @distributed (+) for i in 1:100000\n    f(x)\nend```\nbut it complains that worker 2  does not know about `x`. So before the loop I had every worker print out the value `x[1]`and that worked. There’s something about entering @distributed which makes it invisible again. I also tried `f(fetch(x))`but that didn’t work. Thoughts?","user":"U01655DR0JZ","ts":"1613076341.054900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"P4Fx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Another elementary question here. I am doing the following\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@everywhere include(\"file.jl\") # loads f()\n@everywhere x = load(\"file.jld2\")\ny = @distributed (+) for i in 1:100000\n    f(x)\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"but it complains that worker 2  does not know about "},{"type":"text","text":"x","style":{"code":true}},{"type":"text","text":". So before the loop I had every worker print out the value "},{"type":"text","text":"x[1]","style":{"code":true}},{"type":"text","text":"and that worked. There’s something about entering @distributed which makes it invisible again. I also tried "},{"type":"text","text":"f(fetch(x))","style":{"code":true}},{"type":"text","text":"but that didn’t work. Thoughts?"}]}]}]},{"client_msg_id":"81eb01cf-65bd-40bd-a531-8cf9c068f81b","type":"message","text":"Try doing `global x; f(x)` in your loop","user":"U6A0PD8CR","ts":"1613079475.055700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"b0S","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Try doing "},{"type":"text","text":"global x; f(x)","style":{"code":true}},{"type":"text","text":" in your loop"}]}]}]},{"client_msg_id":"67025b6c-d76f-4a68-89ed-0a4d60cae87e","type":"message","text":"`@everywhere` executes in global scope, so `x` is global","user":"U6A0PD8CR","ts":"1613079490.056100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kFTh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"@everywhere","style":{"code":true}},{"type":"text","text":" executes in global scope, so "},{"type":"text","text":"x","style":{"code":true}},{"type":"text","text":" is global"}]}]}]},{"client_msg_id":"618d0d34-92e6-483e-9ed6-db94cdd23d0b","type":"message","text":"What probably happens is that `@distributed` constructs a function containing the loop body, so the `x` in there, being global, isn't accessible","user":"U6A0PD8CR","ts":"1613079581.056800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GtkE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What probably happens is that "},{"type":"text","text":"@distributed","style":{"code":true}},{"type":"text","text":" constructs a function containing the loop body, so the "},{"type":"text","text":"x","style":{"code":true}},{"type":"text","text":" in there, being global, isn't accessible"}]}]}],"reactions":[{"name":"+1","users":["U7THT3TM3"],"count":1}]},{"client_msg_id":"cb17b800-8fac-46dd-bead-90c5130f6ebe","type":"message","text":"If that fixes it, I would recommend adjusting your program so that you never have to do `global x`. It's slow and introduces dynamic dispatch where it's not needed. Instead, create a function, say `g(x)` (using `@everywhere` to create it on all workers), which contains the `@distributed` loop, and just call it on the head node as `y = g(x)`.","user":"U6A0PD8CR","ts":"1613079716.059100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Odr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If that fixes it, I would recommend adjusting your program so that you never have to do "},{"type":"text","text":"global x","style":{"code":true}},{"type":"text","text":". It's slow and introduces dynamic dispatch where it's not needed. Instead, create a function, say "},{"type":"text","text":"g(x)","style":{"code":true}},{"type":"text","text":" (using "},{"type":"text","text":"@everywhere","style":{"code":true}},{"type":"text","text":" to create it on all workers), which contains the "},{"type":"text","text":"@distributed","style":{"code":true}},{"type":"text","text":" loop, and just call it on the head node as "},{"type":"text","text":"y = g(x)","style":{"code":true}},{"type":"text","text":"."}]}]}]},{"client_msg_id":"048a73db-91fc-48cd-8617-fe1cc0bfdcbd","type":"message","text":"Actually, you probably don't need to make `g(x)` with `@everywhere`, because it's only called on the head node","user":"U6A0PD8CR","ts":"1613079764.059800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Cgb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Actually, you probably don't need to make "},{"type":"text","text":"g(x)","style":{"code":true}},{"type":"text","text":" with "},{"type":"text","text":"@everywhere","style":{"code":true}},{"type":"text","text":", because it's only called on the head node"}]}]}]},{"client_msg_id":"eb267fe1-4400-4425-adb2-868bef09f02b","type":"message","text":"<@U6A0PD8CR> thanks for your dagger ml presentation. I had to leave early, but is the idea that even in models with a  serial dependency structure you can just start a new forward pass as long as the first gpu is cleared ?","user":"UDGT4PM41","ts":"1613143198.062900","team":"T68168MUP","edited":{"user":"UDGT4PM41","ts":"1613143213.000000"},"blocks":[{"type":"rich_text","block_id":"wmP","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U6A0PD8CR"},{"type":"text","text":" thanks for your dagger ml presentation. I had to leave early, but is the idea that even in models with a  serial dependency structure you can just start a new forward pass as long as the first gpu is cleared ?"}]}]}]},{"type":"message","subtype":"channel_join","ts":"1613151214.064700","user":"UH9KWTTD3","text":"<@UH9KWTTD3> has joined the channel","inviter":"UDGT4PM41"},{"client_msg_id":"a30f512d-a5bc-4164-a678-b8c046a6fea9","type":"message","text":"Hello.\nAnyone has a good idea for a (recent?) review on parallel many-particle simulations (with interacting particles)?\nSorry for this slightly off-topic question.","user":"U01HC60USTH","ts":"1613152675.071400","team":"T68168MUP","edited":{"user":"U01HC60USTH","ts":"1613153087.000000"},"blocks":[{"type":"rich_text","block_id":"RsKUT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello.\nAnyone has a good idea for a (recent?) review on parallel many-particle simulations (with interacting particles)?\nSorry for this slightly off-topic question."}]}]}]},{"client_msg_id":"a69e0e0f-45d1-4922-bd8f-05cb2c2a9d47","type":"message","text":"Hi, I'm sure I'm being stupid but for some reason I can't increase the number of MPI processes beyond 127 using MPI.jl, which sounds a suspiciously `char`-y kind of number to fail at. I assume that other people have gone beyond this number without problems?","user":"U7C8KRZKL","ts":"1613573526.073600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tWkD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I'm sure I'm being stupid but for some reason I can't increase the number of MPI processes beyond 127 using MPI.jl, which sounds a suspiciously "},{"type":"text","text":"char","style":{"code":true}},{"type":"text","text":"-y kind of number to fail at. I assume that other people have gone beyond this number without problems?"}]}]}]}]}