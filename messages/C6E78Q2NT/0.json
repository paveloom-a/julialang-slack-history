{"cursor": 0, "messages": [{"client_msg_id":"8f5e66fa-9e14-47bf-8639-50a99b22bd38","type":"message","text":"<https://github.com/JuliaParallel/ClusterManagers.jl/pull/153>\nis there someone willing to review this?","user":"UH8A351DJ","ts":"1608713958.180000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zMXNb","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/JuliaParallel/ClusterManagers.jl/pull/153"},{"type":"text","text":"\nis there someone willing to review this?"}]}]}]},{"client_msg_id":"c7983177-ede9-4d6a-9cae-28c8f25b4801","type":"message","text":"I have a large amount of data which is needed by a dozen of different processes (running on the same machine) as a base for calculations. At the moment this is a set of arrays, each one ~500Mb .\nI am thinking of using SharedArrays so that I don't have to load a copy of the data in each process. Any better solution?","user":"UKNN7CJ86","ts":"1610557179.188400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xx2+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a large amount of data which is needed by a dozen of different processes (running on the same machine) as a base for calculations. At the moment this is a set of arrays, each one ~500Mb .\nI am thinking of using SharedArrays so that I don't have to load a copy of the data in each process. Any better solution?"}]}]}]},{"client_msg_id":"7acf43be-132e-445e-902a-9b4295f72068","type":"message","text":"Seems like a good way to go","user":"U67461GUB","ts":"1610557617.188700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bMFS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Seems like a good way to go"}]}]}]},{"client_msg_id":"e9762f36-f423-4b12-9ae0-aaf2291996c2","type":"message","text":"Thanks <@U67461GUB>!","user":"UKNN7CJ86","ts":"1610558472.188900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1Y0a","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks "},{"type":"user","user_id":"U67461GUB"},{"type":"text","text":"!"}]}]}]},{"client_msg_id":"51ab4246-cdf8-4b64-bb6f-272e1f79c7d9","type":"message","text":"Could someone explain to me how to use a custom reduction function for @distributed loops? I’m trying to do something like\n```using Distributed\nxs = [1]\nreduce_x(xs,x) = x in xs ? xs : push!(xs,x)\nxs = @distributed (reduce_x) for _ in 1:10\n    rand(1:20)   # some costly function\nend```\nSo that a costly functions is executed in parallel on some workers, and the main process reduces those with a custom function `reduce_x` that extends an array `xs` in case the costly function returned a new unique result.","user":"UHZUXBR2P","ts":"1611838549.004000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"krOVQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Could someone explain to me how to use a custom reduction function for @distributed loops? I’m trying to do something like\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Distributed\nxs = [1]\nreduce_x(xs,x) = x in xs ? xs : push!(xs,x)\nxs = @distributed (reduce_x) for _ in 1:10\n    rand(1:20)   # some costly function\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"So that a costly functions is executed in parallel on some workers, and the main process reduces those with a custom function "},{"type":"text","text":"reduce_x","style":{"code":true}},{"type":"text","text":" that extends an array "},{"type":"text","text":"xs","style":{"code":true}},{"type":"text","text":" in case the costly function returned a new unique result."}]}]}],"thread_ts":"1611838549.004000","reply_count":1,"reply_users_count":1,"latest_reply":"1611839711.004100","reply_users":["UHZUXBR2P"],"subscribed":false},{"client_msg_id":"5b7202f1-cb46-49f0-b3d7-749d76680a19","type":"message","text":"For people who test packages that use MPI.jl, how do you design rank-specific tests? This seems to work but feels a little clunky\n```if MPI.Comm_rank(MPI.COMM_WORLD) == 0\n    # @test rank 0 stuff...\nelseif MPI.Comm_rank(MPI.COMM_WORLD) == 1\n    # @test rank 1 stuff...\n...\nend```","user":"UEP056STX","ts":"1612533263.007300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VdM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"For people who test packages that use MPI.jl, how do you design rank-specific tests? This seems to work but feels a little clunky\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"if MPI.Comm_rank(MPI.COMM_WORLD) == 0\n    # @test rank 0 stuff...\nelseif MPI.Comm_rank(MPI.COMM_WORLD) == 1\n    # @test rank 1 stuff...\n...\nend"}]}]}],"thread_ts":"1612533263.007300","reply_count":1,"reply_users_count":1,"latest_reply":"1612534435.007400","reply_users":["UGU761DU2"],"subscribed":false},{"type":"message","subtype":"tombstone","text":"This message was deleted.","user":"USLACKBOT","hidden":true,"ts":"1612555862.010400","thread_ts":"1612555862.010400","reply_count":1,"reply_users_count":1,"latest_reply":"1612555935.010500","reply_users":["U6A0PD8CR"],"subscribed":false},{"client_msg_id":"3a304a06-931a-405f-885a-5e22666967ea","type":"message","text":"Hi, I am having trouble with some basic parallel programming in Julia, and I was hoping someone could point out my misunderstanding here. I have the following example\n`using Distributed`\n`addprocs(1)`\n`@everywhere using JLD2`\n`@everywhere include(\"file1.jl\"); # contains foo()`\n`@everywhere include(\"file2.jl\"); # contains bar`\n\n`# @spawnat 2 @load wasn't working`\n`@load \"data.jld2\" Z1 Z2`\n`put!(RemoteChannel(2), Z1)`\n`put!(RemoteChannel(2), Z2)`\n\n`# variables a..c created with \"a = @spawnat 2 ...\"`\n`@spawnat foo(bar, a, b, c, true, 'Z')`","user":"U01655DR0JZ","ts":"1612556057.012600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"x2KpO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I am having trouble with some basic parallel programming in Julia, and I was hoping someone could point out my misunderstanding here. I have the following example\n"},{"type":"text","text":"using Distributed","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"addprocs(1)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"@everywhere using JLD2","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"@everywhere include(\"file1.jl\"); # contains foo()","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"@everywhere include(\"file2.jl\"); # contains bar","style":{"code":true}},{"type":"text","text":"\n\n"},{"type":"text","text":"# @spawnat 2 @load wasn't working","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"@load \"data.jld2\" Z1 Z2","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"put!(RemoteChannel(2), Z1)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"put!(RemoteChannel(2), Z2)","style":{"code":true}},{"type":"text","text":"\n\n"},{"type":"text","text":"# variables a..c created with \"a = @spawnat 2 ...\"","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"@spawnat foo(bar, a, b, c, true, 'Z')","style":{"code":true}}]}]}]},{"client_msg_id":"4ebb214e-91c9-424c-9f92-8741db5ea314","type":"message","text":"If I have\n`wait(@spawnat 2 x2 = f())`\n`wait(@spawnat 2 x2 = g(x2))`\n`x1 = f(x)`\n`x1 = g(x1)`\n`x1 + fetch(x2)`\nare the commands on worker 1 running simultaneously with the commands on worker 2 or has `wait` “paused” it?","user":"U01655DR0JZ","ts":"1612718554.016300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Uz4L","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If I have\n"},{"type":"text","text":"wait(@spawnat 2 x2 = f())","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"wait(@spawnat 2 x2 = g(x2))","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"x1 = f(x)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"x1 = g(x1)","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"x1 + fetch(x2)","style":{"code":true}},{"type":"text","text":"\nare the commands on worker 1 running simultaneously with the commands on worker 2 or has "},{"type":"text","text":"wait","style":{"code":true}},{"type":"text","text":" “paused” it?"}]}]}]},{"client_msg_id":"fa7ced5e-b2cb-4a4c-98f2-9451ad1e56a3","type":"message","text":"I’m having issues with some basic stuff:\n```(partest) pkg&gt; st\nProject partest v0.1.0\nStatus `~/repos/scratch/julia/partest/Project.toml`\n  [6e4b80f9] BenchmarkTools v0.5.0\n  [a93c6f00] DataFrames v0.22.5\n  [38e38edf] GLM v1.3.11\n  [ff71e718] MixedModels v3.1.5\n  [91a5bcdd] Plots v1.10.4\n  [f3b207a7] StatsPlots v0.14.19\n\njulia&gt; using GLM\n\njulia&gt; @everywhere using GLM\nERROR: On worker 2:\nArgumentError: Package GLM not found in current path:\n- Run `import Pkg; Pkg.add(\"GLM\")` to install the GLM package.\n\nrequire at ./loading.jl:893\neval at ./boot.jl:331\n#103 at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Distributed/src/process_messages.jl:290\nrun_work_thunk at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Distributed/src/process_messages.jl:79\nrun_work_thunk at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Distributed/src/process_messages.jl:88\n#96 at ./task.jl:356\n\n...and 2 more exception(s).\n\nStacktrace:\n [1] sync_end(::Channel{Any}) at ./task.jl:314\n [2] macro expansion at ./task.jl:333 [inlined]\n [3] remotecall_eval(::Module, ::Array{Int64,1}, ::Expr) at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Distributed/src/macros.jl:218\n [4] top-level scope at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Distributed/src/macros.jl:202\n\njulia&gt; versioninfo()\nJulia Version 1.5.2\nCommit 539f3ce943 (2020-09-23 23:17 UTC)\nPlatform Info:\n  OS: macOS (x86_64-apple-darwin18.7.0)\n  CPU: Intel(R) Core(TM) i7-8700B CPU @ 3.20GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)```","user":"U017JTQFNEQ","ts":"1612940309.021200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"50WEr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’m having issues with some basic stuff:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"(partest) pkg> st\nProject partest v0.1.0\nStatus `~/repos/scratch/julia/partest/Project.toml`\n  [6e4b80f9] BenchmarkTools v0.5.0\n  [a93c6f00] DataFrames v0.22.5\n  [38e38edf] GLM v1.3.11\n  [ff71e718] MixedModels v3.1.5\n  [91a5bcdd] Plots v1.10.4\n  [f3b207a7] StatsPlots v0.14.19\n\njulia> using GLM\n\njulia> @everywhere using GLM\nERROR: On worker 2:\nArgumentError: Package GLM not found in current path:\n- Run `import Pkg; Pkg.add(\"GLM\")` to install the GLM package.\n\nrequire at ./loading.jl:893\neval at ./boot.jl:331\n#103 at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Distributed/src/process_messages.jl:290\nrun_work_thunk at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Distributed/src/process_messages.jl:79\nrun_work_thunk at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Distributed/src/process_messages.jl:88\n#96 at ./task.jl:356\n\n...and 2 more exception(s).\n\nStacktrace:\n [1] sync_end(::Channel{Any}) at ./task.jl:314\n [2] macro expansion at ./task.jl:333 [inlined]\n [3] remotecall_eval(::Module, ::Array{Int64,1}, ::Expr) at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Distributed/src/macros.jl:218\n [4] top-level scope at /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.5/Distributed/src/macros.jl:202\n\njulia> versioninfo()\nJulia Version 1.5.2\nCommit 539f3ce943 (2020-09-23 23:17 UTC)\nPlatform Info:\n  OS: macOS (x86_64-apple-darwin18.7.0)\n  CPU: Intel(R) Core(TM) i7-8700B CPU @ 3.20GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-9.0.1 (ORCJIT, skylake)"}]}]}]},{"client_msg_id":"0b9cbdaa-14fb-4bce-8b40-8e66cc20b5ae","type":"message","text":"Not sure if this is the right/best place for this, but does anyone know of a web site that discusses some of the features/capabilities/nuances of Parallel HDF5 and use cases for when (and when not) to use it?  Some of the logistics around which/when operations need to be collective and which/when they can be independent would be very helpful.  I've also encountered some limitations of Parallel HDF5 and would like to know how others work within these limits (or if I'm just doing things improperly).  The two I've found so far are:\n1. Parallel HDF5 does not seem to support variable length strings, but fixed strings are OK.  I've worked around this by padding my strings to a fixed length and converting them to `SVector{N,UInt8}`.  That works, but maybe there's an easier way?\n2. Parallel HDF5 does not seem to support using filters (e.g. compression) when performing independent writes.  This seems to be true even when the writes are all in units of complete chunks.  I think I could use filters if I were doing collective writes, but in some cases the writes are very independent and forcing them to be collective seems rather awkward (which might be suggesting that parallel HDF5 is not the right tool for these cases).\nFWIW, I think these are both limitations of HDF5 itself (i.e. not specific to HDF5.jl).","user":"U01FKQQ7J0J","ts":"1612986144.036000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=+fQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not sure if this is the right/best place for this, but does anyone know of a web site that discusses some of the features/capabilities/nuances of Parallel HDF5 and use cases for when (and when not) to use it?  Some of the logistics around which/when operations need to be collective and which/when they can be independent would be very helpful.  I've also encountered some limitations of Parallel HDF5 and would like to know how others work within these limits (or if I'm just doing things improperly).  The two I've found so far are:\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Parallel HDF5 does not seem to support variable length strings, but fixed strings are OK.  I've worked around this by padding my strings to a fixed length and converting them to "},{"type":"text","text":"SVector{N,UInt8}","style":{"code":true}},{"type":"text","text":".  That works, but maybe there's an easier way?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Parallel HDF5 does not seem to support using filters (e.g. compression) when performing independent writes.  This seems to be true even when the writes are all in units of complete chunks.  I think I could use filters if I were doing collective writes, but in some cases the writes are very independent and forcing them to be collective seems rather awkward (which might be suggesting that parallel HDF5 is not the right tool for these cases)."}]}],"style":"ordered","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"FWIW, I think these are both limitations of HDF5 itself (i.e. not specific to HDF5.jl)."}]}]}]},{"client_msg_id":"9f8b68ce-7cba-46a3-9f57-a36c93bd77fe","type":"message","text":"1. That seems to be correct according to <https://support.hdfgroup.org/HDF5/doc/TechNotes/VLTypes.html>. You might be able to convert it to `FixedString` (a HDF5.jl type)","user":"U68P09RFZ","ts":"1613064474.037500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"v+9","elements":[{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That seems to be correct according to "},{"type":"link","url":"https://support.hdfgroup.org/HDF5/doc/TechNotes/VLTypes.html"},{"type":"text","text":". You might be able to convert it to "},{"type":"text","text":"FixedString","style":{"code":true}},{"type":"text","text":" (a HDF5.jl type)"}]}],"style":"ordered","indent":0}]}]},{"client_msg_id":"15ee84ec-f753-4c76-967f-b6b385f04f2f","type":"message","text":"2. It should be possible with the latest hdf5 release (<https://forum.hdfgroup.org/t/parallel-i-o-does-not-support-filters-yet/884>)","user":"U68P09RFZ","ts":"1613064559.038000","team":"T68168MUP","attachments":[{"service_name":"HDF Forum","title":"Parallel I/O does not support filters yet","title_link":"https://forum.hdfgroup.org/t/parallel-i-o-does-not-support-filters-yet/884","text":"Hi, I’ve got this error message (in the subject of this topic) when I try to setup compression on datasets in the MPI environment. Is there any workaround to compress datasets in parallel I/O or any plans to support this in C API? Regards, Rafal","fallback":"HDF Forum: Parallel I/O does not support filters yet","thumb_url":"https://dbkt.hdfgroup.org/original/1X/5fc85e124523a961f1933b7843c6d80f3d26b72a.png","from_url":"https://forum.hdfgroup.org/t/parallel-i-o-does-not-support-filters-yet/884","thumb_width":512,"thumb_height":512,"service_icon":"https://dbkt.hdfgroup.org/original/1X/5fc85e124523a961f1933b7843c6d80f3d26b72a.png","id":1,"original_url":"https://forum.hdfgroup.org/t/parallel-i-o-does-not-support-filters-yet/884"}],"blocks":[{"type":"rich_text","block_id":"qx2RQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"2. It should be possible with the latest hdf5 release ("},{"type":"link","url":"https://forum.hdfgroup.org/t/parallel-i-o-does-not-support-filters-yet/884"},{"type":"text","text":")"}]}]}]},{"client_msg_id":"a9d794c5-f139-48fb-a05f-59709acdad01","type":"message","text":"Another elementary question here. I am doing the following\n```@everywhere include(\"file.jl\") # loads f()\n@everywhere x = load(\"file.jld2\")\ny = @distributed (+) for i in 1:100000\n    f(x)\nend```\nbut it complains that worker 2  does not know about `x`. So before the loop I had every worker print out the value `x[1]`and that worked. There’s something about entering @distributed which makes it invisible again. I also tried `f(fetch(x))`but that didn’t work. Thoughts?","user":"U01655DR0JZ","ts":"1613076341.054900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"P4Fx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Another elementary question here. I am doing the following\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@everywhere include(\"file.jl\") # loads f()\n@everywhere x = load(\"file.jld2\")\ny = @distributed (+) for i in 1:100000\n    f(x)\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"but it complains that worker 2  does not know about "},{"type":"text","text":"x","style":{"code":true}},{"type":"text","text":". So before the loop I had every worker print out the value "},{"type":"text","text":"x[1]","style":{"code":true}},{"type":"text","text":"and that worked. There’s something about entering @distributed which makes it invisible again. I also tried "},{"type":"text","text":"f(fetch(x))","style":{"code":true}},{"type":"text","text":"but that didn’t work. Thoughts?"}]}]}]},{"client_msg_id":"81eb01cf-65bd-40bd-a531-8cf9c068f81b","type":"message","text":"Try doing `global x; f(x)` in your loop","user":"U6A0PD8CR","ts":"1613079475.055700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"b0S","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Try doing "},{"type":"text","text":"global x; f(x)","style":{"code":true}},{"type":"text","text":" in your loop"}]}]}]},{"client_msg_id":"67025b6c-d76f-4a68-89ed-0a4d60cae87e","type":"message","text":"`@everywhere` executes in global scope, so `x` is global","user":"U6A0PD8CR","ts":"1613079490.056100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kFTh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"@everywhere","style":{"code":true}},{"type":"text","text":" executes in global scope, so "},{"type":"text","text":"x","style":{"code":true}},{"type":"text","text":" is global"}]}]}]},{"client_msg_id":"618d0d34-92e6-483e-9ed6-db94cdd23d0b","type":"message","text":"What probably happens is that `@distributed` constructs a function containing the loop body, so the `x` in there, being global, isn't accessible","user":"U6A0PD8CR","ts":"1613079581.056800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GtkE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What probably happens is that "},{"type":"text","text":"@distributed","style":{"code":true}},{"type":"text","text":" constructs a function containing the loop body, so the "},{"type":"text","text":"x","style":{"code":true}},{"type":"text","text":" in there, being global, isn't accessible"}]}]}],"reactions":[{"name":"+1","users":["U7THT3TM3"],"count":1}]},{"client_msg_id":"cb17b800-8fac-46dd-bead-90c5130f6ebe","type":"message","text":"If that fixes it, I would recommend adjusting your program so that you never have to do `global x`. It's slow and introduces dynamic dispatch where it's not needed. Instead, create a function, say `g(x)` (using `@everywhere` to create it on all workers), which contains the `@distributed` loop, and just call it on the head node as `y = g(x)`.","user":"U6A0PD8CR","ts":"1613079716.059100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Odr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If that fixes it, I would recommend adjusting your program so that you never have to do "},{"type":"text","text":"global x","style":{"code":true}},{"type":"text","text":". It's slow and introduces dynamic dispatch where it's not needed. Instead, create a function, say "},{"type":"text","text":"g(x)","style":{"code":true}},{"type":"text","text":" (using "},{"type":"text","text":"@everywhere","style":{"code":true}},{"type":"text","text":" to create it on all workers), which contains the "},{"type":"text","text":"@distributed","style":{"code":true}},{"type":"text","text":" loop, and just call it on the head node as "},{"type":"text","text":"y = g(x)","style":{"code":true}},{"type":"text","text":"."}]}]}]},{"client_msg_id":"048a73db-91fc-48cd-8617-fe1cc0bfdcbd","type":"message","text":"Actually, you probably don't need to make `g(x)` with `@everywhere`, because it's only called on the head node","user":"U6A0PD8CR","ts":"1613079764.059800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Cgb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Actually, you probably don't need to make "},{"type":"text","text":"g(x)","style":{"code":true}},{"type":"text","text":" with "},{"type":"text","text":"@everywhere","style":{"code":true}},{"type":"text","text":", because it's only called on the head node"}]}]}]},{"client_msg_id":"eb267fe1-4400-4425-adb2-868bef09f02b","type":"message","text":"<@U6A0PD8CR> thanks for your dagger ml presentation. I had to leave early, but is the idea that even in models with a  serial dependency structure you can just start a new forward pass as long as the first gpu is cleared ?","user":"UDGT4PM41","ts":"1613143198.062900","team":"T68168MUP","edited":{"user":"UDGT4PM41","ts":"1613143213.000000"},"blocks":[{"type":"rich_text","block_id":"wmP","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U6A0PD8CR"},{"type":"text","text":" thanks for your dagger ml presentation. I had to leave early, but is the idea that even in models with a  serial dependency structure you can just start a new forward pass as long as the first gpu is cleared ?"}]}]}]},{"type":"message","subtype":"channel_join","ts":"1613151214.064700","user":"UH9KWTTD3","text":"<@UH9KWTTD3> has joined the channel","inviter":"UDGT4PM41"},{"client_msg_id":"a30f512d-a5bc-4164-a678-b8c046a6fea9","type":"message","text":"Hello.\nAnyone has a good idea for a (recent?) review on parallel many-particle simulations (with interacting particles)?\nSorry for this slightly off-topic question.","user":"U01HC60USTH","ts":"1613152675.071400","team":"T68168MUP","edited":{"user":"U01HC60USTH","ts":"1613153087.000000"},"blocks":[{"type":"rich_text","block_id":"RsKUT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello.\nAnyone has a good idea for a (recent?) review on parallel many-particle simulations (with interacting particles)?\nSorry for this slightly off-topic question."}]}]}]},{"client_msg_id":"a69e0e0f-45d1-4922-bd8f-05cb2c2a9d47","type":"message","text":"Hi, I'm sure I'm being stupid but for some reason I can't increase the number of MPI processes beyond 127 using MPI.jl, which sounds a suspiciously `char`-y kind of number to fail at. I assume that other people have gone beyond this number without problems?","user":"U7C8KRZKL","ts":"1613573526.073600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tWkD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I'm sure I'm being stupid but for some reason I can't increase the number of MPI processes beyond 127 using MPI.jl, which sounds a suspiciously "},{"type":"text","text":"char","style":{"code":true}},{"type":"text","text":"-y kind of number to fail at. I assume that other people have gone beyond this number without problems?"}]}]}]},{"type":"message","text":"","user":"U82LX4ACB","ts":"1614039438.000100","team":"T68168MUP","attachments":[{"fallback":"[February 22nd, 2021 7:16 PM] naoyabpr: Anyone knows what the following error message might related to?\n```slurmstepd: error: *** STEP 19296973.0 ON udc-aj37-9c0 CANCELLED AT 2020-12-22T22:49:46 ***\nsrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\n\nslurmstepd: error: *** JOB 19296973 ON udc-aj37-9c0 CANCELLED AT 2020-12-22T22:49:46 ***\nsignal (15): Terminated\nin expression starting at /sfs/qumulo/qhome/jbs3hp/Rivanna/data/sys/myjobs/projects/default/12/main.jl:13\npthread_cond_wait at /lib64/libpthread.so.0 (unknown line)\nuv_cond_wait at /workspace/srcdir/libuv/src/unix/thread.c:827\njl_task_get_next at /buildworker/worker/package_linux64/build/src/partr.c:509\npoptask at ./task.jl:704\nwait at ./task.jl:712 [inlined]\ntask_done_hook at ./task.jl:442\n_jl_invoke at /buildworker/worker/package_linux64/build/src/gf.c:2214 [inlined]\njl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:2398\njl_apply at /buildworker/worker/package_linux64/build/src/julia.h:1690 [inlined]\njl_finish_task at /buildworker/worker/package_linux64/build/src/task.c:196\njl_threadfun at /buildworker/worker/package_linux64/build/src/partr.c:265\nstart_thread at /lib64/libpthread.so.0 (unknown line)\nclone at /lib64/libc.so.6 (unknown line)\nunknown function (ip: (nil))\npthread_cond_wait at /lib64/libpthread.so.0 (unknown line)\nuv_cond_wait at /workspace/srcdir/libuv/src/unix/thread.c:827\njl_task_get_next at /buildworker/worker/package_linux64/build/src/partr.c:509\npoptask at ./task.jl:704```","ts":"1614039364.000800","author_id":"U82LX4ACB","author_subname":"José Bayoán Santiago Calderón","channel_id":"CBFP2PTTR","channel_name":"hpc","is_msg_unfurl":true,"text":"Anyone knows what the following error message might related to?\n```slurmstepd: error: *** STEP 19296973.0 ON udc-aj37-9c0 CANCELLED AT 2020-12-22T22:49:46 ***\nsrun: Job step aborted: Waiting up to 32 seconds for job step to finish.\n\nslurmstepd: error: *** JOB 19296973 ON udc-aj37-9c0 CANCELLED AT 2020-12-22T22:49:46 ***\nsignal (15): Terminated\nin expression starting at /sfs/qumulo/qhome/jbs3hp/Rivanna/data/sys/myjobs/projects/default/12/main.jl:13\npthread_cond_wait at /lib64/libpthread.so.0 (unknown line)\nuv_cond_wait at /workspace/srcdir/libuv/src/unix/thread.c:827\njl_task_get_next at /buildworker/worker/package_linux64/build/src/partr.c:509\npoptask at ./task.jl:704\nwait at ./task.jl:712 [inlined]\ntask_done_hook at ./task.jl:442\n_jl_invoke at /buildworker/worker/package_linux64/build/src/gf.c:2214 [inlined]\njl_apply_generic at /buildworker/worker/package_linux64/build/src/gf.c:2398\njl_apply at /buildworker/worker/package_linux64/build/src/julia.h:1690 [inlined]\njl_finish_task at /buildworker/worker/package_linux64/build/src/task.c:196\njl_threadfun at /buildworker/worker/package_linux64/build/src/partr.c:265\nstart_thread at /lib64/libpthread.so.0 (unknown line)\nclone at /lib64/libc.so.6 (unknown line)\nunknown function (ip: (nil))\npthread_cond_wait at /lib64/libpthread.so.0 (unknown line)\nuv_cond_wait at /workspace/srcdir/libuv/src/unix/thread.c:827\njl_task_get_next at /buildworker/worker/package_linux64/build/src/partr.c:509\npoptask at ./task.jl:704```","author_name":"José Bayoán Santiago Calderón","author_link":"https://julialang.slack.com/team/U82LX4ACB","author_icon":"https://secure.gravatar.com/avatar/a8070ca24f919e30e60dce8a763c8177.jpg?s=48&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0025-48.png","mrkdwn_in":["text"],"color":"D0D0D0","from_url":"https://julialang.slack.com/archives/CBFP2PTTR/p1614039364000800","is_share":true,"footer":"Posted in #hpc"}]},{"client_msg_id":"b724f2a9-9eb1-4ac4-9e20-800b3e8bc928","type":"message","text":"Am I missing something about scoping when defining macros across workers? This fails\n```using Distributed \naddprocs()\n\n@everywhere begin \n    using AWS: @service\n    @service S3\nend\n\nERROR: On worker 2:\nLoadError: UndefVarError: @service not defined\ntop-level scope\neval at ./boot.jl:331\n#103 at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/process_messages.jl:290\nrun_work_thunk at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/process_messages.jl:79\nrun_work_thunk at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/process_messages.jl:88\n#96 at ./task.jl:356\nin expression starting at REPL[3]:3\n\n...and 11 more exception(s).\n\nStacktrace:\n [1] sync_end(::Channel{Any}) at ./task.jl:314\n [2] macro expansion at ./task.jl:333 [inlined]\n [3] remotecall_eval(::Module, ::Array{Int64,1}, ::Expr) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/macros.jl:218\n [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/macros.jl:202```\nbut this works\n```using Distributed\naddprocs()\n@everywhere using AWS: @service \n@everywhere @service S3```","user":"UNH0PT5D3","ts":"1614116007.001800","team":"T68168MUP","edited":{"user":"UNH0PT5D3","ts":"1614116115.000000"},"blocks":[{"type":"rich_text","block_id":"DL13","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Am I missing something about scoping when defining macros across workers? This fails\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Distributed \naddprocs()\n\n@everywhere begin \n    using AWS: @service\n    @service S3\nend\n\nERROR: On worker 2:\nLoadError: UndefVarError: @service not defined\ntop-level scope\neval at ./boot.jl:331\n#103 at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/process_messages.jl:290\nrun_work_thunk at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/process_messages.jl:79\nrun_work_thunk at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/process_messages.jl:88\n#96 at ./task.jl:356\nin expression starting at REPL[3]:3\n\n...and 11 more exception(s).\n\nStacktrace:\n [1] sync_end(::Channel{Any}) at ./task.jl:314\n [2] macro expansion at ./task.jl:333 [inlined]\n [3] remotecall_eval(::Module, ::Array{Int64,1}, ::Expr) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/macros.jl:218\n [4] top-level scope at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/Distributed/src/macros.jl:202"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"but this works\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Distributed\naddprocs()\n@everywhere using AWS: @service \n@everywhere @service S3"}]}]}]},{"client_msg_id":"1df45570-8ffa-4c17-97e2-7f1f044e9ad6","type":"message","text":"I’m getting a 6x slow down (1 hr vs 10 minutes) in loading a package @everywhere when using ClusterMangers vs running locally on a single node. I can run the script locally (Load time is ~10 minutes). Any ideas why there’s such a large performance hit?","user":"U01E7DK9HPA","ts":"1614140531.005800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TxALl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’m getting a 6x slow down (1 hr vs 10 minutes) in loading a package @everywhere when using ClusterMangers vs running locally on a single node. I can run the script locally (Load time is ~10 minutes). Any ideas why there’s such a large performance hit?"}]}]}]},{"client_msg_id":"239eb67b-35a1-4790-b433-311a3ce2fc91","type":"message","text":"Was the package already precompiled?","user":"U67BJLYCS","ts":"1614172138.006400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FiSa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Was the package already precompiled?"}]}]}]},{"client_msg_id":"6f27b5e9-7ae6-473a-ac0a-0da63a709bad","type":"message","text":"(and which package has 10min load times on a good day)","user":"U67BJLYCS","ts":"1614172155.007100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"p8D","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(and which package has 10min load times on a good day)"}]}]}]},{"client_msg_id":"14ebcf71-8ccd-4537-8f0b-c9d13bcdbedd","type":"message","text":"Too long or too short:joy:. It’s my code but most of the load it likely DiffEq.\nIt was not precompiled, would that help?\nedit: I’ll go try precompiling","user":"U01E7DK9HPA","ts":"1614173038.010300","team":"T68168MUP","edited":{"user":"U01E7DK9HPA","ts":"1614173063.000000"},"blocks":[{"type":"rich_text","block_id":"5KjH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Too long or too short"},{"type":"emoji","name":"joy"},{"type":"text","text":". It’s my code but most of the load it likely DiffEq.\nIt was not precompiled, would that help?\nedit: I’ll go try precompiling"}]}]}]},{"client_msg_id":"e1bf1862-305d-434f-bd63-ed10a2f484e8","type":"message","text":"precompile locally, IIRC that helps distributed a ton.","user":"U69BL50BF","ts":"1614173993.010900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HKd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"precompile locally, IIRC that helps distributed a ton."}]}]}]},{"client_msg_id":"2d228ab0-4e35-4be8-8a52-f253f4118758","type":"message","text":"Not precompiling was the issue! @everywhere import … is back down to ~10 minutes. Thank you!","user":"U01E7DK9HPA","ts":"1614184542.013500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"h60","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not precompiling was the issue! @everywhere import … is back down to ~10 minutes. Thank you!"}]}]}]},{"client_msg_id":"7b049de5-853c-4573-9f08-09d0a5b79124","type":"message","text":"<@U6A0PD8CR> possibly relevant to the distributed Flux on Dagger work? Somebody is trying to implement a ring all-reduce for ML training using `Distributed`: <https://discourse.julialang.org/t/julia-distributed-allreduce-and-distributed-training/55808>","user":"UMY1LV01G","ts":"1614203847.014900","team":"T68168MUP","edited":{"user":"UMY1LV01G","ts":"1614204013.000000"},"attachments":[{"service_name":"JuliaLang","title":"Julia Distributed, AllReduce, and Distributed Training","title_link":"https://discourse.julialang.org/t/julia-distributed-allreduce-and-distributed-training/55808","text":"I’m working with Distributed Julia on multiple hosts and want to write some code that does distributed training with a machine learning model and distributed data. I don’t want to use MPI. I have the ClusterManager working correctly and can use addproc_mysystem(n) with out any issues. The traditional way to do distributed training is to put an identical copy of the model on each worker, then partition the training data and give each worker one partition. Each worker will compute a gradient with...","fallback":"JuliaLang: Julia Distributed, AllReduce, and Distributed Training","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","ts":1614036002,"from_url":"https://discourse.julialang.org/t/julia-distributed-allreduce-and-distributed-training/55808","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/julia-distributed-allreduce-and-distributed-training/55808"}],"blocks":[{"type":"rich_text","block_id":"Wj7","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U6A0PD8CR"},{"type":"text","text":" possibly relevant to the distributed Flux on Dagger work? Somebody is trying to implement a ring all-reduce for ML training using "},{"type":"text","text":"Distributed","style":{"code":true}},{"type":"text","text":": "},{"type":"link","url":"https://discourse.julialang.org/t/julia-distributed-allreduce-and-distributed-training/55808"}]}]}]},{"client_msg_id":"b5a2e71a-69c7-49ae-ae63-f9b028369ebd","type":"message","text":"Any advice on getting logging working from with an @distributed for loop? I’ve tried just putting @infos inside the loop but I’m not getting anything out in the worker log files (I’m using CluterManagers.jl) or the main slurm log file","user":"U01E7DK9HPA","ts":"1614707509.017400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UY=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Any advice on getting logging working from with an @distributed for loop? I’ve tried just putting @infos inside the loop but I’m not getting anything out in the worker log files (I’m using CluterManagers.jl) or the main slurm log file"}]}]}],"thread_ts":"1614707509.017400","reply_count":1,"reply_users_count":1,"latest_reply":"1614707765.017500","reply_users":["U6A0PD8CR"],"subscribed":false},{"client_msg_id":"a31fbeff-de27-403f-9e82-ae7a20a8b9e1","type":"message","text":"What's the effective difference between `@spawnat` and `remotecall`?","user":"U68P09RFZ","ts":"1615576841.000800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9ruVj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What's the effective difference between "},{"type":"text","text":"@spawnat","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"remotecall","style":{"code":true}},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"247de24e-bfa0-434d-a15d-49613b2297d1","type":"message","text":"oh, I guess I should have looked at the code: `@spawnat` (1) creates a closure and calls `remotecall`, and (2) adds it to the `@sync` list var if available","user":"U68P09RFZ","ts":"1615577070.002700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"f=s4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"oh, I guess I should have looked at the code: "},{"type":"text","text":"@spawnat","style":{"code":true}},{"type":"text","text":" (1) creates a closure and calls "},{"type":"text","text":"remotecall","style":{"code":true}},{"type":"text","text":", and (2) adds it to the "},{"type":"text","text":"@sync","style":{"code":true}},{"type":"text","text":" list var if available"}]}]}]},{"client_msg_id":"1520564b-785c-4719-b740-cf15b58c5d28","type":"message","text":"So my second question: is there any difference between creating a closure and passing variables via arguments in `remotecall`?","user":"U68P09RFZ","ts":"1615577133.003600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9i=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So my second question: is there any difference between creating a closure and passing variables via arguments in "},{"type":"text","text":"remotecall","style":{"code":true}},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"2d715134-fa13-4bdf-a66f-4e83dd8f9cfa","type":"message","text":"i.e. `remotecall(f, pid, arg1)` vs `remotecall(() -&gt; f(arg1), pid)`?","user":"U68P09RFZ","ts":"1615577170.004300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2si","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"i.e. "},{"type":"text","text":"remotecall(f, pid, arg1)","style":{"code":true}},{"type":"text","text":" vs "},{"type":"text","text":"remotecall(() -> f(arg1), pid)","style":{"code":true}},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"cad801dc-241e-4753-9aed-c4e0b24e6449","type":"message","text":"`remotecall(f, ...)` assumes that f if it is not a closure exists on the other side as  a name","user":"U67BJLYCS","ts":"1615580030.005000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wg9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"remotecall(f, ...)","style":{"code":true}},{"type":"text","text":" assumes that f if it is not a closure exists on the other side as  a name"}]}]}]},{"client_msg_id":"f832600a-5d3a-451e-83c6-e4e767f10583","type":"message","text":"`remotecall(()-&gt;f(arg))` will serialize the closure and send it over the wire","user":"U67BJLYCS","ts":"1615580060.005500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ePf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"remotecall(()->f(arg))","style":{"code":true}},{"type":"text","text":" will serialize the closure and send it over the wire"}]}]}]},{"client_msg_id":"f35d9b73-8e29-497f-8fe4-08915b9eaf03","type":"message","text":"so `remotecall(f, pid, arg1)` is better since you only need to send arg1 and a cheap reference to f","user":"U67BJLYCS","ts":"1615580108.006100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8x4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"so "},{"type":"text","text":"remotecall(f, pid, arg1)","style":{"code":true}},{"type":"text","text":" is better since you only need to send arg1 and a cheap reference to f"}]}]}]},{"client_msg_id":"ae90ad7e-9cf5-4297-ad9e-a82046b14e06","type":"message","text":"ah ok","user":"U68P09RFZ","ts":"1615582104.006300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7MPjl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ah ok"}]}]}]},{"client_msg_id":"10882834-f6fc-4bcc-bccc-3e3fec1b6fc2","type":"message","text":"and `@everywhere` will `@eval` the expression on the remote","user":"U68P09RFZ","ts":"1615582126.006800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"m5Bi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"and "},{"type":"text","text":"@everywhere","style":{"code":true}},{"type":"text","text":" will "},{"type":"text","text":"@eval","style":{"code":true}},{"type":"text","text":" the expression on the remote"}]}]}]},{"client_msg_id":"9e41a7a9-aa7b-4451-81ea-5f1bcdcc2209","type":"message","text":"What I think I want is a variant of `@everywhere` that (a) runs only on worker procs, and (b) returns a value that wraps a vector of `Future`s","user":"U68P09RFZ","ts":"1615584779.008000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"p2h4k","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What I think I want is a variant of "},{"type":"text","text":"@everywhere","style":{"code":true}},{"type":"text","text":" that (a) runs only on worker procs, and (b) returns a value that wraps a vector of "},{"type":"text","text":"Future","style":{"code":true}},{"type":"text","text":"s"}]}]}],"thread_ts":"1615584779.008000","reply_count":2,"reply_users_count":2,"latest_reply":"1615586336.009200","reply_users":["ULG5V164A","U68P09RFZ"],"subscribed":false},{"client_msg_id":"45cb06a7-fbaa-42f1-8200-2b0a3d34c910","type":"message","text":"this object would then have nice printing","user":"U68P09RFZ","ts":"1615585062.008800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fplIR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"this object would then have nice printing"}]}]}],"reactions":[{"name":"printer","users":["U680THK2S"],"count":1}]},{"client_msg_id":"e8da4a7d-0459-4b46-a314-598156933b1e","type":"message","text":"```julia&gt; @spmd rand(10)\n12-element SPMDResult:\n 2&gt; [0.767898, 0.790709, 0.484959, 0.870386, 0.818502, 0.821423, 0.623448, 0.128856, 0.498782, 0.840323]\n 3&gt; [0.456791, 0.0461876, 0.0304607, 0.628646, 0.736393, 0.223175, 0.970945, 0.0204162, 0.459929, 0.623279]\n 4&gt; [0.87047, 0.913365, 0.699531, 0.583619, 0.136024, 0.663682, 0.973483, 0.782167, 0.469208, 0.745066]\n 5&gt; [0.249254, 0.386779, 0.612737, 0.703663, 0.60932, 0.788052, 0.46126, 0.800824, 0.765937, 0.908195]\n ⋮\n 12&gt; [0.0428894, 0.640495, 0.239763, 0.824342, 0.83397, 0.00682966, 0.035369, 0.468548, 0.304129, 0.969085]\n 13&gt; [0.212066, 0.710291, 0.284874, 0.342315, 0.206075, 0.359978, 0.542292, 0.911778, 0.511406, 0.0468176]```","user":"U68P09RFZ","ts":"1615587543.009700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vFrf","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> @spmd rand(10)\n12-element SPMDResult:\n 2> [0.767898, 0.790709, 0.484959, 0.870386, 0.818502, 0.821423, 0.623448, 0.128856, 0.498782, 0.840323]\n 3> [0.456791, 0.0461876, 0.0304607, 0.628646, 0.736393, 0.223175, 0.970945, 0.0204162, 0.459929, 0.623279]\n 4> [0.87047, 0.913365, 0.699531, 0.583619, 0.136024, 0.663682, 0.973483, 0.782167, 0.469208, 0.745066]\n 5> [0.249254, 0.386779, 0.612737, 0.703663, 0.60932, 0.788052, 0.46126, 0.800824, 0.765937, 0.908195]\n ⋮\n 12> [0.0428894, 0.640495, 0.239763, 0.824342, 0.83397, 0.00682966, 0.035369, 0.468548, 0.304129, 0.969085]\n 13> [0.212066, 0.710291, 0.284874, 0.342315, 0.206075, 0.359978, 0.542292, 0.911778, 0.511406, 0.0468176]"}]}]}]},{"client_msg_id":"42444eef-d540-4d9d-99de-b810b1071afb","type":"message","text":"Now the question is how to get plotting to work...","user":"U68P09RFZ","ts":"1615587871.010000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TvyeR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Now the question is how to get plotting to work..."}]}]}]},{"client_msg_id":"460acbad-81fa-4a69-8113-0e8ea79dc23c","type":"message","text":"Damn the project thing with `addprocs` is annoying","user":"U68P09RFZ","ts":"1615591805.010300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"szQcn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Damn the project thing with "},{"type":"text","text":"addprocs","style":{"code":true}},{"type":"text","text":" is annoying"}]}]}],"thread_ts":"1615591805.010300","reply_count":1,"reply_users_count":1,"latest_reply":"1615592047.010400","reply_users":["U680THK2S"],"subscribed":false},{"client_msg_id":"a1eab364-a1bb-4507-aeb1-d73f8d7fdc67","type":"message","text":"I actually want slightly different behaviour: I want to use a different project on the manager process and the workers","user":"U68P09RFZ","ts":"1615608817.012100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lJrZ3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I actually want slightly different behaviour: I want to use a different project on the manager process and the workers"}]}]}]},{"client_msg_id":"c751dd0d-14b0-4c6a-80e8-1f3760ed9052","type":"message","text":"but still want to `remotecall` certain functions from the controllers to the workers","user":"U68P09RFZ","ts":"1615608922.013100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"trV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"but still want to "},{"type":"text","text":"remotecall","style":{"code":true}},{"type":"text","text":" certain functions from the controllers to the workers"}]}]}]},{"client_msg_id":"b6907701-9abd-476e-b41a-585afc3c016e","type":"message","text":"And I think I'm being bitten by <https://julialang.slack.com/archives/C6E78Q2NT/p1615580030005000>","user":"U68P09RFZ","ts":"1615608951.013400","team":"T68168MUP","edited":{"user":"U68P09RFZ","ts":"1615608956.000000"},"attachments":[{"from_url":"https://julialang.slack.com/archives/C6E78Q2NT/p1615580030005000","fallback":"[March 12th, 2021 12:13 PM] vchuravy: `remotecall(f, ...)` assumes that f if it is not a closure exists on the other side as  a name","ts":"1615580030.005000","author_id":"U67BJLYCS","author_subname":"Valentin Churavy","channel_id":"C6E78Q2NT","channel_name":"distributed","is_msg_unfurl":true,"text":"`remotecall(f, ...)` assumes that f if it is not a closure exists on the other side as  a name","author_name":"Valentin Churavy","author_link":"https://julialang.slack.com/team/U67BJLYCS","author_icon":"https://secure.gravatar.com/avatar/1cdd019b591282ce27f057fdab2ee80e.jpg?s=48&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0012-48.png","mrkdwn_in":["text"],"id":1,"original_url":"https://julialang.slack.com/archives/C6E78Q2NT/p1615580030005000","footer":"Posted in #distributed"}],"blocks":[{"type":"rich_text","block_id":"wGYA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And I think I'm being bitten by "},{"type":"link","url":"https://julialang.slack.com/archives/C6E78Q2NT/p1615580030005000"}]}]}]},{"client_msg_id":"43e81b39-c670-4090-8a06-72336882820b","type":"message","text":"It works if I define the function in Main, but not if I have it in a package","user":"U68P09RFZ","ts":"1615609110.014000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=+bcc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It works if I define the function in Main, but not if I have it in a package"}]}]}]},{"client_msg_id":"426bf17a-dd02-470b-8b7a-07e5ca6fa24f","type":"message","text":"Ok, so stuff in `Main` is handled specially...","user":"U68P09RFZ","ts":"1615614760.015100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2JO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ok, so stuff in "},{"type":"text","text":"Main","style":{"code":true}},{"type":"text","text":" is handled specially..."}]}]}]},{"client_msg_id":"23c6051a-ec98-47d7-9db8-79d684c3080e","type":"message","text":"Welcome to Distributed life","user":"U680T6770","ts":"1615617450.015400","team":"T68168MUP","edited":{"user":"U680T6770","ts":"1615617455.000000"},"blocks":[{"type":"rich_text","block_id":"TErbL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Welcome to Distributed life"}]}]}]},{"client_msg_id":"b647373b-d4ff-42fb-93f8-1275b56708ea","type":"message","text":"So if I refer to an object which has a `binding_module` other than `Main`, does it assume that it exists on all machines?","user":"U68P09RFZ","ts":"1615671235.019000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9wgx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So if I refer to an object which has a "},{"type":"text","text":"binding_module","style":{"code":true}},{"type":"text","text":" other than "},{"type":"text","text":"Main","style":{"code":true}},{"type":"text","text":", does it assume that it exists on all machines?"}]}]}]},{"client_msg_id":"1dea2864-83b6-4e9b-bfa7-08a961d69375","type":"message","text":"Yes","user":"U67BJLYCS","ts":"1615732762.019400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yxob","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes"}]}]}]},{"client_msg_id":"205d2063-c633-4ccd-93d3-78d9fbd434db","type":"message","text":"Iirc","user":"U67BJLYCS","ts":"1615732773.019800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9=K","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Iirc"}]}]}]}]}