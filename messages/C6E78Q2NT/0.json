{"cursor": 0, "messages": [{"client_msg_id":"8f5e66fa-9e14-47bf-8639-50a99b22bd38","type":"message","text":"<https://github.com/JuliaParallel/ClusterManagers.jl/pull/153>\nis there someone willing to review this?","user":"UH8A351DJ","ts":"1608713958.180000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zMXNb","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/JuliaParallel/ClusterManagers.jl/pull/153"},{"type":"text","text":"\nis there someone willing to review this?"}]}]}]},{"client_msg_id":"c7983177-ede9-4d6a-9cae-28c8f25b4801","type":"message","text":"I have a large amount of data which is needed by a dozen of different processes (running on the same machine) as a base for calculations. At the moment this is a set of arrays, each one ~500Mb .\nI am thinking of using SharedArrays so that I don't have to load a copy of the data in each process. Any better solution?","user":"UKNN7CJ86","ts":"1610557179.188400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xx2+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a large amount of data which is needed by a dozen of different processes (running on the same machine) as a base for calculations. At the moment this is a set of arrays, each one ~500Mb .\nI am thinking of using SharedArrays so that I don't have to load a copy of the data in each process. Any better solution?"}]}]}]},{"client_msg_id":"7acf43be-132e-445e-902a-9b4295f72068","type":"message","text":"Seems like a good way to go","user":"U67461GUB","ts":"1610557617.188700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bMFS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Seems like a good way to go"}]}]}]},{"client_msg_id":"e9762f36-f423-4b12-9ae0-aaf2291996c2","type":"message","text":"Thanks <@U67461GUB>!","user":"UKNN7CJ86","ts":"1610558472.188900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1Y0a","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks "},{"type":"user","user_id":"U67461GUB"},{"type":"text","text":"!"}]}]}]},{"client_msg_id":"51ab4246-cdf8-4b64-bb6f-272e1f79c7d9","type":"message","text":"Could someone explain to me how to use a custom reduction function for @distributed loops? I’m trying to do something like\n```using Distributed\nxs = [1]\nreduce_x(xs,x) = x in xs ? xs : push!(xs,x)\nxs = @distributed (reduce_x) for _ in 1:10\n    rand(1:20)   # some costly function\nend```\nSo that a costly functions is executed in parallel on some workers, and the main process reduces those with a custom function `reduce_x` that extends an array `xs` in case the costly function returned a new unique result.","user":"UHZUXBR2P","ts":"1611838549.004000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"krOVQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Could someone explain to me how to use a custom reduction function for @distributed loops? I’m trying to do something like\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Distributed\nxs = [1]\nreduce_x(xs,x) = x in xs ? xs : push!(xs,x)\nxs = @distributed (reduce_x) for _ in 1:10\n    rand(1:20)   # some costly function\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"So that a costly functions is executed in parallel on some workers, and the main process reduces those with a custom function "},{"type":"text","text":"reduce_x","style":{"code":true}},{"type":"text","text":" that extends an array "},{"type":"text","text":"xs","style":{"code":true}},{"type":"text","text":" in case the costly function returned a new unique result."}]}]}],"thread_ts":"1611838549.004000","reply_count":1,"reply_users_count":1,"latest_reply":"1611839711.004100","reply_users":["UHZUXBR2P"],"subscribed":false}]}