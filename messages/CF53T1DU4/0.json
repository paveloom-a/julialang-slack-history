{"cursor": 0, "messages": [{"client_msg_id":"2d884b8f-de6c-48ac-bdea-871c4ef24636","type":"message","text":"Is appropriate to mention ~6% performance loss on nightly (as compared to 1.5.3), even if so, it might be hard to strip it down to a small metric. Or is this something to do once there's an alpha, etc.","user":"ULTB7E6UW","ts":"1607980182.098000","team":"T68168MUP","edited":{"user":"ULTB7E6UW","ts":"1607980205.000000"},"blocks":[{"type":"rich_text","block_id":"6WM9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is appropriate to mention ~6% performance loss on nightly (as compared to 1.5.3), even if so, it might be hard to strip it down to a small metric. Or is this something to do once there's an alpha, etc."}]}]}]},{"client_msg_id":"b2731467-5773-4ea5-86f8-db55eb37a958","type":"message","text":"If you have a reproducible example (best small and self-contained)","user":"U67BJLYCS","ts":"1607980532.098700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1tC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If you have a reproducible example (best small and self-contained)"}]}]}],"thread_ts":"1607980532.098700","reply_count":27,"reply_users_count":2,"latest_reply":"1607994990.104300","reply_users":["ULTB7E6UW","U67BJLYCS"],"subscribed":false},{"client_msg_id":"3f97b8d5-4090-4cd7-ac5b-0f304ad1781e","type":"message","text":"then yes, please open an issue","user":"U67BJLYCS","ts":"1607980542.099000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+ZcN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"then yes, please open an issue"}]}]}],"thread_ts":"1607980542.099000","reply_count":1,"reply_users_count":1,"latest_reply":"1608154024.115800","reply_users":["ULTB7E6UW"],"subscribed":false},{"client_msg_id":"778dd2d1-9a9e-4be9-bead-dc77d9bc01a0","type":"message","text":"I posted the following in <#C6A044SQH|helpdesk>, but someone mentioned this channel, so I'll just paste it below:\n\nI coded up 3 solutions to Day 15 of Advent of Code (spoilers), and got the following timings:\n<https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15-unsafe-vector.rkt|Racket w/ unsafe vector/arithmetic> =&gt; 0.68 seconds\n<https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15.c|C version> =&gt; 0.47 seconds\n<https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15.jl|Julia w/ mutable vector> =&gt; 0.41 seconds\nI was impressed that Julia was faster than C. On the one hand, I haven't coded C seriously since 1996, but on the other, I'm very much a Julia newbie.\n\nI'm just curious if an experienced Julia coder can spot any obvious inefficiencies in the <https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15.jl|little Julia program> - even though it came in faster than C, that doesn't mean I wouldn't like it to be faster :slightly_smiling_face:","user":"U014ATN949F","ts":"1608079617.105300","team":"T68168MUP","edited":{"user":"U014ATN949F","ts":"1608133861.000000"},"blocks":[{"type":"rich_text","block_id":"DvIU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I posted the following in "},{"type":"channel","channel_id":"C6A044SQH"},{"type":"text","text":", but someone mentioned this channel, so I'll just paste it below:\n\nI coded up 3 solutions to Day 15 of Advent of Code (spoilers), and got the following timings:\n"},{"type":"link","url":"https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15-unsafe-vector.rkt","text":"Racket w/ unsafe vector/arithmetic"},{"type":"text","text":" => 0.68 seconds\n"},{"type":"link","url":"https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15.c","text":"C version"},{"type":"text","text":" => 0.47 seconds\n"},{"type":"link","url":"https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15.jl","text":"Julia w/ mutable vector"},{"type":"text","text":" => 0.41 seconds\nI was impressed that Julia was faster than C. On the one hand, I haven't coded C seriously since 1996, but on the other, I'm very much a Julia newbie.\n\nI'm just curious if an experienced Julia coder can spot any obvious inefficiencies in the "},{"type":"link","url":"https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15.jl","text":"little Julia program"},{"type":"text","text":" - even though it came in faster than C, that doesn't mean I wouldn't like it to be faster "},{"type":"emoji","name":"slightly_smiling_face"}]}]}],"thread_ts":"1608079617.105300","reply_count":33,"reply_users_count":6,"latest_reply":"1608400503.142400","reply_users":["UD0NS8PDF","U014ATN949F","U0179G7FG4F","U7HAYKY9X","UH24GRBLL","UB7JS9CHF"],"subscribed":false},{"client_msg_id":"21ba142e-6d2e-4446-aae1-07ab427f9059","type":"message","text":"So, I'm looking at the Julia  <https://github.com/JuliaLang/julia/issues/29841|#29841>, where I learned that `HTML` and `Text` are errors and would be removed from Julia 2.0.   So, I've been looking to replace them with local code as advised (since <https://github.com/JuliaLang/julia/pull/38892|pull #38892> was denied).   Here is the code in question.\n```mutable struct Text{T}\n    content::T\nend\n\nBase.print(io::IO, t::Text) = print(io, t.content)\nBase.print(io::IO, t::Text{&lt;:Function}) = t.content(io)\nBase.show(io::IO, t::Text) = print(io, t)```\nWhen I do a straight-up replacement, calling this type `Reprint` (let's say), it seems to work.  At first I thought it was slower; but I think that's a sampling error on an imperfect laptop.  I have a question.\n\n1. When I drop \"mutable\" things get slower, almost 40% slower.\n2. If I replace `content::T` with `content::Function` things also get much slower.\nI'm using this in a `Text() do io ... end` style, so I was seeing if it could be improved or simplified.  I guess not?  What are the impacts of using `{T}`?\nSorry for the multiple edits (slack UI is frustrating).","user":"ULTB7E6UW","ts":"1608159461.119200","team":"T68168MUP","edited":{"user":"ULTB7E6UW","ts":"1608160314.000000"},"blocks":[{"type":"rich_text","block_id":"/dQf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So, I'm looking at the Julia  "},{"type":"link","url":"https://github.com/JuliaLang/julia/issues/29841","text":"#29841"},{"type":"text","text":", where I learned that "},{"type":"text","text":"HTML","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"Text","style":{"code":true}},{"type":"text","text":" are errors and would be removed from Julia 2.0.   So, I've been looking to replace them with local code as advised (since "},{"type":"link","url":"https://github.com/JuliaLang/julia/pull/38892","text":"pull #38892"},{"type":"text","text":" was denied).   Here is the code in question.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"mutable struct Text{T}\n    content::T\nend\n\nBase.print(io::IO, t::Text) = print(io, t.content)\nBase.print(io::IO, t::Text{<:Function}) = t.content(io)\nBase.show(io::IO, t::Text) = print(io, t)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"When I do a straight-up replacement, calling this type "},{"type":"text","text":"Reprint","style":{"code":true}},{"type":"text","text":" (let's say), it seems to work.  At first I thought it was slower; but I think that's a sampling error on an imperfect laptop.  I have a question.\n\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"When I drop \"mutable\" things get slower, almost 40% slower."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"If I replace "},{"type":"text","text":"content::T","style":{"code":true}},{"type":"text","text":" with "},{"type":"text","text":"content::Function","style":{"code":true}},{"type":"text","text":" things also get much slower."}]}],"style":"ordered","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"\nI'm using this in a "},{"type":"text","text":"Text() do io ... end","style":{"code":true}},{"type":"text","text":" style, so I was seeing if it could be improved or simplified.  I guess not?  What are the impacts of using "},{"type":"text","text":"{T}","style":{"code":true}},{"type":"text","text":"?\nSorry for the multiple edits (slack UI is frustrating)."}]}]}],"thread_ts":"1608159461.119200","reply_count":70,"reply_users_count":2,"latest_reply":"1608164821.136600","reply_users":["ULTB7E6UW","U0179G7FG4F"],"subscribed":false},{"client_msg_id":"8fbf43e1-574f-4e16-97ec-71fb6c3218ea","type":"message","text":"Are there usually issues with type inference when shadowing local variables? Consider this relatively nonsensical example:\n```module Stuff\nfunction solve_b(filename)\n    your = Vector{Int}()\n    _nearby = Vector{Vector{Int}}()\n    ruledict = Dict{String, Vector{UnitRange}}()\n    nearby = filter(true, _nearby)\n    Dict(\n        rulename =&gt; Set(filter(_ -&gt; all(true, nearby), 1:length(your)))\n        for rulename in ruledict\n    )\nend\nend```\nThere are no issues when you run `@code_warntype Stuff.solve_b(\"a\")`, but if `_nearby` is changed to `nearby`, then it infers `nearby@_6::Core.Box` and `nearby@_8::Union{}` instead of `Vector{Vector{Int64}}` as it's supposed to.","user":"UC81ESVH6","ts":"1608159805.120800","team":"T68168MUP","edited":{"user":"UC81ESVH6","ts":"1608161244.000000"},"blocks":[{"type":"rich_text","block_id":"FuXcB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Are there usually issues with type inference when shadowing local variables? Consider this relatively nonsensical example:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"module Stuff\nfunction solve_b(filename)\n    your = Vector{Int}()\n    _nearby = Vector{Vector{Int}}()\n    ruledict = Dict{String, Vector{UnitRange}}()\n    nearby = filter(true, _nearby)\n    Dict(\n        rulename => Set(filter(_ -> all(true, nearby), 1:length(your)))\n        for rulename in ruledict\n    )\nend\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"There are no issues when you run "},{"type":"text","text":"@code_warntype Stuff.solve_b(\"a\")","style":{"code":true}},{"type":"text","text":", but if "},{"type":"text","text":"_nearby","style":{"code":true}},{"type":"text","text":" is changed to "},{"type":"text","text":"nearby","style":{"code":true}},{"type":"text","text":", then it infers "},{"type":"text","text":"nearby@_6::Core.Box","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"nearby@_8::Union{}","style":{"code":true}},{"type":"text","text":" instead of "},{"type":"text","text":"Vector{Vector{Int64}}","style":{"code":true}},{"type":"text","text":" as it's supposed to."}]}]}],"thread_ts":"1608159805.120800","reply_count":3,"reply_users_count":1,"latest_reply":"1608163294.130800","reply_users":["U67BJLYCS"],"subscribed":false},{"client_msg_id":"098d0cf6-d109-41cc-a560-36f954d58aaa","type":"message","text":"TranscodingStreams.jl is the best bet we have for very fast IO buffering, right? It seems like some performance is still left on the table:\nComparing this\n```function bufferfile(path, buffer::Vector{UInt8})\n    rdr = NoopStream(open(path))\n    while !eof(rdr)\n        readbytes!(rdr, buffer)\n    end\nend```\nwith `cat my_file &gt; /dev/null` , I get 370 ms for TranscodingStreams and 240 ms for `cat` . Does anyone know if a thorough optimization has been attempted?","user":"U7HAYKY9X","ts":"1608240108.139500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3wJn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"TranscodingStreams.jl is the best bet we have for very fast IO buffering, right? It seems like some performance is still left on the table:\nComparing this\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function bufferfile(path, buffer::Vector{UInt8})\n    rdr = NoopStream(open(path))\n    while !eof(rdr)\n        readbytes!(rdr, buffer)\n    end\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"with "},{"type":"text","text":"cat my_file > /dev/null","style":{"code":true}},{"type":"text","text":" , I get 370 ms for TranscodingStreams and 240 ms for "},{"type":"text","text":"cat","style":{"code":true}},{"type":"text","text":" . Does anyone know if a thorough optimization has been attempted?"}]}]}],"thread_ts":"1608240108.139500","reply_count":12,"reply_users_count":3,"latest_reply":"1608319538.142000","reply_users":["UH24GRBLL","U7HAYKY9X","U01GRS159T8"],"subscribed":false},{"client_msg_id":"8cfdea73-ff2d-4724-b473-e38adf55faa1","type":"message","text":"In which case running `GC.gc()` manually not really run the gc in the backend ?\nI have disabled gc by running `GC.enable(false)` first and then calling GC.gc() manually","user":"U01GZQ6B0JU","ts":"1608532265.145000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xUn6X","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In which case running "},{"type":"text","text":"GC.gc()","style":{"code":true}},{"type":"text","text":" manually not really run the gc in the backend ?\nI have disabled gc by running "},{"type":"text","text":"GC.enable(false) ","style":{"code":true}},{"type":"text","text":"first and then calling GC.gc() manually"}]}]}],"thread_ts":"1608532265.145000","reply_count":3,"reply_users_count":2,"latest_reply":"1608532740.145600","reply_users":["U0179G7FG4F","U01GZQ6B0JU"],"subscribed":false},{"client_msg_id":"4baa4893-1ef7-4e55-ada9-167090e2c74f","type":"message","text":"hi","user":"UTAQ23XTR","ts":"1608559692.146000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"72q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hi"}]}]}],"reactions":[{"name":"wave","users":["U7HAYKY9X","UH24GRBLL"],"count":2}]},{"client_msg_id":"56fccb0f-dd1e-43d0-9215-60ac601c0de1","type":"message","text":"I am attempting to translate some python code to julia to speed it up. But maybe as it's almost christmas I could just give it as a puzzle.","user":"UTAQ23XTR","ts":"1608560858.146900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eXPR9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am attempting to translate some python code to julia to speed it up. But maybe as it's almost christmas I could just give it as a puzzle."}]}]}],"thread_ts":"1608560858.146900","reply_count":107,"reply_users_count":4,"latest_reply":"1608649127.172200","reply_users":["U0179G7FG4F","UTAQ23XTR","UH24GRBLL","UDD5Z7FLZ"],"subscribed":false},{"client_msg_id":"4a8f2f20-8da9-42e9-9b65-18bf8a5a0891","type":"message","text":"Here it is: Given the string aaaaaaaaaa and an alphabet of size 100, find all other strings with edit (Levenshtein) distance at most 2. Your code must run in less than 1 second.  There are 1631129 distinct such strings","user":"UTAQ23XTR","ts":"1608560908.147500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6Rg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Here it is: Given the string aaaaaaaaaa and an alphabet of size 100, find all other strings with edit (Levenshtein) distance at most 2. Your code must run in less than 1 second.  There are 1631129 distinct such strings"}]}]}]},{"client_msg_id":"49c077da-71a4-42c7-a86c-ec79d68cc2f9","type":"message","text":"Ideally the code would only output the distinct strings but that is not necessary to win the prize :slightly_smiling_face:","user":"UTAQ23XTR","ts":"1608560933.148000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VmuNC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ideally the code would only output the distinct strings but that is not necessary to win the prize "},{"type":"emoji","name":"slightly_smiling_face"}]}]}]},{"type":"message","subtype":"thread_broadcast","text":"there are only 95 printable ASCII characters...","user":"UH24GRBLL","ts":"1608561577.149300","thread_ts":"1608560858.146900","root":{"client_msg_id":"56fccb0f-dd1e-43d0-9215-60ac601c0de1","type":"message","text":"I am attempting to translate some python code to julia to speed it up. But maybe as it's almost christmas I could just give it as a puzzle.","user":"UTAQ23XTR","ts":"1608560858.146900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eXPR9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am attempting to translate some python code to julia to speed it up. But maybe as it's almost christmas I could just give it as a puzzle."}]}]}],"thread_ts":"1608560858.146900","reply_count":107,"reply_users_count":4,"latest_reply":"1608649127.172200","reply_users":["U0179G7FG4F","UTAQ23XTR","UH24GRBLL","UDD5Z7FLZ"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"F0sjL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"there are only 95 printable ASCII characters..."}]}]}],"client_msg_id":"01f58c3d-e40c-48b2-a020-52ab028e7bc2"},{"client_msg_id":"b31b26ad-193d-47c8-8f6b-a2946097c724","type":"message","text":"My code was taking 2 hours 40 mins to run and now it takes 7 minutes :star-struck:.","user":"UDSG73JTH","ts":"1608580172.162200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wT3J","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"My code was taking 2 hours 40 mins to run and now it takes 7 minutes "},{"type":"emoji","name":"star-struck"},{"type":"text","text":"."}]}]}],"thread_ts":"1608580172.162200","reply_count":2,"reply_users_count":2,"latest_reply":"1608585656.162800","reply_users":["U0179G7FG4F","UDSG73JTH"],"subscribed":false},{"client_msg_id":"a0c70155-54aa-4266-bfce-642164dc5aa1","type":"message","text":"Just out of curiosity: On the systems have access to, allocations larger than 16 KiB take two operations:\n```julia&gt; @time Vector{UInt8}(undef, 2^14 - 1);\n  0.000005 seconds (1 allocation: 16.125 KiB)\n\njulia&gt; @time Vector{UInt8}(undef, 2^14);\n  0.000006 seconds (2 allocations: 16.141 KiB)\n\njulia&gt; @time Vector{UInt8}(undef, 2^40);\n  0.008465 seconds (2 allocations: 1.000 TiB, 99.61% gc time)```\nDoes this mean that the array is no longer contiguous in memory? What sets this limit? Hardware, OS, LLVM, or julia?","user":"U01DD7Z0D89","ts":"1608742654.177700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sbMEx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Just out of curiosity: On the systems have access to, allocations larger than 16 KiB take two operations:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> @time Vector{UInt8}(undef, 2^14 - 1);\n  0.000005 seconds (1 allocation: 16.125 KiB)\n\njulia> @time Vector{UInt8}(undef, 2^14);\n  0.000006 seconds (2 allocations: 16.141 KiB)\n\njulia> @time Vector{UInt8}(undef, 2^40);\n  0.008465 seconds (2 allocations: 1.000 TiB, 99.61% gc time)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Does this mean that the array is no longer contiguous in memory? What sets this limit? Hardware, OS, LLVM, or julia?"}]}]}],"thread_ts":"1608742654.177700","reply_count":1,"reply_users_count":1,"latest_reply":"1608742835.177800","reply_users":["U0179G7FG4F"],"subscribed":false},{"client_msg_id":"37a19605-0c34-4710-8d56-332eaa6a2fd9","type":"message","text":"hi all","user":"UTAQ23XTR","ts":"1608998510.182200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3+Ul","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hi all"}]}]}],"thread_ts":"1608998510.182200","reply_count":1,"reply_users_count":1,"latest_reply":"1608998540.182300","reply_users":["U7HAYKY9X"],"subscribed":false},{"client_msg_id":"e1a77c61-12be-4aab-a362-3c5c96d34348","type":"message","text":"it would be great if there were more/some Julia answers on <http://codegolf.stackexchange.com|codegolf.stackexchange.com> for fastest-code questions.","user":"UTAQ23XTR","ts":"1609002646.183400","team":"T68168MUP","edited":{"user":"UTAQ23XTR","ts":"1609002661.000000"},"blocks":[{"type":"rich_text","block_id":"kx0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it would be great if there were more/some Julia answers on "},{"type":"link","url":"http://codegolf.stackexchange.com","text":"codegolf.stackexchange.com"},{"type":"text","text":" for fastest-code questions."}]}]}]},{"client_msg_id":"7f7a2c00-9ab1-4bd2-bcfd-adac86a282b6","type":"message","text":"My main issue with such questions is that it sometimes veers into the \"this-looks-like-homework\" territory, and I would be a little hesitant to help.","user":"UDD5Z7FLZ","ts":"1609014202.186300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IO8i","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"My main issue with such questions is that it sometimes veers into the \"this-looks-like-homework\" territory, and I would be a little hesitant to help."}]}]}]},{"client_msg_id":"7e136000-a164-491e-a412-59320db940b8","type":"message","text":"I wrote a simple function to sum integers from 1 to n as a simple test of BigInt:\n```function sum_n(n::BigInt)::BigInt\n  val = BigInt(0)\n\n  while n &gt; 0\n    val += n\n    n -=  1\n  end\n\n  val\nend\nsum_n(BigInt(10))\n@time println(sum_n(BigInt(6000000))) # 6 Million```\nIt takes roughly 2.2 seconds to sum 6M numbers. By comparison, the following Racket function can sum 1B numbers in about the same amount of time. The Julia code has 30M in allocations which seems crazy. Does the BigInt implementation allocate a new BigInt for each arithmetic operation?\n```(define (sum-n n [result 0])\n  (if (&gt; n 0) \n      (sum-n (- n 1) (+ result n))\n      result))\n\n(time (sum-n 1000000000)) ; 1 Billion```","user":"U014ATN949F","ts":"1609015995.188900","team":"T68168MUP","edited":{"user":"U014ATN949F","ts":"1609016087.000000"},"blocks":[{"type":"rich_text","block_id":"cIfOH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I wrote a simple function to sum integers from 1 to n as a simple test of BigInt:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function sum_n(n::BigInt)::BigInt\n  val = BigInt(0)\n\n  while n > 0\n    val += n\n    n -=  1\n  end\n\n  val\nend\nsum_n(BigInt(10))\n@time println(sum_n(BigInt(6000000))) # 6 Million"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"It takes roughly 2.2 seconds to sum 6M numbers. By comparison, the following Racket function can sum 1B numbers in about the same amount of time. The Julia code has 30M in allocations which seems crazy. Does the BigInt implementation allocate a new BigInt for each arithmetic operation?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"(define (sum-n n [result 0])\n  (if (> n 0) \n      (sum-n (- n 1) (+ result n))\n      result))\n\n(time (sum-n 1000000000)) ; 1 Billion"}]}]}],"thread_ts":"1609015995.188900","reply_count":46,"reply_users_count":5,"latest_reply":"1609017193.198800","reply_users":["UH24GRBLL","U014ATN949F","U7HAYKY9X","UDD5Z7FLZ","UD0NS8PDF"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"should `write(io, 1234)` allocate?","user":"UH24GRBLL","ts":"1609275144.246300","thread_ts":"1608240108.139500","root":{"client_msg_id":"098d0cf6-d109-41cc-a560-36f954d58aaa","type":"message","text":"TranscodingStreams.jl is the best bet we have for very fast IO buffering, right? It seems like some performance is still left on the table:\nComparing this\n```function bufferfile(path, buffer::Vector{UInt8})\n    rdr = NoopStream(open(path))\n    while !eof(rdr)\n        readbytes!(rdr, buffer)\n    end\nend```\nwith `cat my_file &gt; /dev/null` , I get 370 ms for TranscodingStreams and 240 ms for `cat` . Does anyone know if a thorough optimization has been attempted?","user":"U7HAYKY9X","ts":"1608240108.139500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3wJn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"TranscodingStreams.jl is the best bet we have for very fast IO buffering, right? It seems like some performance is still left on the table:\nComparing this\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function bufferfile(path, buffer::Vector{UInt8})\n    rdr = NoopStream(open(path))\n    while !eof(rdr)\n        readbytes!(rdr, buffer)\n    end\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"with "},{"type":"text","text":"cat my_file > /dev/null","style":{"code":true}},{"type":"text","text":" , I get 370 ms for TranscodingStreams and 240 ms for "},{"type":"text","text":"cat","style":{"code":true}},{"type":"text","text":" . Does anyone know if a thorough optimization has been attempted?"}]}]}],"thread_ts":"1608240108.139500","reply_count":52,"reply_users_count":3,"latest_reply":"1609275155.246600","reply_users":["UH24GRBLL","U7HAYKY9X","U01GRS159T8"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"GeR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"should "},{"type":"text","text":"write(io, 1234)","style":{"code":true}},{"type":"text","text":" allocate?"}]}]}],"client_msg_id":"1007e5bd-1825-4639-8b12-135136e5da00"},{"client_msg_id":"0a390832-7399-4f7b-9a00-c72225a5794f","type":"message","text":"I have a function like this (what it does isn't really important):\n```function stabilize(curr, tolerance, distance=maximum(size(curr)))\n    prev = nothing\n    while prev != curr\n        prev = deepcopy(curr)\n        for I in CartesianIndices(curr)\n            prev[I] == 'L' &amp;&amp; count_occupied(prev, I, distance) == 0         &amp;&amp; (curr[I] = '#')\n            prev[I] == '#' &amp;&amp; count_occupied(prev, I, distance) &gt;= tolerance &amp;&amp; (curr[I] = 'L')\n        end\n    end\n    count(==('#'), curr)\nend```\nAnd it takes this long to run:\n```julia&gt; @btime stabilize(......, 4, 1)\n  38.350 ms (720 allocations: 4.00 MiB)```\nBut the moment I replace `tolerance` and `distance` with the respective values in the function I see an improvement:\n```julia&gt; @btime stabilize(......)\n  13.499 ms (720 allocations: 4.00 MiB)```\nWhy is this the case?","user":"UC81ESVH6","ts":"1609280309.249400","team":"T68168MUP","edited":{"user":"UC81ESVH6","ts":"1609280384.000000"},"blocks":[{"type":"rich_text","block_id":"H9Z6C","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a function like this (what it does isn't really important):\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function stabilize(curr, tolerance, distance=maximum(size(curr)))\n    prev = nothing\n    while prev != curr\n        prev = deepcopy(curr)\n        for I in CartesianIndices(curr)\n            prev[I] == 'L' && count_occupied(prev, I, distance) == 0         && (curr[I] = '#')\n            prev[I] == '#' && count_occupied(prev, I, distance) >= tolerance && (curr[I] = 'L')\n        end\n    end\n    count(==('#'), curr)\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"And it takes this long to run:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> @btime stabilize(......, 4, 1)\n  38.350 ms (720 allocations: 4.00 MiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"But the moment I replace "},{"type":"text","text":"tolerance","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"distance","style":{"code":true}},{"type":"text","text":" with the respective values in the function I see an improvement:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> @btime stabilize(......)\n  13.499 ms (720 allocations: 4.00 MiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Why is this the case?"}]}]}],"thread_ts":"1609280309.249400","reply_count":7,"reply_users_count":3,"latest_reply":"1609282422.250900","reply_users":["UH24GRBLL","UC81ESVH6","U01GMP3HF9C"],"subscribed":false},{"client_msg_id":"964d3841-c6d5-44e7-99fe-5925b196e25e","type":"message","text":"Hello all, I'm trying to optimize the following code. Essentially, I'm want to find the minimum value from `value` for each unique combination of 2 vectors (for the purposes of this code, v1 = 1 and v2 = 2 is the same as v1 = 2 and v2 = 1, which is why I do that hacky part with `min` and `max`). I need to scale this up pretty massively, so I'm hoping to find ways to speed it up. Any suggestions? Thanks very much!!\n```using Base.Threads\n# Vector 1\nv1 = rand(collect(1:1000), 100000)\n# Vector 2\nv2 = rand(collect(1:1000), 100000)\n# Values\nvalue = rand(100000)\n\n# Get unique combos of v1 and v2\nminv = min.(v1, v2)\nmaxv = max.(v1, v2)\nunique_combos = unique(tuple.(minv, maxv))\nn_unique = length(unique_combos)\n\n# init a vector to house minimum of `value` per unique v1 and v2 combination\nvalue_mins = Vector{Float64}(undef, n_unique)\n\n# loop through to find minimum per unique group\n@threads for i in 1:n_unique\n    unique_combo = unique_combos[i]\n    min_value_for_combo_i = minimum(value[(minv .== unique_combo[1]) .&amp; (maxv .== unique_combo[2])])\n    value_mins[i] = min_value_for_combo_i\nend```","user":"U011NV8FNF7","ts":"1609287683.257400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JQpt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello all, I'm trying to optimize the following code. Essentially, I'm want to find the minimum value from "},{"type":"text","text":"value","style":{"code":true}},{"type":"text","text":" for each unique combination of 2 vectors (for the purposes of this code, v1 = 1 and v2 = 2 is the same as v1 = 2 and v2 = 1, which is why I do that hacky part with "},{"type":"text","text":"min","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"max","style":{"code":true}},{"type":"text","text":"). I need to scale this up pretty massively, so I'm hoping to find ways to speed it up. Any suggestions? Thanks very much!!\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Base.Threads\n# Vector 1\nv1 = rand(collect(1:1000), 100000)\n# Vector 2\nv2 = rand(collect(1:1000), 100000)\n# Values\nvalue = rand(100000)\n\n# Get unique combos of v1 and v2\nminv = min.(v1, v2)\nmaxv = max.(v1, v2)\nunique_combos = unique(tuple.(minv, maxv))\nn_unique = length(unique_combos)\n\n# init a vector to house minimum of `value` per unique v1 and v2 combination\nvalue_mins = Vector{Float64}(undef, n_unique)\n\n# loop through to find minimum per unique group\n@threads for i in 1:n_unique\n    unique_combo = unique_combos[i]\n    min_value_for_combo_i = minimum(value[(minv .== unique_combo[1]) .& (maxv .== unique_combo[2])])\n    value_mins[i] = min_value_for_combo_i\nend"}]}]}],"thread_ts":"1609287683.257400","reply_count":4,"reply_users_count":2,"latest_reply":"1609326180.258400","reply_users":["UDD5Z7FLZ","U01GMP3HF9C"],"subscribed":false},{"client_msg_id":"be34b342-27bf-423f-9c7f-06c335bf262b","type":"message","text":"Which types of operations is julia still lacking behind compared with C in terms of preformamce?\nAre these performance issues related to inherent issues with Julia or simply due to julias relative immaturity?","user":"U01FAHWCMFF","ts":"1609345469.261100","team":"T68168MUP","edited":{"user":"U01FAHWCMFF","ts":"1609345524.000000"},"blocks":[{"type":"rich_text","block_id":"8WzB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Which types of operations is julia still lacking behind compared with C in terms of preformamce?\nAre these performance issues related to inherent issues with Julia or simply due to julias relative immaturity?"}]}]}]},{"type":"message","text":"Come compete for a $10 starbucks gift card by being the first to solve my performance issue","user":"U01FAHWCMFF","ts":"1609366811.265400","team":"T68168MUP","edited":{"user":"U01FAHWCMFF","ts":"1609366852.000000"},"attachments":[{"fallback":"[December 30th, 2020 5:09 PM] ayman: If anyone manages to get rid of the last forloop with mapping, without using `MappedArrays.jl`  or any library that doesn't come shipped with Julia (e.g. Statistics.jl is allowed, GLM.jl isnt), *with the same performance*, I will give you a $10 starbucks gift card. First person to do so wins. Have fun all :slightly_smiling_face:\n```function pairwise_analysis2(x::Array{Int16, 2})\n    m, n = size(x)\n    mean_matrix = Matrix{Float64}(undef, (n, n))\n    @views @inbounds for i = 1:n\n        for j = 1:n\n            total::Int16 = 0\n            for k = 1:m\n                total += x[k, i] == x[k, j]\n            end\n            mean_matrix[i, j] = total / m\n        end\n    end\n    return mean_matrix\nend\n\nX = Array{Int16}(rand(0:1, (1000, 1000)));\n@btime pairwise_analysis(X)```","ts":"1609366179.234900","author_id":"U01FAHWCMFF","author_subname":"Ayman Al Baz","channel_id":"C67TK21LJ","channel_name":"gripes","is_msg_unfurl":true,"is_reply_unfurl":true,"text":"If anyone manages to get rid of the last forloop with mapping, without using `MappedArrays.jl`  or any library that doesn't come shipped with Julia (e.g. Statistics.jl is allowed, GLM.jl isnt), *with the same performance*, I will give you a $10 starbucks gift card. First person to do so wins. Have fun all :slightly_smiling_face:\n```function pairwise_analysis2(x::Array{Int16, 2})\n    m, n = size(x)\n    mean_matrix = Matrix{Float64}(undef, (n, n))\n    @views @inbounds for i = 1:n\n        for j = 1:n\n            total::Int16 = 0\n            for k = 1:m\n                total += x[k, i] == x[k, j]\n            end\n            mean_matrix[i, j] = total / m\n        end\n    end\n    return mean_matrix\nend\n\nX = Array{Int16}(rand(0:1, (1000, 1000)));\n@btime pairwise_analysis(X)```","author_name":"Ayman Al Baz","author_link":"https://julialang.slack.com/team/U01FAHWCMFF","author_icon":"https://avatars.slack-edge.com/2020-11-23/1516965530134_1cf6ed3767c34aaf6d40_48.png","mrkdwn_in":["text"],"color":"D0D0D0","from_url":"https://julialang.slack.com/archives/C67TK21LJ/p1609366179234900?thread_ts=1609362482231100&cid=C67TK21LJ","is_share":true,"footer":"From a thread in #gripes"}],"blocks":[{"type":"rich_text","block_id":"cgZ5M","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Come compete for a $10 starbucks gift card by being the first to solve my performance issue"}]}]}]},{"client_msg_id":"4b492bfb-da24-4057-85b8-2a5c0239793b","type":"message","text":"<@UDD5Z7FLZ>  I guess if you answer slowly enough any homework deadline will have passed :)","user":"UTAQ23XTR","ts":"1609445310.283500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8enX=","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UDD5Z7FLZ"},{"type":"text","text":"  I guess if you answer slowly enough any homework deadline will have passed :)"}]}]}],"thread_ts":"1609445310.283500","reply_count":3,"reply_users_count":2,"latest_reply":"1609445879.284000","reply_users":["UDD5Z7FLZ","UTAQ23XTR"],"subscribed":false},{"client_msg_id":"36d87105-e9da-4cc1-b965-26d187398af0","type":"message","text":"Hi all. I have a question about getting good performance out of threading. I don't have a really minimal example, but can point you to a repo with the code (~200 lines):\n\n<https://github.com/vancleve/threading_example>\n\nOn my macbookpro, running the `@time evolve_replicates!(pop, rp)` from <https://github.com/vancleve/threading_example/blob/master/popsim.jl#L30>\nI get something like (after running once to compile)\nno threads  `3.936467 seconds (7.94 M allocations: 4.620 GiB, 14.60% gc time)`\n1 thread      `3.890999 seconds (7.94 M allocations: 4.620 GiB, 16.57% gc time)`\n2 threads    `2.340651 seconds (7.94 M allocations: 4.620 GiB, 25.03% gc time)`\n4 threads    `1.786248 seconds (7.94 M allocations: 4.620 GiB, 41.74% gc time)`\n8 threads    `1.658927 seconds (7.96 M allocations: 4.621 GiB, 58.21% gc time)`\n\nHere are flame graphs for 1 thread (left) and 8 threads (right).\n\nI know I have to reduce allocations, but was hoping to get some advice how best to do this since quite of a bit of the allocation time is coming in basic linear algebra stuff like matrix division and using `I` as identity matrix.\n\nI've also thought about using `Distributed` but I ran into world age issues when passing functions to the workers...","user":"UKFUT8N5A","ts":"1609540973.289000","team":"T68168MUP","edited":{"user":"UKFUT8N5A","ts":"1609541695.000000"},"blocks":[{"type":"rich_text","block_id":"UGvV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi all. I have a question about getting good performance out of threading. I don't have a really minimal example, but can point you to a repo with the code (~200 lines):\n\n"},{"type":"link","url":"https://github.com/vancleve/threading_example"},{"type":"text","text":"\n\nOn my macbookpro, running the "},{"type":"text","text":"@time evolve_replicates!(pop, rp)","style":{"code":true}},{"type":"text","text":" from "},{"type":"link","url":"https://github.com/vancleve/threading_example/blob/master/popsim.jl#L30"},{"type":"text","text":"\nI get something like (after running once to compile)\nno threads  "},{"type":"text","text":"3.936467 seconds (7.94 M allocations: 4.620 GiB, 14.60% gc time)","style":{"code":true}},{"type":"text","text":"\n1 thread      "},{"type":"text","text":"3.890999 seconds (7.94 M allocations: 4.620 GiB, 16.57% gc time)","style":{"code":true}},{"type":"text","text":"\n2 threads    "},{"type":"text","text":"2.340651 seconds (7.94 M allocations: 4.620 GiB, 25.03% gc time)","style":{"code":true}},{"type":"text","text":"\n4 threads    "},{"type":"text","text":"1.786248 seconds (7.94 M allocations: 4.620 GiB, 41.74% gc time)","style":{"code":true}},{"type":"text","text":"\n8 threads    "},{"type":"text","text":"1.658927 seconds (7.96 M allocations: 4.621 GiB, 58.21% gc time)","style":{"code":true}},{"type":"text","text":"\n\nHere are flame graphs for 1 thread (left) and 8 threads (right).\n\nI know I have to reduce allocations, but was hoping to get some advice how best to do this since quite of a bit of the allocation time is coming in basic linear algebra stuff like matrix division and using "},{"type":"text","text":"I","style":{"code":true}},{"type":"text","text":" as identity matrix.\n\nI've also thought about using "},{"type":"text","text":"Distributed","style":{"code":true}},{"type":"text","text":" but I ran into world age issues when passing functions to the workers..."}]}]}]},{"type":"message","text":"","files":[{"id":"F01HVUH9VU3","created":1609541094,"timestamp":1609541094,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"UKFUT8N5A","editable":false,"size":440298,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01HVUH9VU3/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01HVUH9VU3/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01HVUH9VU3-66bb381d09/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01HVUH9VU3-66bb381d09/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01HVUH9VU3-66bb381d09/image_360.png","thumb_360_w":360,"thumb_360_h":163,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01HVUH9VU3-66bb381d09/image_480.png","thumb_480_w":480,"thumb_480_h":218,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01HVUH9VU3-66bb381d09/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01HVUH9VU3-66bb381d09/image_720.png","thumb_720_w":720,"thumb_720_h":326,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01HVUH9VU3-66bb381d09/image_800.png","thumb_800_w":800,"thumb_800_h":363,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01HVUH9VU3-66bb381d09/image_960.png","thumb_960_w":960,"thumb_960_h":435,"thumb_1024":"https://files.slack.com/files-tmb/T68168MUP-F01HVUH9VU3-66bb381d09/image_1024.png","thumb_1024_w":1024,"thumb_1024_h":464,"original_w":3238,"original_h":1468,"thumb_tiny":"AwAVADDR2L/dFVMkdRV2oZIQ3TqTUtFJkP4igcehqX7P9KaYcMAO4qbMq6GbhnGBSqQTwBT/ACh523sRmnxRgAHHUUcorktFFFaEBSY+bPtilooATaN+7vjFKBgYFFFAH//Z","permalink":"https://julialang.slack.com/files/UKFUT8N5A/F01HVUH9VU3/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01HVUH9VU3-a57f3c3707","is_starred":false,"has_rich_preview":false},{"id":"F01HNV0NTFY","created":1609541122,"timestamp":1609541122,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"UKFUT8N5A","editable":false,"size":415089,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01HNV0NTFY/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01HNV0NTFY/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01HNV0NTFY-3349dbff2d/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01HNV0NTFY-3349dbff2d/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01HNV0NTFY-3349dbff2d/image_360.png","thumb_360_w":360,"thumb_360_h":151,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01HNV0NTFY-3349dbff2d/image_480.png","thumb_480_w":480,"thumb_480_h":201,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01HNV0NTFY-3349dbff2d/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01HNV0NTFY-3349dbff2d/image_720.png","thumb_720_w":720,"thumb_720_h":302,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01HNV0NTFY-3349dbff2d/image_800.png","thumb_800_w":800,"thumb_800_h":335,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01HNV0NTFY-3349dbff2d/image_960.png","thumb_960_w":960,"thumb_960_h":402,"thumb_1024":"https://files.slack.com/files-tmb/T68168MUP-F01HNV0NTFY-3349dbff2d/image_1024.png","thumb_1024_w":1024,"thumb_1024_h":429,"original_w":3238,"original_h":1356,"thumb_tiny":"AwAUADDRZF2n5R+VVB6EYq7VaSFmfK4xUyRUWM/KjIHXFOaAqjM2OBmkKfut3TFTYrQTcD2FKCD2FNXBYDPU1YSMAkUJXB6EtFFFaGYdaQgYxjilooATaPQUtFFAH//Z","permalink":"https://julialang.slack.com/files/UKFUT8N5A/F01HNV0NTFY/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01HNV0NTFY-fb2a1c796e","is_starred":false,"has_rich_preview":false}],"upload":false,"user":"UKFUT8N5A","ts":"1609541199.289200"},{"client_msg_id":"f8cbc126-a382-4696-8a81-ba33a7553958","type":"message","text":"I have a section of a `Vector{UInt8}` which corresponds to some text. I want to get the hash of that text - stripped from any whitespace is both ends. My current solution involves copying to a string, then calling `strip` , copying the result to another string, then using `crc32c` .\nIs there a way to do this more efficiently?","user":"U7HAYKY9X","ts":"1609876769.297000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4QrWV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a section of a "},{"type":"text","text":"Vector{UInt8}","style":{"code":true}},{"type":"text","text":" which corresponds to some text. I want to get the hash of that text - stripped from any whitespace is both ends. My current solution involves copying to a string, then calling "},{"type":"text","text":"strip","style":{"code":true}},{"type":"text","text":" , copying the result to another string, then using "},{"type":"text","text":"crc32c","style":{"code":true}},{"type":"text","text":" .\nIs there a way to do this more efficiently?"}]}]}],"thread_ts":"1609876769.297000","reply_count":4,"reply_users_count":2,"latest_reply":"1609877893.297700","reply_users":["U012XER8K4M","U7HAYKY9X"],"subscribed":false},{"type":"message","text":"Is there a sparse 2d data structure that supports random insertions of x,y coordinate pairs well, or a dense matrix data structure that grows in small (maybe 8 by 8) blocks when a new block gets written to?","user":"U9MD78Z9N","ts":"1609921521.298800","team":"T68168MUP"},{"client_msg_id":"b9c1be21-656b-40e2-a4bb-0aa28567ff28","type":"message","text":"When working with gzipped files, you can make it go faster by doing the (de)compression in a separate thread from the other computation. But I can't find any good packages for easy asynchronous/parallel (de)compression. Does such a package exist?","user":"U7HAYKY9X","ts":"1609932745.302100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hpY5i","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"When working with gzipped files, you can make it go faster by doing the (de)compression in a separate thread from the other computation. But I can't find any good packages for easy asynchronous/parallel (de)compression. Does such a package exist?"}]}]}],"thread_ts":"1609932745.302100","reply_count":11,"reply_users_count":2,"latest_reply":"1609936069.304700","reply_users":["U6A936746","U7HAYKY9X"],"subscribed":false},{"client_msg_id":"a88591ab-bad4-4d56-ab5a-1adf45a23704","type":"message","text":"hmmm... I wonder if we can improve this <https://github.com/zero-one-group/geni/blob/develop/docs/simple_performance_benchmark.md>","user":"U013V2CFZAN","ts":"1609948057.305400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hTO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hmmm... I wonder if we can improve this "},{"type":"link","url":"https://github.com/zero-one-group/geni/blob/develop/docs/simple_performance_benchmark.md"}]}]}],"thread_ts":"1609948057.305400","reply_count":7,"reply_users_count":2,"latest_reply":"1609951222.306900","reply_users":["U013V2CFZAN","UH24GRBLL"],"subscribed":false},{"client_msg_id":"6229696a-662f-4440-ba0c-dd8bbb60f03f","type":"message","text":"Hi guys I managed to patch together an example using the 'indirectbr' llvm instruction, which is how computed gotos might be implemented.  Does anyone know how to turn this into a macro?","user":"U01GRS159T8","ts":"1609952031.308500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"AtkW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi guys I managed to patch together an example using the 'indirectbr' llvm instruction, which is how computed gotos might be implemented.  Does anyone know how to turn this into a macro?"}]}]}],"thread_ts":"1609952031.308500","reply_count":2,"reply_users_count":1,"latest_reply":"1609952071.308800","reply_users":["U01GRS159T8"],"subscribed":false},{"type":"message","text":"Is there a way to hint/force a particular function call to be inlined? [Inlined from the caller site.]","user":"U9MD78Z9N","ts":"1610383922.313400","team":"T68168MUP","edited":{"user":"U9MD78Z9N","ts":"1610384693.000000"},"thread_ts":"1610383922.313400","reply_count":22,"reply_users_count":4,"latest_reply":"1610385597.318000","reply_users":["UAUPJLBQX","U9MD78Z9N","U0179G7FG4F","U67BJLYCS"],"subscribed":false},{"client_msg_id":"d78dcc8f-9490-4f0f-b68a-24ac96b5cf30","type":"message","text":"are there any tricks for converting an integer which respresents a fixed point decimal into a float the fastest possible way?  Is int_val * scale_factor the best you can do?","user":"U01GRS159T8","ts":"1610394662.321100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oVA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"are there any tricks for converting an integer which respresents a fixed point decimal into a float the fastest possible way?  Is int_val * scale_factor the best you can do?"}]}]}],"thread_ts":"1610394662.321100","reply_count":9,"reply_users_count":2,"latest_reply":"1610395609.323400","reply_users":["U7HAYKY9X","U01GRS159T8"],"subscribed":false},{"client_msg_id":"a4944fd3-9bfc-4a80-9721-f923e1ff7e8b","type":"message","text":"I'm trying to write a simple ode solver (like the ones <https://perso.crans.org/besson/publis/notebooks/Runge-Kutta_methods_for_ODE_integration_in_Julia.html|here>) but with a custom adaptive stepsize. What is a good way to pre-allocate the vectors given that I don't know how many steps I'll ultimately need?","user":"U91Q3595Y","ts":"1610401442.325100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"cWG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm trying to write a simple ode solver (like the ones "},{"type":"link","url":"https://perso.crans.org/besson/publis/notebooks/Runge-Kutta_methods_for_ODE_integration_in_Julia.html","text":"here"},{"type":"text","text":") but with a custom adaptive stepsize. What is a good way to pre-allocate the vectors given that I don't know how many steps I'll ultimately need?"}]}]}],"thread_ts":"1610401442.325100","reply_count":26,"reply_users_count":4,"latest_reply":"1610403185.331600","reply_users":["U91Q3595Y","U0179G7FG4F","U9MED583T","U69BL50BF"],"subscribed":false},{"type":"message","text":"Assuming Julia properly inlines and the method is type stable. Are there any advantages to use value types to encourage a partial evaluation at compile time, if you call a function again and again with some arguments kept fixed and trusting constant propagation?","user":"U9MD78Z9N","ts":"1610403841.331800","team":"T68168MUP","edited":{"user":"U9MD78Z9N","ts":"1610404139.000000"}},{"client_msg_id":"65184213-14cf-4739-87d3-98a224964999","type":"message","text":"cross-posting <https://stackoverflow.com/questions/65786407/how-to-concatenate-apache-arrow-files-with-identical-structure-in-julia> since it seems like an interesting problem","user":"U0179G7FG4F","ts":"1611040656.015000","team":"T68168MUP","attachments":[{"service_name":"Stack Overflow","title":"How to concatenate Apache Arrow files with identical structure in julia","title_link":"https://stackoverflow.com/questions/65786407/how-to-concatenate-apache-arrow-files-with-identical-structure-in-julia","text":"How can I concatenate several Arrow files with identical structure into a single Arrow file without reading each file into memory? I am using Arrow.jl and the Arrow files represent dataframes with","fallback":"Stack Overflow: How to concatenate Apache Arrow files with identical structure in julia","thumb_url":"https://cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon@2.png?v=73d79a89bded","from_url":"https://stackoverflow.com/questions/65786407/how-to-concatenate-apache-arrow-files-with-identical-structure-in-julia","thumb_width":316,"thumb_height":316,"service_icon":"https://cdn.sstatic.net/Sites/stackoverflow/Img/apple-touch-icon.png?v=c78bd457575a","id":1,"original_url":"https://stackoverflow.com/questions/65786407/how-to-concatenate-apache-arrow-files-with-identical-structure-in-julia"}],"blocks":[{"type":"rich_text","block_id":"P+44z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"cross-posting "},{"type":"link","url":"https://stackoverflow.com/questions/65786407/how-to-concatenate-apache-arrow-files-with-identical-structure-in-julia"},{"type":"text","text":" since it seems like an interesting problem"}]}]}]},{"client_msg_id":"b976bdce-f924-49e1-879a-4fbc2f1107b8","type":"message","text":"<https://news.ycombinator.com/item?id=25825657>","user":"UDGT4PM41","ts":"1611103586.015300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"H/s3","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://news.ycombinator.com/item?id=25825657"}]}]}]},{"client_msg_id":"731d9538-9799-4f46-ad62-263e57aef28b","type":"message","text":"if anyone wants to take a crack at that","user":"UDGT4PM41","ts":"1611103592.015500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"T9E","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"if anyone wants to take a crack at that"}]}]}],"thread_ts":"1611103592.015500","reply_count":1,"reply_users_count":1,"latest_reply":"1611103710.015600","reply_users":["USU9FRPEU"],"subscribed":false},{"client_msg_id":"fed300f3-4e1c-408f-a8ac-57c49460a7fa","type":"message","text":"Implemented a simple agent based SIR model in julia as it's been a long time since I've written anything agent based. Here is the main function, any low hanging fruit I am missing?\n```function agents_step!(u_next,u,graph,params,t,rand_gen)\n    @unpack p, recovery_rate  = params  \n    u_next .= u\n    for (v,agent) in enumerate(u)\n        if agent == Susceptible\n            for w in LightGraphs.neighbors(graph,v)\n                if u[w] == Infected &amp;&amp; rand(rand_gen) &lt; p\n                    u_next[v] = Infected\n                end\n            end\n        elseif agent == Infected\n            if rand(rand_gen) &lt; recovery_rate\n                u_next[v] = Recovered\n            end\n        end\n    end\nend```","user":"U011V2YN59N","ts":"1611109097.016900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Jmfh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Implemented a simple agent based SIR model in julia as it's been a long time since I've written anything agent based. Here is the main function, any low hanging fruit I am missing?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function agents_step!(u_next,u,graph,params,t,rand_gen)\n    @unpack p, recovery_rate  = params  \n    u_next .= u\n    for (v,agent) in enumerate(u)\n        if agent == Susceptible\n            for w in LightGraphs.neighbors(graph,v)\n                if u[w] == Infected && rand(rand_gen) < p\n                    u_next[v] = Infected\n                end\n            end\n        elseif agent == Infected\n            if rand(rand_gen) < recovery_rate\n                u_next[v] = Recovered\n            end\n        end\n    end\nend"}]}]}],"thread_ts":"1611109097.016900","reply_count":1,"reply_users_count":1,"latest_reply":"1611109637.019400","reply_users":["U011V2YN59N"],"subscribed":false},{"client_msg_id":"a1925297-1d7f-45b0-884f-93f5f31c2446","type":"message","text":"`rand_gen` is an Xorshift RNG, it's faster but not significantly so.","user":"U011V2YN59N","ts":"1611109187.017900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qdfUt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"rand_gen","style":{"code":true}},{"type":"text","text":" is an Xorshift RNG, it's faster but not significantly so."}]}]}]},{"client_msg_id":"5e878dde-c501-4bf5-a31b-b6610ea5221b","type":"message","text":"I'm also curious if anyone has had luck running ABMs on the GPU, it seems like there's too much branching for it to be efficient but I am not sure.","user":"U011V2YN59N","ts":"1611109602.019300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dEUCM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm also curious if anyone has had luck running ABMs on the GPU, it seems like there's too much branching for it to be efficient but I am not sure."}]}]}],"thread_ts":"1611109602.019300","reply_count":4,"reply_users_count":2,"latest_reply":"1611113730.020800","reply_users":["U0179G7FG4F","U011V2YN59N"],"subscribed":false},{"client_msg_id":"84fa04c2-a72e-41cc-b66a-c4f720703ef9","type":"message","text":"You could run that on GPU in DynamicGrids.jl in only a few lines of code. But it will be faster on threaded CPUs using `SparseOpt()`.","user":"URN898S15","ts":"1611121709.028100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ca/K","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You could run that on GPU in DynamicGrids.jl in only a few lines of code. But it will be faster on threaded CPUs using "},{"type":"text","text":"SparseOpt()","style":{"code":true}},{"type":"text","text":"."}]}]}],"thread_ts":"1611121709.028100","reply_count":3,"reply_users_count":2,"latest_reply":"1611199810.000100","reply_users":["U011V2YN59N","URN898S15"],"subscribed":false},{"client_msg_id":"f753a398-ad73-4cc2-bba2-8b86bcdfd0db","type":"message","text":"Although I'm only guessing thats a grid based network from your code, I don't know how lightgraphs works :joy:","user":"URN898S15","ts":"1611121837.029200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vgut","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Although I'm only guessing thats a grid based network from your code, I don't know how lightgraphs works "},{"type":"emoji","name":"joy"}]}]}]},{"client_msg_id":"07a1b69d-af1f-47e1-839b-5740dd2dd19f","type":"message","text":"How long does a task need to take before it exceeds the time taken using the threading infrastructure? (Is it ~1 ms?)","user":"UDSG73JTH","ts":"1611220149.002700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kla/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How long does a task need to take before it exceeds the time taken using the threading infrastructure? (Is it ~1 ms?)"}]}]}],"thread_ts":"1611220149.002700","reply_count":6,"reply_users_count":3,"latest_reply":"1611240505.006100","reply_users":["U7HAYKY9X","UDSG73JTH","URN898S15"],"subscribed":false},{"client_msg_id":"468584d3-ae05-40dc-bbc9-26b2ff406991","type":"message","text":"I found a quick comparison code on github that was comparing C vs Java and decided to add Julia and python to the mix. The code is not pretty, but I was wondering if people could give me some feedback. Will post the code inside the thread.","user":"U013V2CFZAN","ts":"1611227612.005500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1DZK4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I found a quick comparison code on github that was comparing C vs Java and decided to add Julia and python to the mix. The code is not pretty, but I was wondering if people could give me some feedback. Will post the code inside the thread."}]}]}],"thread_ts":"1611227612.005500","reply_count":2,"reply_users_count":1,"latest_reply":"1611227688.005800","reply_users":["U013V2CFZAN"],"subscribed":false},{"client_msg_id":"7e4b6d12-09a8-4673-98b9-a145b7b65752","type":"message","text":"Do views into matrices impact the cache behavior? I was implementing some BLIS style matrix-matrix multiplies in Julia, and expected to need to add manual packing of the data to get performance for large matrices and did not need to do this. I haven't done a deep dive yet to compare with using offsets instead of views.","user":"UCRHP2GHE","ts":"1611336765.013200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nHesu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do views into matrices impact the cache behavior? I was implementing some BLIS style matrix-matrix multiplies in Julia, and expected to need to add manual packing of the data to get performance for large matrices and did not need to do this. I haven't done a deep dive yet to compare with using offsets instead of views."}]}]}],"thread_ts":"1611336765.013200","reply_count":3,"reply_users_count":2,"latest_reply":"1611337884.013700","reply_users":["UH24GRBLL","UCRHP2GHE"],"subscribed":false},{"client_msg_id":"61f157da-4046-4410-a607-a96c351d492c","type":"message","text":"Is there a way to do the following without manually unrolling or (pre)allocating an array? I accumulate over some scalar `Float64` variables and want to do this\n```u1,u2,u3 = 0.0, 0.0, 0.0\nfor i = 1:10\n  u1 += 1 \n  u2 += 3\n  u3 += 2\nend```\nbut for `N` variables (assuming `N` is encoded in some static type info and `N` is small, e.g., 3-10ish).\n\nI can do this with a preallocated array, but Id like to avoid that allocation too if possible.","user":"U011LUQ182G","ts":"1611383893.021500","team":"T68168MUP","edited":{"user":"U011LUQ182G","ts":"1611384216.000000"},"blocks":[{"type":"rich_text","block_id":"Xskab","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to do the following without manually unrolling or (pre)allocating an array? I accumulate over some scalar "},{"type":"text","text":"Float64","style":{"code":true}},{"type":"text","text":" variables and want to do this\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"u1,u2,u3 = 0.0, 0.0, 0.0\nfor i = 1:10\n  u1 += 1 \n  u2 += 3\n  u3 += 2\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"but for "},{"type":"text","text":"N","style":{"code":true}},{"type":"text","text":" variables (assuming "},{"type":"text","text":"N","style":{"code":true}},{"type":"text","text":" is encoded in some static type info and "},{"type":"text","text":"N","style":{"code":true}},{"type":"text","text":" is small, e.g., 3-10ish).\n\nI can do this with a preallocated array, but Id like to avoid that allocation too if possible."}]}]}],"thread_ts":"1611383893.021500","reply_count":7,"reply_users_count":3,"latest_reply":"1611437212.038500","reply_users":["U011LUQ182G","UD0NS8PDF","UAUPJLBQX"],"subscribed":false},{"client_msg_id":"9667088b-fbb9-4040-8889-7e05cb7a4180","type":"message","text":"Can anyone explain to me why this code allocates memory (looking at `@benchmark` ), and if it's possible to avoid it? i.e. I want to create a view of a struct's array and use it to construct another instance of the same struct. But even just creating a view of the struct's array field allocates memory. This does not happen if the function argument is an array rather than a struct instance.\n```struct Foo\n  vals\nend\n\nconst A = randn(100)\nfoo = Foo(A)\nfunction testalloc(X,inds)\n  view(X.vals, inds);\n  return nothing;\nend\n\nxs = 1:10\n@benchmark testalloc(foo,xs)```","user":"U01H36BUDJB","ts":"1611418280.027900","team":"T68168MUP","edited":{"user":"U01H36BUDJB","ts":"1611418333.000000"},"blocks":[{"type":"rich_text","block_id":"4I9++","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can anyone explain to me why this code allocates memory (looking at "},{"type":"text","text":"@benchmark","style":{"code":true}},{"type":"text","text":" ), and if it's possible to avoid it? i.e. I want to create a view of a struct's array and use it to construct another instance of the same struct. But even just creating a view of the struct's array field allocates memory. This does not happen if the function argument is an array rather than a struct instance.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"struct Foo\n  vals\nend\n\nconst A = randn(100)\nfoo = Foo(A)\nfunction testalloc(X,inds)\n  view(X.vals, inds);\n  return nothing;\nend\n\nxs = 1:10\n@benchmark testalloc(foo,xs)"}]}]}],"thread_ts":"1611418280.027900","reply_count":35,"reply_users_count":5,"latest_reply":"1611428030.038000","reply_users":["U7HAYKY9X","U0179G7FG4F","U01H36BUDJB","U01GRS159T8","UH24GRBLL"],"subscribed":false},{"client_msg_id":"efce8b63-3113-4f39-988e-f33158a1908f","type":"message","text":"Anyone know if `Interpolations.jl` is supposed to be allocation free? It appears that even a simple example with the interpolation object and query points fixed causes allocation inside of a function:\n```const A = randn(100);\nconst A_i = interpolate(A,BSpline(Linear()));\nfunction interpalloc(y,xs,out)\n    out .= y(xs)\nend\n@benchmark interpalloc(A_i,xs,out)\n\nBenchmarkTools.Trial: \n  memory estimate:  496 bytes\n  allocs estimate:  2\n  --------------\n  minimum time:     142.632 ns (0.00% GC)\n  median time:      146.349 ns (0.00% GC)\n  mean time:        161.094 ns (4.74% GC)\n  maximum time:     1.289 s (84.83% GC)\n  --------------\n  samples:          10000\n  evals/sample:     846```","user":"U01H36BUDJB","ts":"1611496824.039900","team":"T68168MUP","edited":{"user":"U01H36BUDJB","ts":"1611496916.000000"},"blocks":[{"type":"rich_text","block_id":"8KC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Anyone know if "},{"type":"text","text":"Interpolations.jl","style":{"code":true}},{"type":"text","text":" is supposed to be allocation free? It appears that even a simple example with the interpolation object and query points fixed causes allocation inside of a function:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"const A = randn(100);\nconst A_i = interpolate(A,BSpline(Linear()));\nfunction interpalloc(y,xs,out)\n    out .= y(xs)\nend\n@benchmark interpalloc(A_i,xs,out)\n\nBenchmarkTools.Trial: \n  memory estimate:  496 bytes\n  allocs estimate:  2\n  --------------\n  minimum time:     142.632 ns (0.00% GC)\n  median time:      146.349 ns (0.00% GC)\n  mean time:        161.094 ns (4.74% GC)\n  maximum time:     1.289 s (84.83% GC)\n  --------------\n  samples:          10000\n  evals/sample:     846"}]}]}],"thread_ts":"1611496824.039900","reply_count":7,"reply_users_count":3,"latest_reply":"1611610936.041400","reply_users":["U67G3QRJM","U01H36BUDJB","USU9FRPEU"],"subscribed":false},{"client_msg_id":"90b8a123-7e58-4953-beff-e9f35ea3c0b3","type":"message","text":"So I currently am having a mini-contest between a co-worker of mine about performance.\n\nHe claims that Java is one of the fastest languages for his toy-problem.\n\nI wrote a Julia version but my code currently runs at &lt;2x the speed of his Java code.\n\nCan anyone suggest to me some ways to increase my performance? Also feel free to give any un-solicited advice about my coding style :slightly_smiling_face:\nNote this has to all follow the same structure (e.g. single threaded, must use recursion)\n\nThe toy problem is just <https://en.wikipedia.org/wiki/Peg_solitaire|peg solitaire> but on a triangular board. It involves lots and lots of array append and lots of recursion.\n\n<https://github.com/ayman-albaz/peg-performance/blob/master/src/main/julia/performance.jl>","user":"U01FAHWCMFF","ts":"1611856909.043900","team":"T68168MUP","edited":{"user":"U01FAHWCMFF","ts":"1611856973.000000"},"attachments":[{"image_url":"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0b/Peg_Solitaire_1687_on_Portrait_of_Princess_Soubise_by_Claude-Auguste_Berey.jpg/1200px-Peg_Solitaire_1687_on_Portrait_of_Princess_Soubise_by_Claude-Auguste_Berey.jpg","image_width":1200,"image_height":1626,"image_bytes":797938,"title":"Peg solitaire","title_link":"https://en.wikipedia.org/wiki/Peg_solitaire","from_url":"https://en.wikipedia.org/wiki/Peg_solitaire","author_name":"Wikipedia","author_link":"https://en.wikipedia.org/","text":"Peg solitaire (or Solo Noble) is a board game for one player involving movement of pegs on a board with holes.  Some sets use marbles in a board with indentations. The game is known simply as Solitaire in the United Kingdom where the card games are called Patience.  It is also referred to as Brainvita (mainly in India, where sets are sold commercially under this name).\nThe first evidence of the game can be traced back to the court of Louis XIV, and the specific date of 1697, with an engraving made ten years later by Claude Auguste Berey of Anne de Rohan-Chabot, Princess of Soubise, with the puzzle by her side.  The August 1687 edition of the French literary magazine Mercure galant contains a description of the board, rules and sample problems.  This is the first known reference to the game in print.\nThe standard game fills the entire board with pegs except for the central hole. The objective is, making valid moves, to empty the entire board except for a solitary peg in the central hole.","fallback":"wikipedia: Peg solitaire","service_icon":"https://a.slack-edge.com/80588/img/unfurl_icons/wikipedia.png","id":1,"original_url":"https://en.wikipedia.org/wiki/Peg_solitaire"}],"blocks":[{"type":"rich_text","block_id":"SkErK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So I currently am having a mini-contest between a co-worker of mine about performance.\n\nHe claims that Java is one of the fastest languages for his toy-problem.\n\nI wrote a Julia version but my code currently runs at <2x the speed of his Java code.\n\nCan anyone suggest to me some ways to increase my performance? Also feel free to give any un-solicited advice about my coding style "},{"type":"emoji","name":"slightly_smiling_face"},{"type":"text","text":"\nNote this has to all follow the same structure (e.g. single threaded, must use recursion)\n\nThe toy problem is just "},{"type":"link","url":"https://en.wikipedia.org/wiki/Peg_solitaire","text":"peg solitaire"},{"type":"text","text":" but on a triangular board. It involves lots and lots of array append and lots of recursion.\n\n"},{"type":"link","url":"https://github.com/ayman-albaz/peg-performance/blob/master/src/main/julia/performance.jl"}]}]}],"thread_ts":"1611856909.043900","reply_count":2,"reply_users_count":2,"latest_reply":"1611858024.047800","reply_users":["U01GMP3HF9C","U01FAHWCMFF"],"subscribed":false},{"client_msg_id":"756bdf61-d65d-4506-8ce7-1132b4862fd2","type":"message","text":"How long does the vectors typically get? Could you get away with tuples instead of vectors to avoid the many allocations? Alternatively, preallocate arrays and use sizehint or send the current size along","user":"UJ7DVTVQ8","ts":"1611857350.045900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"y6Y7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How long does the vectors typically get? Could you get away with tuples instead of vectors to avoid the many allocations? Alternatively, preallocate arrays and use sizehint or send the current size along"}]}]}],"thread_ts":"1611857350.045900","reply_count":2,"reply_users_count":2,"latest_reply":"1611857606.047400","reply_users":["U0179G7FG4F","UJ7DVTVQ8"],"subscribed":false},{"client_msg_id":"55e79744-8434-41ae-9418-76de876e1766","type":"message","text":"I'm not too familiar with the math behind the game, but the final size of the array i guess is determined through exhaustively iterating through every possible combination of moves.","user":"U01FAHWCMFF","ts":"1611857421.046900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5lDxf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm not too familiar with the math behind the game, but the final size of the array i guess is determined through exhaustively iterating through every possible combination of moves."}]}]}]},{"client_msg_id":"09afd3a8-6707-4ac0-a024-7edab5a1c8e6","type":"message","text":"Can you post the Java version?","user":"U0179G7FG4F","ts":"1611857534.047100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3VVXT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can you post the Java version?"}]}]}]},{"client_msg_id":"d7ba8eaf-11e9-4570-9c3d-991f59eb4e58","type":"message","text":"<https://github.com/ayman-albaz/peg-performance/tree/master/src/main/java>\nJust run the .sh file.","user":"U01FAHWCMFF","ts":"1611857557.047300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pEAbz","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/ayman-albaz/peg-performance/tree/master/src/main/java"},{"type":"text","text":"\nJust run the .sh file."}]}]}]},{"client_msg_id":"f9ac3b8d-f6b8-4d96-9611-c45788550d34","type":"message","text":"I just want to point out that data structures and models are not part of a problem but part of a solution. It seems to me that you're contest is slightly biased.\nNow I haven't look at the code or anything so ... Sorry to disrupt your thread","user":"U01FR2HFJ7M","ts":"1611880767.055400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TfJ5u","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I just want to point out that data structures and models are not part of a problem but part of a solution. It seems to me that you're contest is slightly biased.\nNow I haven't look at the code or anything so ... Sorry to disrupt your thread"}]}]}]}]}