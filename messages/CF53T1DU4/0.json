{"cursor": 2, "messages": [{"client_msg_id":"58ebbfb1-37ed-4237-b082-940f95585a9a","type":"message","text":"I'll keep an eye on the new version then","user":"U7PD3M3L5","ts":"1613045744.236500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"q7o","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'll keep an eye on the new version then"}]}]}]},{"client_msg_id":"60165e23-d31a-40d4-9bea-4c8903403f6e","type":"message","text":"but I'm actually very impressed by the loop speed here","user":"U7PD3M3L5","ts":"1613045752.236800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TXtZ+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"but I'm actually very impressed by the loop speed here"}]}]}],"reactions":[{"name":"sonic","users":["UH24GRBLL"],"count":1}]},{"client_msg_id":"05de277b-c2f9-4c03-906d-8ac50a8c8eb7","type":"message","text":"and the easiness of adding `@avx`  whatever it does","user":"U7PD3M3L5","ts":"1613045768.237100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"g45","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"and the easiness of adding "},{"type":"text","text":"@avx","style":{"code":true}},{"type":"text","text":"  whatever it does"}]}]}]},{"client_msg_id":"1a807543-008a-4a51-83d8-1b04935bc9a2","type":"message","text":"That’s some real black magic.","user":"UD0NS8PDF","ts":"1613045800.237500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"z9Nz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That’s some real black magic."}]}]}]},{"client_msg_id":"fb55657d-429b-4869-aaa6-96067eda334a","type":"message","text":"it (on one hand) does loop reordering, on the other it merges loads (almost) optimally, such that the arithmetic unit in your CPU is always busy","user":"UH24GRBLL","ts":"1613045836.238300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VWL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it (on one hand) does loop reordering, on the other it merges loads (almost) optimally, such that the arithmetic unit in your CPU is always busy"}]}]}]},{"client_msg_id":"1a6bec46-b759-41c5-957e-f19631212747","type":"message","text":"you have to be careful though - if you don't respect its rules, it can wreck your computation","user":"UH24GRBLL","ts":"1613045872.238700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"spa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you have to be careful though - if you don't respect its rules, it can wreck your computation"}]}]}]},{"client_msg_id":"25a8487e-9226-4438-95b6-105cbd62938f","type":"message","text":"Always respect the warning! :) <https://juliahub.com/ui/Packages/LoopVectorization/4TogI/0.11.2#Warning>","user":"UH24GRBLL","ts":"1613045882.238900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"EjZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Always respect the warning! :) "},{"type":"link","url":"https://juliahub.com/ui/Packages/LoopVectorization/4TogI/0.11.2#Warning"}]}]}]},{"client_msg_id":"abd12932-fcee-4a62-899b-c5b030b05b47","type":"message","text":"haha ok thanks","user":"U7PD3M3L5","ts":"1613046602.239100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"P0M","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"haha ok thanks"}]}]}]},{"client_msg_id":"3bc06c81-cd6b-4c6b-860a-1cf31bdd37a3","type":"message","text":"actually I just copied it from your code, did not read the docs or the warning","user":"U7PD3M3L5","ts":"1613046613.239600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+tgr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"actually I just copied it from your code, did not read the docs or the warning"}]}]}]},{"client_msg_id":"3b54ac71-1390-45be-95c6-e5f2a4147bab","type":"message","text":"the not iterating `1:2:N`  could have gotten me at some point","user":"U7PD3M3L5","ts":"1613046624.239900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SV404","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"the not iterating "},{"type":"text","text":"1:2:N","style":{"code":true}},{"type":"text","text":"  could have gotten me at some point"}]}]}]},{"client_msg_id":"3cb2ca98-a2d6-4048-bb05-a09393974124","type":"message","text":"I'll probably remove the `1:2:N` limitation fairly soon.","user":"UAUPJLBQX","ts":"1613076272.241000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jLN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'll probably remove the "},{"type":"text","text":"1:2:N","style":{"code":true}},{"type":"text","text":" limitation fairly soon."}]}]}]},{"client_msg_id":"e073d807-a592-4178-98a0-1c70cd67c63a","type":"message","text":"what package is @cast in?","user":"U01GRS159T8","ts":"1613175525.241800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yABHt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"what package is @cast in?"}]}]}],"thread_ts":"1613175525.241800","reply_count":1,"reply_users_count":1,"latest_reply":"1613175558.241900","reply_users":["U0179G7FG4F"],"subscribed":false},{"client_msg_id":"ab3f7a05-3711-42d3-b97d-9de42916424e","type":"message","text":"Does anyone have experience benchmarking code in CI? I have manual benchmarks and I would like to add performance tests, but this feels like a terrible idea due to how inconsistent the benchmarks may be. The median time per sample is about 11 microseconds and CI would be GitHub actions. I would want to write a test that verifies the median sample time is 11 +/- 1 uS and fails if it falls outside that range.","user":"U01537M2E9W","ts":"1613596642.245300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SBs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does anyone have experience benchmarking code in CI? I have manual benchmarks and I would like to add performance tests, but this feels like a terrible idea due to how inconsistent the benchmarks may be. The median time per sample is about 11 microseconds and CI would be GitHub actions. I would want to write a test that verifies the median sample time is 11 +/- 1 uS and fails if it falls outside that range."}]}]}],"thread_ts":"1613596642.245300","reply_count":1,"reply_users_count":1,"latest_reply":"1613597124.245400","reply_users":["UCZ7VBGUD"],"subscribed":false},{"client_msg_id":"e6b257d3-cf7e-4737-b274-9511bd567a2b","type":"message","text":"I need to numerically integrate a slightly complicated function many times, so I'm trying to find a fast way to do it. I don't necessarily need super high precision (ideally beyond 1e-5 but I could make do with less)\n\nI'm currently trying the following with `FastGaussQuadrature` . Might anyone have a suggestion for how to improve performance?\n```function fastgq(f, a, b)\n    nodes, weights = gausslegendre( 10000 );\n    \n    x = 0.5*((b .- a).*nodes .+ (b.+a))\n    \n    out = 0.5*(b.-a)*dot( weights, f.(x) )\n    \n    return(out)\nend```","user":"U91Q3595Y","ts":"1613615785.248800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+0fn9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I need to numerically integrate a slightly complicated function many times, so I'm trying to find a fast way to do it. I don't necessarily need super high precision (ideally beyond 1e-5 but I could make do with less)\n\nI'm currently trying the following with "},{"type":"text","text":"FastGaussQuadrature","style":{"code":true}},{"type":"text","text":" . Might anyone have a suggestion for how to improve performance?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function fastgq(f, a, b)\n    nodes, weights = gausslegendre( 10000 );\n    \n    x = 0.5*((b .- a).*nodes .+ (b.+a))\n    \n    out = 0.5*(b.-a)*dot( weights, f.(x) )\n    \n    return(out)\nend"}]}]}],"thread_ts":"1613615785.248800","reply_count":9,"reply_users_count":2,"latest_reply":"1613616936.250500","reply_users":["U0179G7FG4F","U91Q3595Y"],"subscribed":false},{"client_msg_id":"826e815f-b75c-4389-be99-72a8fba07e21","type":"message","text":"When you return an array from a function, it always allocates, correct? Is there a way to just return a pointer/reference to that array? Wrap it in `Ref` maybe?","user":"U01H36BUDJB","ts":"1613733455.261300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"y84x","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"When you return an array from a function, it always allocates, correct? Is there a way to just return a pointer/reference to that array? Wrap it in "},{"type":"text","text":"Ref","style":{"code":true}},{"type":"text","text":" maybe?"}]}]}],"thread_ts":"1613733455.261300","reply_count":1,"reply_users_count":1,"latest_reply":"1613733532.261400","reply_users":["UD0NS8PDF"],"subscribed":false},{"client_msg_id":"e54b6091-4f3d-4635-af2a-2d0f41b6350c","type":"message","text":"Could anyone familiar with ComponentArrays explain why this line seems to be type-unstable with `@code_warntype` despite all of the present variables (i.e. `values` ,`start` ,`stop`) being fully known? It's really perplexing me.\n```newvalues = ComponentArray(values,(Axis{(edges=start:2:stop,cells=start+1:2:stop-1)}(),))```","user":"U01H36BUDJB","ts":"1613749045.266000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OpeP3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Could anyone familiar with ComponentArrays explain why this line seems to be type-unstable with "},{"type":"text","text":"@code_warntype","style":{"code":true}},{"type":"text","text":" despite all of the present variables (i.e. "},{"type":"text","text":"values","style":{"code":true}},{"type":"text","text":" ,"},{"type":"text","text":"start","style":{"code":true}},{"type":"text","text":" ,"},{"type":"text","text":"stop","style":{"code":true}},{"type":"text","text":") being fully known? It's really perplexing me.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"newvalues = ComponentArray(values,(Axis{(edges=start:2:stop,cells=start+1:2:stop-1)}(),))"}]}]}],"thread_ts":"1613749045.266000","reply_count":1,"reply_users_count":1,"latest_reply":"1613749075.266100","reply_users":["U01H36BUDJB"],"subscribed":false},{"type":"message","subtype":"channel_join","ts":"1613754988.266600","user":"US4A6G6B0","text":"<@US4A6G6B0> has joined the channel","inviter":"U69BL50BF"},{"client_msg_id":"17b57b98-42b8-4788-82fd-dc8f71819b25","type":"message","text":"What's the best way to debug long compilation times for a function? i.e. I want to figure out what's slowing it down.","user":"U01H36BUDJB","ts":"1613832812.000700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"F8a4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What's the best way to debug long compilation times for a function? i.e. I want to figure out what's slowing it down."}]}]}]},{"client_msg_id":"5181e65a-5662-48ab-962a-bfc6a60373d2","type":"message","text":"Hey","user":"U01MG0TN079","ts":"1613894462.015400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"d1p","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey"}]}]}]},{"client_msg_id":"5c3c9cbd-b681-420a-b1b9-4ff9ba57ef9f","type":"message","text":"Say I'm working with rational numbers, with a known bound on the denominator (i.e. all can be written as n/K for some known integer K), how much slower is it to work with Rational{Int} rather than Int ?","user":"U01MG0TN079","ts":"1613894514.016400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lYt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Say I'm working with rational numbers, with a known bound on the denominator (i.e. all can be written as n/K for some known integer K), how much slower is it to work with Rational{Int} rather than Int ?"}]}]}]},{"client_msg_id":"0378c30a-b2c0-4b0a-8144-d9fe520b1c8c","type":"message","text":"Another question: What would be the best way to type a vector of collections whose types depend on their index in the vector","user":"U01MG0TN079","ts":"1613910244.018000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/8QL6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Another question: What would be the best way to type a vector of collections whose types depend on their index in the vector"}]}]}]},{"client_msg_id":"d0161884-6800-4511-b9db-e1f06161fb68","type":"message","text":"`[Vector{SVector{1}},Vector{SVector{2}},…]`","user":"U01MG0TN079","ts":"1613910283.018800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xOEC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"[Vector{SVector{1}},Vector{SVector{2}},…]","style":{"code":true}}]}]}]},{"client_msg_id":"c49b7c69-850b-403b-87f1-253c9649d246","type":"message","text":"Since the length of such a vector of collection is bounded in my case, by around 20, I wonder if I should just define 20 custom types, or do it with a macro?","user":"U01MG0TN079","ts":"1613910458.019700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xJl9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Since the length of such a vector of collection is bounded in my case, by around 20, I wonder if I should just define 20 custom types, or do it with a macro?"}]}]}],"thread_ts":"1613910458.019700","reply_count":2,"reply_users_count":2,"latest_reply":"1613911670.020000","reply_users":["U7HAYKY9X","U01MG0TN079"],"subscribed":false},{"client_msg_id":"71d5d8c1-a106-4932-95c4-b34190f1f523","type":"message","text":"Is accessing nested named tuples supposed to allocate? Like for example, if I do `a.b.c.X` where `a`,`b`,`c` are all nested `NamedTuple` s, and `X` is the name of an array stored in `c` this shouldn't allocate right?","user":"U01H36BUDJB","ts":"1613999608.023300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5Vp5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is accessing nested named tuples supposed to allocate? Like for example, if I do "},{"type":"text","text":"a.b.c.X","style":{"code":true}},{"type":"text","text":" where "},{"type":"text","text":"a","style":{"code":true}},{"type":"text","text":","},{"type":"text","text":"b","style":{"code":true}},{"type":"text","text":","},{"type":"text","text":"c","style":{"code":true}},{"type":"text","text":" are all nested "},{"type":"text","text":"NamedTuple","style":{"code":true}},{"type":"text","text":" s, and "},{"type":"text","text":"X","style":{"code":true}},{"type":"text","text":" is the name of an array stored in "},{"type":"text","text":"c","style":{"code":true}},{"type":"text","text":" this shouldn't allocate right?"}]}]}],"thread_ts":"1613999608.023300","reply_count":1,"reply_users_count":1,"latest_reply":"1613999699.023400","reply_users":["U7HAYKY9X"],"subscribed":false},{"client_msg_id":"d6813445-8124-4ede-8951-47178b0c4ee9","type":"message","text":"<@UAUPJLBQX> thoughts on <https://github.com/dzhang314/EFTBase.jl/pull/2>? specifically, whether `fast_two_sum` or `two_sum` is likely to be faster in practice.","user":"U0179G7FG4F","ts":"1614028162.024800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pGDzI","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UAUPJLBQX"},{"type":"text","text":" thoughts on "},{"type":"link","url":"https://github.com/dzhang314/EFTBase.jl/pull/2"},{"type":"text","text":"? specifically, whether "},{"type":"text","text":"fast_two_sum","style":{"code":true}},{"type":"text","text":" or "},{"type":"text","text":"two_sum","style":{"code":true}},{"type":"text","text":" is likely to be faster in practice."}]}]}]},{"client_msg_id":"dc93d659-140f-42ff-8693-4c7e2b268b9c","type":"message","text":"not Chris, but as the comments say - it's highly dependent on what happens in the CPU/microcode","user":"UH24GRBLL","ts":"1614028521.025300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dB0F","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"not Chris, but as the comments say - it's highly dependent on what happens in the CPU/microcode"}]}]}]},{"client_msg_id":"72e55605-a2fd-4c97-a9c3-f9c2e523a946","type":"message","text":"Is there a way we could automatically (at build time) pick the best one?","user":"U0179G7FG4F","ts":"1614028619.025800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ArUN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way we could automatically (at build time) pick the best one?"}]}]}]},{"client_msg_id":"67c3a264-36a1-4ee0-9dfc-d649310b1660","type":"message","text":"have a large lookup table checking which is faster on which specific architecture and `@static` which method you define","user":"UH24GRBLL","ts":"1614028652.026400","team":"T68168MUP","edited":{"user":"UH24GRBLL","ts":"1614028661.000000"},"blocks":[{"type":"rich_text","block_id":"5bnE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"have a large lookup table checking which is faster on which specific architecture and "},{"type":"text","text":"@static","style":{"code":true}},{"type":"text","text":" which method you define"}]}]}]},{"client_msg_id":"aa49fb2d-e16c-4e20-aee1-61c75727742c","type":"message","text":"Do you think that's actually feasible? Would an approach where you benchmark both during pre-compile work?","user":"U0179G7FG4F","ts":"1614028872.027300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"it2D","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do you think that's actually feasible? Would an approach where you benchmark both during pre-compile work?"}]}]}]},{"client_msg_id":"9bb70c70-ab4f-497d-aaf2-2355c83d580f","type":"message","text":"You'd have to benchmark after compilation","user":"UH24GRBLL","ts":"1614030207.027800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VO2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You'd have to benchmark after compilation"}]}]}]},{"client_msg_id":"425e1a78-d9ea-4373-9f39-16e603927b5b","type":"message","text":"As I understand it, that's how all the smart optimizations in LLVM and gcc are compared - benchmark it and find out which is faster, then emit that code.","user":"UH24GRBLL","ts":"1614030283.028600","team":"T68168MUP","edited":{"user":"UH24GRBLL","ts":"1614030312.000000"},"blocks":[{"type":"rich_text","block_id":"XAJa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"As I understand it, that's how all the smart optimizations in LLVM and gcc are compared - benchmark it and find out which is faster, then emit that code."}]}]}]},{"client_msg_id":"33cde73e-7c55-41f6-bce7-6ce31c8d5735","type":"message","text":"Maybe we could use Preferences.jl","user":"UAUPJLBQX","ts":"1614034597.029000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HuL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Maybe we could use Preferences.jl"}]}]}]},{"client_msg_id":"75f9fccb-c758-472b-b439-2bb8bdbc7a16","type":"message","text":"pick a default (`two_sum`), and have a function `tune!` where it runs benchmarks, and then sets preferences accordingly?","user":"UAUPJLBQX","ts":"1614034615.029500","team":"T68168MUP","edited":{"user":"UAUPJLBQX","ts":"1614034632.000000"},"blocks":[{"type":"rich_text","block_id":"WiC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"pick a default ("},{"type":"text","text":"two_sum","style":{"code":true}},{"type":"text","text":"), and have a function "},{"type":"text","text":"tune!","style":{"code":true}},{"type":"text","text":" where it runs benchmarks, and then sets preferences accordingly?"}]}]}]},{"client_msg_id":"50137ac0-dc9b-4bad-be53-aea0b4cffdb0","type":"message","text":"You'd have to take care not to fall into benchmarking artifacts due to other programs running at the same time - could very well mess with the branching version","user":"UH24GRBLL","ts":"1614062149.030900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OxE7P","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You'd have to take care not to fall into benchmarking artifacts due to other programs running at the same time - could very well mess with the branching version"}]}]}]},{"client_msg_id":"c5dbdcbb-84e1-46f3-b1c1-ce09e7246032","type":"message","text":"Does anyone have a good way of making a struct use a memory arena? <https://github.com/tonyrubak/MemoryArena.jl> Looks unmaintained, and it would be nice to have a go-to idiom for this","user":"U0179G7FG4F","ts":"1614393044.032100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LpmUQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does anyone have a good way of making a struct use a memory arena? "},{"type":"link","url":"https://github.com/tonyrubak/MemoryArena.jl"},{"type":"text","text":" Looks unmaintained, and it would be nice to have a go-to idiom for this"}]}]}]},{"client_msg_id":"25c54e4e-8987-4d0b-93f8-16ac41f953f0","type":"message","text":"Hi there! I am stuck with a perf comparison with C code which is almost twice as fast as my julia version (I gain 10% if call julia with fastmath mode, but it still is far from C). Of course I can use threads easily, but that is kind of not the point. Any obvious improvement from the following julia code?\n```using BenchmarkTools\n\ncx(x, side, scale) = x / side * 2 * scale - scale\ncy(y...) = cx(y...) * 1im\n\nfunction solvePixel(i, i_n, side, center, scale)\n    y, x = divrem(i, side)\n    c = cx(x, side, scale) + cy(y, side, scale) + center\n    z = 0im\n    k = 0\n    while (k &lt; i_n)\n        z = z * z + c\n        abs(z) &lt;= 2 ? k += 1 : break \n    end\n    return 1 - k / i_n\nend\n\nfunction res_(idx, i_n, side, center, scale)\n    return UInt8(floor(solvePixel(idx, i_n, side, center, scale) * 255))\nend\n\nfunction main(; i_n=512, side=2048, center=0.4 + 0.4im, scale=0.3)\n    open(\"out.ppm\", \"w\") do io\n        write(io, \"P6 $side $side 255 \")\n        buffer = Matrix{UInt8}(undef, side, side * 3)\n        for i in 0:side - 1\n            tmp = i * side\n            for j in 1:side\n                idx = tmp + j\n                res = res_(idx, i_n, side, center, scale)\n                @inbounds buffer[i + 1,j * 3 - 2:j * 3] .= res\n            end\n        end                     \n        write(io, buffer)       \n    end\nend\n\n@benchmark main()```","user":"U01FR2HFJ7M","ts":"1614914041.036100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"K5nyJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi there! I am stuck with a perf comparison with C code which is almost twice as fast as my julia version (I gain 10% if call julia with fastmath mode, but it still is far from C). Of course I can use threads easily, but that is kind of not the point. Any obvious improvement from the following julia code?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using BenchmarkTools\n\ncx(x, side, scale) = x / side * 2 * scale - scale\ncy(y...) = cx(y...) * 1im\n\nfunction solvePixel(i, i_n, side, center, scale)\n    y, x = divrem(i, side)\n    c = cx(x, side, scale) + cy(y, side, scale) + center\n    z = 0im\n    k = 0\n    while (k < i_n)\n        z = z * z + c\n        abs(z) <= 2 ? k += 1 : break \n    end\n    return 1 - k / i_n\nend\n\nfunction res_(idx, i_n, side, center, scale)\n    return UInt8(floor(solvePixel(idx, i_n, side, center, scale) * 255))\nend\n\nfunction main(; i_n=512, side=2048, center=0.4 + 0.4im, scale=0.3)\n    open(\"out.ppm\", \"w\") do io\n        write(io, \"P6 $side $side 255 \")\n        buffer = Matrix{UInt8}(undef, side, side * 3)\n        for i in 0:side - 1\n            tmp = i * side\n            for j in 1:side\n                idx = tmp + j\n                res = res_(idx, i_n, side, center, scale)\n                @inbounds buffer[i + 1,j * 3 - 2:j * 3] .= res\n            end\n        end                     \n        write(io, buffer)       \n    end\nend\n\n@benchmark main()"}]}]}]},{"client_msg_id":"e78c4329-9240-4895-9759-d32c8c900699","type":"message","text":"Is there a way to speed up this calculate this function faster? `x` and `y` are vectors `W` and `W_in` are matrices and activation is `tanh`\n```function foo(activation,W, W_in, x, y)\n    return activation.((W*x)+(W_in*y))\nend```","user":"U01C3624SGJ","ts":"1614956001.041400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lBLx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to speed up this calculate this function faster? "},{"type":"text","text":"x","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" are vectors "},{"type":"text","text":"W","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"W_in","style":{"code":true}},{"type":"text","text":" are matrices and activation is "},{"type":"text","text":"tanh","style":{"code":true}},{"type":"text","text":"\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function foo(activation,W, W_in, x, y)\n    return activation.((W*x)+(W_in*y))\nend"}]}]}],"thread_ts":"1614956001.041400","reply_count":3,"reply_users_count":3,"latest_reply":"1614956755.041900","reply_users":["U7HAYKY9X","U0179G7FG4F","U01C3624SGJ"],"subscribed":false},{"client_msg_id":"6603f50a-416b-4553-a5c4-5325513394d8","type":"message","text":"I would greatly appreciate if someone experienced with threading design commented in <https://github.com/JuliaData/DataFrames.jl/pull/2647> what conditions we should use in DataFrames.jl to turn-on using multi-threading (I can do it by trial and error, but maybe there are some concrete recommendations about conditions when one can expect to get speedup from using `Threads.@threads` - essentially the question is what is the cost of setting up and handling task switching). Thank you! CC <@U67431ELR>","user":"U8JAMQGQY","ts":"1615212213.050700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"syM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I would greatly appreciate if someone experienced with threading design commented in "},{"type":"link","url":"https://github.com/JuliaData/DataFrames.jl/pull/2647"},{"type":"text","text":" what conditions we should use in DataFrames.jl to turn-on using multi-threading (I can do it by trial and error, but maybe there are some concrete recommendations about conditions when one can expect to get speedup from using "},{"type":"text","text":"Threads.@threads","style":{"code":true}},{"type":"text","text":" - essentially the question is what is the cost of setting up and handling task switching). Thank you! CC "},{"type":"user","user_id":"U67431ELR"}]}]}],"thread_ts":"1615212213.050700","reply_count":5,"reply_users_count":2,"latest_reply":"1615213067.051600","reply_users":["U67431ELR","U8JAMQGQY"],"subscribed":false},{"client_msg_id":"7c432943-a5bc-429c-ad3c-32fbb3a2f18b","type":"message","text":"If I have an immutable isbits struct, and I create a Vector of them, is there any way (unsafe, hacky) to modify a single field of an element? Similar to what I could do with a reference in c++.","user":"UCNPT22MQ","ts":"1615324052.056100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6sN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If I have an immutable isbits struct, and I create a Vector of them, is there any way (unsafe, hacky) to modify a single field of an element? Similar to what I could do with a reference in c++."}]}]}]},{"client_msg_id":"e1e271c2-52e8-4f22-a15b-e2a682590320","type":"message","text":"if you have a function that returns a new instance of that immutable type with that field set differently, the compiler will probably just write that for you since you're semantically unable to observe whether the element changed to a new instance","user":"UH24GRBLL","ts":"1615324245.057100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7WwM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"if you have a function that returns a new instance of that immutable type with that field set differently, the compiler will probably just write that for you since you're semantically unable to observe whether the element changed to a new instance"}]}]}]},{"client_msg_id":"eb89ae35-b4cb-403d-9f84-fdc8b4d83a7e","type":"message","text":"that's the whole point of immutability","user":"UH24GRBLL","ts":"1615324261.057400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8GuGr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"that's the whole point of immutability"}]}]}]},{"client_msg_id":"d95e40be-91b2-45c0-90f6-fbc45ce1ba35","type":"message","text":"Hmm, interesting, thanks. It does look like the assembly only modifies the single value, though there is no SIMD happening (unlike the C++ version which generates interesting scatter / strided ops)","user":"UCNPT22MQ","ts":"1615324655.061600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kHy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hmm, interesting, thanks. It does look like the assembly only modifies the single value, though there is no SIMD happening (unlike the C++ version which generates interesting scatter / strided ops)"}]}]}]},{"client_msg_id":"07b79084-1407-4810-8ac7-0840867c1129","type":"message","text":"Not sure if the fact that the llvm is far more complex and indirect for the Julia version prevents it from seeing it","user":"UCNPT22MQ","ts":"1615324678.062400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fQic","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not sure if the fact that the llvm is far more complex and indirect for the Julia version prevents it from seeing it"}]}]}]},{"client_msg_id":"10557393-6712-46e7-a0d4-a8c913bd04ba","type":"message","text":"I'm just doing something like:\n```for i in 1:length(vals)\n    old = vals[i]\n    vals[i] = MyStruct(old.a, old.b, old.c + 10)\nend```","user":"UCNPT22MQ","ts":"1615324721.063200","team":"T68168MUP","edited":{"user":"UCNPT22MQ","ts":"1615324757.000000"},"blocks":[{"type":"rich_text","block_id":"yLLr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm just doing something like:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"for i in 1:length(vals)\n    old = vals[i]\n    vals[i] = MyStruct(old.a, old.b, old.c + 10)\nend"}]}]}]},{"client_msg_id":"7b5140e3-d9e6-4736-ab14-0ebb6a253c74","type":"message","text":"Does anyone know whats wrong with this:\n`@reduce expectation[i] := @sum(n) 1/ensemble_size * κ(Θ[:, n], Φ[:, i])`?\n`Tensorcast` gives me\n```LoadError: don't know what to do with $(Expr(:(:=), :(expectation[i]), :(#= In[37]:7 =# @sum n))) \n    @reduce expectation[i] := #= In[37]:7 =# @sum n  (1 / ensemble_size) * κ(Θ[:, n], Φ[:, i])\nin expression starting at In[37]:7```","user":"U7PD3M3L5","ts":"1615886346.000700","team":"T68168MUP","edited":{"user":"U7PD3M3L5","ts":"1615886360.000000"},"blocks":[{"type":"rich_text","block_id":"+Hga","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does anyone know whats wrong with this:\n`@reduce expectation[i] := @sum(n) 1/ensemble_size * κ(Θ[:, n], Φ[:, i])`?\n"},{"type":"text","text":"Tensorcast","style":{"code":true}},{"type":"text","text":" gives me\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"LoadError: don't know what to do with $(Expr(:(:=), :(expectation[i]), :(#= In[37]:7 =# @sum n))) \n    @reduce expectation[i] := #= In[37]:7 =# @sum n  (1 / ensemble_size) * κ(Θ[:, n], Φ[:, i])\nin expression starting at In[37]:7"}]}]}]},{"client_msg_id":"16cd862c-b805-4ab0-8a35-d21ca6e6c1d2","type":"message","text":"If I have a function that needs _some_ arguments from a struct but not all, is it quicker to pass arguments individually or is there no performance penalty for passing the whole struct (assuming it is fully typed, doesn't generate type instability etc.)?\n\nFrom tests some small tests it appears that there is a performance penalty. Is this something that can be improved by inlining?\n\nIdeally I'd like to pass the full struct to keep the code neat (lots of arguments required). Cheers!","user":"U017FUJDBT7","ts":"1615949804.002300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QOZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If I have a function that needs "},{"type":"text","text":"some","style":{"italic":true}},{"type":"text","text":" arguments from a struct but not all, is it quicker to pass arguments individually or is there no performance penalty for passing the whole struct (assuming it is fully typed, doesn't generate type instability etc.)?\n\nFrom tests some small tests it appears that there is a performance penalty. Is this something that can be improved by inlining?\n\nIdeally I'd like to pass the full struct to keep the code neat (lots of arguments required). Cheers!"}]}]}],"thread_ts":"1615949804.002300","reply_count":4,"reply_users_count":3,"latest_reply":"1615951001.004000","reply_users":["U017FUJDBT7","U0179G7FG4F","UDD5Z7FLZ"],"subscribed":false},{"type":"message","text":"I have many parametric structs like:\n```\nstruct GeneralConstraint{IndexType,WeightType,N} &lt;: AbstractClause\n    Lb::WeightType\n    Ub::WeightType\n    Weight::SVector{N,WeightType}\n    Literals::SVector{N,Literal{IndexType}}\nend\n```\nwhere the N differs in general. IndexType and WeightType are always the same.\nHow do i best store them, order does not matter but i should be able to iterate all of them and append.","user":"U9MD78Z9N","ts":"1616041467.005300","team":"T68168MUP"},{"type":"message","text":"Next step question, is there something more performant i can do if i have a collection that stays unchanged and i can sort as i want and am in control how to access it?","user":"U9MD78Z9N","ts":"1616043494.005400","team":"T68168MUP","edited":{"user":"U9MD78Z9N","ts":"1616043532.000000"}},{"client_msg_id":"5d00fb9c-6248-42de-9814-a861f0f6707d","type":"message","text":"<@U9MD78Z9N> Maybe <https://github.com/tkoolen/TypeSortedCollections.jl|https://github.com/tkoolen/TypeSortedCollections.jl>?","user":"U67G3QRJM","ts":"1616074141.006100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"s0L","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U9MD78Z9N"},{"type":"text","text":" Maybe "},{"type":"link","url":"https://github.com/tkoolen/TypeSortedCollections.jl","text":"https://github.com/tkoolen/TypeSortedCollections.jl"},{"type":"text","text":"?"}]}]}]},{"type":"message","text":"What does it mean when my flamegraph looks like this?","files":[{"id":"F01RMMU7L5B","created":1616083745,"timestamp":1616083745,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U7PD3M3L5","editable":false,"size":19930,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01RMMU7L5B/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01RMMU7L5B/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01RMMU7L5B-50d3870fc0/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01RMMU7L5B-50d3870fc0/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01RMMU7L5B-50d3870fc0/image_360.png","thumb_360_w":360,"thumb_360_h":353,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01RMMU7L5B-50d3870fc0/image_480.png","thumb_480_w":480,"thumb_480_h":470,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01RMMU7L5B-50d3870fc0/image_160.png","original_w":530,"original_h":519,"thumb_tiny":"AwAvADCnRQOlApiDFGKWigBMUGlpDQADpQKB0ooAWikzRmgBaQ0ZooAB0pKXNJQAUUUUDCiiigD/2Q==","permalink":"https://julialang.slack.com/files/U7PD3M3L5/F01RMMU7L5B/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01RMMU7L5B-51547f65c8","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"UcS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What does it mean when my flamegraph looks like this?"}]}]}],"user":"U7PD3M3L5","display_as_bot":false,"ts":"1616083748.006500"},{"client_msg_id":"e4e5bcae-4f54-4b64-9f0c-a0cfbd3e08b6","type":"message","text":"Only on the left side one of the bottom things is the actual function which is running all the time. All the big bars on the right are C functions, the topmost being `clone`","user":"U7PD3M3L5","ts":"1616083784.007400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3BBod","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Only on the left side one of the bottom things is the actual function which is running all the time. All the big bars on the right are C functions, the topmost being "},{"type":"text","text":"clone","style":{"code":true}}]}]}]},{"client_msg_id":"87307027-c794-430d-ac8d-b9b31af6f3a0","type":"message","text":"Could it be that Julia performs\nA*B*v\nas\n(A*B)*v ? seems very much like it from a performance test","user":"U7PD3M3L5","ts":"1616084234.008100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"cYy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Could it be that Julia performs\nA*B*v\nas\n(A*B)*v ? seems very much like it from a performance test"}]}]}],"thread_ts":"1616084234.008100","reply_count":2,"reply_users_count":2,"latest_reply":"1616084879.008400","reply_users":["U0179G7FG4F","U67D54KS8"],"subscribed":false},{"client_msg_id":"b4d5e37c-e945-4bf5-ac03-fcd0ab0aae59","type":"message","text":"Is it possible to copy an arbitrary pixel in an file (png) without loading the entire image? I'm trying to speed up the simplified code below where the png image is one in a collection with a total size of 25GB+. I'm open to preprocessing the collection to make it easier to access the pixels\n```for frame = 1:Ntotalframes\n    newimg = copy(preloaded_img)\n    for j = 1:ysize\n        for i = 1:xsize\n            fname, pixelcoords = getDesiredFilenameAndPixelIndex(i, j, frame)\n            newimg[i, j] = load(fname)[pixelcoords[1], pixelcoords[2]]\n        end\n    end\n    newimg_file_name = @sprintf(\"%04d.png\", frame);\n    save(newimg_file_name, newimg)\nend```","user":"U0138UTB7A4","ts":"1616196756.012000","team":"T68168MUP","edited":{"user":"U0138UTB7A4","ts":"1616197210.000000"},"blocks":[{"type":"rich_text","block_id":"M+Dn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is it possible to copy an arbitrary pixel in an file (png) without loading the entire image? I'm trying to speed up the simplified code below where the png image is one in a collection with a total size of 25GB+. I'm open to preprocessing the collection to make it easier to access the pixels\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"for frame = 1:Ntotalframes\n    newimg = copy(preloaded_img)\n    for j = 1:ysize\n        for i = 1:xsize\n            fname, pixelcoords = getDesiredFilenameAndPixelIndex(i, j, frame)\n            newimg[i, j] = load(fname)[pixelcoords[1], pixelcoords[2]]\n        end\n    end\n    newimg_file_name = @sprintf(\"%04d.png\", frame);\n    save(newimg_file_name, newimg)\nend"}]}]}],"thread_ts":"1616196756.012000","reply_count":10,"reply_users_count":2,"latest_reply":"1616198230.015200","reply_users":["U0179G7FG4F","U0138UTB7A4"],"subscribed":false},{"client_msg_id":"dc994942-070f-4568-80c2-9200e38e0767","type":"message","text":"`@avx` is equivalent to something like `@inbounds @simd` if the loop is only 1D, right?","user":"U011V2YN59N","ts":"1616735350.019000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uPi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"@avx","style":{"code":true}},{"type":"text","text":" is equivalent to something like "},{"type":"text","text":"@inbounds @simd","style":{"code":true}},{"type":"text","text":" if the loop is only 1D, right?"}]}]}],"thread_ts":"1616735350.019000","reply_count":2,"reply_users_count":1,"latest_reply":"1616735951.019900","reply_users":["U0179G7FG4F"],"is_locked":false,"subscribed":false},{"client_msg_id":"013325f4-7684-4d59-a775-b08637e7d21e","type":"message","text":"Hello. Running some code of mine with `@time` , I get :`7821.040149 seconds (23.55 G allocations: 1.653 TiB, 21.28% gc time)` . The code essentially consists in iterating over lots of small vectors constructed during the runtime. There is no big data that is read from disk or streamed from outside. The 1.6TiB makes me think I allocate way too much. How extreme are those numbers?","user":"U01MG0TN079","ts":"1616739073.022700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ab7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello. Running some code of mine with "},{"type":"text","text":"@time","style":{"code":true}},{"type":"text","text":" , I get :"},{"type":"text","text":"7821.040149 seconds (23.55 G allocations: 1.653 TiB, 21.28% gc time)","style":{"code":true}},{"type":"text","text":" . The code essentially consists in iterating over lots of small vectors constructed during the runtime. There is no big data that is read from disk or streamed from outside. The 1.6TiB makes me think I allocate way too much. How extreme are those numbers?"}]}]}]},{"client_msg_id":"6283876b-c698-47fd-a24d-dbedd61ea696","type":"message","text":"perhaps slightly off-topic, but does somebody know why `SLEEF.jl` got forked into `SLEEFPirates.jl` ? ... just wondering ...","user":"U013V2CFZAN","ts":"1616754077.030200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+o423","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"perhaps slightly off-topic, but does somebody know why "},{"type":"text","text":"SLEEF.jl","style":{"code":true}},{"type":"text","text":" got forked into "},{"type":"text","text":"SLEEFPirates.jl","style":{"code":true}},{"type":"text","text":" ? ... just wondering ..."}]}]}]},{"client_msg_id":"5effe515-275e-45ef-90e7-55196ac40b59","type":"message","text":"Is there a clever way to get a loop with a conditional `break` to vectorize? If I convert it to a while loop, then it's slower.","user":"U011V2YN59N","ts":"1616774934.039300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RdRTQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a clever way to get a loop with a conditional "},{"type":"text","text":"break","style":{"code":true}},{"type":"text","text":" to vectorize? If I convert it to a while loop, then it's slower."}]}]}],"thread_ts":"1616774934.039300","reply_count":18,"reply_users_count":3,"latest_reply":"1616775429.043100","reply_users":["U6QGE7S86","U011V2YN59N","U67BJLYCS"],"is_locked":false,"subscribed":false},{"client_msg_id":"37ef2864-5b02-45e8-8e77-c167154ac38e","type":"message","text":"where `needles` and `hay` are relatively small (0..32) tuples of symbols, to find the symbols in hay that are *not* in needles:\nCan I do better in performance (more important) / memory use than:\n```not_occurs_in(needles, hay) = \nhay[[foldl(.&amp;,map(.!,[n .== hay for n in needles]))...]]```","user":"U68QW0PUZ","ts":"1616792336.045700","team":"T68168MUP","edited":{"user":"U68QW0PUZ","ts":"1616792441.000000"},"blocks":[{"type":"rich_text","block_id":"JDfxR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"where "},{"type":"text","text":"needles","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"hay","style":{"code":true}},{"type":"text","text":" are relatively small (0..32) tuples of symbols, to find the symbols in hay that are "},{"type":"text","text":"not","style":{"bold":true}},{"type":"text","text":" in needles:\nCan I do better in performance (more important) / memory use than:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"not_occurs_in(needles, hay) = \nhay[[foldl(.&,map(.!,[n .== hay for n in needles]))...]]"}]}]}]},{"client_msg_id":"58798804-012e-410e-9c53-1e9f5b515ec3","type":"message","text":"Does anyone see why the following\n```function g_eps(theta, theta_, Σ_inv)\n    exp(-0.25 * (theta - theta_)' * Σ_inv * (theta-theta_))\nend\n\nfunction calc_T(Theta)\n    @cast g[i,j] := g_eps(Theta[:, i], Theta[:, j], I)\n    g\nend\n\nTheta = randn(3, 10)\ncalc_T(Theta)```\nerrors with\n```julia&gt; calc_T(Theta)\nERROR: MethodError: no method matching length(::UniformScaling{Bool})```\n?","user":"U7PD3M3L5","ts":"1617019352.052400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2gCk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does anyone see why the following\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function g_eps(theta, theta_, Σ_inv)\n    exp(-0.25 * (theta - theta_)' * Σ_inv * (theta-theta_))\nend\n\nfunction calc_T(Theta)\n    @cast g[i,j] := g_eps(Theta[:, i], Theta[:, j], I)\n    g\nend\n\nTheta = randn(3, 10)\ncalc_T(Theta)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"errors with\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> calc_T(Theta)\nERROR: MethodError: no method matching length(::UniformScaling{Bool})"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"?"}]}]}]},{"client_msg_id":"df512ee5-9c53-4abd-8b66-e1bfac21ac6f","type":"message","text":"where does it error?","user":"UH24GRBLL","ts":"1617019788.052700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"aPWaS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"where does it error?"}]}]}]},{"client_msg_id":"d2921370-1d80-4194-ae5c-3f7fde778149","type":"message","text":"In the `cast`  from tensorcast:\n```MethodError: no method matching length(::UniformScaling{Bool})\nClosest candidates are:\n  length(!Matched::DataStructures.IntSet) at /home/*/.julia/packages/DataStructures/DLSxi/src/int_set.jl:197\n  length(!Matched::LaTeXStrings.LaTeXString) at /home/*/.julia/packages/LaTeXStrings/anRaX/src/LaTeXStrings.jl:115\n  length(!Matched::Base.EnvDict) at env.jl:132\n  ...\n_similar_for(::UnitRange{Int64}, ::Type{Bool}, ::UniformScaling{Bool}, ::Base.HasLength) at array.jl:597\n_collect(::UnitRange{Int64}, ::UniformScaling{Bool}, ::Base.HasEltype, ::Base.HasLength) at array.jl:630\ncollect(::UniformScaling{Bool}) at array.jl:624\nbroadcastable(::UniformScaling{Bool}) at broadcast.jl:682\nmap(::typeof(Base.Broadcast.broadcastable), ::Tuple{UniformScaling{Bool}}) at tuple.jl:157\nbroadcasted at broadcast.jl:1262 [inlined]\nmacro expansion at macro.jl:214 [inlined]\ncalc_T(::Array{Float64,2}) at example_2.jl:45\ntop-level scope at example_2.jl:49```","user":"U7PD3M3L5","ts":"1617019884.053500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9Y6YB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In the "},{"type":"text","text":"cast","style":{"code":true}},{"type":"text","text":"  from tensorcast:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"MethodError: no method matching length(::UniformScaling{Bool})\nClosest candidates are:\n  length(!Matched::DataStructures.IntSet) at /home/*/.julia/packages/DataStructures/DLSxi/src/int_set.jl:197\n  length(!Matched::LaTeXStrings.LaTeXString) at /home/*/.julia/packages/LaTeXStrings/anRaX/src/LaTeXStrings.jl:115\n  length(!Matched::Base.EnvDict) at env.jl:132\n  ...\n_similar_for(::UnitRange{Int64}, ::Type{Bool}, ::UniformScaling{Bool}, ::Base.HasLength) at array.jl:597\n_collect(::UnitRange{Int64}, ::UniformScaling{Bool}, ::Base.HasEltype, ::Base.HasLength) at array.jl:630\ncollect(::UniformScaling{Bool}) at array.jl:624\nbroadcastable(::UniformScaling{Bool}) at broadcast.jl:682\nmap(::typeof(Base.Broadcast.broadcastable), ::Tuple{UniformScaling{Bool}}) at tuple.jl:157\nbroadcasted at broadcast.jl:1262 [inlined]\nmacro expansion at macro.jl:214 [inlined]\ncalc_T(::Array{Float64,2}) at example_2.jl:45\ntop-level scope at example_2.jl:49"}]}]}]},{"client_msg_id":"bd0c1542-5706-42c3-9d6a-7fc44d5e6688","type":"message","text":"it ends up trying to broadcast over `I`, which doesn't have a length","user":"UH24GRBLL","ts":"1617020166.053900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ALmB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it ends up trying to broadcast over "},{"type":"text","text":"I","style":{"code":true}},{"type":"text","text":", which doesn't have a length"}]}]}],"reactions":[{"name":"pray","users":["U7PD3M3L5"],"count":1}]},{"client_msg_id":"2757b67c-298c-4286-a2f2-5adf67736e70","type":"message","text":"It will work with Ref, i.e. `@cast g[i,j] := g_eps(Theta[:, i], Theta[:, j], Ref(I))`.","user":"UD0NS8PDF","ts":"1617031122.055100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"XeZ3Z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It will work with Ref, i.e. "},{"type":"text","text":"@cast g[i,j] := g_eps(Theta[:, i], Theta[:, j], Ref(I))","style":{"code":true}},{"type":"text","text":"."}]}]}]},{"client_msg_id":"519a811e-865d-4eb0-9957-9634627c220f","type":"message","text":"although perhaps ideally it would figure out that you only intend broadcasting to happen over the things you mark with indices?","user":"UD0NS8PDF","ts":"1617031204.056300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KaO5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"although perhaps ideally it would figure out that you only intend broadcasting to happen over the things you mark with indices?"}]}]}]},{"client_msg_id":"2df9cd5d-9491-4e36-becb-d039a656a289","type":"message","text":"(There might be more efficient ways of doing this, too.)","user":"UD0NS8PDF","ts":"1617031208.056500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1154a","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(There might be more efficient ways of doing this, too.)"}]}]}]},{"client_msg_id":"eea93d37-2bcb-4aff-8237-9afc383717e8","type":"message","text":"I was trying some things out and got an error. Does LoopVectorization not support a loop of the form\n```function mymean4(func,iter)\n    len = length(iter)\n    total = zero(eltype(iter))\n    @avx for i in iter\n        total += func(i)\n    end\n    total/len\nend```\nIt gives this error\n`ERROR: LoadError: MethodError: no method matching canonicalize_range(::Vector{Float64})`","user":"U01C3624SGJ","ts":"1617125499.076400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"i=kiT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I was trying some things out and got an error. Does LoopVectorization not support a loop of the form\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function mymean4(func,iter)\n    len = length(iter)\n    total = zero(eltype(iter))\n    @avx for i in iter\n        total += func(i)\n    end\n    total/len\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"It gives this error\n"},{"type":"text","text":"ERROR: LoadError: MethodError: no method matching canonicalize_range(::Vector{Float64})","style":{"code":true}}]}]}]},{"client_msg_id":"d12f6624-f9ba-4ce5-9132-251184c5e92e","type":"message","text":"How can I understand the very considerable differences in performance for `A_ij b_j` (no Einstein sum)?\n\n```function bmul1(A,b)\n  C = zero(A)\n  for j in 1:size(A,1)\n    @simd for i in 1:size(A,2)\n      @inbounds C[i,j] = A[i,j] * b[i]\n    end\n  end\n  return C\nend\n\nbmul2(A,b) = A .* b```\n```A = rand(64,64); b = rand(64)```\n```@btime bmul1($A, $b);\n4.770 μs (2 allocations: 32.08 KiB)```\n```@btime bmul2($A, $b);\n3.286 μs (2 allocations: 32.08 KiB)```\nI tried looking at `@code_typed` but found nothing suspicious!","user":"U01PLQWQXPV","ts":"1617267253.085100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=mmgH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I understand the very considerable differences in performance for "},{"type":"text","text":"A_ij b_j","style":{"code":true}},{"type":"text","text":" (no Einstein sum)?\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function bmul1(A,b)\n  C = zero(A)\n  for j in 1:size(A,1)\n    @simd for i in 1:size(A,2)\n      @inbounds C[i,j] = A[i,j] * b[i]\n    end\n  end\n  return C\nend\n\nbmul2(A,b) = A .* b"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"A = rand(64,64); b = rand(64)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@btime bmul1($A, $b);\n4.770 μs (2 allocations: 32.08 KiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@btime bmul2($A, $b);\n3.286 μs (2 allocations: 32.08 KiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nI tried looking at "},{"type":"text","text":"@code_typed","style":{"code":true}},{"type":"text","text":" but found nothing suspicious!"}]}]}]},{"client_msg_id":"4e940f00-5495-4663-8c0d-2b55a325ee93","type":"message","text":"I have a complicated function that at some point uses 100 times this one that I defined previously:\n```@inline nth_neighbor(i,n,N) = mod1(i + n,N) ```\nOther things also happen 100 times but this takes most of the time according to the profiler. Is mod1 expected to be slow? If it is something not known I can try and do a MWE to post a better question.","user":"UEGRU91B2","ts":"1617309052.089600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Aw3f","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a complicated function that at some point uses 100 times this one that I defined previously:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@inline nth_neighbor(i,n,N) = mod1(i + n,N) "}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Other things also happen 100 times but this takes most of the time according to the profiler. Is mod1 expected to be slow? If it is something not known I can try and do a MWE to post a better question."}]}]}],"thread_ts":"1617309052.089600","reply_count":2,"reply_users_count":2,"latest_reply":"1617309401.089900","reply_users":["UEGRU91B2","U7HAYKY9X"],"is_locked":false,"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"I don't have a solution to your problem, but I have also found `mod1` to be very slow. I am not sure if that can be fixed though, as I have no frame of reference for the implementation in other languages.","user":"U011V2YN59N","ts":"1617313403.090400","thread_ts":"1617309052.089600","root":{"client_msg_id":"4e940f00-5495-4663-8c0d-2b55a325ee93","type":"message","text":"I have a complicated function that at some point uses 100 times this one that I defined previously:\n```@inline nth_neighbor(i,n,N) = mod1(i + n,N) ```\nOther things also happen 100 times but this takes most of the time according to the profiler. Is mod1 expected to be slow? If it is something not known I can try and do a MWE to post a better question.","user":"UEGRU91B2","ts":"1617309052.089600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Aw3f","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a complicated function that at some point uses 100 times this one that I defined previously:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"@inline nth_neighbor(i,n,N) = mod1(i + n,N) "}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Other things also happen 100 times but this takes most of the time according to the profiler. Is mod1 expected to be slow? If it is something not known I can try and do a MWE to post a better question."}]}]}],"thread_ts":"1617309052.089600","reply_count":4,"reply_users_count":3,"latest_reply":"1617313403.090400","reply_users":["UEGRU91B2","U7HAYKY9X","U011V2YN59N"],"is_locked":false,"subscribed":false},"blocks":[{"type":"rich_text","block_id":"uDv5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I don't have a solution to your problem, but I have also found "},{"type":"text","text":"mod1","style":{"code":true}},{"type":"text","text":" to be very slow. I am not sure if that can be fixed though, as I have no frame of reference for the implementation in other languages."}]}]}],"client_msg_id":"bea45f7a-117e-49fd-b978-cf58406220ff"},{"client_msg_id":"c51b4210-36eb-4c88-8194-2d0d2b12033d","type":"message","text":"Hey. I'm puzzled by the following: I've been trying to optimize some code of mine: after doing some changes I got to this version: <https://github.com/bottine/VinbergsAlgorithmNF/blob/685fec1b92ed9f21eb8a725b933a32be1bed8130/src/vinbergs_algo.jl#L392> (the function `_extend_root_stem` is where stuff is happening) which ran under an hour on a specific \"benchmark\" example I have. I then cleanup up my changes to <https://github.com/bottine/VinbergsAlgorithmNF/blob/34f4d951541baf2b6132eeaeea2887dcd968b4d3/src/vinbergs_algo.jl#L389> and just like that my code took 2-3× longer. I'm now at this version <https://github.com/bottine/VinbergsAlgorithmNF/blob/c8bcd88897659a03d9990b15b2daad2d421a7779/src/vinbergs_algo.jl#L503> which is still quite slower than the commit `685fec1b` and I have no idea what made this commit so much better than everything coming after. Using Cthulhu, I don't see any type instabilities in my latest code, and it looks better in general. I think there is probably some \"one line\" that I changed which killed performance, but have no idea how to find it. Any idea ?","user":"U01MG0TN079","ts":"1617342979.093300","team":"T68168MUP","edited":{"user":"U01MG0TN079","ts":"1617343383.000000"},"blocks":[{"type":"rich_text","block_id":"R1R9U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey. I'm puzzled by the following: I've been trying to optimize some code of mine: after doing some changes I got to this version: "},{"type":"link","url":"https://github.com/bottine/VinbergsAlgorithmNF/blob/685fec1b92ed9f21eb8a725b933a32be1bed8130/src/vinbergs_algo.jl#L392"},{"type":"text","text":" (the function "},{"type":"text","text":"_extend_root_stem","style":{"code":true}},{"type":"text","text":" is where stuff is happening) which ran under an hour on a specific \"benchmark\" example I have. I then cleanup up my changes to "},{"type":"link","url":"https://github.com/bottine/VinbergsAlgorithmNF/blob/34f4d951541baf2b6132eeaeea2887dcd968b4d3/src/vinbergs_algo.jl#L389"},{"type":"text","text":" and just like that my code took 2-3× longer. I'm now at this version "},{"type":"link","url":"https://github.com/bottine/VinbergsAlgorithmNF/blob/c8bcd88897659a03d9990b15b2daad2d421a7779/src/vinbergs_algo.jl#L503"},{"type":"text","text":" which is still quite slower than the commit "},{"type":"text","text":"685fec1b","style":{"code":true}},{"type":"text","text":" and I have no idea what made this commit so much better than everything coming after. Using Cthulhu, I don't see any type instabilities in my latest code, and it looks better in general. I think there is probably some \"one line\" that I changed which killed performance, but have no idea how to find it. Any idea ?"}]}]}],"thread_ts":"1617342979.093300","reply_count":3,"reply_users_count":2,"latest_reply":"1617343796.093900","reply_users":["U0179G7FG4F","U01MG0TN079"],"is_locked":false,"subscribed":false},{"client_msg_id":"e8021019-3541-4b03-88a9-066e151c22b4","type":"message","text":"Hmm ... this is a bit disappointing ... I prefer the vector-like notation, but I would like to have the BLAS speed\n```julia&gt; @benchmark axpy!(-a, $v1, $v2)\nBenchmarkTools.Trial: \n  memory estimate:  16 bytes\n  allocs estimate:  1\n  --------------\n  minimum time:     23.732 μs (0.00% GC)\n  median time:      25.812 μs (0.00% GC)\n  mean time:        26.578 μs (0.00% GC)\n  maximum time:     403.066 μs (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     1\n\njulia&gt; @benchmark @avx $v2 .-= a.*$v1\nBenchmarkTools.Trial: \n  memory estimate:  128 bytes\n  allocs estimate:  4\n  --------------\n  minimum time:     317.518 μs (0.00% GC)\n  median time:      323.328 μs (0.00% GC)\n  mean time:        325.628 μs (0.00% GC)\n  maximum time:     676.790 μs (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     1\n\njulia&gt; @benchmark $v2 .-= a.*$v1\nBenchmarkTools.Trial: \n  memory estimate:  128 bytes\n  allocs estimate:  4\n  --------------\n  minimum time:     445.999 μs (0.00% GC)\n  median time:      460.991 μs (0.00% GC)\n  mean time:        473.982 μs (0.00% GC)\n  maximum time:     7.687 ms (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     1\n\njulia&gt; v1 |&gt; length\n1000000```","user":"U013V2CFZAN","ts":"1617350999.094900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3YiUa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hmm ... this is a bit disappointing ... I prefer the vector-like notation, but I would like to have the BLAS speed\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> @benchmark axpy!(-a, $v1, $v2)\nBenchmarkTools.Trial: \n  memory estimate:  16 bytes\n  allocs estimate:  1\n  --------------\n  minimum time:     23.732 μs (0.00% GC)\n  median time:      25.812 μs (0.00% GC)\n  mean time:        26.578 μs (0.00% GC)\n  maximum time:     403.066 μs (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     1\n\njulia> @benchmark @avx $v2 .-= a.*$v1\nBenchmarkTools.Trial: \n  memory estimate:  128 bytes\n  allocs estimate:  4\n  --------------\n  minimum time:     317.518 μs (0.00% GC)\n  median time:      323.328 μs (0.00% GC)\n  mean time:        325.628 μs (0.00% GC)\n  maximum time:     676.790 μs (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     1\n\njulia> @benchmark $v2 .-= a.*$v1\nBenchmarkTools.Trial: \n  memory estimate:  128 bytes\n  allocs estimate:  4\n  --------------\n  minimum time:     445.999 μs (0.00% GC)\n  median time:      460.991 μs (0.00% GC)\n  mean time:        473.982 μs (0.00% GC)\n  maximum time:     7.687 ms (0.00% GC)\n  --------------\n  samples:          10000\n  evals/sample:     1\n\njulia> v1 |> length\n1000000"}]}]}],"thread_ts":"1617350999.094900","reply_count":15,"reply_users_count":3,"latest_reply":"1617352079.098500","reply_users":["UH24GRBLL","U013V2CFZAN","UAUPJLBQX"],"is_locked":false,"subscribed":false},{"client_msg_id":"4f1b6eca-40ba-4352-8f82-0bb9d462a54c","type":"message","text":"If I want a package to precompile a bunch of methods so that compilation is not necessary when external packages call it, what's the best way to do that? Can this be done with `precompile`? Or would I need to make a system image / use binary builder?","user":"UEN48T0BT","ts":"1617385741.109700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"X89","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If I want a package to precompile a bunch of methods so that compilation is not necessary when external packages call it, what's the best way to do that? Can this be done with "},{"type":"text","text":"precompile","style":{"code":true}},{"type":"text","text":"? Or would I need to make a system image / use binary builder?"}]}]}]},{"client_msg_id":"b08286c7-8b02-4b64-8742-6ef25f70e026","type":"message","text":"<@UAUPJLBQX> Are there any branchless techniques references that you could recommend for Julia?","user":"U013V2CFZAN","ts":"1617443388.116700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"im2b","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UAUPJLBQX"},{"type":"text","text":" Are there any branchless techniques references that you could recommend for Julia?"}]}]}]},{"client_msg_id":"71a475db-689a-4c6e-9a2e-46a1cba65de3","type":"message","text":"<https://github.com/0x0f0f0f/Metatheory.jl/issues/6>","user":"UDGT4PM41","ts":"1617638742.125400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gpTA","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/0x0f0f0f/Metatheory.jl/issues/6"}]}]}]},{"client_msg_id":"9bfa960e-0709-4118-8ec6-69d8d268fe8f","type":"message","text":"Trying to optimize the `fastmod` function here. Any thoughts?\n```struct Mod\n    modulo::UInt32\n    modulo_inv::UInt64\n    function Mod(d::UInt32)\n        cmod::UInt64 = div(typemax(UInt64), d) + 1\n        return new(d,cmod)\n    end\n    function Mod(d::Int)\n        ud = convert(UInt32, d)\n        return Mod(ud)\n    end\nend\n\n@inline function fastmod(n::UInt32,T::Mod)::UInt32\n    lowbits::UInt64 = T.modulo_inv * n\n    return (convert(UInt128,lowbits) * T.modulo) &gt;&gt; 64\nend```\ntranslation from this code from Lemire et al (2019)\n```uint32_t d = ...;// your divisor &gt; 0\n\nuint64_t c = UINT64_C(0xFFFFFFFFFFFFFFFF) / d + 1;\n\n// fastmod computes (n mod d) given precomputed c\nuint32_t fastmod(uint32_t n ) {\n  uint64_t lowbits = c * n;\n  return ((__uint128_t)lowbits * d) &gt;&gt; 64; \n}```\nI also made a <https://discourse.julialang.org/t/optimizing-remainder-with-constant-modulus-implementation/58586|discourse post> last night with more info and copypastable benchmarking code but it hasn't gotten any traffic.","user":"U011V2YN59N","ts":"1617651204.127300","team":"T68168MUP","edited":{"user":"U011V2YN59N","ts":"1617651220.000000"},"blocks":[{"type":"rich_text","block_id":"Dx7=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Trying to optimize the "},{"type":"text","text":"fastmod","style":{"code":true}},{"type":"text","text":" function here. Any thoughts?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"struct Mod\n    modulo::UInt32\n    modulo_inv::UInt64\n    function Mod(d::UInt32)\n        cmod::UInt64 = div(typemax(UInt64), d) + 1\n        return new(d,cmod)\n    end\n    function Mod(d::Int)\n        ud = convert(UInt32, d)\n        return Mod(ud)\n    end\nend\n\n@inline function fastmod(n::UInt32,T::Mod)::UInt32\n    lowbits::UInt64 = T.modulo_inv * n\n    return (convert(UInt128,lowbits) * T.modulo) >> 64\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"translation from this code from Lemire et al (2019)\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"uint32_t d = ...;// your divisor > 0\n\nuint64_t c = UINT64_C(0xFFFFFFFFFFFFFFFF) / d + 1;\n\n// fastmod computes (n mod d) given precomputed c\nuint32_t fastmod(uint32_t n ) {\n  uint64_t lowbits = c * n;\n  return ((__uint128_t)lowbits * d) >> 64; \n}"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I also made a "},{"type":"link","url":"https://discourse.julialang.org/t/optimizing-remainder-with-constant-modulus-implementation/58586","text":"discourse post"},{"type":"text","text":" last night with more info and copypastable benchmarking code but it hasn't gotten any traffic."}]}]}]},{"client_msg_id":"daab6db2-b753-443c-b222-10f40f02beb3","type":"message","text":"I might be writing a short blog post comparing R/Python/Rust/Julia. Some folks have been golfing performance and here's my fastest version for Julia - any performance juice left to squeeze?\n\n```q = [0.001,0.002,0.003,0.003,0.004,0.004,0.005,0.007,0.009,0.011]\nw = [0.05,0.07,0.08,0.10,0.14,0.20,0.20,0.20,0.10,0.04]\nP = 100\nS = 25000\nr = 0.02\n\n# calculate present value of basic insurance policy\n@inline function npv4(q,w,P,S,r,term=nothing)\n    term = term === nothing ? length(q) : term\n    inforce = 1.0\n    result = 0.0\n    v = (1 / ( 1 + r))\n    v_t = v\n    \n    for (t,(q,w)) in enumerate(zip(q,w))\n        t &gt; term &amp;&amp; return result\n        result += inforce * (P - S * q) * v_t\n        inforce -= inforce * q + inforce * w\n        v_t *= v\n    end\n    \n    return result\nend```","user":"UFWQ6DP0S","ts":"1617762478.131400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"137WB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I might be writing a short blog post comparing R/Python/Rust/Julia. Some folks have been golfing performance and here's my fastest version for Julia - any performance juice left to squeeze?\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"q = [0.001,0.002,0.003,0.003,0.004,0.004,0.005,0.007,0.009,0.011]\nw = [0.05,0.07,0.08,0.10,0.14,0.20,0.20,0.20,0.10,0.04]\nP = 100\nS = 25000\nr = 0.02\n\n# calculate present value of basic insurance policy\n@inline function npv4(q,w,P,S,r,term=nothing)\n    term = term === nothing ? length(q) : term\n    inforce = 1.0\n    result = 0.0\n    v = (1 / ( 1 + r))\n    v_t = v\n    \n    for (t,(q,w)) in enumerate(zip(q,w))\n        t > term && return result\n        result += inforce * (P - S * q) * v_t\n        inforce -= inforce * q + inforce * w\n        v_t *= v\n    end\n    \n    return result\nend"}]}]}],"thread_ts":"1617762478.131400","reply_count":46,"reply_users_count":4,"latest_reply":"1617766290.141200","reply_users":["UFWQ6DP0S","U6795JH6H","U011V2YN59N","UD0NS8PDF"],"is_locked":false,"subscribed":false},{"client_msg_id":"ccceea81-4779-4a24-a30a-c2431eb65cd5","type":"message","text":"Saw <https://discourse.julialang.org/t/optimizing-linear-algebra-code/58797/2|this> discourse post about optimizing a linear algebra computation. They are correct in that preallocating doesn't improve the speed much at all (by 0.2% or something). Why is that?","user":"U011V2YN59N","ts":"1617854377.146800","team":"T68168MUP","edited":{"user":"U011V2YN59N","ts":"1617854663.000000"},"blocks":[{"type":"rich_text","block_id":"ZW2K","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Saw "},{"type":"link","url":"https://discourse.julialang.org/t/optimizing-linear-algebra-code/58797/2","text":"this"},{"type":"text","text":" discourse post about optimizing a linear algebra computation. They are correct in that preallocating doesn't improve the speed much at all (by 0.2% or something). Why is that?"}]}]}],"thread_ts":"1617854377.146800","reply_count":2,"reply_users_count":2,"latest_reply":"1617855367.150000","reply_users":["UD0NS8PDF","U011V2YN59N"],"is_locked":false,"subscribed":false},{"client_msg_id":"d3602c4b-f134-468b-8116-bcb46a1c322c","type":"message","text":"<@UAUPJLBQX> I was going through <https://juliasimd.github.io/LoopVectorization.jl/stable/examples/matrix_multiplication/#Matrix-Multiplication|this>  example in the `LoopVectorization.jl`  documentation for matrix multiplication and I don't understand why, but it is not updating my `C` output matrix.\n```N = 2\nA1 = rand(N, N)\nA2 = rand(N,N)\nA3 = similar(A1)\n\nfunction A_mul_B!(C, A, B)\n    @avx for n ∈ indices((C,B), 2), m ∈ indices((C,A), 1)\n        Cmn = zero(eltype(C))\n        for k ∈ indices((A,B), (2,1))\n            Cmn += C[m,k] * B[k,n]\n        end\n        C[m,n] = Cmn\n    end\nend\n\njulia&gt; A_mul_B!(A3, A1, A2)\n\njulia&gt; A3\n2×2 Matrix{Float64}:\n 6.22527e-310  9.15541e-310\n 6.22543e-310  9.15553e-310```\nIs this a bug?","user":"U013V2CFZAN","ts":"1617969888.151700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GTw","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UAUPJLBQX"},{"type":"text","text":" I was going through "},{"type":"link","url":"https://juliasimd.github.io/LoopVectorization.jl/stable/examples/matrix_multiplication/#Matrix-Multiplication","text":"this"},{"type":"text","text":"  example in the "},{"type":"text","text":"LoopVectorization.jl","style":{"code":true}},{"type":"text","text":"  documentation for matrix multiplication and I don't understand why, but it is not updating my "},{"type":"text","text":"C","style":{"code":true}},{"type":"text","text":" output matrix.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"N = 2\nA1 = rand(N, N)\nA2 = rand(N,N)\nA3 = similar(A1)\n\nfunction A_mul_B!(C, A, B)\n    @avx for n ∈ indices((C,B), 2), m ∈ indices((C,A), 1)\n        Cmn = zero(eltype(C))\n        for k ∈ indices((A,B), (2,1))\n            Cmn += C[m,k] * B[k,n]\n        end\n        C[m,n] = Cmn\n    end\nend\n\njulia> A_mul_B!(A3, A1, A2)\n\njulia> A3\n2×2 Matrix{Float64}:\n 6.22527e-310  9.15541e-310\n 6.22543e-310  9.15553e-310"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Is this a bug?"}]}]}],"thread_ts":"1617969888.151700","reply_count":1,"reply_users_count":1,"latest_reply":"1617970478.151800","reply_users":["U013V2CFZAN"],"is_locked":false,"subscribed":false},{"client_msg_id":"97903825-ad1c-445c-8aa6-20f2f4d7b852","type":"message","text":"Hey I have two matrices of dimensions `D, M` , where `M`  is relatively large (100-1000) and `D`  as large as possible. I want to calculate all the inner products of the columns, i.e.\n`res[i,j]=dot(A[:, i], B[:, j])` . These are the two approaches I have taken\n```using BenchmarkTools\n\nfunction pairwise_inner_products1!(res, A, B)\n    D, M = size(A)\n    Temp = similar(A)\n    for i=1:M\n        Temp .= A .* B\n        res[:, i] .= sum(Temp, dims=1)[:]\n    end\n    res\nend\n\nfunction pairwise_inner_products2!(res, A, B)\n    D, M = size(A)\n    for j=1:M\n        for i=1:M\n            for k=1:D\n                @inbounds res[i, j] += A[k, i] * B[k, j]\n            end\n        end\n    end\nend\n\nD = 500\nM = 300\n\nbench1 = @benchmark pairwise_inner_products1!(res, A, B) setup=(\n    res = rand(M, M);\n    A = rand(D, M);\n    B = rand(D, M);\n)\n\nbench2 = @benchmark pairwise_inner_products2!(res, A, B) setup=(\n    res = rand(M, M);\n    A = rand(D, M);\n    B = rand(D, M);\n)```\n","user":"U7PD3M3L5","ts":"1617980182.157300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/m4X","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey I have two matrices of dimensions "},{"type":"text","text":"D, M","style":{"code":true}},{"type":"text","text":" , where "},{"type":"text","text":"M","style":{"code":true}},{"type":"text","text":"  is relatively large (100-1000) and "},{"type":"text","text":"D","style":{"code":true}},{"type":"text","text":"  as large as possible. I want to calculate all the inner products of the columns, i.e.\n"},{"type":"text","text":"res[i,j]=dot(A[:, i], B[:, j])","style":{"code":true}},{"type":"text","text":" . These are the two approaches I have taken\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using BenchmarkTools\n\nfunction pairwise_inner_products1!(res, A, B)\n    D, M = size(A)\n    Temp = similar(A)\n    for i=1:M\n        Temp .= A .* B\n        res[:, i] .= sum(Temp, dims=1)[:]\n    end\n    res\nend\n\nfunction pairwise_inner_products2!(res, A, B)\n    D, M = size(A)\n    for j=1:M\n        for i=1:M\n            for k=1:D\n                @inbounds res[i, j] += A[k, i] * B[k, j]\n            end\n        end\n    end\nend\n\nD = 500\nM = 300\n\nbench1 = @benchmark pairwise_inner_products1!(res, A, B) setup=(\n    res = rand(M, M);\n    A = rand(D, M);\n    B = rand(D, M);\n)\n\nbench2 = @benchmark pairwise_inner_products2!(res, A, B) setup=(\n    res = rand(M, M);\n    A = rand(D, M);\n    B = rand(D, M);\n)"}]},{"type":"rich_text_section","elements":[]}]}],"thread_ts":"1617980182.157300","reply_count":14,"reply_users_count":4,"latest_reply":"1617981530.163900","reply_users":["UH24GRBLL","U7PD3M3L5","UMDEUKM29","UD0NS8PDF"],"is_locked":false,"subscribed":false},{"client_msg_id":"05d4e4c8-8796-401a-be94-ba118fcd69a7","type":"message","text":"They result in\n```julia&gt; bench1\nBenchmarkTools.Trial: \n  memory estimate:  2.61 MiB\n  allocs estimate:  602\n  --------------\n  minimum time:     16.487 ms (0.00% GC)\n  median time:      16.816 ms (0.00% GC)\n  mean time:        17.147 ms (1.26% GC)\n  maximum time:     20.134 ms (6.76% GC)\n  --------------\n  samples:          285\n  evals/sample:     1\n\njulia&gt; bench2\nBenchmarkTools.Trial: \n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     32.597 ms (0.00% GC)\n  median time:      33.231 ms (0.00% GC)\n  mean time:        33.268 ms (0.00% GC)\n  maximum time:     36.697 ms (0.00% GC)\n  --------------\n  samples:          149\n  evals/sample:     1```\nDoes anyone see any obvious improvements?","user":"U7PD3M3L5","ts":"1617980209.157800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RJI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"They result in\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> bench1\nBenchmarkTools.Trial: \n  memory estimate:  2.61 MiB\n  allocs estimate:  602\n  --------------\n  minimum time:     16.487 ms (0.00% GC)\n  median time:      16.816 ms (0.00% GC)\n  mean time:        17.147 ms (1.26% GC)\n  maximum time:     20.134 ms (6.76% GC)\n  --------------\n  samples:          285\n  evals/sample:     1\n\njulia> bench2\nBenchmarkTools.Trial: \n  memory estimate:  0 bytes\n  allocs estimate:  0\n  --------------\n  minimum time:     32.597 ms (0.00% GC)\n  median time:      33.231 ms (0.00% GC)\n  mean time:        33.268 ms (0.00% GC)\n  maximum time:     36.697 ms (0.00% GC)\n  --------------\n  samples:          149\n  evals/sample:     1"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Does anyone see any obvious improvements?"}]}]}]},{"client_msg_id":"10689df6-3fbe-476c-b414-34fdae06757c","type":"message","text":"Ok, as someone noticed, the problem I posted above was actually trivial. Thanks. The problem I actually want to solve is\n```res[i, j] = (A[:, i]-A[:,j])' * Sigma * (A[:, i] - A[:, j])```\nI refactored this into\n```using BenchmarkTools\nusing LinearAlgebra\n\nfunction differences!(res, A, Σ)\n    D, M = size(A)\n    Temp = similar(A)\n    Temp2 = similar(A)\n    Temp3 = similar(A)\n    for i=1:M\n        Temp .= A[:, i] .- A\n        mul!(Temp2, Σ, A[:, i] .- A)\n        Temp3 .= Temp .* Temp2\n        sum!(res[:, i]', Temp3)\n    end\n    res\nend\n\nD = 500\nM = 300\n\nbench1 = @benchmark differences!(res, A, Σ) setup=(\n    res = rand(M, M);\n    A = rand(D, M);\n    Σ = rand(D, D);\n)```\nand am getting\n```julia&gt; bench1\nBenchmarkTools.Trial: \n  memory estimate:  349.89 MiB\n  allocs estimate:  1506\n  --------------\n  minimum time:     416.358 ms (1.70% GC)\n  median time:      429.999 ms (1.66% GC)\n  mean time:        443.107 ms (4.95% GC)\n  maximum time:     598.346 ms (31.10% GC)\n  --------------\n  samples:          12\n  evals/sample:     1```\n","user":"U7PD3M3L5","ts":"1617982181.166500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Mjxh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ok, as someone noticed, the problem I posted above was actually trivial. Thanks. The problem I actually want to solve is\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"res[i, j] = (A[:, i]-A[:,j])' * Sigma * (A[:, i] - A[:, j])"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I refactored this into\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using BenchmarkTools\nusing LinearAlgebra\n\nfunction differences!(res, A, Σ)\n    D, M = size(A)\n    Temp = similar(A)\n    Temp2 = similar(A)\n    Temp3 = similar(A)\n    for i=1:M\n        Temp .= A[:, i] .- A\n        mul!(Temp2, Σ, A[:, i] .- A)\n        Temp3 .= Temp .* Temp2\n        sum!(res[:, i]', Temp3)\n    end\n    res\nend\n\nD = 500\nM = 300\n\nbench1 = @benchmark differences!(res, A, Σ) setup=(\n    res = rand(M, M);\n    A = rand(D, M);\n    Σ = rand(D, D);\n)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"and am getting\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> bench1\nBenchmarkTools.Trial: \n  memory estimate:  349.89 MiB\n  allocs estimate:  1506\n  --------------\n  minimum time:     416.358 ms (1.70% GC)\n  median time:      429.999 ms (1.66% GC)\n  mean time:        443.107 ms (4.95% GC)\n  maximum time:     598.346 ms (31.10% GC)\n  --------------\n  samples:          12\n  evals/sample:     1"}]},{"type":"rich_text_section","elements":[]}]}],"thread_ts":"1617982181.166500","reply_count":3,"reply_users_count":2,"latest_reply":"1617982383.168200","reply_users":["UH24GRBLL","U7PD3M3L5"],"is_locked":false,"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"A really cool problem, I wish I had the time/knowledge to tackle it.","user":"U011V2YN59N","ts":"1618067859.181100","thread_ts":"1617638742.125400","root":{"client_msg_id":"71a475db-689a-4c6e-9a2e-46a1cba65de3","type":"message","text":"<https://github.com/0x0f0f0f/Metatheory.jl/issues/6>","user":"UDGT4PM41","ts":"1617638742.125400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gpTA","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/0x0f0f0f/Metatheory.jl/issues/6"}]}]}],"thread_ts":"1617638742.125400","reply_count":1,"reply_users_count":1,"latest_reply":"1618067859.181100","reply_users":["U011V2YN59N"],"is_locked":false,"subscribed":false},"blocks":[{"type":"rich_text","block_id":"Hkgu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"A really cool problem, I wish I had the time/knowledge to tackle it."}]}]}],"client_msg_id":"da01a16e-1960-445c-811f-60e9b4d7b123"},{"client_msg_id":"187661c2-2df2-4c1d-a05a-b8673cdfac90","type":"message","text":"is it possible to use LoopVectorization’s threaded broadcasting `@avxt` on StructArrays? e.g.,\n```using StructArrays\nusing BenchmarkTools\nusing LoopVectorization\n\nu,v = ntuple(x-&gt;randn(10,1000),2)\nU = StructArray(u=u,v=v)\nf(U) = exp(U.u)*sin(U.v)\n\n@avxt f.(U)```\nWhen I tried on v.12.12, `@avxt f.(U)` wasn’t any faster than the non-threaded broadcast version.","user":"U011LUQ182G","ts":"1618189873.188800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"b5gYL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is it possible to use LoopVectorization’s threaded broadcasting "},{"type":"text","text":"@avxt","style":{"code":true}},{"type":"text","text":" on StructArrays? e.g.,\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using StructArrays\nusing BenchmarkTools\nusing LoopVectorization\n\nu,v = ntuple(x->randn(10,1000),2)\nU = StructArray(u=u,v=v)\nf(U) = exp(U.u)*sin(U.v)\n\n@avxt f.(U)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"When I tried on v.12.12, "},{"type":"text","text":"@avxt f.(U)","style":{"code":true}},{"type":"text","text":" wasn’t any faster than the non-threaded broadcast version."}]}]}],"thread_ts":"1618189873.188800","reply_count":45,"reply_users_count":2,"latest_reply":"1618193518.199000","reply_users":["UAUPJLBQX","U011LUQ182G"],"is_locked":false,"subscribed":false},{"client_msg_id":"448ae3ec-949c-4024-8c36-6b0e886a0e5b","type":"message","text":"Hey guys, so I was trying to compare Forwardiff and Zygote for my use case, and tried optimizing the code to my best. the results are weird though, ForwardDiff allocates less than zygote, but is not faster than Zygote. Am I going wrong somewhere?\n```julia&gt; using ForwardDiff: gradient!, Chunk, GradientConfig\njulia&gt; using Zygote: gradient\njulia&gt; using BenchmarkTools\njulia&gt; function opt_fwdiff!(x, f)\n           grad = similar(x)\n           cfg = GradientConfig(f, x, Chunk{3}())\n           for i in 1:30\n               gradient!(grad, f, x, cfg)\n               x .-= 0.1 .* grad\n           end\n           return x\n       end\nopt_fwdiff! (generic function with 1 method)\n\njulia&gt; function opt_zyg!(x, f)\n           grad = similar(x)\n           for i in 1:30\n               grad = gradient(x -&gt; f(x), x)[1]\n               x .-= 0.1 .* grad\n           end\n           return x\n       end\nopt_zyg! (generic function with 1 method)\n\njulia&gt; x =[3, 4.0, 5]; f(x) = sum(x.^2); y =copy(x);\n\njulia&gt; @btime opt_zyg!($x, $f)\n  8.021 μs (91 allocations: 7.14 KiB)\n\njulia&gt; @btime opt_fwdiff!($y, $f)\n  8.151 μs (33 allocations: 5.53 KiB)```","user":"U017D621ELC","ts":"1618222090.205700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1ksE2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey guys, so I was trying to compare Forwardiff and Zygote for my use case, and tried optimizing the code to my best. the results are weird though, ForwardDiff allocates less than zygote, but is not faster than Zygote. Am I going wrong somewhere?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> using ForwardDiff: gradient!, Chunk, GradientConfig\njulia> using Zygote: gradient\njulia> using BenchmarkTools\njulia> function opt_fwdiff!(x, f)\n           grad = similar(x)\n           cfg = GradientConfig(f, x, Chunk{3}())\n           for i in 1:30\n               gradient!(grad, f, x, cfg)\n               x .-= 0.1 .* grad\n           end\n           return x\n       end\nopt_fwdiff! (generic function with 1 method)\n\njulia> function opt_zyg!(x, f)\n           grad = similar(x)\n           for i in 1:30\n               grad = gradient(x -> f(x), x)[1]\n               x .-= 0.1 .* grad\n           end\n           return x\n       end\nopt_zyg! (generic function with 1 method)\n\njulia> x =[3, 4.0, 5]; f(x) = sum(x.^2); y =copy(x);\n\njulia> @btime opt_zyg!($x, $f)\n  8.021 μs (91 allocations: 7.14 KiB)\n\njulia> @btime opt_fwdiff!($y, $f)\n  8.151 μs (33 allocations: 5.53 KiB)"}]}]}]},{"client_msg_id":"03517a6b-3e32-4a02-86fc-a4e349a7db9c","type":"message","text":"&gt; The results are weird though, ForwardDiff allocates less than zygote, but is not faster than Zygote.\nIs that the weird part? That's not too weird since allocations are not the only thing that takes time in an algorithm.","user":"U67D54KS8","ts":"1618224245.208400","team":"T68168MUP","edited":{"user":"U67D54KS8","ts":"1618224257.000000"},"blocks":[{"type":"rich_text","block_id":"ZLoM","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"The results are weird though, ForwardDiff allocates less than zygote, but is not faster than Zygote."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Is that the weird part? That's not too weird since allocations are not the only thing that takes time in an algorithm."}]}]}]}]}