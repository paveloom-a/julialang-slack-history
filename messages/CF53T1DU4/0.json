{"cursor": 0, "messages": [{"client_msg_id":"2d884b8f-de6c-48ac-bdea-871c4ef24636","type":"message","text":"Is appropriate to mention ~6% performance loss on nightly (as compared to 1.5.3), even if so, it might be hard to strip it down to a small metric. Or is this something to do once there's an alpha, etc.","user":"ULTB7E6UW","ts":"1607980182.098000","team":"T68168MUP","edited":{"user":"ULTB7E6UW","ts":"1607980205.000000"},"blocks":[{"type":"rich_text","block_id":"6WM9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is appropriate to mention ~6% performance loss on nightly (as compared to 1.5.3), even if so, it might be hard to strip it down to a small metric. Or is this something to do once there's an alpha, etc."}]}]}]},{"client_msg_id":"b2731467-5773-4ea5-86f8-db55eb37a958","type":"message","text":"If you have a reproducible example (best small and self-contained)","user":"U67BJLYCS","ts":"1607980532.098700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1tC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If you have a reproducible example (best small and self-contained)"}]}]}],"thread_ts":"1607980532.098700","reply_count":27,"reply_users_count":2,"latest_reply":"1607994990.104300","reply_users":["ULTB7E6UW","U67BJLYCS"],"subscribed":false},{"client_msg_id":"3f97b8d5-4090-4cd7-ac5b-0f304ad1781e","type":"message","text":"then yes, please open an issue","user":"U67BJLYCS","ts":"1607980542.099000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+ZcN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"then yes, please open an issue"}]}]}],"thread_ts":"1607980542.099000","reply_count":1,"reply_users_count":1,"latest_reply":"1608154024.115800","reply_users":["ULTB7E6UW"],"subscribed":false},{"client_msg_id":"778dd2d1-9a9e-4be9-bead-dc77d9bc01a0","type":"message","text":"I posted the following in <#C6A044SQH|helpdesk>, but someone mentioned this channel, so I'll just paste it below:\n\nI coded up 3 solutions to Day 15 of Advent of Code (spoilers), and got the following timings:\n<https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15-unsafe-vector.rkt|Racket w/ unsafe vector/arithmetic> =&gt; 0.68 seconds\n<https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15.c|C version> =&gt; 0.47 seconds\n<https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15.jl|Julia w/ mutable vector> =&gt; 0.41 seconds\nI was impressed that Julia was faster than C. On the one hand, I haven't coded C seriously since 1996, but on the other, I'm very much a Julia newbie.\n\nI'm just curious if an experienced Julia coder can spot any obvious inefficiencies in the <https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15.jl|little Julia program> - even though it came in faster than C, that doesn't mean I wouldn't like it to be faster :slightly_smiling_face:","user":"U014ATN949F","ts":"1608079617.105300","team":"T68168MUP","edited":{"user":"U014ATN949F","ts":"1608133861.000000"},"blocks":[{"type":"rich_text","block_id":"DvIU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I posted the following in "},{"type":"channel","channel_id":"C6A044SQH"},{"type":"text","text":", but someone mentioned this channel, so I'll just paste it below:\n\nI coded up 3 solutions to Day 15 of Advent of Code (spoilers), and got the following timings:\n"},{"type":"link","url":"https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15-unsafe-vector.rkt","text":"Racket w/ unsafe vector/arithmetic"},{"type":"text","text":" => 0.68 seconds\n"},{"type":"link","url":"https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15.c","text":"C version"},{"type":"text","text":" => 0.47 seconds\n"},{"type":"link","url":"https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15.jl","text":"Julia w/ mutable vector"},{"type":"text","text":" => 0.41 seconds\nI was impressed that Julia was faster than C. On the one hand, I haven't coded C seriously since 1996, but on the other, I'm very much a Julia newbie.\n\nI'm just curious if an experienced Julia coder can spot any obvious inefficiencies in the "},{"type":"link","url":"https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15.jl","text":"little Julia program"},{"type":"text","text":" - even though it came in faster than C, that doesn't mean I wouldn't like it to be faster "},{"type":"emoji","name":"slightly_smiling_face"}]}]}],"thread_ts":"1608079617.105300","reply_count":33,"reply_users_count":6,"latest_reply":"1608400503.142400","reply_users":["UD0NS8PDF","U014ATN949F","U0179G7FG4F","U7HAYKY9X","UH24GRBLL","UB7JS9CHF"],"subscribed":false},{"client_msg_id":"21ba142e-6d2e-4446-aae1-07ab427f9059","type":"message","text":"So, I'm looking at the Julia  <https://github.com/JuliaLang/julia/issues/29841|#29841>, where I learned that `HTML` and `Text` are errors and would be removed from Julia 2.0.   So, I've been looking to replace them with local code as advised (since <https://github.com/JuliaLang/julia/pull/38892|pull #38892> was denied).   Here is the code in question.\n```mutable struct Text{T}\n    content::T\nend\n\nBase.print(io::IO, t::Text) = print(io, t.content)\nBase.print(io::IO, t::Text{&lt;:Function}) = t.content(io)\nBase.show(io::IO, t::Text) = print(io, t)```\nWhen I do a straight-up replacement, calling this type `Reprint` (let's say), it seems to work.  At first I thought it was slower; but I think that's a sampling error on an imperfect laptop.  I have a question.\n\n1. When I drop \"mutable\" things get slower, almost 40% slower.\n2. If I replace `content::T` with `content::Function` things also get much slower.\nI'm using this in a `Text() do io ... end` style, so I was seeing if it could be improved or simplified.  I guess not?  What are the impacts of using `{T}`?\nSorry for the multiple edits (slack UI is frustrating).","user":"ULTB7E6UW","ts":"1608159461.119200","team":"T68168MUP","edited":{"user":"ULTB7E6UW","ts":"1608160314.000000"},"blocks":[{"type":"rich_text","block_id":"/dQf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So, I'm looking at the Julia  "},{"type":"link","url":"https://github.com/JuliaLang/julia/issues/29841","text":"#29841"},{"type":"text","text":", where I learned that "},{"type":"text","text":"HTML","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"Text","style":{"code":true}},{"type":"text","text":" are errors and would be removed from Julia 2.0.   So, I've been looking to replace them with local code as advised (since "},{"type":"link","url":"https://github.com/JuliaLang/julia/pull/38892","text":"pull #38892"},{"type":"text","text":" was denied).   Here is the code in question.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"mutable struct Text{T}\n    content::T\nend\n\nBase.print(io::IO, t::Text) = print(io, t.content)\nBase.print(io::IO, t::Text{<:Function}) = t.content(io)\nBase.show(io::IO, t::Text) = print(io, t)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"When I do a straight-up replacement, calling this type "},{"type":"text","text":"Reprint","style":{"code":true}},{"type":"text","text":" (let's say), it seems to work.  At first I thought it was slower; but I think that's a sampling error on an imperfect laptop.  I have a question.\n\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"When I drop \"mutable\" things get slower, almost 40% slower."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"If I replace "},{"type":"text","text":"content::T","style":{"code":true}},{"type":"text","text":" with "},{"type":"text","text":"content::Function","style":{"code":true}},{"type":"text","text":" things also get much slower."}]}],"style":"ordered","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"\nI'm using this in a "},{"type":"text","text":"Text() do io ... end","style":{"code":true}},{"type":"text","text":" style, so I was seeing if it could be improved or simplified.  I guess not?  What are the impacts of using "},{"type":"text","text":"{T}","style":{"code":true}},{"type":"text","text":"?\nSorry for the multiple edits (slack UI is frustrating)."}]}]}],"thread_ts":"1608159461.119200","reply_count":70,"reply_users_count":2,"latest_reply":"1608164821.136600","reply_users":["ULTB7E6UW","U0179G7FG4F"],"subscribed":false},{"client_msg_id":"8fbf43e1-574f-4e16-97ec-71fb6c3218ea","type":"message","text":"Are there usually issues with type inference when shadowing local variables? Consider this relatively nonsensical example:\n```module Stuff\nfunction solve_b(filename)\n    your = Vector{Int}()\n    _nearby = Vector{Vector{Int}}()\n    ruledict = Dict{String, Vector{UnitRange}}()\n    nearby = filter(true, _nearby)\n    Dict(\n        rulename =&gt; Set(filter(_ -&gt; all(true, nearby), 1:length(your)))\n        for rulename in ruledict\n    )\nend\nend```\nThere are no issues when you run `@code_warntype Stuff.solve_b(\"a\")`, but if `_nearby` is changed to `nearby`, then it infers `nearby@_6::Core.Box` and `nearby@_8::Union{}` instead of `Vector{Vector{Int64}}` as it's supposed to.","user":"UC81ESVH6","ts":"1608159805.120800","team":"T68168MUP","edited":{"user":"UC81ESVH6","ts":"1608161244.000000"},"blocks":[{"type":"rich_text","block_id":"FuXcB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Are there usually issues with type inference when shadowing local variables? Consider this relatively nonsensical example:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"module Stuff\nfunction solve_b(filename)\n    your = Vector{Int}()\n    _nearby = Vector{Vector{Int}}()\n    ruledict = Dict{String, Vector{UnitRange}}()\n    nearby = filter(true, _nearby)\n    Dict(\n        rulename => Set(filter(_ -> all(true, nearby), 1:length(your)))\n        for rulename in ruledict\n    )\nend\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"There are no issues when you run "},{"type":"text","text":"@code_warntype Stuff.solve_b(\"a\")","style":{"code":true}},{"type":"text","text":", but if "},{"type":"text","text":"_nearby","style":{"code":true}},{"type":"text","text":" is changed to "},{"type":"text","text":"nearby","style":{"code":true}},{"type":"text","text":", then it infers "},{"type":"text","text":"nearby@_6::Core.Box","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"nearby@_8::Union{}","style":{"code":true}},{"type":"text","text":" instead of "},{"type":"text","text":"Vector{Vector{Int64}}","style":{"code":true}},{"type":"text","text":" as it's supposed to."}]}]}],"thread_ts":"1608159805.120800","reply_count":3,"reply_users_count":1,"latest_reply":"1608163294.130800","reply_users":["U67BJLYCS"],"subscribed":false},{"client_msg_id":"098d0cf6-d109-41cc-a560-36f954d58aaa","type":"message","text":"TranscodingStreams.jl is the best bet we have for very fast IO buffering, right? It seems like some performance is still left on the table:\nComparing this\n```function bufferfile(path, buffer::Vector{UInt8})\n    rdr = NoopStream(open(path))\n    while !eof(rdr)\n        readbytes!(rdr, buffer)\n    end\nend```\nwith `cat my_file &gt; /dev/null` , I get 370 ms for TranscodingStreams and 240 ms for `cat` . Does anyone know if a thorough optimization has been attempted?","user":"U7HAYKY9X","ts":"1608240108.139500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3wJn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"TranscodingStreams.jl is the best bet we have for very fast IO buffering, right? It seems like some performance is still left on the table:\nComparing this\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function bufferfile(path, buffer::Vector{UInt8})\n    rdr = NoopStream(open(path))\n    while !eof(rdr)\n        readbytes!(rdr, buffer)\n    end\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"with "},{"type":"text","text":"cat my_file > /dev/null","style":{"code":true}},{"type":"text","text":" , I get 370 ms for TranscodingStreams and 240 ms for "},{"type":"text","text":"cat","style":{"code":true}},{"type":"text","text":" . Does anyone know if a thorough optimization has been attempted?"}]}]}],"thread_ts":"1608240108.139500","reply_count":12,"reply_users_count":3,"latest_reply":"1608319538.142000","reply_users":["UH24GRBLL","U7HAYKY9X","U01GRS159T8"],"subscribed":false},{"client_msg_id":"8cfdea73-ff2d-4724-b473-e38adf55faa1","type":"message","text":"In which case running `GC.gc()` manually not really run the gc in the backend ?\nI have disabled gc by running `GC.enable(false)` first and then calling GC.gc() manually","user":"U01GZQ6B0JU","ts":"1608532265.145000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xUn6X","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In which case running "},{"type":"text","text":"GC.gc()","style":{"code":true}},{"type":"text","text":" manually not really run the gc in the backend ?\nI have disabled gc by running "},{"type":"text","text":"GC.enable(false) ","style":{"code":true}},{"type":"text","text":"first and then calling GC.gc() manually"}]}]}],"thread_ts":"1608532265.145000","reply_count":3,"reply_users_count":2,"latest_reply":"1608532740.145600","reply_users":["U0179G7FG4F","U01GZQ6B0JU"],"subscribed":false},{"client_msg_id":"4baa4893-1ef7-4e55-ada9-167090e2c74f","type":"message","text":"hi","user":"UTAQ23XTR","ts":"1608559692.146000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"72q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hi"}]}]}],"reactions":[{"name":"wave","users":["U7HAYKY9X","UH24GRBLL"],"count":2}]},{"client_msg_id":"56fccb0f-dd1e-43d0-9215-60ac601c0de1","type":"message","text":"I am attempting to translate some python code to julia to speed it up. But maybe as it's almost christmas I could just give it as a puzzle.","user":"UTAQ23XTR","ts":"1608560858.146900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eXPR9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am attempting to translate some python code to julia to speed it up. But maybe as it's almost christmas I could just give it as a puzzle."}]}]}],"thread_ts":"1608560858.146900","reply_count":107,"reply_users_count":4,"latest_reply":"1608649127.172200","reply_users":["U0179G7FG4F","UTAQ23XTR","UH24GRBLL","UDD5Z7FLZ"],"subscribed":false},{"client_msg_id":"4a8f2f20-8da9-42e9-9b65-18bf8a5a0891","type":"message","text":"Here it is: Given the string aaaaaaaaaa and an alphabet of size 100, find all other strings with edit (Levenshtein) distance at most 2. Your code must run in less than 1 second.  There are 1631129 distinct such strings","user":"UTAQ23XTR","ts":"1608560908.147500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6Rg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Here it is: Given the string aaaaaaaaaa and an alphabet of size 100, find all other strings with edit (Levenshtein) distance at most 2. Your code must run in less than 1 second.  There are 1631129 distinct such strings"}]}]}]},{"client_msg_id":"49c077da-71a4-42c7-a86c-ec79d68cc2f9","type":"message","text":"Ideally the code would only output the distinct strings but that is not necessary to win the prize :slightly_smiling_face:","user":"UTAQ23XTR","ts":"1608560933.148000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VmuNC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ideally the code would only output the distinct strings but that is not necessary to win the prize "},{"type":"emoji","name":"slightly_smiling_face"}]}]}]},{"type":"message","subtype":"thread_broadcast","text":"there are only 95 printable ASCII characters...","user":"UH24GRBLL","ts":"1608561577.149300","thread_ts":"1608560858.146900","root":{"client_msg_id":"56fccb0f-dd1e-43d0-9215-60ac601c0de1","type":"message","text":"I am attempting to translate some python code to julia to speed it up. But maybe as it's almost christmas I could just give it as a puzzle.","user":"UTAQ23XTR","ts":"1608560858.146900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eXPR9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am attempting to translate some python code to julia to speed it up. But maybe as it's almost christmas I could just give it as a puzzle."}]}]}],"thread_ts":"1608560858.146900","reply_count":107,"reply_users_count":4,"latest_reply":"1608649127.172200","reply_users":["U0179G7FG4F","UTAQ23XTR","UH24GRBLL","UDD5Z7FLZ"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"F0sjL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"there are only 95 printable ASCII characters..."}]}]}],"client_msg_id":"01f58c3d-e40c-48b2-a020-52ab028e7bc2"},{"client_msg_id":"b31b26ad-193d-47c8-8f6b-a2946097c724","type":"message","text":"My code was taking 2 hours 40 mins to run and now it takes 7 minutes :star-struck:.","user":"UDSG73JTH","ts":"1608580172.162200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wT3J","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"My code was taking 2 hours 40 mins to run and now it takes 7 minutes "},{"type":"emoji","name":"star-struck"},{"type":"text","text":"."}]}]}],"thread_ts":"1608580172.162200","reply_count":2,"reply_users_count":2,"latest_reply":"1608585656.162800","reply_users":["U0179G7FG4F","UDSG73JTH"],"subscribed":false},{"client_msg_id":"a0c70155-54aa-4266-bfce-642164dc5aa1","type":"message","text":"Just out of curiosity: On the systems have access to, allocations larger than 16 KiB take two operations:\n```julia&gt; @time Vector{UInt8}(undef, 2^14 - 1);\n  0.000005 seconds (1 allocation: 16.125 KiB)\n\njulia&gt; @time Vector{UInt8}(undef, 2^14);\n  0.000006 seconds (2 allocations: 16.141 KiB)\n\njulia&gt; @time Vector{UInt8}(undef, 2^40);\n  0.008465 seconds (2 allocations: 1.000 TiB, 99.61% gc time)```\nDoes this mean that the array is no longer contiguous in memory? What sets this limit? Hardware, OS, LLVM, or julia?","user":"U01DD7Z0D89","ts":"1608742654.177700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sbMEx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Just out of curiosity: On the systems have access to, allocations larger than 16 KiB take two operations:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> @time Vector{UInt8}(undef, 2^14 - 1);\n  0.000005 seconds (1 allocation: 16.125 KiB)\n\njulia> @time Vector{UInt8}(undef, 2^14);\n  0.000006 seconds (2 allocations: 16.141 KiB)\n\njulia> @time Vector{UInt8}(undef, 2^40);\n  0.008465 seconds (2 allocations: 1.000 TiB, 99.61% gc time)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Does this mean that the array is no longer contiguous in memory? What sets this limit? Hardware, OS, LLVM, or julia?"}]}]}],"thread_ts":"1608742654.177700","reply_count":1,"reply_users_count":1,"latest_reply":"1608742835.177800","reply_users":["U0179G7FG4F"],"subscribed":false},{"client_msg_id":"37a19605-0c34-4710-8d56-332eaa6a2fd9","type":"message","text":"hi all","user":"UTAQ23XTR","ts":"1608998510.182200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3+Ul","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hi all"}]}]}],"thread_ts":"1608998510.182200","reply_count":1,"reply_users_count":1,"latest_reply":"1608998540.182300","reply_users":["U7HAYKY9X"],"subscribed":false},{"client_msg_id":"e1a77c61-12be-4aab-a362-3c5c96d34348","type":"message","text":"it would be great if there were more/some Julia answers on <http://codegolf.stackexchange.com|codegolf.stackexchange.com> for fastest-code questions.","user":"UTAQ23XTR","ts":"1609002646.183400","team":"T68168MUP","edited":{"user":"UTAQ23XTR","ts":"1609002661.000000"},"blocks":[{"type":"rich_text","block_id":"kx0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it would be great if there were more/some Julia answers on "},{"type":"link","url":"http://codegolf.stackexchange.com","text":"codegolf.stackexchange.com"},{"type":"text","text":" for fastest-code questions."}]}]}]},{"client_msg_id":"7f7a2c00-9ab1-4bd2-bcfd-adac86a282b6","type":"message","text":"My main issue with such questions is that it sometimes veers into the \"this-looks-like-homework\" territory, and I would be a little hesitant to help.","user":"UDD5Z7FLZ","ts":"1609014202.186300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IO8i","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"My main issue with such questions is that it sometimes veers into the \"this-looks-like-homework\" territory, and I would be a little hesitant to help."}]}]}]},{"client_msg_id":"7e136000-a164-491e-a412-59320db940b8","type":"message","text":"I wrote a simple function to sum integers from 1 to n as a simple test of BigInt:\n```function sum_n(n::BigInt)::BigInt\n  val = BigInt(0)\n\n  while n &gt; 0\n    val += n\n    n -=  1\n  end\n\n  val\nend\nsum_n(BigInt(10))\n@time println(sum_n(BigInt(6000000))) # 6 Million```\nIt takes roughly 2.2 seconds to sum 6M numbers. By comparison, the following Racket function can sum 1B numbers in about the same amount of time. The Julia code has 30M in allocations which seems crazy. Does the BigInt implementation allocate a new BigInt for each arithmetic operation?\n```(define (sum-n n [result 0])\n  (if (&gt; n 0) \n      (sum-n (- n 1) (+ result n))\n      result))\n\n(time (sum-n 1000000000)) ; 1 Billion```","user":"U014ATN949F","ts":"1609015995.188900","team":"T68168MUP","edited":{"user":"U014ATN949F","ts":"1609016087.000000"},"blocks":[{"type":"rich_text","block_id":"cIfOH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I wrote a simple function to sum integers from 1 to n as a simple test of BigInt:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function sum_n(n::BigInt)::BigInt\n  val = BigInt(0)\n\n  while n > 0\n    val += n\n    n -=  1\n  end\n\n  val\nend\nsum_n(BigInt(10))\n@time println(sum_n(BigInt(6000000))) # 6 Million"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"It takes roughly 2.2 seconds to sum 6M numbers. By comparison, the following Racket function can sum 1B numbers in about the same amount of time. The Julia code has 30M in allocations which seems crazy. Does the BigInt implementation allocate a new BigInt for each arithmetic operation?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"(define (sum-n n [result 0])\n  (if (> n 0) \n      (sum-n (- n 1) (+ result n))\n      result))\n\n(time (sum-n 1000000000)) ; 1 Billion"}]}]}],"thread_ts":"1609015995.188900","reply_count":46,"reply_users_count":5,"latest_reply":"1609017193.198800","reply_users":["UH24GRBLL","U014ATN949F","U7HAYKY9X","UDD5Z7FLZ","UD0NS8PDF"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"should `write(io, 1234)` allocate?","user":"UH24GRBLL","ts":"1609275144.246300","thread_ts":"1608240108.139500","root":{"client_msg_id":"098d0cf6-d109-41cc-a560-36f954d58aaa","type":"message","text":"TranscodingStreams.jl is the best bet we have for very fast IO buffering, right? It seems like some performance is still left on the table:\nComparing this\n```function bufferfile(path, buffer::Vector{UInt8})\n    rdr = NoopStream(open(path))\n    while !eof(rdr)\n        readbytes!(rdr, buffer)\n    end\nend```\nwith `cat my_file &gt; /dev/null` , I get 370 ms for TranscodingStreams and 240 ms for `cat` . Does anyone know if a thorough optimization has been attempted?","user":"U7HAYKY9X","ts":"1608240108.139500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3wJn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"TranscodingStreams.jl is the best bet we have for very fast IO buffering, right? It seems like some performance is still left on the table:\nComparing this\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function bufferfile(path, buffer::Vector{UInt8})\n    rdr = NoopStream(open(path))\n    while !eof(rdr)\n        readbytes!(rdr, buffer)\n    end\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"with "},{"type":"text","text":"cat my_file > /dev/null","style":{"code":true}},{"type":"text","text":" , I get 370 ms for TranscodingStreams and 240 ms for "},{"type":"text","text":"cat","style":{"code":true}},{"type":"text","text":" . Does anyone know if a thorough optimization has been attempted?"}]}]}],"thread_ts":"1608240108.139500","reply_count":52,"reply_users_count":3,"latest_reply":"1609275155.246600","reply_users":["UH24GRBLL","U7HAYKY9X","U01GRS159T8"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"GeR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"should "},{"type":"text","text":"write(io, 1234)","style":{"code":true}},{"type":"text","text":" allocate?"}]}]}],"client_msg_id":"1007e5bd-1825-4639-8b12-135136e5da00"},{"client_msg_id":"0a390832-7399-4f7b-9a00-c72225a5794f","type":"message","text":"I have a function like this (what it does isn't really important):\n```function stabilize(curr, tolerance, distance=maximum(size(curr)))\n    prev = nothing\n    while prev != curr\n        prev = deepcopy(curr)\n        for I in CartesianIndices(curr)\n            prev[I] == 'L' &amp;&amp; count_occupied(prev, I, distance) == 0         &amp;&amp; (curr[I] = '#')\n            prev[I] == '#' &amp;&amp; count_occupied(prev, I, distance) &gt;= tolerance &amp;&amp; (curr[I] = 'L')\n        end\n    end\n    count(==('#'), curr)\nend```\nAnd it takes this long to run:\n```julia&gt; @btime stabilize(......, 4, 1)\n  38.350 ms (720 allocations: 4.00 MiB)```\nBut the moment I replace `tolerance` and `distance` with the respective values in the function I see an improvement:\n```julia&gt; @btime stabilize(......)\n  13.499 ms (720 allocations: 4.00 MiB)```\nWhy is this the case?","user":"UC81ESVH6","ts":"1609280309.249400","team":"T68168MUP","edited":{"user":"UC81ESVH6","ts":"1609280384.000000"},"blocks":[{"type":"rich_text","block_id":"H9Z6C","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a function like this (what it does isn't really important):\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function stabilize(curr, tolerance, distance=maximum(size(curr)))\n    prev = nothing\n    while prev != curr\n        prev = deepcopy(curr)\n        for I in CartesianIndices(curr)\n            prev[I] == 'L' && count_occupied(prev, I, distance) == 0         && (curr[I] = '#')\n            prev[I] == '#' && count_occupied(prev, I, distance) >= tolerance && (curr[I] = 'L')\n        end\n    end\n    count(==('#'), curr)\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"And it takes this long to run:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> @btime stabilize(......, 4, 1)\n  38.350 ms (720 allocations: 4.00 MiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"But the moment I replace "},{"type":"text","text":"tolerance","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"distance","style":{"code":true}},{"type":"text","text":" with the respective values in the function I see an improvement:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> @btime stabilize(......)\n  13.499 ms (720 allocations: 4.00 MiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Why is this the case?"}]}]}],"thread_ts":"1609280309.249400","reply_count":7,"reply_users_count":3,"latest_reply":"1609282422.250900","reply_users":["UH24GRBLL","UC81ESVH6","U01GMP3HF9C"],"subscribed":false},{"client_msg_id":"964d3841-c6d5-44e7-99fe-5925b196e25e","type":"message","text":"Hello all, I'm trying to optimize the following code. Essentially, I'm want to find the minimum value from `value` for each unique combination of 2 vectors (for the purposes of this code, v1 = 1 and v2 = 2 is the same as v1 = 2 and v2 = 1, which is why I do that hacky part with `min` and `max`). I need to scale this up pretty massively, so I'm hoping to find ways to speed it up. Any suggestions? Thanks very much!!\n```using Base.Threads\n# Vector 1\nv1 = rand(collect(1:1000), 100000)\n# Vector 2\nv2 = rand(collect(1:1000), 100000)\n# Values\nvalue = rand(100000)\n\n# Get unique combos of v1 and v2\nminv = min.(v1, v2)\nmaxv = max.(v1, v2)\nunique_combos = unique(tuple.(minv, maxv))\nn_unique = length(unique_combos)\n\n# init a vector to house minimum of `value` per unique v1 and v2 combination\nvalue_mins = Vector{Float64}(undef, n_unique)\n\n# loop through to find minimum per unique group\n@threads for i in 1:n_unique\n    unique_combo = unique_combos[i]\n    min_value_for_combo_i = minimum(value[(minv .== unique_combo[1]) .&amp; (maxv .== unique_combo[2])])\n    value_mins[i] = min_value_for_combo_i\nend```","user":"U011NV8FNF7","ts":"1609287683.257400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JQpt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello all, I'm trying to optimize the following code. Essentially, I'm want to find the minimum value from "},{"type":"text","text":"value","style":{"code":true}},{"type":"text","text":" for each unique combination of 2 vectors (for the purposes of this code, v1 = 1 and v2 = 2 is the same as v1 = 2 and v2 = 1, which is why I do that hacky part with "},{"type":"text","text":"min","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"max","style":{"code":true}},{"type":"text","text":"). I need to scale this up pretty massively, so I'm hoping to find ways to speed it up. Any suggestions? Thanks very much!!\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Base.Threads\n# Vector 1\nv1 = rand(collect(1:1000), 100000)\n# Vector 2\nv2 = rand(collect(1:1000), 100000)\n# Values\nvalue = rand(100000)\n\n# Get unique combos of v1 and v2\nminv = min.(v1, v2)\nmaxv = max.(v1, v2)\nunique_combos = unique(tuple.(minv, maxv))\nn_unique = length(unique_combos)\n\n# init a vector to house minimum of `value` per unique v1 and v2 combination\nvalue_mins = Vector{Float64}(undef, n_unique)\n\n# loop through to find minimum per unique group\n@threads for i in 1:n_unique\n    unique_combo = unique_combos[i]\n    min_value_for_combo_i = minimum(value[(minv .== unique_combo[1]) .& (maxv .== unique_combo[2])])\n    value_mins[i] = min_value_for_combo_i\nend"}]}]}],"thread_ts":"1609287683.257400","reply_count":4,"reply_users_count":2,"latest_reply":"1609326180.258400","reply_users":["UDD5Z7FLZ","U01GMP3HF9C"],"subscribed":false},{"client_msg_id":"be34b342-27bf-423f-9c7f-06c335bf262b","type":"message","text":"Which types of operations is julia still lacking behind compared with C in terms of preformamce?\nAre these performance issues related to inherent issues with Julia or simply due to julias relative immaturity?","user":"U01FAHWCMFF","ts":"1609345469.261100","team":"T68168MUP","edited":{"user":"U01FAHWCMFF","ts":"1609345524.000000"},"blocks":[{"type":"rich_text","block_id":"8WzB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Which types of operations is julia still lacking behind compared with C in terms of preformamce?\nAre these performance issues related to inherent issues with Julia or simply due to julias relative immaturity?"}]}]}]},{"type":"message","text":"Come compete for a $10 starbucks gift card by being the first to solve my performance issue","user":"U01FAHWCMFF","ts":"1609366811.265400","team":"T68168MUP","edited":{"user":"U01FAHWCMFF","ts":"1609366852.000000"},"attachments":[{"fallback":"[December 30th, 2020 5:09 PM] ayman: If anyone manages to get rid of the last forloop with mapping, without using `MappedArrays.jl`  or any library that doesn't come shipped with Julia (e.g. Statistics.jl is allowed, GLM.jl isnt), *with the same performance*, I will give you a $10 starbucks gift card. First person to do so wins. Have fun all :slightly_smiling_face:\n```function pairwise_analysis2(x::Array{Int16, 2})\n    m, n = size(x)\n    mean_matrix = Matrix{Float64}(undef, (n, n))\n    @views @inbounds for i = 1:n\n        for j = 1:n\n            total::Int16 = 0\n            for k = 1:m\n                total += x[k, i] == x[k, j]\n            end\n            mean_matrix[i, j] = total / m\n        end\n    end\n    return mean_matrix\nend\n\nX = Array{Int16}(rand(0:1, (1000, 1000)));\n@btime pairwise_analysis(X)```","ts":"1609366179.234900","author_id":"U01FAHWCMFF","author_subname":"Ayman Al Baz","channel_id":"C67TK21LJ","channel_name":"gripes","is_msg_unfurl":true,"is_reply_unfurl":true,"text":"If anyone manages to get rid of the last forloop with mapping, without using `MappedArrays.jl`  or any library that doesn't come shipped with Julia (e.g. Statistics.jl is allowed, GLM.jl isnt), *with the same performance*, I will give you a $10 starbucks gift card. First person to do so wins. Have fun all :slightly_smiling_face:\n```function pairwise_analysis2(x::Array{Int16, 2})\n    m, n = size(x)\n    mean_matrix = Matrix{Float64}(undef, (n, n))\n    @views @inbounds for i = 1:n\n        for j = 1:n\n            total::Int16 = 0\n            for k = 1:m\n                total += x[k, i] == x[k, j]\n            end\n            mean_matrix[i, j] = total / m\n        end\n    end\n    return mean_matrix\nend\n\nX = Array{Int16}(rand(0:1, (1000, 1000)));\n@btime pairwise_analysis(X)```","author_name":"Ayman Al Baz","author_link":"https://julialang.slack.com/team/U01FAHWCMFF","author_icon":"https://avatars.slack-edge.com/2020-11-23/1516965530134_1cf6ed3767c34aaf6d40_48.png","mrkdwn_in":["text"],"color":"D0D0D0","from_url":"https://julialang.slack.com/archives/C67TK21LJ/p1609366179234900?thread_ts=1609362482231100&cid=C67TK21LJ","is_share":true,"footer":"From a thread in #gripes"}],"blocks":[{"type":"rich_text","block_id":"cgZ5M","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Come compete for a $10 starbucks gift card by being the first to solve my performance issue"}]}]}]}]}