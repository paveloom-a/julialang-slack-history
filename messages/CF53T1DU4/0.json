{"cursor": 0, "messages": [{"client_msg_id":"2d884b8f-de6c-48ac-bdea-871c4ef24636","type":"message","text":"Is appropriate to mention ~6% performance loss on nightly (as compared to 1.5.3), even if so, it might be hard to strip it down to a small metric. Or is this something to do once there's an alpha, etc.","user":"ULTB7E6UW","ts":"1607980182.098000","team":"T68168MUP","edited":{"user":"ULTB7E6UW","ts":"1607980205.000000"},"blocks":[{"type":"rich_text","block_id":"6WM9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is appropriate to mention ~6% performance loss on nightly (as compared to 1.5.3), even if so, it might be hard to strip it down to a small metric. Or is this something to do once there's an alpha, etc."}]}]}]},{"client_msg_id":"b2731467-5773-4ea5-86f8-db55eb37a958","type":"message","text":"If you have a reproducible example (best small and self-contained)","user":"U67BJLYCS","ts":"1607980532.098700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1tC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If you have a reproducible example (best small and self-contained)"}]}]}],"thread_ts":"1607980532.098700","reply_count":27,"reply_users_count":2,"latest_reply":"1607994990.104300","reply_users":["ULTB7E6UW","U67BJLYCS"],"subscribed":false},{"client_msg_id":"3f97b8d5-4090-4cd7-ac5b-0f304ad1781e","type":"message","text":"then yes, please open an issue","user":"U67BJLYCS","ts":"1607980542.099000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+ZcN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"then yes, please open an issue"}]}]}],"thread_ts":"1607980542.099000","reply_count":1,"reply_users_count":1,"latest_reply":"1608154024.115800","reply_users":["ULTB7E6UW"],"subscribed":false},{"client_msg_id":"778dd2d1-9a9e-4be9-bead-dc77d9bc01a0","type":"message","text":"I posted the following in <#C6A044SQH|helpdesk>, but someone mentioned this channel, so I'll just paste it below:\n\nI coded up 3 solutions to Day 15 of Advent of Code (spoilers), and got the following timings:\n<https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15-unsafe-vector.rkt|Racket w/ unsafe vector/arithmetic> =&gt; 0.68 seconds\n<https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15.c|C version> =&gt; 0.47 seconds\n<https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15.jl|Julia w/ mutable vector> =&gt; 0.41 seconds\nI was impressed that Julia was faster than C. On the one hand, I haven't coded C seriously since 1996, but on the other, I'm very much a Julia newbie.\n\nI'm just curious if an experienced Julia coder can spot any obvious inefficiencies in the <https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15.jl|little Julia program> - even though it came in faster than C, that doesn't mean I wouldn't like it to be faster :slightly_smiling_face:","user":"U014ATN949F","ts":"1608079617.105300","team":"T68168MUP","edited":{"user":"U014ATN949F","ts":"1608133861.000000"},"blocks":[{"type":"rich_text","block_id":"DvIU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I posted the following in "},{"type":"channel","channel_id":"C6A044SQH"},{"type":"text","text":", but someone mentioned this channel, so I'll just paste it below:\n\nI coded up 3 solutions to Day 15 of Advent of Code (spoilers), and got the following timings:\n"},{"type":"link","url":"https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15-unsafe-vector.rkt","text":"Racket w/ unsafe vector/arithmetic"},{"type":"text","text":" => 0.68 seconds\n"},{"type":"link","url":"https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15.c","text":"C version"},{"type":"text","text":" => 0.47 seconds\n"},{"type":"link","url":"https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15.jl","text":"Julia w/ mutable vector"},{"type":"text","text":" => 0.41 seconds\nI was impressed that Julia was faster than C. On the one hand, I haven't coded C seriously since 1996, but on the other, I'm very much a Julia newbie.\n\nI'm just curious if an experienced Julia coder can spot any obvious inefficiencies in the "},{"type":"link","url":"https://github.com/lojic/LearningRacket/blob/master/advent-of-code-2020/day15.jl","text":"little Julia program"},{"type":"text","text":" - even though it came in faster than C, that doesn't mean I wouldn't like it to be faster "},{"type":"emoji","name":"slightly_smiling_face"}]}]}],"thread_ts":"1608079617.105300","reply_count":33,"reply_users_count":6,"latest_reply":"1608400503.142400","reply_users":["UD0NS8PDF","U014ATN949F","U0179G7FG4F","U7HAYKY9X","UH24GRBLL","UB7JS9CHF"],"subscribed":false},{"client_msg_id":"21ba142e-6d2e-4446-aae1-07ab427f9059","type":"message","text":"So, I'm looking at the Julia  <https://github.com/JuliaLang/julia/issues/29841|#29841>, where I learned that `HTML` and `Text` are errors and would be removed from Julia 2.0.   So, I've been looking to replace them with local code as advised (since <https://github.com/JuliaLang/julia/pull/38892|pull #38892> was denied).   Here is the code in question.\n```mutable struct Text{T}\n    content::T\nend\n\nBase.print(io::IO, t::Text) = print(io, t.content)\nBase.print(io::IO, t::Text{&lt;:Function}) = t.content(io)\nBase.show(io::IO, t::Text) = print(io, t)```\nWhen I do a straight-up replacement, calling this type `Reprint` (let's say), it seems to work.  At first I thought it was slower; but I think that's a sampling error on an imperfect laptop.  I have a question.\n\n1. When I drop \"mutable\" things get slower, almost 40% slower.\n2. If I replace `content::T` with `content::Function` things also get much slower.\nI'm using this in a `Text() do io ... end` style, so I was seeing if it could be improved or simplified.  I guess not?  What are the impacts of using `{T}`?\nSorry for the multiple edits (slack UI is frustrating).","user":"ULTB7E6UW","ts":"1608159461.119200","team":"T68168MUP","edited":{"user":"ULTB7E6UW","ts":"1608160314.000000"},"blocks":[{"type":"rich_text","block_id":"/dQf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So, I'm looking at the Julia  "},{"type":"link","url":"https://github.com/JuliaLang/julia/issues/29841","text":"#29841"},{"type":"text","text":", where I learned that "},{"type":"text","text":"HTML","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"Text","style":{"code":true}},{"type":"text","text":" are errors and would be removed from Julia 2.0.   So, I've been looking to replace them with local code as advised (since "},{"type":"link","url":"https://github.com/JuliaLang/julia/pull/38892","text":"pull #38892"},{"type":"text","text":" was denied).   Here is the code in question.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"mutable struct Text{T}\n    content::T\nend\n\nBase.print(io::IO, t::Text) = print(io, t.content)\nBase.print(io::IO, t::Text{<:Function}) = t.content(io)\nBase.show(io::IO, t::Text) = print(io, t)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"When I do a straight-up replacement, calling this type "},{"type":"text","text":"Reprint","style":{"code":true}},{"type":"text","text":" (let's say), it seems to work.  At first I thought it was slower; but I think that's a sampling error on an imperfect laptop.  I have a question.\n\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"When I drop \"mutable\" things get slower, almost 40% slower."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"If I replace "},{"type":"text","text":"content::T","style":{"code":true}},{"type":"text","text":" with "},{"type":"text","text":"content::Function","style":{"code":true}},{"type":"text","text":" things also get much slower."}]}],"style":"ordered","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"\nI'm using this in a "},{"type":"text","text":"Text() do io ... end","style":{"code":true}},{"type":"text","text":" style, so I was seeing if it could be improved or simplified.  I guess not?  What are the impacts of using "},{"type":"text","text":"{T}","style":{"code":true}},{"type":"text","text":"?\nSorry for the multiple edits (slack UI is frustrating)."}]}]}],"thread_ts":"1608159461.119200","reply_count":70,"reply_users_count":2,"latest_reply":"1608164821.136600","reply_users":["ULTB7E6UW","U0179G7FG4F"],"subscribed":false},{"client_msg_id":"8fbf43e1-574f-4e16-97ec-71fb6c3218ea","type":"message","text":"Are there usually issues with type inference when shadowing local variables? Consider this relatively nonsensical example:\n```module Stuff\nfunction solve_b(filename)\n    your = Vector{Int}()\n    _nearby = Vector{Vector{Int}}()\n    ruledict = Dict{String, Vector{UnitRange}}()\n    nearby = filter(true, _nearby)\n    Dict(\n        rulename =&gt; Set(filter(_ -&gt; all(true, nearby), 1:length(your)))\n        for rulename in ruledict\n    )\nend\nend```\nThere are no issues when you run `@code_warntype Stuff.solve_b(\"a\")`, but if `_nearby` is changed to `nearby`, then it infers `nearby@_6::Core.Box` and `nearby@_8::Union{}` instead of `Vector{Vector{Int64}}` as it's supposed to.","user":"UC81ESVH6","ts":"1608159805.120800","team":"T68168MUP","edited":{"user":"UC81ESVH6","ts":"1608161244.000000"},"blocks":[{"type":"rich_text","block_id":"FuXcB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Are there usually issues with type inference when shadowing local variables? Consider this relatively nonsensical example:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"module Stuff\nfunction solve_b(filename)\n    your = Vector{Int}()\n    _nearby = Vector{Vector{Int}}()\n    ruledict = Dict{String, Vector{UnitRange}}()\n    nearby = filter(true, _nearby)\n    Dict(\n        rulename => Set(filter(_ -> all(true, nearby), 1:length(your)))\n        for rulename in ruledict\n    )\nend\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"There are no issues when you run "},{"type":"text","text":"@code_warntype Stuff.solve_b(\"a\")","style":{"code":true}},{"type":"text","text":", but if "},{"type":"text","text":"_nearby","style":{"code":true}},{"type":"text","text":" is changed to "},{"type":"text","text":"nearby","style":{"code":true}},{"type":"text","text":", then it infers "},{"type":"text","text":"nearby@_6::Core.Box","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"nearby@_8::Union{}","style":{"code":true}},{"type":"text","text":" instead of "},{"type":"text","text":"Vector{Vector{Int64}}","style":{"code":true}},{"type":"text","text":" as it's supposed to."}]}]}],"thread_ts":"1608159805.120800","reply_count":3,"reply_users_count":1,"latest_reply":"1608163294.130800","reply_users":["U67BJLYCS"],"subscribed":false},{"client_msg_id":"098d0cf6-d109-41cc-a560-36f954d58aaa","type":"message","text":"TranscodingStreams.jl is the best bet we have for very fast IO buffering, right? It seems like some performance is still left on the table:\nComparing this\n```function bufferfile(path, buffer::Vector{UInt8})\n    rdr = NoopStream(open(path))\n    while !eof(rdr)\n        readbytes!(rdr, buffer)\n    end\nend```\nwith `cat my_file &gt; /dev/null` , I get 370 ms for TranscodingStreams and 240 ms for `cat` . Does anyone know if a thorough optimization has been attempted?","user":"U7HAYKY9X","ts":"1608240108.139500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3wJn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"TranscodingStreams.jl is the best bet we have for very fast IO buffering, right? It seems like some performance is still left on the table:\nComparing this\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function bufferfile(path, buffer::Vector{UInt8})\n    rdr = NoopStream(open(path))\n    while !eof(rdr)\n        readbytes!(rdr, buffer)\n    end\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"with "},{"type":"text","text":"cat my_file > /dev/null","style":{"code":true}},{"type":"text","text":" , I get 370 ms for TranscodingStreams and 240 ms for "},{"type":"text","text":"cat","style":{"code":true}},{"type":"text","text":" . Does anyone know if a thorough optimization has been attempted?"}]}]}],"thread_ts":"1608240108.139500","reply_count":12,"reply_users_count":3,"latest_reply":"1608319538.142000","reply_users":["UH24GRBLL","U7HAYKY9X","U01GRS159T8"],"subscribed":false},{"client_msg_id":"8cfdea73-ff2d-4724-b473-e38adf55faa1","type":"message","text":"In which case running `GC.gc()` manually not really run the gc in the backend ?\nI have disabled gc by running `GC.enable(false)` first and then calling GC.gc() manually","user":"U01GZQ6B0JU","ts":"1608532265.145000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xUn6X","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In which case running "},{"type":"text","text":"GC.gc()","style":{"code":true}},{"type":"text","text":" manually not really run the gc in the backend ?\nI have disabled gc by running "},{"type":"text","text":"GC.enable(false) ","style":{"code":true}},{"type":"text","text":"first and then calling GC.gc() manually"}]}]}],"thread_ts":"1608532265.145000","reply_count":3,"reply_users_count":2,"latest_reply":"1608532740.145600","reply_users":["U0179G7FG4F","U01GZQ6B0JU"],"subscribed":false},{"client_msg_id":"4baa4893-1ef7-4e55-ada9-167090e2c74f","type":"message","text":"hi","user":"UTAQ23XTR","ts":"1608559692.146000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"72q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hi"}]}]}],"reactions":[{"name":"wave","users":["U7HAYKY9X","UH24GRBLL"],"count":2}]},{"client_msg_id":"56fccb0f-dd1e-43d0-9215-60ac601c0de1","type":"message","text":"I am attempting to translate some python code to julia to speed it up. But maybe as it's almost christmas I could just give it as a puzzle.","user":"UTAQ23XTR","ts":"1608560858.146900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eXPR9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am attempting to translate some python code to julia to speed it up. But maybe as it's almost christmas I could just give it as a puzzle."}]}]}],"thread_ts":"1608560858.146900","reply_count":107,"reply_users_count":4,"latest_reply":"1608649127.172200","reply_users":["U0179G7FG4F","UTAQ23XTR","UH24GRBLL","UDD5Z7FLZ"],"subscribed":false},{"client_msg_id":"4a8f2f20-8da9-42e9-9b65-18bf8a5a0891","type":"message","text":"Here it is: Given the string aaaaaaaaaa and an alphabet of size 100, find all other strings with edit (Levenshtein) distance at most 2. Your code must run in less than 1 second.  There are 1631129 distinct such strings","user":"UTAQ23XTR","ts":"1608560908.147500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6Rg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Here it is: Given the string aaaaaaaaaa and an alphabet of size 100, find all other strings with edit (Levenshtein) distance at most 2. Your code must run in less than 1 second.  There are 1631129 distinct such strings"}]}]}]},{"client_msg_id":"49c077da-71a4-42c7-a86c-ec79d68cc2f9","type":"message","text":"Ideally the code would only output the distinct strings but that is not necessary to win the prize :slightly_smiling_face:","user":"UTAQ23XTR","ts":"1608560933.148000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VmuNC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ideally the code would only output the distinct strings but that is not necessary to win the prize "},{"type":"emoji","name":"slightly_smiling_face"}]}]}]},{"type":"message","subtype":"thread_broadcast","text":"there are only 95 printable ASCII characters...","user":"UH24GRBLL","ts":"1608561577.149300","thread_ts":"1608560858.146900","root":{"client_msg_id":"56fccb0f-dd1e-43d0-9215-60ac601c0de1","type":"message","text":"I am attempting to translate some python code to julia to speed it up. But maybe as it's almost christmas I could just give it as a puzzle.","user":"UTAQ23XTR","ts":"1608560858.146900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eXPR9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am attempting to translate some python code to julia to speed it up. But maybe as it's almost christmas I could just give it as a puzzle."}]}]}],"thread_ts":"1608560858.146900","reply_count":107,"reply_users_count":4,"latest_reply":"1608649127.172200","reply_users":["U0179G7FG4F","UTAQ23XTR","UH24GRBLL","UDD5Z7FLZ"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"F0sjL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"there are only 95 printable ASCII characters..."}]}]}],"client_msg_id":"01f58c3d-e40c-48b2-a020-52ab028e7bc2"},{"client_msg_id":"b31b26ad-193d-47c8-8f6b-a2946097c724","type":"message","text":"My code was taking 2 hours 40 mins to run and now it takes 7 minutes :star-struck:.","user":"UDSG73JTH","ts":"1608580172.162200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wT3J","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"My code was taking 2 hours 40 mins to run and now it takes 7 minutes "},{"type":"emoji","name":"star-struck"},{"type":"text","text":"."}]}]}],"thread_ts":"1608580172.162200","reply_count":2,"reply_users_count":2,"latest_reply":"1608585656.162800","reply_users":["U0179G7FG4F","UDSG73JTH"],"subscribed":false},{"client_msg_id":"a0c70155-54aa-4266-bfce-642164dc5aa1","type":"message","text":"Just out of curiosity: On the systems have access to, allocations larger than 16 KiB take two operations:\n```julia&gt; @time Vector{UInt8}(undef, 2^14 - 1);\n  0.000005 seconds (1 allocation: 16.125 KiB)\n\njulia&gt; @time Vector{UInt8}(undef, 2^14);\n  0.000006 seconds (2 allocations: 16.141 KiB)\n\njulia&gt; @time Vector{UInt8}(undef, 2^40);\n  0.008465 seconds (2 allocations: 1.000 TiB, 99.61% gc time)```\nDoes this mean that the array is no longer contiguous in memory? What sets this limit? Hardware, OS, LLVM, or julia?","user":"U01DD7Z0D89","ts":"1608742654.177700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sbMEx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Just out of curiosity: On the systems have access to, allocations larger than 16 KiB take two operations:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> @time Vector{UInt8}(undef, 2^14 - 1);\n  0.000005 seconds (1 allocation: 16.125 KiB)\n\njulia> @time Vector{UInt8}(undef, 2^14);\n  0.000006 seconds (2 allocations: 16.141 KiB)\n\njulia> @time Vector{UInt8}(undef, 2^40);\n  0.008465 seconds (2 allocations: 1.000 TiB, 99.61% gc time)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Does this mean that the array is no longer contiguous in memory? What sets this limit? Hardware, OS, LLVM, or julia?"}]}]}],"thread_ts":"1608742654.177700","reply_count":1,"reply_users_count":1,"latest_reply":"1608742835.177800","reply_users":["U0179G7FG4F"],"subscribed":false},{"client_msg_id":"37a19605-0c34-4710-8d56-332eaa6a2fd9","type":"message","text":"hi all","user":"UTAQ23XTR","ts":"1608998510.182200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3+Ul","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hi all"}]}]}],"thread_ts":"1608998510.182200","reply_count":1,"reply_users_count":1,"latest_reply":"1608998540.182300","reply_users":["U7HAYKY9X"],"subscribed":false},{"client_msg_id":"e1a77c61-12be-4aab-a362-3c5c96d34348","type":"message","text":"it would be great if there were more/some Julia answers on <http://codegolf.stackexchange.com|codegolf.stackexchange.com> for fastest-code questions.","user":"UTAQ23XTR","ts":"1609002646.183400","team":"T68168MUP","edited":{"user":"UTAQ23XTR","ts":"1609002661.000000"},"blocks":[{"type":"rich_text","block_id":"kx0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it would be great if there were more/some Julia answers on "},{"type":"link","url":"http://codegolf.stackexchange.com","text":"codegolf.stackexchange.com"},{"type":"text","text":" for fastest-code questions."}]}]}]},{"client_msg_id":"7f7a2c00-9ab1-4bd2-bcfd-adac86a282b6","type":"message","text":"My main issue with such questions is that it sometimes veers into the \"this-looks-like-homework\" territory, and I would be a little hesitant to help.","user":"UDD5Z7FLZ","ts":"1609014202.186300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IO8i","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"My main issue with such questions is that it sometimes veers into the \"this-looks-like-homework\" territory, and I would be a little hesitant to help."}]}]}]},{"client_msg_id":"7e136000-a164-491e-a412-59320db940b8","type":"message","text":"I wrote a simple function to sum integers from 1 to n as a simple test of BigInt:\n```function sum_n(n::BigInt)::BigInt\n  val = BigInt(0)\n\n  while n &gt; 0\n    val += n\n    n -=  1\n  end\n\n  val\nend\nsum_n(BigInt(10))\n@time println(sum_n(BigInt(6000000))) # 6 Million```\nIt takes roughly 2.2 seconds to sum 6M numbers. By comparison, the following Racket function can sum 1B numbers in about the same amount of time. The Julia code has 30M in allocations which seems crazy. Does the BigInt implementation allocate a new BigInt for each arithmetic operation?\n```(define (sum-n n [result 0])\n  (if (&gt; n 0) \n      (sum-n (- n 1) (+ result n))\n      result))\n\n(time (sum-n 1000000000)) ; 1 Billion```","user":"U014ATN949F","ts":"1609015995.188900","team":"T68168MUP","edited":{"user":"U014ATN949F","ts":"1609016087.000000"},"blocks":[{"type":"rich_text","block_id":"cIfOH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I wrote a simple function to sum integers from 1 to n as a simple test of BigInt:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function sum_n(n::BigInt)::BigInt\n  val = BigInt(0)\n\n  while n > 0\n    val += n\n    n -=  1\n  end\n\n  val\nend\nsum_n(BigInt(10))\n@time println(sum_n(BigInt(6000000))) # 6 Million"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"It takes roughly 2.2 seconds to sum 6M numbers. By comparison, the following Racket function can sum 1B numbers in about the same amount of time. The Julia code has 30M in allocations which seems crazy. Does the BigInt implementation allocate a new BigInt for each arithmetic operation?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"(define (sum-n n [result 0])\n  (if (> n 0) \n      (sum-n (- n 1) (+ result n))\n      result))\n\n(time (sum-n 1000000000)) ; 1 Billion"}]}]}],"thread_ts":"1609015995.188900","reply_count":46,"reply_users_count":5,"latest_reply":"1609017193.198800","reply_users":["UH24GRBLL","U014ATN949F","U7HAYKY9X","UDD5Z7FLZ","UD0NS8PDF"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"should `write(io, 1234)` allocate?","user":"UH24GRBLL","ts":"1609275144.246300","thread_ts":"1608240108.139500","root":{"client_msg_id":"098d0cf6-d109-41cc-a560-36f954d58aaa","type":"message","text":"TranscodingStreams.jl is the best bet we have for very fast IO buffering, right? It seems like some performance is still left on the table:\nComparing this\n```function bufferfile(path, buffer::Vector{UInt8})\n    rdr = NoopStream(open(path))\n    while !eof(rdr)\n        readbytes!(rdr, buffer)\n    end\nend```\nwith `cat my_file &gt; /dev/null` , I get 370 ms for TranscodingStreams and 240 ms for `cat` . Does anyone know if a thorough optimization has been attempted?","user":"U7HAYKY9X","ts":"1608240108.139500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3wJn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"TranscodingStreams.jl is the best bet we have for very fast IO buffering, right? It seems like some performance is still left on the table:\nComparing this\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function bufferfile(path, buffer::Vector{UInt8})\n    rdr = NoopStream(open(path))\n    while !eof(rdr)\n        readbytes!(rdr, buffer)\n    end\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"with "},{"type":"text","text":"cat my_file > /dev/null","style":{"code":true}},{"type":"text","text":" , I get 370 ms for TranscodingStreams and 240 ms for "},{"type":"text","text":"cat","style":{"code":true}},{"type":"text","text":" . Does anyone know if a thorough optimization has been attempted?"}]}]}],"thread_ts":"1608240108.139500","reply_count":52,"reply_users_count":3,"latest_reply":"1609275155.246600","reply_users":["UH24GRBLL","U7HAYKY9X","U01GRS159T8"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"GeR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"should "},{"type":"text","text":"write(io, 1234)","style":{"code":true}},{"type":"text","text":" allocate?"}]}]}],"client_msg_id":"1007e5bd-1825-4639-8b12-135136e5da00"},{"client_msg_id":"0a390832-7399-4f7b-9a00-c72225a5794f","type":"message","text":"I have a function like this (what it does isn't really important):\n```function stabilize(curr, tolerance, distance=maximum(size(curr)))\n    prev = nothing\n    while prev != curr\n        prev = deepcopy(curr)\n        for I in CartesianIndices(curr)\n            prev[I] == 'L' &amp;&amp; count_occupied(prev, I, distance) == 0         &amp;&amp; (curr[I] = '#')\n            prev[I] == '#' &amp;&amp; count_occupied(prev, I, distance) &gt;= tolerance &amp;&amp; (curr[I] = 'L')\n        end\n    end\n    count(==('#'), curr)\nend```\nAnd it takes this long to run:\n```julia&gt; @btime stabilize(......, 4, 1)\n  38.350 ms (720 allocations: 4.00 MiB)```\nBut the moment I replace `tolerance` and `distance` with the respective values in the function I see an improvement:\n```julia&gt; @btime stabilize(......)\n  13.499 ms (720 allocations: 4.00 MiB)```\nWhy is this the case?","user":"UC81ESVH6","ts":"1609280309.249400","team":"T68168MUP","edited":{"user":"UC81ESVH6","ts":"1609280384.000000"},"blocks":[{"type":"rich_text","block_id":"H9Z6C","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a function like this (what it does isn't really important):\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function stabilize(curr, tolerance, distance=maximum(size(curr)))\n    prev = nothing\n    while prev != curr\n        prev = deepcopy(curr)\n        for I in CartesianIndices(curr)\n            prev[I] == 'L' && count_occupied(prev, I, distance) == 0         && (curr[I] = '#')\n            prev[I] == '#' && count_occupied(prev, I, distance) >= tolerance && (curr[I] = 'L')\n        end\n    end\n    count(==('#'), curr)\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"And it takes this long to run:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> @btime stabilize(......, 4, 1)\n  38.350 ms (720 allocations: 4.00 MiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"But the moment I replace "},{"type":"text","text":"tolerance","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"distance","style":{"code":true}},{"type":"text","text":" with the respective values in the function I see an improvement:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> @btime stabilize(......)\n  13.499 ms (720 allocations: 4.00 MiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Why is this the case?"}]}]}],"thread_ts":"1609280309.249400","reply_count":7,"reply_users_count":3,"latest_reply":"1609282422.250900","reply_users":["UH24GRBLL","UC81ESVH6","U01GMP3HF9C"],"subscribed":false},{"client_msg_id":"964d3841-c6d5-44e7-99fe-5925b196e25e","type":"message","text":"Hello all, I'm trying to optimize the following code. Essentially, I'm want to find the minimum value from `value` for each unique combination of 2 vectors (for the purposes of this code, v1 = 1 and v2 = 2 is the same as v1 = 2 and v2 = 1, which is why I do that hacky part with `min` and `max`). I need to scale this up pretty massively, so I'm hoping to find ways to speed it up. Any suggestions? Thanks very much!!\n```using Base.Threads\n# Vector 1\nv1 = rand(collect(1:1000), 100000)\n# Vector 2\nv2 = rand(collect(1:1000), 100000)\n# Values\nvalue = rand(100000)\n\n# Get unique combos of v1 and v2\nminv = min.(v1, v2)\nmaxv = max.(v1, v2)\nunique_combos = unique(tuple.(minv, maxv))\nn_unique = length(unique_combos)\n\n# init a vector to house minimum of `value` per unique v1 and v2 combination\nvalue_mins = Vector{Float64}(undef, n_unique)\n\n# loop through to find minimum per unique group\n@threads for i in 1:n_unique\n    unique_combo = unique_combos[i]\n    min_value_for_combo_i = minimum(value[(minv .== unique_combo[1]) .&amp; (maxv .== unique_combo[2])])\n    value_mins[i] = min_value_for_combo_i\nend```","user":"U011NV8FNF7","ts":"1609287683.257400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JQpt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello all, I'm trying to optimize the following code. Essentially, I'm want to find the minimum value from "},{"type":"text","text":"value","style":{"code":true}},{"type":"text","text":" for each unique combination of 2 vectors (for the purposes of this code, v1 = 1 and v2 = 2 is the same as v1 = 2 and v2 = 1, which is why I do that hacky part with "},{"type":"text","text":"min","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"max","style":{"code":true}},{"type":"text","text":"). I need to scale this up pretty massively, so I'm hoping to find ways to speed it up. Any suggestions? Thanks very much!!\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Base.Threads\n# Vector 1\nv1 = rand(collect(1:1000), 100000)\n# Vector 2\nv2 = rand(collect(1:1000), 100000)\n# Values\nvalue = rand(100000)\n\n# Get unique combos of v1 and v2\nminv = min.(v1, v2)\nmaxv = max.(v1, v2)\nunique_combos = unique(tuple.(minv, maxv))\nn_unique = length(unique_combos)\n\n# init a vector to house minimum of `value` per unique v1 and v2 combination\nvalue_mins = Vector{Float64}(undef, n_unique)\n\n# loop through to find minimum per unique group\n@threads for i in 1:n_unique\n    unique_combo = unique_combos[i]\n    min_value_for_combo_i = minimum(value[(minv .== unique_combo[1]) .& (maxv .== unique_combo[2])])\n    value_mins[i] = min_value_for_combo_i\nend"}]}]}],"thread_ts":"1609287683.257400","reply_count":4,"reply_users_count":2,"latest_reply":"1609326180.258400","reply_users":["UDD5Z7FLZ","U01GMP3HF9C"],"subscribed":false},{"client_msg_id":"be34b342-27bf-423f-9c7f-06c335bf262b","type":"message","text":"Which types of operations is julia still lacking behind compared with C in terms of preformamce?\nAre these performance issues related to inherent issues with Julia or simply due to julias relative immaturity?","user":"U01FAHWCMFF","ts":"1609345469.261100","team":"T68168MUP","edited":{"user":"U01FAHWCMFF","ts":"1609345524.000000"},"blocks":[{"type":"rich_text","block_id":"8WzB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Which types of operations is julia still lacking behind compared with C in terms of preformamce?\nAre these performance issues related to inherent issues with Julia or simply due to julias relative immaturity?"}]}]}]},{"type":"message","text":"Come compete for a $10 starbucks gift card by being the first to solve my performance issue","user":"U01FAHWCMFF","ts":"1609366811.265400","team":"T68168MUP","edited":{"user":"U01FAHWCMFF","ts":"1609366852.000000"},"attachments":[{"fallback":"[December 30th, 2020 5:09 PM] ayman: If anyone manages to get rid of the last forloop with mapping, without using `MappedArrays.jl`  or any library that doesn't come shipped with Julia (e.g. Statistics.jl is allowed, GLM.jl isnt), *with the same performance*, I will give you a $10 starbucks gift card. First person to do so wins. Have fun all :slightly_smiling_face:\n```function pairwise_analysis2(x::Array{Int16, 2})\n    m, n = size(x)\n    mean_matrix = Matrix{Float64}(undef, (n, n))\n    @views @inbounds for i = 1:n\n        for j = 1:n\n            total::Int16 = 0\n            for k = 1:m\n                total += x[k, i] == x[k, j]\n            end\n            mean_matrix[i, j] = total / m\n        end\n    end\n    return mean_matrix\nend\n\nX = Array{Int16}(rand(0:1, (1000, 1000)));\n@btime pairwise_analysis(X)```","ts":"1609366179.234900","author_id":"U01FAHWCMFF","author_subname":"Ayman Al Baz","channel_id":"C67TK21LJ","channel_name":"gripes","is_msg_unfurl":true,"is_reply_unfurl":true,"text":"If anyone manages to get rid of the last forloop with mapping, without using `MappedArrays.jl`  or any library that doesn't come shipped with Julia (e.g. Statistics.jl is allowed, GLM.jl isnt), *with the same performance*, I will give you a $10 starbucks gift card. First person to do so wins. Have fun all :slightly_smiling_face:\n```function pairwise_analysis2(x::Array{Int16, 2})\n    m, n = size(x)\n    mean_matrix = Matrix{Float64}(undef, (n, n))\n    @views @inbounds for i = 1:n\n        for j = 1:n\n            total::Int16 = 0\n            for k = 1:m\n                total += x[k, i] == x[k, j]\n            end\n            mean_matrix[i, j] = total / m\n        end\n    end\n    return mean_matrix\nend\n\nX = Array{Int16}(rand(0:1, (1000, 1000)));\n@btime pairwise_analysis(X)```","author_name":"Ayman Al Baz","author_link":"https://julialang.slack.com/team/U01FAHWCMFF","author_icon":"https://avatars.slack-edge.com/2020-11-23/1516965530134_1cf6ed3767c34aaf6d40_48.png","mrkdwn_in":["text"],"color":"D0D0D0","from_url":"https://julialang.slack.com/archives/C67TK21LJ/p1609366179234900?thread_ts=1609362482231100&cid=C67TK21LJ","is_share":true,"footer":"From a thread in #gripes"}],"blocks":[{"type":"rich_text","block_id":"cgZ5M","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Come compete for a $10 starbucks gift card by being the first to solve my performance issue"}]}]}]},{"client_msg_id":"4b492bfb-da24-4057-85b8-2a5c0239793b","type":"message","text":"<@UDD5Z7FLZ>  I guess if you answer slowly enough any homework deadline will have passed :)","user":"UTAQ23XTR","ts":"1609445310.283500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8enX=","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UDD5Z7FLZ"},{"type":"text","text":"  I guess if you answer slowly enough any homework deadline will have passed :)"}]}]}],"thread_ts":"1609445310.283500","reply_count":3,"reply_users_count":2,"latest_reply":"1609445879.284000","reply_users":["UDD5Z7FLZ","UTAQ23XTR"],"subscribed":false},{"client_msg_id":"36d87105-e9da-4cc1-b965-26d187398af0","type":"message","text":"Hi all. I have a question about getting good performance out of threading. I don't have a really minimal example, but can point you to a repo with the code (~200 lines):\n\n<https://github.com/vancleve/threading_example>\n\nOn my macbookpro, running the `@time evolve_replicates!(pop, rp)` from <https://github.com/vancleve/threading_example/blob/master/popsim.jl#L30>\nI get something like (after running once to compile)\nno threads  `3.936467 seconds (7.94 M allocations: 4.620 GiB, 14.60% gc time)`\n1 thread      `3.890999 seconds (7.94 M allocations: 4.620 GiB, 16.57% gc time)`\n2 threads    `2.340651 seconds (7.94 M allocations: 4.620 GiB, 25.03% gc time)`\n4 threads    `1.786248 seconds (7.94 M allocations: 4.620 GiB, 41.74% gc time)`\n8 threads    `1.658927 seconds (7.96 M allocations: 4.621 GiB, 58.21% gc time)`\n\nHere are flame graphs for 1 thread (left) and 8 threads (right).\n\nI know I have to reduce allocations, but was hoping to get some advice how best to do this since quite of a bit of the allocation time is coming in basic linear algebra stuff like matrix division and using `I` as identity matrix.\n\nI've also thought about using `Distributed` but I ran into world age issues when passing functions to the workers...","user":"UKFUT8N5A","ts":"1609540973.289000","team":"T68168MUP","edited":{"user":"UKFUT8N5A","ts":"1609541695.000000"},"blocks":[{"type":"rich_text","block_id":"UGvV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi all. I have a question about getting good performance out of threading. I don't have a really minimal example, but can point you to a repo with the code (~200 lines):\n\n"},{"type":"link","url":"https://github.com/vancleve/threading_example"},{"type":"text","text":"\n\nOn my macbookpro, running the "},{"type":"text","text":"@time evolve_replicates!(pop, rp)","style":{"code":true}},{"type":"text","text":" from "},{"type":"link","url":"https://github.com/vancleve/threading_example/blob/master/popsim.jl#L30"},{"type":"text","text":"\nI get something like (after running once to compile)\nno threads  "},{"type":"text","text":"3.936467 seconds (7.94 M allocations: 4.620 GiB, 14.60% gc time)","style":{"code":true}},{"type":"text","text":"\n1 thread      "},{"type":"text","text":"3.890999 seconds (7.94 M allocations: 4.620 GiB, 16.57% gc time)","style":{"code":true}},{"type":"text","text":"\n2 threads    "},{"type":"text","text":"2.340651 seconds (7.94 M allocations: 4.620 GiB, 25.03% gc time)","style":{"code":true}},{"type":"text","text":"\n4 threads    "},{"type":"text","text":"1.786248 seconds (7.94 M allocations: 4.620 GiB, 41.74% gc time)","style":{"code":true}},{"type":"text","text":"\n8 threads    "},{"type":"text","text":"1.658927 seconds (7.96 M allocations: 4.621 GiB, 58.21% gc time)","style":{"code":true}},{"type":"text","text":"\n\nHere are flame graphs for 1 thread (left) and 8 threads (right).\n\nI know I have to reduce allocations, but was hoping to get some advice how best to do this since quite of a bit of the allocation time is coming in basic linear algebra stuff like matrix division and using "},{"type":"text","text":"I","style":{"code":true}},{"type":"text","text":" as identity matrix.\n\nI've also thought about using "},{"type":"text","text":"Distributed","style":{"code":true}},{"type":"text","text":" but I ran into world age issues when passing functions to the workers..."}]}]}]},{"type":"message","text":"","files":[{"id":"F01HVUH9VU3","created":1609541094,"timestamp":1609541094,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"UKFUT8N5A","editable":false,"size":440298,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01HVUH9VU3/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01HVUH9VU3/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01HVUH9VU3-66bb381d09/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01HVUH9VU3-66bb381d09/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01HVUH9VU3-66bb381d09/image_360.png","thumb_360_w":360,"thumb_360_h":163,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01HVUH9VU3-66bb381d09/image_480.png","thumb_480_w":480,"thumb_480_h":218,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01HVUH9VU3-66bb381d09/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01HVUH9VU3-66bb381d09/image_720.png","thumb_720_w":720,"thumb_720_h":326,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01HVUH9VU3-66bb381d09/image_800.png","thumb_800_w":800,"thumb_800_h":363,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01HVUH9VU3-66bb381d09/image_960.png","thumb_960_w":960,"thumb_960_h":435,"thumb_1024":"https://files.slack.com/files-tmb/T68168MUP-F01HVUH9VU3-66bb381d09/image_1024.png","thumb_1024_w":1024,"thumb_1024_h":464,"original_w":3238,"original_h":1468,"thumb_tiny":"AwAVADDR2L/dFVMkdRV2oZIQ3TqTUtFJkP4igcehqX7P9KaYcMAO4qbMq6GbhnGBSqQTwBT/ACh523sRmnxRgAHHUUcorktFFFaEBSY+bPtilooATaN+7vjFKBgYFFFAH//Z","permalink":"https://julialang.slack.com/files/UKFUT8N5A/F01HVUH9VU3/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01HVUH9VU3-a57f3c3707","is_starred":false,"has_rich_preview":false},{"id":"F01HNV0NTFY","created":1609541122,"timestamp":1609541122,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"UKFUT8N5A","editable":false,"size":415089,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01HNV0NTFY/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01HNV0NTFY/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01HNV0NTFY-3349dbff2d/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01HNV0NTFY-3349dbff2d/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01HNV0NTFY-3349dbff2d/image_360.png","thumb_360_w":360,"thumb_360_h":151,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01HNV0NTFY-3349dbff2d/image_480.png","thumb_480_w":480,"thumb_480_h":201,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01HNV0NTFY-3349dbff2d/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01HNV0NTFY-3349dbff2d/image_720.png","thumb_720_w":720,"thumb_720_h":302,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01HNV0NTFY-3349dbff2d/image_800.png","thumb_800_w":800,"thumb_800_h":335,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01HNV0NTFY-3349dbff2d/image_960.png","thumb_960_w":960,"thumb_960_h":402,"thumb_1024":"https://files.slack.com/files-tmb/T68168MUP-F01HNV0NTFY-3349dbff2d/image_1024.png","thumb_1024_w":1024,"thumb_1024_h":429,"original_w":3238,"original_h":1356,"thumb_tiny":"AwAUADDRZF2n5R+VVB6EYq7VaSFmfK4xUyRUWM/KjIHXFOaAqjM2OBmkKfut3TFTYrQTcD2FKCD2FNXBYDPU1YSMAkUJXB6EtFFFaGYdaQgYxjilooATaPQUtFFAH//Z","permalink":"https://julialang.slack.com/files/UKFUT8N5A/F01HNV0NTFY/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01HNV0NTFY-fb2a1c796e","is_starred":false,"has_rich_preview":false}],"upload":false,"user":"UKFUT8N5A","ts":"1609541199.289200"},{"client_msg_id":"f8cbc126-a382-4696-8a81-ba33a7553958","type":"message","text":"I have a section of a `Vector{UInt8}` which corresponds to some text. I want to get the hash of that text - stripped from any whitespace is both ends. My current solution involves copying to a string, then calling `strip` , copying the result to another string, then using `crc32c` .\nIs there a way to do this more efficiently?","user":"U7HAYKY9X","ts":"1609876769.297000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4QrWV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a section of a "},{"type":"text","text":"Vector{UInt8}","style":{"code":true}},{"type":"text","text":" which corresponds to some text. I want to get the hash of that text - stripped from any whitespace is both ends. My current solution involves copying to a string, then calling "},{"type":"text","text":"strip","style":{"code":true}},{"type":"text","text":" , copying the result to another string, then using "},{"type":"text","text":"crc32c","style":{"code":true}},{"type":"text","text":" .\nIs there a way to do this more efficiently?"}]}]}],"thread_ts":"1609876769.297000","reply_count":4,"reply_users_count":2,"latest_reply":"1609877893.297700","reply_users":["U012XER8K4M","U7HAYKY9X"],"subscribed":false},{"type":"message","text":"Is there a sparse 2d data structure that supports random insertions of x,y coordinate pairs well, or a dense matrix data structure that grows in small (maybe 8 by 8) blocks when a new block gets written to?","user":"U9MD78Z9N","ts":"1609921521.298800","team":"T68168MUP"},{"client_msg_id":"b9c1be21-656b-40e2-a4bb-0aa28567ff28","type":"message","text":"When working with gzipped files, you can make it go faster by doing the (de)compression in a separate thread from the other computation. But I can't find any good packages for easy asynchronous/parallel (de)compression. Does such a package exist?","user":"U7HAYKY9X","ts":"1609932745.302100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hpY5i","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"When working with gzipped files, you can make it go faster by doing the (de)compression in a separate thread from the other computation. But I can't find any good packages for easy asynchronous/parallel (de)compression. Does such a package exist?"}]}]}],"thread_ts":"1609932745.302100","reply_count":11,"reply_users_count":2,"latest_reply":"1609936069.304700","reply_users":["U6A936746","U7HAYKY9X"],"subscribed":false}]}