{"cursor": 0, "messages": [{"client_msg_id":"01febfe7-06f0-4448-92bc-bafb8bd7f16a","type":"message","text":"Hi everyone,\nIs there a Julia interface to Bullet/PyBullet?\nI'm new here (and to Julia and Bullet as well), so I could be asking in the wrong channel","user":"U01HVPNK9R9","ts":"1609530748.123100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mpP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi everyone,\nIs there a Julia interface to Bullet/PyBullet?\nI'm new here (and to Julia and Bullet as well), so I could be asking in the wrong channel"}]}]}],"thread_ts":"1609530748.123100","reply_count":1,"reply_users_count":1,"latest_reply":"1609531346.124000","reply_users":["U7V6YNG04"],"subscribed":false},{"client_msg_id":"2d08eabc-2843-4dee-a98b-484c0733d3b8","type":"message","text":"Hi there,\n\nI’m trying to install ReinforcementLearning.jl\nFollowing the steps on Getting started:\n`] add ReinforcementLearning`\n(or even  `add <https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl>`)\nProduces the following:\n`ERROR: cannot find name corresponding to UUID 4af54fe1-eca0-43a8-85a7-787d91b784e3 in a registry`\n\nI’m able to install the underlying packages. I’ve added registry General. I’ve completely nuked Julia on my system (cache etc) and reinstalled fresh. Same result.\n\nI am a n00b so it’s likely I’m doing something daft. Anyone have any ideas?","user":"U0176DTGS76","ts":"1610360232.127800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Xfw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi there,\n\nI’m trying to install ReinforcementLearning.jl\nFollowing the steps on Getting started:\n"},{"type":"text","text":"] add ReinforcementLearning","style":{"code":true}},{"type":"text","text":"\n(or even  "},{"type":"text","text":"add ","style":{"code":true}},{"type":"link","url":"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl","style":{"code":true}},{"type":"text","text":")\nProduces the following:\n"},{"type":"text","text":"ERROR: cannot find name corresponding to UUID 4af54fe1-eca0-43a8-85a7-787d91b784e3 in a registry","style":{"code":true}},{"type":"text","text":"\n\nI’m able to install the underlying packages. I’ve added registry General. I’ve completely nuked Julia on my system (cache etc) and reinstalled fresh. Same result.\n\nI am a n00b so it’s likely I’m doing something daft. Anyone have any ideas?"}]}]}]},{"client_msg_id":"82df34a7-5d27-46bf-9be2-f63fadc2c8da","type":"message","text":"Really like the docs:\n<https://juliareinforcementlearning.org/blog/how_to_write_a_customized_environment/>","user":"U01BG0NN34J","ts":"1611733827.000400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"flI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Really like the docs:\n"},{"type":"link","url":"https://juliareinforcementlearning.org/blog/how_to_write_a_customized_environment/"}]}]}],"thread_ts":"1611733827.000400","reply_count":2,"reply_users_count":2,"latest_reply":"1611784070.002400","reply_users":["U7V6YNG04","U01BG0NN34J"],"subscribed":false},{"client_msg_id":"3e89cf3e-cb8f-4b94-9de2-b0c4c03d93d9","type":"message","text":"Lots of thought went into this design, the docs help bring it out.","user":"U01BG0NN34J","ts":"1611733862.001000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SCG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Lots of thought went into this design, the docs help bring it out."}]}]}]},{"client_msg_id":"b0629acb-419e-4b6b-b6a8-ae648c746878","type":"message","text":"How the heck can a single person produce so much <@U7V6YNG04>??? Do you even sleep?","user":"U01BG0NN34J","ts":"1611949560.003200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"08+pA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How the heck can a single person produce so much "},{"type":"user","user_id":"U7V6YNG04"},{"type":"text","text":"??? Do you even sleep?"}]}]}],"reactions":[{"name":"smile","users":["U01724Q3PGW","UH9KWTTD3","UR5ASDE8G","UMEN7R0CD","U7V6YNG04"],"count":5}]},{"type":"message","text":"Really like the <https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl|environments> matrix:","files":[{"id":"F01M30KSML0","created":1611949605,"timestamp":1611949605,"name":"Screenshot from 2021-01-29 14-46-30.png","title":"Screenshot from 2021-01-29 14-46-30.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01BG0NN34J","editable":false,"size":50879,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01M30KSML0/screenshot_from_2021-01-29_14-46-30.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01M30KSML0/download/screenshot_from_2021-01-29_14-46-30.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01M30KSML0-76ae6f61be/screenshot_from_2021-01-29_14-46-30_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01M30KSML0-76ae6f61be/screenshot_from_2021-01-29_14-46-30_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01M30KSML0-76ae6f61be/screenshot_from_2021-01-29_14-46-30_360.png","thumb_360_w":276,"thumb_360_h":360,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01M30KSML0-76ae6f61be/screenshot_from_2021-01-29_14-46-30_480.png","thumb_480_w":368,"thumb_480_h":480,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01M30KSML0-76ae6f61be/screenshot_from_2021-01-29_14-46-30_160.png","original_w":445,"original_h":580,"thumb_tiny":"AwAwACTR2c53N+dGz/bb86DnsAfxoBbPIAH1oARcbuHY47Gn01s9dxH0Gabk/wB5v++aAJKKTBx1zRQAhOOxP0oDZONrD6igsoOCcGk3p/eFACt93v8AhTRnP8dOZc88k+gOKbt9n/76oAfRRjAxRQAFgvXP5UgcE45/KlLYOME/QUm//Zb8qABxkdM/jimbf9j/AMepzHt8uO+TTQAOgjyPegCSigcjt+FFAH//2Q==","permalink":"https://julialang.slack.com/files/U01BG0NN34J/F01M30KSML0/screenshot_from_2021-01-29_14-46-30.png","permalink_public":"https://slack-files.com/T68168MUP-F01M30KSML0-1161a2cf7f","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"oT=T3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Really like the "},{"type":"link","url":"https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl","text":"environments"},{"type":"text","text":" matrix:"}]}]}],"user":"U01BG0NN34J","display_as_bot":false,"ts":"1611949616.003700"},{"type":"message","text":"Reading the tutorial I don't quiet understand what action_space_mapping and action_mapping does.\n<https://juliareinforcementlearning.org/blog/how_to_write_a_customized_environment/>","files":[{"id":"F01M47K5724","created":1611986471,"timestamp":1611986471,"name":"Screenshot from 2021-01-30 00-59-52.png","title":"Screenshot from 2021-01-30 00-59-52.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01BG0NN34J","editable":false,"size":28571,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01M47K5724/screenshot_from_2021-01-30_00-59-52.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01M47K5724/download/screenshot_from_2021-01-30_00-59-52.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01M47K5724-81f311e2b2/screenshot_from_2021-01-30_00-59-52_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01M47K5724-81f311e2b2/screenshot_from_2021-01-30_00-59-52_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01M47K5724-81f311e2b2/screenshot_from_2021-01-30_00-59-52_360.png","thumb_360_w":360,"thumb_360_h":71,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01M47K5724-81f311e2b2/screenshot_from_2021-01-30_00-59-52_480.png","thumb_480_w":480,"thumb_480_h":94,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01M47K5724-81f311e2b2/screenshot_from_2021-01-30_00-59-52_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01M47K5724-81f311e2b2/screenshot_from_2021-01-30_00-59-52_720.png","thumb_720_w":720,"thumb_720_h":141,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01M47K5724-81f311e2b2/screenshot_from_2021-01-30_00-59-52_800.png","thumb_800_w":800,"thumb_800_h":157,"original_w":947,"original_h":186,"thumb_tiny":"AwAJADDROMd/wpuR/tflUlFADdwHY/lSg5GaWigAooooA//Z","permalink":"https://julialang.slack.com/files/U01BG0NN34J/F01M47K5724/screenshot_from_2021-01-30_00-59-52.png","permalink_public":"https://slack-files.com/T68168MUP-F01M47K5724-095ad670bd","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"hzG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Reading the tutorial I don't quiet understand what action_space_mapping and action_mapping does.\n"},{"type":"link","url":"https://juliareinforcementlearning.org/blog/how_to_write_a_customized_environment/"}]}]}],"user":"U01BG0NN34J","display_as_bot":false,"ts":"1611986474.004700"},{"client_msg_id":"4c7cf765-51d9-4e68-9e8d-7330f72a75b5","type":"message","text":"Action_space_mapping would take `(:PowerRich, :MegaHaul, nothing)` and map it to [`Base.OneTo(3), Base.OneTo(3), Base.OneTo(3)]`?","user":"U01BG0NN34J","ts":"1611986578.005700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"EFbB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Action_space_mapping would take "},{"type":"text","text":"(:PowerRich, :MegaHaul, nothing)","style":{"code":true}},{"type":"text","text":" and map it to ["},{"type":"text","text":"Base.OneTo(3), Base.OneTo(3), Base.OneTo(3)]","style":{"code":true}},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"c5966823-c608-48cf-b65f-851f7cb319a2","type":"message","text":"`action_mapping` would take `i` and do  `(:PowerRich, :MegaHaul, nothing)[i].`  Ok I got this.","user":"U01BG0NN34J","ts":"1611986720.006600","team":"T68168MUP","edited":{"user":"U01BG0NN34J","ts":"1611986748.000000"},"blocks":[{"type":"rich_text","block_id":"n/pL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"action_mapping","style":{"code":true}},{"type":"text","text":" would take "},{"type":"text","text":"i","style":{"code":true}},{"type":"text","text":" and do  "},{"type":"text","text":"(:PowerRich, :MegaHaul, nothing)[i].","style":{"code":true}},{"type":"text","text":"  Ok I got this."}]}]}]},{"client_msg_id":"25c6fb29-af41-4bb0-978a-8cc6c0271ea6","type":"message","text":"Still not sure about `Action_space_mapping` though","user":"U01BG0NN34J","ts":"1611986757.007100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5KucR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Still not sure about "},{"type":"text","text":"Action_space_mapping","style":{"code":true}},{"type":"text","text":" though"}]}]}],"thread_ts":"1611986757.007100","reply_count":1,"reply_users_count":1,"latest_reply":"1611987224.007200","reply_users":["U7V6YNG04"],"subscribed":false},{"client_msg_id":"37479668-fdbd-41a2-b485-e7d198cbf521","type":"message","text":"ooooo i see","user":"U01BG0NN34J","ts":"1611987289.007500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zaTm/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ooooo i see"}]}]}]},{"client_msg_id":"14092adb-3942-4509-bd01-bc1773cf0aea","type":"message","text":"x -&gt; Base.OneTo(3) is basically  `_-&gt; Base.OneTo(3)` since it just ignores the x (action-space) completely.","user":"U01BG0NN34J","ts":"1611987332.008300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DWC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"x -> Base.OneTo(3) is basically "},{"type":"text","text":" _-> Base.OneTo(3)","style":{"code":true}},{"type":"text","text":" since it just ignores the x (action-space) completely."}]}]}],"reactions":[{"name":"100","users":["U7V6YNG04"],"count":1}]},{"client_msg_id":"44ff3f49-b12d-4a44-b2fc-939b3413375d","type":"message","text":"I thought it was looping over or something.","user":"U01BG0NN34J","ts":"1611987346.008600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=6s=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I thought it was looping over or something."}]}]}]},{"client_msg_id":"f0004de8-1d5f-4d37-af4e-cfde300b9136","type":"message","text":"So if I initialize a hook `hook = TotalRewardPerEpisode()`\nand then run the experiment:\n`run(p, env, StopAfterEpisode(1000), hook)`\nis the hook modified? E.g. do I need to recreate it before I run a new experiment?","user":"U01BG0NN34J","ts":"1611987824.009400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0ut/a","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So if I initialize a hook "},{"type":"text","text":"hook = TotalRewardPerEpisode()","style":{"code":true}},{"type":"text","text":"\nand then run the experiment:\n"},{"type":"text","text":"run(p, env, StopAfterEpisode(1000), hook)","style":{"code":true}},{"type":"text","text":"\nis the hook modified? E.g. do I need to recreate it before I run a new experiment?"}]}]}]},{"client_msg_id":"3a0ba6db-5d01-41b5-8f2a-867fdd435ceb","type":"message","text":"Ah, yes I see it keeps accumulating stuff!","user":"U01BG0NN34J","ts":"1611988122.009700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/f3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah, yes I see it keeps accumulating stuff!"}]}]}],"thread_ts":"1611988122.009700","reply_count":1,"reply_users_count":1,"latest_reply":"1611988448.009800","reply_users":["U7V6YNG04"],"subscribed":false},{"client_msg_id":"e9baa28c-cc06-401d-aa20-f5c74f31340a","type":"message","text":"What does `SART` in VectorSARTTrajectory stand for?","user":"U01BG0NN34J","ts":"1611988746.010500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Nve","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What does `SART` in VectorSARTTrajectory stand for?"}]}]}],"thread_ts":"1611988746.010500","reply_count":4,"reply_users_count":2,"latest_reply":"1612028736.015400","reply_users":["U01724Q3PGW","U01BG0NN34J"],"subscribed":false},{"client_msg_id":"a9664e5e-63ca-4c63-84e4-9e134467e34c","type":"message","text":"\"For environments which support many different kinds of states, developers should specify all the supported state styles. For example:\"\n```StateStyle(tp)```\nMy question is \"how\" do we specify supported state styles? (I'm new to julia so maybe i don't understand...)\nDo we override `StateStyle` function and return a tuple?","user":"U01BG0NN34J","ts":"1611996471.012400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+7oe1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"\"For environments which support many different kinds of states, developers should specify all the supported state styles. For example:\"\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"StateStyle(tp)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nMy question is \"how\" do we specify supported state styles? (I'm new to julia so maybe i don't understand...)\nDo we override "},{"type":"text","text":"StateStyle","style":{"code":true}},{"type":"text","text":" function and return a tuple?"}]}]}],"thread_ts":"1611996471.012400","reply_count":1,"reply_users_count":1,"latest_reply":"1612015151.014800","reply_users":["U7V6YNG04"],"subscribed":false},{"client_msg_id":"150b24e1-bc49-4281-b569-7d295fc50ef1","type":"message","text":"ref: <https://juliareinforcementlearning.org/blog/how_to_write_a_customized_environment/>","user":"U01BG0NN34J","ts":"1611996475.012600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=kmFD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ref: "},{"type":"link","url":"https://juliareinforcementlearning.org/blog/how_to_write_a_customized_environment/"}]}]}]},{"client_msg_id":"8bcacea3-c6dc-4b43-9d56-6ab5080a449e","type":"message","text":"Also, for ActionStyle:\n1. Is `legal_action_space` the action set at the present timestep?\n2. What is `legal_action_space_mask`?","user":"U01BG0NN34J","ts":"1611996690.013300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7RjTq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Also, for ActionStyle:\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is `legal_action_space` the action set at the present timestep?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"What is `legal_action_space_mask`?"}]}],"style":"ordered","indent":0}]}],"thread_ts":"1611996690.013300","reply_count":3,"reply_users_count":2,"latest_reply":"1612062894.019100","reply_users":["U7V6YNG04","U01BG0NN34J"],"subscribed":false},{"client_msg_id":"41fbce56-b089-467a-b2b3-6cb2578a5385","type":"message","text":"OOOOOO this is really nice:\n&gt;  Note that the APIs in single agent is still valid, only that they all fall back to the perspective from the `current_player(env)`.\nI really hated keeping track of the current player haha.","user":"U01BG0NN34J","ts":"1611996778.013900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RwW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"OOOOOO this is really nice:\n"}]},{"type":"rich_text_quote","elements":[{"type":"text","text":" Note that the APIs in single agent is still valid, only that they all fall back to the perspective from the "},{"type":"text","text":"current_player(env)","style":{"code":true}},{"type":"text","text":"."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I really hated keeping track of the current player haha."}]}]}]},{"client_msg_id":"4d6dddc6-192f-496a-9d97-492c7e2a541a","type":"message","text":"&gt;  If all players can see the same state, then we say the `InformationStyle` of these environments are of `PerfectInformation`. They are a special case of `ImperfectInformation` environments.\nSo does ``InformationStyle` still aply to single player games? E.g. where I have POMDP setting would i use the \"ImperfectInformation\"?","user":"U01BG0NN34J","ts":"1611996902.014700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"l18r","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":" If all players can see the same state, then we say the "},{"type":"text","text":"InformationStyle","style":{"code":true}},{"type":"text","text":" of these environments are of "},{"type":"text","text":"PerfectInformation","style":{"code":true}},{"type":"text","text":". They are a special case of "},{"type":"text","text":"ImperfectInformation","style":{"code":true}},{"type":"text","text":" environments."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"So does `"},{"type":"text","text":"InformationStyle","style":{"code":true}},{"type":"text","text":" still aply to single player games? E.g. where I have POMDP setting would i use the \"ImperfectInformation\"?"}]}]}],"thread_ts":"1611996902.014700","reply_count":1,"reply_users_count":1,"latest_reply":"1612017915.015200","reply_users":["U7V6YNG04"],"subscribed":false},{"client_msg_id":"E9BC616C-40DC-40F0-BFFD-9B854E5A709A","type":"message","text":"Just curious if there are any examples of continuous tasks (vs episodic)?\n\n","user":"U01BG0NN34J","ts":"1612062059.017800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ps3pS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Just curious if there are any examples of continuous tasks (vs episodic)?\n"},{"type":"text","text":"\n"}]}]}],"thread_ts":"1612062059.017800","reply_count":1,"reply_users_count":1,"latest_reply":"1612062978.019300","reply_users":["U7V6YNG04"],"subscribed":false},{"client_msg_id":"18645C0A-086C-499B-A3B3-43FF726F2578","type":"message","text":"Have you seen external agents in rllib? <@U7V6YNG04>\nCurious if this would be hard to accomplish with Julia?\n<https://docs.ray.io/en/master/rllib-env.html|https://docs.ray.io/en/master/rllib-env.html>","user":"U01BG0NN34J","ts":"1612062143.019000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OZYR9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Have you seen external agents in rllib? "},{"type":"user","user_id":"U7V6YNG04"},{"type":"text","text":"\nCurious if this would be hard to accomplish with Julia?\n"},{"type":"link","url":"https://docs.ray.io/en/master/rllib-env.html","text":"https://docs.ray.io/en/master/rllib-env.html"}]}]}],"thread_ts":"1612062143.019000","reply_count":5,"reply_users_count":2,"latest_reply":"1612147324.024800","reply_users":["U7V6YNG04","U01BG0NN34J"],"subscribed":false},{"client_msg_id":"0b9f182f-e173-4775-b4a1-70c342e1fa5b","type":"message","text":"I'm trying to understand \"traits\" in Julia. Looking at the example of ActionStyle here:\n<https://github.com/JuliaReinforcementLearning/ReinforcementLearningBase.jl/blob/a5ba00cf0db75c17dcec9bd35a1b49a0d63f1e77/src/interface.jl#L300>\n\nI don't understand how it actually gets used by the ReinforcementLearing.jl packagge","user":"U01BG0NN34J","ts":"1612318730.025900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"imZYI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm trying to understand \"traits\" in Julia. Looking at the example of ActionStyle here:\n"},{"type":"link","url":"https://github.com/JuliaReinforcementLearning/ReinforcementLearningBase.jl/blob/a5ba00cf0db75c17dcec9bd35a1b49a0d63f1e77/src/interface.jl#L300"},{"type":"text","text":"\n\nI don't understand how it actually gets used by the ReinforcementLearing.jl packagge"}]}]}],"thread_ts":"1612318730.025900","reply_count":1,"reply_users_count":1,"latest_reply":"1612351583.034700","reply_users":["U7V6YNG04"],"subscribed":false},{"client_msg_id":"f0209c3c-fcea-4dce-9b75-3546aac10511","type":"message","text":"How can I wrap a CartPole env? For example so that the state contains an extra bit that can is set to either 0 or 1 by the environment?","user":"U01BG0NN34J","ts":"1612321273.027400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"epC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I wrap a CartPole env? For example so that the state contains an extra bit that can is set to either 0 or 1 by the environment?"}]}]}]},{"client_msg_id":"ab1853b3-4970-4ea9-a0fe-7616fe353108","type":"message","text":"I thought this, but then it's not really an int, but rather a range of floats between 0 and 1. I just want it to be a discrete value of 0 or 1.\n```RLBase.state_space(env::CartPoleMyEnv{T}) where {T} = Space(\n    ClosedInterval{T}[\n        (0..1),\n        (-2*env.params.xthreshold)..(2*env.params.xthreshold),\n        -1e38..1e38,\n        (-2*env.params.thetathreshold)..(2*env.params.thetathreshold),\n        -1e38..1e38,\n    ],\n)```","user":"U01BG0NN34J","ts":"1612321356.028200","team":"T68168MUP","edited":{"user":"U01BG0NN34J","ts":"1612321367.000000"},"blocks":[{"type":"rich_text","block_id":"C1mmh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I thought this, but then it's not really an int, but rather a range of floats between 0 and 1. I just want it to be a discrete value of 0 or 1.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"RLBase.state_space(env::CartPoleMyEnv{T}) where {T} = Space(\n    ClosedInterval{T}[\n        (0..1),\n        (-2*env.params.xthreshold)..(2*env.params.xthreshold),\n        -1e38..1e38,\n        (-2*env.params.thetathreshold)..(2*env.params.thetathreshold),\n        -1e38..1e38,\n    ],\n)"}]}]}]},{"client_msg_id":"698b6ead-284f-4392-9f70-881898526df0","type":"message","text":"Also would be nice to wrap it somehow instead of hacking on the underlying environment.","user":"U01BG0NN34J","ts":"1612321385.028700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"X/un","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Also would be nice to wrap it somehow instead of hacking on the underlying environment."}]}]}],"thread_ts":"1612321385.028700","reply_count":1,"reply_users_count":1,"latest_reply":"1612352007.038600","reply_users":["U7V6YNG04"],"subscribed":false},{"client_msg_id":"2c684ce7-8dce-4bf2-a5fe-cf9d0b98ad60","type":"message","text":"Another question, what does this loop do? <https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl/blob/master/src/environments/wrappers/StateOverriddenEnv.jl#L21|https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl/blob/master/src/environments/wrappers/StateOverriddenEnv.jl#L[…]1>\n```for f in vcat(RLBase.ENV_API, RLBase.MULTI_AGENT_ENV_API)\n    if f ∉ (:state,)\n        @eval RLBase.$f(x::StateOverriddenEnv, args...; kwargs...) =\n            $f(x.env, args...; kwargs...)\n    end\nend```\nIs this a macro?","user":"U01BG0NN34J","ts":"1612322788.029300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bQtl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Another question, what does this loop do? "},{"type":"link","url":"https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl/blob/master/src/environments/wrappers/StateOverriddenEnv.jl#L21","text":"https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl/blob/master/src/environments/wrappers/StateOverriddenEnv.jl#L[…]1"},{"type":"text","text":"\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"for f in vcat(RLBase.ENV_API, RLBase.MULTI_AGENT_ENV_API)\n    if f ∉ (:state,)\n        @eval RLBase.$f(x::StateOverriddenEnv, args...; kwargs...) =\n            $f(x.env, args...; kwargs...)\n    end\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Is this a macro?"}]}]}],"thread_ts":"1612322788.029300","reply_count":1,"reply_users_count":1,"latest_reply":"1612351062.033000","reply_users":["U7V6YNG04"],"subscribed":false},{"client_msg_id":"2be8c137-1abd-4e1a-a909-30adb3fd7f67","type":"message","text":"How can I find a definition of this `Space` struct? Is it part of standard library or does it come from some other package?\n<https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl/blob/master/src/environments/examples/PendulumEnv.jl#L16>","user":"U01BG0NN34J","ts":"1612328192.030300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"XqYK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I find a definition of this "},{"type":"text","text":"Space","style":{"code":true}},{"type":"text","text":" struct? Is it part of standard library or does it come from some other package?\n"},{"type":"link","url":"https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl/blob/master/src/environments/examples/PendulumEnv.jl#L16"}]}]}],"thread_ts":"1612328192.030300","reply_count":1,"reply_users_count":1,"latest_reply":"1612350976.031100","reply_users":["U7V6YNG04"],"subscribed":false},{"client_msg_id":"84E8E1C4-55BF-4A19-80F3-C3BFCF02FD4C","type":"message","text":"Thanks for answers!","user":"U01BG0NN34J","ts":"1612372992.039200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vj9P","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for answers!"}]}]}]},{"client_msg_id":"34a0b811-f8c4-4c2f-afa6-a5667ef6a83e","type":"message","text":"Hi, I am Ojasv Kamal, a thrid year ug student at Indian Institute of Technology Kharagpur. I am interested in contributing to reinforcement learning algorithms and environments. Can someone kindly guide me regarding current progress and tasks outstanding?","user":"USR49DFNU","ts":"1612461516.041700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"if2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I am Ojasv Kamal, a thrid year ug student at Indian Institute of Technology Kharagpur. I am interested in contributing to reinforcement learning algorithms and environments. Can someone kindly guide me regarding current progress and tasks outstanding?"}]}]}]},{"client_msg_id":"de45af5e-3a3c-4ac2-bb1e-a771abc836ed","type":"message","text":"Hi, what is the purpose of RLBase.state_space()? Why do we need to explicitly enumerate the state space?","user":"U01MJMZTS11","ts":"1612895648.043200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"e710U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, what is the purpose of RLBase.state_space()? Why do we need to explicitly enumerate the state space?"}]}]}]},{"client_msg_id":"8c403d63-7c52-4036-be18-62ca00db0ec0","type":"message","text":"Is there a doc describing all trajectories available in JuliaRL? I remember there was an issue created but can't find it.","user":"U01BLDVSCG6","ts":"1613452151.050700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gM7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a doc describing all trajectories available in JuliaRL? I remember there was an issue created but can't find it."}]}]}]},{"client_msg_id":"99ef1245-d7c5-449c-99ff-a02021f756fb","type":"message","text":"\"trajectories\"?","user":"U01BG0NN34J","ts":"1613661146.051500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"x=6U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"\"trajectories\"?"}]}]}]},{"client_msg_id":"fcaf910b-e5c9-495d-96c5-5f1fcaa554c8","type":"message","text":"I'm trying to set up an experiment with PPO and an environment with an action space with multiple real valued outputs. So I started looking at the RLZoo PPOPendulum experiment with the RLEnvs PendulumEnv and tried to just add a second dimension in the action space. I figure I needed to change the action space, the policy network outputs and the trajectory size for the action. But have not managed to get it to work and after looking at both the MultiThreadEnv and PPOPolicy it seems that they assume single input action spaces. Is this correct and if so are there any good workarounds? I had a small try to update the spots I had errors to support multi dimensional actions, but felt like I just went deeper and understood less as the errors changed...\n\nAny help or pointers appreciated, here is the code I used for testing it out.\n\n","user":"UU6QUN0LD","ts":"1614096797.017000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+2DL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm trying to set up an experiment with PPO and an environment with an action space with multiple real valued outputs. So I started looking at the RLZoo PPOPendulum experiment with the RLEnvs PendulumEnv and tried to just add a second dimension in the action space. I figure I needed to change the action space, the policy network outputs and the trajectory size for the action. But have not managed to get it to work and after looking at both the MultiThreadEnv and PPOPolicy it seems that they assume single input action spaces. Is this correct and if so are there any good workarounds? I had a small try to update the spots I had errors to support multi dimensional actions, but felt like I just went deeper and understood less as the errors changed...\n\nAny help or pointers appreciated, here is the code I used for testing it out.\n\n"}]}]}]},{"client_msg_id":"fcaf910b-e5c9-495d-96c5-5f1fcaa554c8","type":"message","text":"```using ReinforcementLearning\nusing IntervalSets\nusing Random\nusing Dates\nusing Logging\nusing TensorBoardLogger\nusing StableRNGs\nusing Flux\nusing Distributions\n\nstruct PendulumEnvParams{T}\n    max_speed::T\n    max_torque::T\n    g::T\n    m::T\n    l::T\n    dt::T\n    max_steps::Int\nend\n\nmutable struct PendulumEnv{A,T,R&lt;:AbstractRNG} &lt;: AbstractEnv\n    params::PendulumEnvParams{T}\n    action_space::A\n    observation_space::Space{Vector{ClosedInterval{T}}}\n    state::Vector{T}\n    done::Bool\n    t::Int\n    rng::R\n    reward::T\n    n_actions::Int\nend\n\n\"\"\"\n    PendulumEnv(;kwargs...)\n# Keyword arguments\n- `T = Float64`\n- `max_speed = T(8)`\n- `max_torque = T(2)`\n- `g = T(10)`\n- `m = T(1)`\n- `l = T(1)`\n- `dt = T(0.05)`\n- `max_steps = 200`\n- `continuous::Bool = true`\n- `n_actions::Int = 3`\n- `rng = Random.GLOBAL_RNG`\n\"\"\"\nfunction PendulumEnv(;\n    T = Float64,\n    max_speed = T(8),\n    max_torque = T(2),\n    g = T(10),\n    m = T(1),\n    l = T(1),\n    dt = T(0.05),\n    max_steps = 200,\n    continuous::Bool = true,\n    n_actions::Int = 3,\n    rng = Random.GLOBAL_RNG,\n)\n    high = T.([1, 1, max_speed])\n    action_space = Space(ClosedInterval[-2.0..2.0, -2.0..3.0])\n    env = PendulumEnv(\n        PendulumEnvParams(max_speed, max_torque, g, m, l, dt, max_steps),\n        action_space,\n        Space(ClosedInterval{T}.(-high, high)),\n        zeros(T, 2),\n        false,\n        0,\n        rng,\n        zero(T),\n        n_actions,\n    )\n    RLBase.reset!(env)\n    env\nend\n\nRandom.seed!(env::PendulumEnv, seed) = Random.seed!(env.rng, seed)\n\npendulum_observation(s) = [cos(s[1]), sin(s[1]), s[2]]\nangle_normalize(x) = Base.mod((x + Base.π), (2 * Base.π)) - Base.π\n\nRLBase.action_space(env::PendulumEnv) = env.action_space\nRLBase.state_space(env::PendulumEnv) = env.observation_space\nRLBase.reward(env::PendulumEnv) = env.reward\nRLBase.is_terminated(env::PendulumEnv) = env.done\nRLBase.state(env::PendulumEnv) = pendulum_observation(env.state)\n\nfunction RLBase.reset!(env::PendulumEnv{A,T}) where {A,T}\n    env.state[1] = 2 * π * (rand(env.rng, T) .- 1)\n    env.state[2] = 2 * (rand(env.rng, T) .- 1)\n    env.t = 0\n    env.done = false\n    env.reward = zero(T)\n    nothing\nend\n\nfunction (env::PendulumEnv{&lt;:ClosedInterval})(a::AbstractFloat)\n    @assert a in env.action_space\n    _step!(env, a[1])\nend\n\nfunction _step!(env::PendulumEnv, a)\n    env.t += 1\n    th, thdot = env.state\n    a = clamp(a, -env.params.max_torque, env.params.max_torque)\n    costs = angle_normalize(th)^2 + 0.1 * thdot^2 + 0.001 * a^2\n    newthdot =\n        thdot +\n        (\n            -3 * env.params.g / (2 * env.params.l) * sin(th + pi) +\n            3 * a / (env.params.m * env.params.l^2)\n        ) * env.params.dt\n    th += newthdot * env.params.dt\n    newthdot = clamp(newthdot, -env.params.max_speed, env.params.max_speed)\n    env.state[1] = th\n    env.state[2] = newthdot\n    env.done = env.t &gt;= env.params.max_steps\n    env.reward = -costs\n    nothing\nend\n\nfunction create_experiment(;\n    save_dir = nothing,\n    seed = 123,\n)\n    if isnothing(save_dir)\n        t = Dates.format(now(), \"yyyy_mm_dd_HH_MM_SS\")\n        save_dir = joinpath(pwd(), \"checkpoints\", \"JuliaRL_PPO_Pendulum_$(t)\")\n    end\n\n    lg = TBLogger(joinpath(save_dir, \"tb_log\"), min_level = <http://Logging.Info|Logging.Info>)\n    rng = StableRNG(seed)\n    inner_env = PendulumEnv(T = Float32, rng = rng)\n    action_space = RLBase.action_space(inner_env)\n    na = length(action_space)\n    low = map(x -&gt; x.left, action_space)\n    high = map(x -&gt; x.right, action_space)\n    ns = length(state(inner_env))\n    @show na ns\n\n    N_ENV = 8\n    UPDATE_FREQ = 2048\n    env = MultiThreadEnv([\n        ActionTransformedEnv(PendulumEnv(T = Float32, rng = StableRNG(hash(seed + i))), action_mapping = x -&gt; clamp.(x * 2, low, high)) for i in 1:N_ENV\n    ])\n\n    init = glorot_uniform(rng)\n\n    agent = Agent(\n        policy = PPOPolicy(\n            approximator = ActorCritic(\n                actor = GaussianNetwork(\n                    pre = Chain(\n                        Dense(ns, 64, relu; initW = glorot_uniform(rng)),\n                        Dense(64, 64, relu; initW = glorot_uniform(rng)),\n                    ),\n                    μ = Dense(64, na, tanh; initW = glorot_uniform(rng)),\n                    σ = Dense(64, na; initW = glorot_uniform(rng)),\n                ),\n                critic = Chain(\n                    Dense(ns, 64, relu; initW = glorot_uniform(rng)),\n                    Dense(64, 64, relu; initW = glorot_uniform(rng)),\n                    Dense(64, 1; initW = glorot_uniform(rng)),\n                ),\n                optimizer = ADAM(3e-4),\n            ) |&gt; cpu,\n            γ = 0.99f0,\n            λ = 0.95f0,\n            clip_range = 0.2f0,\n            max_grad_norm = 0.5f0,\n            n_epochs = 10,\n            n_microbatches = 32,\n            actor_loss_weight = 1.0f0,\n            critic_loss_weight = 0.5f0,\n            entropy_loss_weight = 0.00f0,\n            dist = Normal,\n            rng = rng,\n            update_freq = UPDATE_FREQ,\n        ),\n        trajectory = PPOTrajectory(;\n            capacity = UPDATE_FREQ,\n            state = Array{Float32, 2} =&gt; (ns, N_ENV),\n            action = Array{Float32, 2} =&gt; (na, N_ENV),\n            action_log_prob = Array{Float32, 2} =&gt; (na, N_ENV),\n            reward = Array{Float32, 1} =&gt; (N_ENV,),\n            terminal = Array{Bool, 1} =&gt; (N_ENV,),\n        ),\n    )\n\n    stop_condition = StopAfterStep(50_000)\n    total_reward_per_episode = TotalBatchRewardPerEpisode(N_ENV)\n    hook = ComposedHook(\n        total_reward_per_episode,\n        DoEveryNStep() do t, agent, env\n            @show t agent\n            with_logger(lg) do\n                @info(\n                    \"training\",\n                    actor_loss = agent.policy.actor_loss[end, end],\n                    critic_loss = agent.policy.critic_loss[end, end],\n                    loss = agent.policy.loss[end, end],\n                )\n                for i in 1:length(env)\n                    if is_terminated(env[i])\n                        @info \"training\" reward = total_reward_per_episode.rewards[i][end] log_step_increment =\n                            0\n                        break\n                    end\n                end\n            end\n        end,\n    )\n\n    Experiment(agent, env, stop_condition, hook, \"# Play Pendulum with PPO\")\nend\n\nexperiment = create_experiment()\nrun(experiment.policy, experiment.env, experiment.stop_condition, experiment.hook)```","user":"UU6QUN0LD","ts":"1614096797.017100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+2DL-8vn","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using ReinforcementLearning\nusing IntervalSets\nusing Random\nusing Dates\nusing Logging\nusing TensorBoardLogger\nusing StableRNGs\nusing Flux\nusing Distributions\n\nstruct PendulumEnvParams{T}\n    max_speed::T\n    max_torque::T\n    g::T\n    m::T\n    l::T\n    dt::T\n    max_steps::Int\nend\n\nmutable struct PendulumEnv{A,T,R<:AbstractRNG} <: AbstractEnv\n    params::PendulumEnvParams{T}\n    action_space::A\n    observation_space::Space{Vector{ClosedInterval{T}}}\n    state::Vector{T}\n    done::Bool\n    t::Int\n    rng::R\n    reward::T\n    n_actions::Int\nend\n\n\"\"\"\n    PendulumEnv(;kwargs...)\n# Keyword arguments\n- `T = Float64`\n- `max_speed = T(8)`\n- `max_torque = T(2)`\n- `g = T(10)`\n- `m = T(1)`\n- `l = T(1)`\n- `dt = T(0.05)`\n- `max_steps = 200`\n- `continuous::Bool = true`\n- `n_actions::Int = 3`\n- `rng = Random.GLOBAL_RNG`\n\"\"\"\nfunction PendulumEnv(;\n    T = Float64,\n    max_speed = T(8),\n    max_torque = T(2),\n    g = T(10),\n    m = T(1),\n    l = T(1),\n    dt = T(0.05),\n    max_steps = 200,\n    continuous::Bool = true,\n    n_actions::Int = 3,\n    rng = Random.GLOBAL_RNG,\n)\n    high = T.([1, 1, max_speed])\n    action_space = Space(ClosedInterval[-2.0..2.0, -2.0..3.0])\n    env = PendulumEnv(\n        PendulumEnvParams(max_speed, max_torque, g, m, l, dt, max_steps),\n        action_space,\n        Space(ClosedInterval{T}.(-high, high)),\n        zeros(T, 2),\n        false,\n        0,\n        rng,\n        zero(T),\n        n_actions,\n    )\n    RLBase.reset!(env)\n    env\nend\n\nRandom.seed!(env::PendulumEnv, seed) = Random.seed!(env.rng, seed)\n\npendulum_observation(s) = [cos(s[1]), sin(s[1]), s[2]]\nangle_normalize(x) = Base.mod((x + Base.π), (2 * Base.π)) - Base.π\n\nRLBase.action_space(env::PendulumEnv) = env.action_space\nRLBase.state_space(env::PendulumEnv) = env.observation_space\nRLBase.reward(env::PendulumEnv) = env.reward\nRLBase.is_terminated(env::PendulumEnv) = env.done\nRLBase.state(env::PendulumEnv) = pendulum_observation(env.state)\n\nfunction RLBase.reset!(env::PendulumEnv{A,T}) where {A,T}\n    env.state[1] = 2 * π * (rand(env.rng, T) .- 1)\n    env.state[2] = 2 * (rand(env.rng, T) .- 1)\n    env.t = 0\n    env.done = false\n    env.reward = zero(T)\n    nothing\nend\n\nfunction (env::PendulumEnv{<:ClosedInterval})(a::AbstractFloat)\n    @assert a in env.action_space\n    _step!(env, a[1])\nend\n\nfunction _step!(env::PendulumEnv, a)\n    env.t += 1\n    th, thdot = env.state\n    a = clamp(a, -env.params.max_torque, env.params.max_torque)\n    costs = angle_normalize(th)^2 + 0.1 * thdot^2 + 0.001 * a^2\n    newthdot =\n        thdot +\n        (\n            -3 * env.params.g / (2 * env.params.l) * sin(th + pi) +\n            3 * a / (env.params.m * env.params.l^2)\n        ) * env.params.dt\n    th += newthdot * env.params.dt\n    newthdot = clamp(newthdot, -env.params.max_speed, env.params.max_speed)\n    env.state[1] = th\n    env.state[2] = newthdot\n    env.done = env.t >= env.params.max_steps\n    env.reward = -costs\n    nothing\nend\n\nfunction create_experiment(;\n    save_dir = nothing,\n    seed = 123,\n)\n    if isnothing(save_dir)\n        t = Dates.format(now(), \"yyyy_mm_dd_HH_MM_SS\")\n        save_dir = joinpath(pwd(), \"checkpoints\", \"JuliaRL_PPO_Pendulum_$(t)\")\n    end\n\n    lg = TBLogger(joinpath(save_dir, \"tb_log\"), min_level = Logging.Info)\n    rng = StableRNG(seed)\n    inner_env = PendulumEnv(T = Float32, rng = rng)\n    action_space = RLBase.action_space(inner_env)\n    na = length(action_space)\n    low = map(x -> x.left, action_space)\n    high = map(x -> x.right, action_space)\n    ns = length(state(inner_env))\n    @show na ns\n\n    N_ENV = 8\n    UPDATE_FREQ = 2048\n    env = MultiThreadEnv([\n        ActionTransformedEnv(PendulumEnv(T = Float32, rng = StableRNG(hash(seed + i))), action_mapping = x -> clamp.(x * 2, low, high)) for i in 1:N_ENV\n    ])\n\n    init = glorot_uniform(rng)\n\n    agent = Agent(\n        policy = PPOPolicy(\n            approximator = ActorCritic(\n                actor = GaussianNetwork(\n                    pre = Chain(\n                        Dense(ns, 64, relu; initW = glorot_uniform(rng)),\n                        Dense(64, 64, relu; initW = glorot_uniform(rng)),\n                    ),\n                    μ = Dense(64, na, tanh; initW = glorot_uniform(rng)),\n                    σ = Dense(64, na; initW = glorot_uniform(rng)),\n                ),\n                critic = Chain(\n                    Dense(ns, 64, relu; initW = glorot_uniform(rng)),\n                    Dense(64, 64, relu; initW = glorot_uniform(rng)),\n                    Dense(64, 1; initW = glorot_uniform(rng)),\n                ),\n                optimizer = ADAM(3e-4),\n            ) |> cpu,\n            γ = 0.99f0,\n            λ = 0.95f0,\n            clip_range = 0.2f0,\n            max_grad_norm = 0.5f0,\n            n_epochs = 10,\n            n_microbatches = 32,\n            actor_loss_weight = 1.0f0,\n            critic_loss_weight = 0.5f0,\n            entropy_loss_weight = 0.00f0,\n            dist = Normal,\n            rng = rng,\n            update_freq = UPDATE_FREQ,\n        ),\n        trajectory = PPOTrajectory(;\n            capacity = UPDATE_FREQ,\n            state = Array{Float32, 2} => (ns, N_ENV),\n            action = Array{Float32, 2} => (na, N_ENV),\n            action_log_prob = Array{Float32, 2} => (na, N_ENV),\n            reward = Array{Float32, 1} => (N_ENV,),\n            terminal = Array{Bool, 1} => (N_ENV,),\n        ),\n    )\n\n    stop_condition = StopAfterStep(50_000)\n    total_reward_per_episode = TotalBatchRewardPerEpisode(N_ENV)\n    hook = ComposedHook(\n        total_reward_per_episode,\n        DoEveryNStep() do t, agent, env\n            @show t agent\n            with_logger(lg) do\n                @info(\n                    \"training\",\n                    actor_loss = agent.policy.actor_loss[end, end],\n                    critic_loss = agent.policy.critic_loss[end, end],\n                    loss = agent.policy.loss[end, end],\n                )\n                for i in 1:length(env)\n                    if is_terminated(env[i])\n                        @info \"training\" reward = total_reward_per_episode.rewards[i][end] log_step_increment =\n                            0\n                        break\n                    end\n                end\n            end\n        end,\n    )\n\n    Experiment(agent, env, stop_condition, hook, \"# Play Pendulum with PPO\")\nend\n\nexperiment = create_experiment()\nrun(experiment.policy, experiment.env, experiment.stop_condition, experiment.hook)"}]}]}]},{"client_msg_id":"93028f75-191f-41f8-a506-500f94e1a580","type":"message","text":"Hello <@UU6QUN0LD>,\n\n&gt; I had a small try to update the spots I had errors to support multi dimensional actions, but felt like I just went deeper and understood less as the errors changed...\nObviously, you hit some corner cases here :joy:\nDuring the last time we tried to support multi dimensional actions in continuous control, we found that it was not that easy to do backpropagation with multi variant gaussian distribution. So we only kept the single variant version in the code base.\n\n&gt;  Is this correct and if so are there any good workarounds? \nSo there are two possible solutions here.\n\n1. Fix the backward propagation with multi variant gaussian distribution in the following line:\n<https://github.com/JuliaReinforcementLearning/ReinforcementLearningZoo.jl/blob/a9ad73153d42a8c1aac595fdfffd227b830bdc9a/src/algorithms/policy_gradient/ppo.jl#L259>\n\n2. Use some other variants of PPO to solve the multi variable continuous control problems.\n\nFor example, in this paper <https://ojs.aaai.org/index.php/AAAI/article/view/6059/5915> it simply split each dimension of the action space into K discrete regions. It's relatively easier to implement if you've finished reading the **Network Architecture** part.\n\nAnyway, contribution is welcomed. I bet you'll not be the last one to have this problem:grin:","user":"U7V6YNG04","ts":"1614135801.024300","team":"T68168MUP","edited":{"user":"U7V6YNG04","ts":"1614136110.000000"},"blocks":[{"type":"rich_text","block_id":"M=66","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello "},{"type":"user","user_id":"UU6QUN0LD"},{"type":"text","text":",\n\n"}]},{"type":"rich_text_quote","elements":[{"type":"text","text":"I had a small try to update the spots I had errors to support multi dimensional actions, but felt like I just went deeper and understood less as the errors changed..."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Obviously, you hit some corner cases here "},{"type":"emoji","name":"joy"},{"type":"text","text":"\nDuring the last time we tried to support multi dimensional actions in continuous control, we found that it was not that easy to do backpropagation with multi variant gaussian distribution. So we only kept the single variant version in the code base.\n\n"}]},{"type":"rich_text_quote","elements":[{"type":"text","text":" Is this correct and if so are there any good workarounds? "}]},{"type":"rich_text_section","elements":[{"type":"text","text":"So there are two possible solutions here.\n\n1. Fix the backward propagation with multi variant gaussian distribution in the following line:\n"},{"type":"link","url":"https://github.com/JuliaReinforcementLearning/ReinforcementLearningZoo.jl/blob/a9ad73153d42a8c1aac595fdfffd227b830bdc9a/src/algorithms/policy_gradient/ppo.jl#L259"},{"type":"text","text":"\n\n2. Use some other variants of PPO to solve the multi variable continuous control problems.\n\nFor example, in this paper "},{"type":"link","url":"https://ojs.aaai.org/index.php/AAAI/article/view/6059/5915"},{"type":"text","text":" it simply split each dimension of the action space into K discrete regions. It's relatively easier to implement if you've finished reading the *"},{"type":"text","text":"Network Architecture","style":{"bold":true}},{"type":"text","text":"* part.\n\nAnyway, contribution is welcomed. I bet you'll not be the last one to have this problem"},{"type":"emoji","name":"grin"}]}]}]},{"client_msg_id":"c77bbe0f-5117-4274-807d-affaa6c30de2","type":"message","text":"Thanks for the response!\n\nI also ended up at that line in the end (among a few others) that seemed to need some small updates for the multi dimensional case.\n\nIs it a performance problem if you do some too naive implementations or what do you mean with \"found that it was not that easy\"?\n\nI found a \"fix\" that seems to work for me for now, only thing is that it has broken the single dimensional case. But I'll see if I can get it to work for both in a nice way and can put up a PR in that case.","user":"UU6QUN0LD","ts":"1614156257.029000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GzQM4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for the response!\n\nI also ended up at that line in the end (among a few others) that seemed to need some small updates for the multi dimensional case.\n\nIs it a performance problem if you do some too naive implementations or what do you mean with \"found that it was not that easy\"?\n\nI found a \"fix\" that seems to work for me for now, only thing is that it has broken the single dimensional case. But I'll see if I can get it to work for both in a nice way and can put up a PR in that case."}]}]}]},{"client_msg_id":"687195f6-ff1f-47cd-9b84-b1cba63717d4","type":"message","text":"I mean <https://github.com/JuliaStats/Distributions.jl/issues/1183>","user":"U7V6YNG04","ts":"1614158808.029200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+4CI1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I mean "},{"type":"link","url":"https://github.com/JuliaStats/Distributions.jl/issues/1183"}]}]}]},{"client_msg_id":"89483843-1a25-4d5a-938b-a3444313d4fd","type":"message","text":"Hi folks, if folks are interested I've made a repo for examples in using the Lyceum framework for continuous control RL problems and trajectory optimization problems <https://github.com/klowrey/FrankaTest> built around the Franka Emika robot and the MuJoCo Simulator. There's code to do interactive Jacobian control, MPPI-MPC, and Natural Policy Gradient learning, but the reward function for the environment isn't well tested; it's a good start for people curious about Lyceum (you'll have to `registry add <https://github.com/Lyceum/LyceumRegistry>` to get the relevant packages).","user":"U9NH09E58","ts":"1614719279.032500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"esoGM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi folks, if folks are interested I've made a repo for examples in using the Lyceum framework for continuous control RL problems and trajectory optimization problems "},{"type":"link","url":"https://github.com/klowrey/FrankaTest"},{"type":"text","text":" built around the Franka Emika robot and the MuJoCo Simulator. There's code to do interactive Jacobian control, MPPI-MPC, and Natural Policy Gradient learning, but the reward function for the environment isn't well tested; it's a good start for people curious about Lyceum (you'll have to "},{"type":"text","text":"registry add ","style":{"code":true}},{"type":"link","url":"https://github.com/Lyceum/LyceumRegistry","style":{"code":true}},{"type":"text","text":" to get the relevant packages)."}]}]}]},{"client_msg_id":"a998d5c6-b3d0-444f-9765-9ff4142ba86b","type":"message","text":"Hey folks, I am pretty new to reinforcement learning, but much more familiar with say traditional ml and deep learning. I wanted to try some simple to medium complexity Markov decision processes to get a feel for the method. So I was wondering which packages to begin with in Julia. I would need something to setup MDPs and then do policy evaluation or value iteration, etc. I was looking around but there are definitely a lot of julia packages out there. But I was hoping to find perhaps the most common packages that people use for stuff like that. Can anyone point me in the right direction.","user":"UDDSTBX19","ts":"1614880263.036000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ymfcJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey folks, I am pretty new to reinforcement learning, but much more familiar with say traditional ml and deep learning. I wanted to try some simple to medium complexity Markov decision processes to get a feel for the method. So I was wondering which packages to begin with in Julia. I would need something to setup MDPs and then do policy evaluation or value iteration, etc. I was looking around but there are definitely a lot of julia packages out there. But I was hoping to find perhaps the most common packages that people use for stuff like that. Can anyone point me in the right direction."}]}]}]},{"client_msg_id":"4f478032-a7c7-4106-91f0-07344ad2f4e7","type":"message","text":"Might be simplest to start with <https://github.com/JuliaPOMDP/QuickPOMDPs.jl> ?","user":"U6BJXUZHR","ts":"1614881302.036300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"g3K=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Might be simplest to start with "},{"type":"link","url":"https://github.com/JuliaPOMDP/QuickPOMDPs.jl"},{"type":"text","text":" ?"}]}]}]},{"client_msg_id":"1115d751-a3d5-407e-b787-a3904b62d70c","type":"message","text":"<@U6BJXUZHR> thanks for the pointer. I read about that one last night, so I will take a look.","user":"UDDSTBX19","ts":"1614881574.036700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JAxD","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U6BJXUZHR"},{"type":"text","text":" thanks for the pointer. I read about that one last night, so I will take a look."}]}]}]},{"client_msg_id":"09e7589f-5f1b-4a26-92b9-0cafe700b392","type":"message","text":"Anyone here interested in doing some pair programming with ReinforcementLearning.jl on zoom? I wanted to convert an environment from python and could use some help (since I'm not a julia expert)","user":"U01BG0NN34J","ts":"1615601136.038400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lbVL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Anyone here interested in doing some pair programming with ReinforcementLearning.jl on zoom? I wanted to convert an environment from python and could use some help (since I'm not a julia expert)"}]}]}]},{"type":"message","text":"I really appreciate <@U01BG0NN34J>’s comments/suggestions on ReinforcementLearning.jl","files":[{"id":"F01RJF07933","created":1615657506,"timestamp":1615657506,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U7V6YNG04","editable":false,"size":78604,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01RJF07933/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01RJF07933/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01RJF07933-76310d085f/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01RJF07933-76310d085f/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01RJF07933-76310d085f/image_360.png","thumb_360_w":360,"thumb_360_h":315,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01RJF07933-76310d085f/image_480.png","thumb_480_w":480,"thumb_480_h":420,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01RJF07933-76310d085f/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01RJF07933-76310d085f/image_720.png","thumb_720_w":720,"thumb_720_h":629,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01RJF07933-76310d085f/image_800.png","thumb_800_w":800,"thumb_800_h":699,"original_w":883,"original_h":772,"thumb_tiny":"AwApADDS57Uc+1BA700ADof1oAdz7UU0DHf8zTufagAooooADjvTPk9vyqT8Kbn/AGWoAb8nt+VKCo7/AKUo57EfWnYoAKKKKADntTAT6LT6KAGZPov50oJ9F/OnUUAJ83oPzoGe4A/GlooA/9k=","permalink":"https://julialang.slack.com/files/U7V6YNG04/F01RJF07933/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01RJF07933-cdd4b08396","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"5=g9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I really appreciate "},{"type":"user","user_id":"U01BG0NN34J"},{"type":"text","text":"’s comments/suggestions on ReinforcementLearning.jl"}]}]}],"user":"U7V6YNG04","display_as_bot":false,"ts":"1615657557.039800"},{"client_msg_id":"be81a805-6ccd-41d9-b6f6-8054a8d97572","type":"message","text":"What's the equivalent of `render` function of open ai gym in RL.jl?","user":"U01BG0NN34J","ts":"1615663614.041500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"EbM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What's the equivalent of "},{"type":"text","text":"render","style":{"code":true}},{"type":"text","text":" function of open ai gym in RL.jl?"}]}]}]}]}