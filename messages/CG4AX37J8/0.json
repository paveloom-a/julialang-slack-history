{"cursor": 0, "messages": [{"client_msg_id":"01febfe7-06f0-4448-92bc-bafb8bd7f16a","type":"message","text":"Hi everyone,\nIs there a Julia interface to Bullet/PyBullet?\nI'm new here (and to Julia and Bullet as well), so I could be asking in the wrong channel","user":"U01HVPNK9R9","ts":"1609530748.123100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mpP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi everyone,\nIs there a Julia interface to Bullet/PyBullet?\nI'm new here (and to Julia and Bullet as well), so I could be asking in the wrong channel"}]}]}],"thread_ts":"1609530748.123100","reply_count":1,"reply_users_count":1,"latest_reply":"1609531346.124000","reply_users":["U7V6YNG04"],"subscribed":false},{"client_msg_id":"2d08eabc-2843-4dee-a98b-484c0733d3b8","type":"message","text":"Hi there,\n\nI’m trying to install ReinforcementLearning.jl\nFollowing the steps on Getting started:\n`] add ReinforcementLearning`\n(or even  `add <https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl>`)\nProduces the following:\n`ERROR: cannot find name corresponding to UUID 4af54fe1-eca0-43a8-85a7-787d91b784e3 in a registry`\n\nI’m able to install the underlying packages. I’ve added registry General. I’ve completely nuked Julia on my system (cache etc) and reinstalled fresh. Same result.\n\nI am a n00b so it’s likely I’m doing something daft. Anyone have any ideas?","user":"U0176DTGS76","ts":"1610360232.127800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Xfw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi there,\n\nI’m trying to install ReinforcementLearning.jl\nFollowing the steps on Getting started:\n"},{"type":"text","text":"] add ReinforcementLearning","style":{"code":true}},{"type":"text","text":"\n(or even  "},{"type":"text","text":"add ","style":{"code":true}},{"type":"link","url":"https://github.com/JuliaReinforcementLearning/ReinforcementLearning.jl","style":{"code":true}},{"type":"text","text":")\nProduces the following:\n"},{"type":"text","text":"ERROR: cannot find name corresponding to UUID 4af54fe1-eca0-43a8-85a7-787d91b784e3 in a registry","style":{"code":true}},{"type":"text","text":"\n\nI’m able to install the underlying packages. I’ve added registry General. I’ve completely nuked Julia on my system (cache etc) and reinstalled fresh. Same result.\n\nI am a n00b so it’s likely I’m doing something daft. Anyone have any ideas?"}]}]}]},{"client_msg_id":"82df34a7-5d27-46bf-9be2-f63fadc2c8da","type":"message","text":"Really like the docs:\n<https://juliareinforcementlearning.org/blog/how_to_write_a_customized_environment/>","user":"U01BG0NN34J","ts":"1611733827.000400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"flI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Really like the docs:\n"},{"type":"link","url":"https://juliareinforcementlearning.org/blog/how_to_write_a_customized_environment/"}]}]}],"thread_ts":"1611733827.000400","reply_count":2,"reply_users_count":2,"latest_reply":"1611784070.002400","reply_users":["U7V6YNG04","U01BG0NN34J"],"subscribed":false},{"client_msg_id":"3e89cf3e-cb8f-4b94-9de2-b0c4c03d93d9","type":"message","text":"Lots of thought went into this design, the docs help bring it out.","user":"U01BG0NN34J","ts":"1611733862.001000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SCG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Lots of thought went into this design, the docs help bring it out."}]}]}]},{"client_msg_id":"b0629acb-419e-4b6b-b6a8-ae648c746878","type":"message","text":"How the heck can a single person produce so much <@U7V6YNG04>??? Do you even sleep?","user":"U01BG0NN34J","ts":"1611949560.003200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"08+pA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How the heck can a single person produce so much "},{"type":"user","user_id":"U7V6YNG04"},{"type":"text","text":"??? Do you even sleep?"}]}]}],"reactions":[{"name":"smile","users":["U01724Q3PGW","UH9KWTTD3","UR5ASDE8G","UMEN7R0CD","U7V6YNG04"],"count":5}]},{"type":"message","text":"Really like the <https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl|environments> matrix:","files":[{"id":"F01M30KSML0","created":1611949605,"timestamp":1611949605,"name":"Screenshot from 2021-01-29 14-46-30.png","title":"Screenshot from 2021-01-29 14-46-30.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01BG0NN34J","editable":false,"size":50879,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01M30KSML0/screenshot_from_2021-01-29_14-46-30.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01M30KSML0/download/screenshot_from_2021-01-29_14-46-30.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01M30KSML0-76ae6f61be/screenshot_from_2021-01-29_14-46-30_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01M30KSML0-76ae6f61be/screenshot_from_2021-01-29_14-46-30_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01M30KSML0-76ae6f61be/screenshot_from_2021-01-29_14-46-30_360.png","thumb_360_w":276,"thumb_360_h":360,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01M30KSML0-76ae6f61be/screenshot_from_2021-01-29_14-46-30_480.png","thumb_480_w":368,"thumb_480_h":480,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01M30KSML0-76ae6f61be/screenshot_from_2021-01-29_14-46-30_160.png","original_w":445,"original_h":580,"thumb_tiny":"AwAwACTR2c53N+dGz/bb86DnsAfxoBbPIAH1oARcbuHY47Gn01s9dxH0Gabk/wB5v++aAJKKTBx1zRQAhOOxP0oDZONrD6igsoOCcGk3p/eFACt93v8AhTRnP8dOZc88k+gOKbt9n/76oAfRRjAxRQAFgvXP5UgcE45/KlLYOME/QUm//Zb8qABxkdM/jimbf9j/AMepzHt8uO+TTQAOgjyPegCSigcjt+FFAH//2Q==","permalink":"https://julialang.slack.com/files/U01BG0NN34J/F01M30KSML0/screenshot_from_2021-01-29_14-46-30.png","permalink_public":"https://slack-files.com/T68168MUP-F01M30KSML0-1161a2cf7f","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"oT=T3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Really like the "},{"type":"link","url":"https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl","text":"environments"},{"type":"text","text":" matrix:"}]}]}],"user":"U01BG0NN34J","display_as_bot":false,"ts":"1611949616.003700"},{"type":"message","text":"Reading the tutorial I don't quiet understand what action_space_mapping and action_mapping does.\n<https://juliareinforcementlearning.org/blog/how_to_write_a_customized_environment/>","files":[{"id":"F01M47K5724","created":1611986471,"timestamp":1611986471,"name":"Screenshot from 2021-01-30 00-59-52.png","title":"Screenshot from 2021-01-30 00-59-52.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01BG0NN34J","editable":false,"size":28571,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01M47K5724/screenshot_from_2021-01-30_00-59-52.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01M47K5724/download/screenshot_from_2021-01-30_00-59-52.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01M47K5724-81f311e2b2/screenshot_from_2021-01-30_00-59-52_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01M47K5724-81f311e2b2/screenshot_from_2021-01-30_00-59-52_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01M47K5724-81f311e2b2/screenshot_from_2021-01-30_00-59-52_360.png","thumb_360_w":360,"thumb_360_h":71,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01M47K5724-81f311e2b2/screenshot_from_2021-01-30_00-59-52_480.png","thumb_480_w":480,"thumb_480_h":94,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01M47K5724-81f311e2b2/screenshot_from_2021-01-30_00-59-52_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01M47K5724-81f311e2b2/screenshot_from_2021-01-30_00-59-52_720.png","thumb_720_w":720,"thumb_720_h":141,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01M47K5724-81f311e2b2/screenshot_from_2021-01-30_00-59-52_800.png","thumb_800_w":800,"thumb_800_h":157,"original_w":947,"original_h":186,"thumb_tiny":"AwAJADDROMd/wpuR/tflUlFADdwHY/lSg5GaWigAooooA//Z","permalink":"https://julialang.slack.com/files/U01BG0NN34J/F01M47K5724/screenshot_from_2021-01-30_00-59-52.png","permalink_public":"https://slack-files.com/T68168MUP-F01M47K5724-095ad670bd","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"hzG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Reading the tutorial I don't quiet understand what action_space_mapping and action_mapping does.\n"},{"type":"link","url":"https://juliareinforcementlearning.org/blog/how_to_write_a_customized_environment/"}]}]}],"user":"U01BG0NN34J","display_as_bot":false,"ts":"1611986474.004700"},{"client_msg_id":"4c7cf765-51d9-4e68-9e8d-7330f72a75b5","type":"message","text":"Action_space_mapping would take `(:PowerRich, :MegaHaul, nothing)` and map it to [`Base.OneTo(3), Base.OneTo(3), Base.OneTo(3)]`?","user":"U01BG0NN34J","ts":"1611986578.005700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"EFbB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Action_space_mapping would take "},{"type":"text","text":"(:PowerRich, :MegaHaul, nothing)","style":{"code":true}},{"type":"text","text":" and map it to ["},{"type":"text","text":"Base.OneTo(3), Base.OneTo(3), Base.OneTo(3)]","style":{"code":true}},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"c5966823-c608-48cf-b65f-851f7cb319a2","type":"message","text":"`action_mapping` would take `i` and do  `(:PowerRich, :MegaHaul, nothing)[i].`  Ok I got this.","user":"U01BG0NN34J","ts":"1611986720.006600","team":"T68168MUP","edited":{"user":"U01BG0NN34J","ts":"1611986748.000000"},"blocks":[{"type":"rich_text","block_id":"n/pL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"action_mapping","style":{"code":true}},{"type":"text","text":" would take "},{"type":"text","text":"i","style":{"code":true}},{"type":"text","text":" and do  "},{"type":"text","text":"(:PowerRich, :MegaHaul, nothing)[i].","style":{"code":true}},{"type":"text","text":"  Ok I got this."}]}]}]},{"client_msg_id":"25c6fb29-af41-4bb0-978a-8cc6c0271ea6","type":"message","text":"Still not sure about `Action_space_mapping` though","user":"U01BG0NN34J","ts":"1611986757.007100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5KucR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Still not sure about "},{"type":"text","text":"Action_space_mapping","style":{"code":true}},{"type":"text","text":" though"}]}]}],"thread_ts":"1611986757.007100","reply_count":1,"reply_users_count":1,"latest_reply":"1611987224.007200","reply_users":["U7V6YNG04"],"subscribed":false},{"client_msg_id":"37479668-fdbd-41a2-b485-e7d198cbf521","type":"message","text":"ooooo i see","user":"U01BG0NN34J","ts":"1611987289.007500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zaTm/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ooooo i see"}]}]}]},{"client_msg_id":"14092adb-3942-4509-bd01-bc1773cf0aea","type":"message","text":"x -&gt; Base.OneTo(3) is basically  `_-&gt; Base.OneTo(3)` since it just ignores the x (action-space) completely.","user":"U01BG0NN34J","ts":"1611987332.008300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DWC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"x -> Base.OneTo(3) is basically "},{"type":"text","text":" _-> Base.OneTo(3)","style":{"code":true}},{"type":"text","text":" since it just ignores the x (action-space) completely."}]}]}],"reactions":[{"name":"100","users":["U7V6YNG04"],"count":1}]},{"client_msg_id":"44ff3f49-b12d-4a44-b2fc-939b3413375d","type":"message","text":"I thought it was looping over or something.","user":"U01BG0NN34J","ts":"1611987346.008600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=6s=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I thought it was looping over or something."}]}]}]},{"client_msg_id":"f0004de8-1d5f-4d37-af4e-cfde300b9136","type":"message","text":"So if I initialize a hook `hook = TotalRewardPerEpisode()`\nand then run the experiment:\n`run(p, env, StopAfterEpisode(1000), hook)`\nis the hook modified? E.g. do I need to recreate it before I run a new experiment?","user":"U01BG0NN34J","ts":"1611987824.009400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0ut/a","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So if I initialize a hook "},{"type":"text","text":"hook = TotalRewardPerEpisode()","style":{"code":true}},{"type":"text","text":"\nand then run the experiment:\n"},{"type":"text","text":"run(p, env, StopAfterEpisode(1000), hook)","style":{"code":true}},{"type":"text","text":"\nis the hook modified? E.g. do I need to recreate it before I run a new experiment?"}]}]}]},{"client_msg_id":"3a0ba6db-5d01-41b5-8f2a-867fdd435ceb","type":"message","text":"Ah, yes I see it keeps accumulating stuff!","user":"U01BG0NN34J","ts":"1611988122.009700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/f3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah, yes I see it keeps accumulating stuff!"}]}]}],"thread_ts":"1611988122.009700","reply_count":1,"reply_users_count":1,"latest_reply":"1611988448.009800","reply_users":["U7V6YNG04"],"subscribed":false},{"client_msg_id":"e9baa28c-cc06-401d-aa20-f5c74f31340a","type":"message","text":"What does `SART` in VectorSARTTrajectory stand for?","user":"U01BG0NN34J","ts":"1611988746.010500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Nve","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What does `SART` in VectorSARTTrajectory stand for?"}]}]}],"thread_ts":"1611988746.010500","reply_count":4,"reply_users_count":2,"latest_reply":"1612028736.015400","reply_users":["U01724Q3PGW","U01BG0NN34J"],"subscribed":false},{"client_msg_id":"a9664e5e-63ca-4c63-84e4-9e134467e34c","type":"message","text":"\"For environments which support many different kinds of states, developers should specify all the supported state styles. For example:\"\n```StateStyle(tp)```\nMy question is \"how\" do we specify supported state styles? (I'm new to julia so maybe i don't understand...)\nDo we override `StateStyle` function and return a tuple?","user":"U01BG0NN34J","ts":"1611996471.012400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+7oe1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"\"For environments which support many different kinds of states, developers should specify all the supported state styles. For example:\"\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"StateStyle(tp)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nMy question is \"how\" do we specify supported state styles? (I'm new to julia so maybe i don't understand...)\nDo we override "},{"type":"text","text":"StateStyle","style":{"code":true}},{"type":"text","text":" function and return a tuple?"}]}]}],"thread_ts":"1611996471.012400","reply_count":1,"reply_users_count":1,"latest_reply":"1612015151.014800","reply_users":["U7V6YNG04"],"subscribed":false},{"client_msg_id":"150b24e1-bc49-4281-b569-7d295fc50ef1","type":"message","text":"ref: <https://juliareinforcementlearning.org/blog/how_to_write_a_customized_environment/>","user":"U01BG0NN34J","ts":"1611996475.012600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=kmFD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ref: "},{"type":"link","url":"https://juliareinforcementlearning.org/blog/how_to_write_a_customized_environment/"}]}]}]},{"client_msg_id":"8bcacea3-c6dc-4b43-9d56-6ab5080a449e","type":"message","text":"Also, for ActionStyle:\n1. Is `legal_action_space` the action set at the present timestep?\n2. What is `legal_action_space_mask`?","user":"U01BG0NN34J","ts":"1611996690.013300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7RjTq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Also, for ActionStyle:\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is `legal_action_space` the action set at the present timestep?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"What is `legal_action_space_mask`?"}]}],"style":"ordered","indent":0}]}],"thread_ts":"1611996690.013300","reply_count":3,"reply_users_count":2,"latest_reply":"1612062894.019100","reply_users":["U7V6YNG04","U01BG0NN34J"],"subscribed":false},{"client_msg_id":"41fbce56-b089-467a-b2b3-6cb2578a5385","type":"message","text":"OOOOOO this is really nice:\n&gt;  Note that the APIs in single agent is still valid, only that they all fall back to the perspective from the `current_player(env)`.\nI really hated keeping track of the current player haha.","user":"U01BG0NN34J","ts":"1611996778.013900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RwW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"OOOOOO this is really nice:\n"}]},{"type":"rich_text_quote","elements":[{"type":"text","text":" Note that the APIs in single agent is still valid, only that they all fall back to the perspective from the "},{"type":"text","text":"current_player(env)","style":{"code":true}},{"type":"text","text":"."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I really hated keeping track of the current player haha."}]}]}]},{"client_msg_id":"4d6dddc6-192f-496a-9d97-492c7e2a541a","type":"message","text":"&gt;  If all players can see the same state, then we say the `InformationStyle` of these environments are of `PerfectInformation`. They are a special case of `ImperfectInformation` environments.\nSo does ``InformationStyle` still aply to single player games? E.g. where I have POMDP setting would i use the \"ImperfectInformation\"?","user":"U01BG0NN34J","ts":"1611996902.014700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"l18r","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":" If all players can see the same state, then we say the "},{"type":"text","text":"InformationStyle","style":{"code":true}},{"type":"text","text":" of these environments are of "},{"type":"text","text":"PerfectInformation","style":{"code":true}},{"type":"text","text":". They are a special case of "},{"type":"text","text":"ImperfectInformation","style":{"code":true}},{"type":"text","text":" environments."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"So does `"},{"type":"text","text":"InformationStyle","style":{"code":true}},{"type":"text","text":" still aply to single player games? E.g. where I have POMDP setting would i use the \"ImperfectInformation\"?"}]}]}],"thread_ts":"1611996902.014700","reply_count":1,"reply_users_count":1,"latest_reply":"1612017915.015200","reply_users":["U7V6YNG04"],"subscribed":false},{"client_msg_id":"E9BC616C-40DC-40F0-BFFD-9B854E5A709A","type":"message","text":"Just curious if there are any examples of continuous tasks (vs episodic)?\n\n","user":"U01BG0NN34J","ts":"1612062059.017800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ps3pS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Just curious if there are any examples of continuous tasks (vs episodic)?\n"},{"type":"text","text":"\n"}]}]}],"thread_ts":"1612062059.017800","reply_count":1,"reply_users_count":1,"latest_reply":"1612062978.019300","reply_users":["U7V6YNG04"],"subscribed":false},{"client_msg_id":"18645C0A-086C-499B-A3B3-43FF726F2578","type":"message","text":"Have you seen external agents in rllib? <@U7V6YNG04>\nCurious if this would be hard to accomplish with Julia?\n<https://docs.ray.io/en/master/rllib-env.html|https://docs.ray.io/en/master/rllib-env.html>","user":"U01BG0NN34J","ts":"1612062143.019000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OZYR9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Have you seen external agents in rllib? "},{"type":"user","user_id":"U7V6YNG04"},{"type":"text","text":"\nCurious if this would be hard to accomplish with Julia?\n"},{"type":"link","url":"https://docs.ray.io/en/master/rllib-env.html","text":"https://docs.ray.io/en/master/rllib-env.html"}]}]}],"thread_ts":"1612062143.019000","reply_count":5,"reply_users_count":2,"latest_reply":"1612147324.024800","reply_users":["U7V6YNG04","U01BG0NN34J"],"subscribed":false},{"client_msg_id":"0b9f182f-e173-4775-b4a1-70c342e1fa5b","type":"message","text":"I'm trying to understand \"traits\" in Julia. Looking at the example of ActionStyle here:\n<https://github.com/JuliaReinforcementLearning/ReinforcementLearningBase.jl/blob/a5ba00cf0db75c17dcec9bd35a1b49a0d63f1e77/src/interface.jl#L300>\n\nI don't understand how it actually gets used by the ReinforcementLearing.jl packagge","user":"U01BG0NN34J","ts":"1612318730.025900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"imZYI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm trying to understand \"traits\" in Julia. Looking at the example of ActionStyle here:\n"},{"type":"link","url":"https://github.com/JuliaReinforcementLearning/ReinforcementLearningBase.jl/blob/a5ba00cf0db75c17dcec9bd35a1b49a0d63f1e77/src/interface.jl#L300"},{"type":"text","text":"\n\nI don't understand how it actually gets used by the ReinforcementLearing.jl packagge"}]}]}],"thread_ts":"1612318730.025900","reply_count":1,"reply_users_count":1,"latest_reply":"1612351583.034700","reply_users":["U7V6YNG04"],"subscribed":false},{"client_msg_id":"f0209c3c-fcea-4dce-9b75-3546aac10511","type":"message","text":"How can I wrap a CartPole env? For example so that the state contains an extra bit that can is set to either 0 or 1 by the environment?","user":"U01BG0NN34J","ts":"1612321273.027400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"epC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I wrap a CartPole env? For example so that the state contains an extra bit that can is set to either 0 or 1 by the environment?"}]}]}]},{"client_msg_id":"ab1853b3-4970-4ea9-a0fe-7616fe353108","type":"message","text":"I thought this, but then it's not really an int, but rather a range of floats between 0 and 1. I just want it to be a discrete value of 0 or 1.\n```RLBase.state_space(env::CartPoleMyEnv{T}) where {T} = Space(\n    ClosedInterval{T}[\n        (0..1),\n        (-2*env.params.xthreshold)..(2*env.params.xthreshold),\n        -1e38..1e38,\n        (-2*env.params.thetathreshold)..(2*env.params.thetathreshold),\n        -1e38..1e38,\n    ],\n)```","user":"U01BG0NN34J","ts":"1612321356.028200","team":"T68168MUP","edited":{"user":"U01BG0NN34J","ts":"1612321367.000000"},"blocks":[{"type":"rich_text","block_id":"C1mmh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I thought this, but then it's not really an int, but rather a range of floats between 0 and 1. I just want it to be a discrete value of 0 or 1.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"RLBase.state_space(env::CartPoleMyEnv{T}) where {T} = Space(\n    ClosedInterval{T}[\n        (0..1),\n        (-2*env.params.xthreshold)..(2*env.params.xthreshold),\n        -1e38..1e38,\n        (-2*env.params.thetathreshold)..(2*env.params.thetathreshold),\n        -1e38..1e38,\n    ],\n)"}]}]}]},{"client_msg_id":"698b6ead-284f-4392-9f70-881898526df0","type":"message","text":"Also would be nice to wrap it somehow instead of hacking on the underlying environment.","user":"U01BG0NN34J","ts":"1612321385.028700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"X/un","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Also would be nice to wrap it somehow instead of hacking on the underlying environment."}]}]}],"thread_ts":"1612321385.028700","reply_count":1,"reply_users_count":1,"latest_reply":"1612352007.038600","reply_users":["U7V6YNG04"],"subscribed":false},{"client_msg_id":"2c684ce7-8dce-4bf2-a5fe-cf9d0b98ad60","type":"message","text":"Another question, what does this loop do? <https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl/blob/master/src/environments/wrappers/StateOverriddenEnv.jl#L21|https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl/blob/master/src/environments/wrappers/StateOverriddenEnv.jl#L[…]1>\n```for f in vcat(RLBase.ENV_API, RLBase.MULTI_AGENT_ENV_API)\n    if f ∉ (:state,)\n        @eval RLBase.$f(x::StateOverriddenEnv, args...; kwargs...) =\n            $f(x.env, args...; kwargs...)\n    end\nend```\nIs this a macro?","user":"U01BG0NN34J","ts":"1612322788.029300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bQtl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Another question, what does this loop do? "},{"type":"link","url":"https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl/blob/master/src/environments/wrappers/StateOverriddenEnv.jl#L21","text":"https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl/blob/master/src/environments/wrappers/StateOverriddenEnv.jl#L[…]1"},{"type":"text","text":"\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"for f in vcat(RLBase.ENV_API, RLBase.MULTI_AGENT_ENV_API)\n    if f ∉ (:state,)\n        @eval RLBase.$f(x::StateOverriddenEnv, args...; kwargs...) =\n            $f(x.env, args...; kwargs...)\n    end\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Is this a macro?"}]}]}],"thread_ts":"1612322788.029300","reply_count":1,"reply_users_count":1,"latest_reply":"1612351062.033000","reply_users":["U7V6YNG04"],"subscribed":false},{"client_msg_id":"2be8c137-1abd-4e1a-a909-30adb3fd7f67","type":"message","text":"How can I find a definition of this `Space` struct? Is it part of standard library or does it come from some other package?\n<https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl/blob/master/src/environments/examples/PendulumEnv.jl#L16>","user":"U01BG0NN34J","ts":"1612328192.030300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"XqYK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I find a definition of this "},{"type":"text","text":"Space","style":{"code":true}},{"type":"text","text":" struct? Is it part of standard library or does it come from some other package?\n"},{"type":"link","url":"https://github.com/JuliaReinforcementLearning/ReinforcementLearningEnvironments.jl/blob/master/src/environments/examples/PendulumEnv.jl#L16"}]}]}],"thread_ts":"1612328192.030300","reply_count":1,"reply_users_count":1,"latest_reply":"1612350976.031100","reply_users":["U7V6YNG04"],"subscribed":false},{"client_msg_id":"84E8E1C4-55BF-4A19-80F3-C3BFCF02FD4C","type":"message","text":"Thanks for answers!","user":"U01BG0NN34J","ts":"1612372992.039200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vj9P","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for answers!"}]}]}]},{"client_msg_id":"34a0b811-f8c4-4c2f-afa6-a5667ef6a83e","type":"message","text":"Hi, I am Ojasv Kamal, a thrid year ug student at Indian Institute of Technology Kharagpur. I am interested in contributing to reinforcement learning algorithms and environments. Can someone kindly guide me regarding current progress and tasks outstanding?","user":"USR49DFNU","ts":"1612461516.041700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"if2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I am Ojasv Kamal, a thrid year ug student at Indian Institute of Technology Kharagpur. I am interested in contributing to reinforcement learning algorithms and environments. Can someone kindly guide me regarding current progress and tasks outstanding?"}]}]}]},{"client_msg_id":"de45af5e-3a3c-4ac2-bb1e-a771abc836ed","type":"message","text":"Hi, what is the purpose of RLBase.state_space()? Why do we need to explicitly enumerate the state space?","user":"U01MJMZTS11","ts":"1612895648.043200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"e710U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, what is the purpose of RLBase.state_space()? Why do we need to explicitly enumerate the state space?"}]}]}]}]}