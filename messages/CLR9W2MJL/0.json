{"cursor": 0, "messages": [{"client_msg_id":"65878fb9-c825-496d-9f28-5ebb2c5e200b","type":"message","text":"<https://ai.facebook.com/blog/ppl-bench-creating-a-standard-for-benchmarking-probabilistic-programming-languages/|https://ai.facebook.com/blog/ppl-bench-creating-a-standard-for-benchmarking-probabilistic-programming-languages/>","user":"UDGT4PM41","ts":"1608004645.405100","team":"T68168MUP","attachments":[{"title":"PPL Bench: Creating a standard for benchmarking probabilistic programming languages","title_link":"https://ai.facebook.com/blog/ppl-bench-creating-a-standard-for-benchmarking-probabilistic-programming-languages/","text":"PPL Bench is an open-source, standardized benchmarking framework for measuring inference performance in probabilistic programming languages.","fallback":"PPL Bench: Creating a standard for benchmarking probabilistic programming languages","thumb_url":"https://scontent-iad3-1.xx.fbcdn.net/v/t39.2365-6/122174564_823527068401904_6421496198039070291_n.png?_nc_cat=101&ccb=2&_nc_sid=ad8a9d&_nc_ohc=LLIQ4Qo4DU4AX8GoIF6&_nc_ht=scontent-iad3-1.xx&oh=3df55048ed07e801738638cef4e0721d&oe=5FFD90E3","from_url":"https://ai.facebook.com/blog/ppl-bench-creating-a-standard-for-benchmarking-probabilistic-programming-languages/","thumb_width":4756,"thumb_height":1427,"service_icon":"https://static.xx.fbcdn.net/rsrc.php/v3/yh/r/5ZP3mZo0oAE.png","service_name":"ai.facebook.com","id":1,"original_url":"https://ai.facebook.com/blog/ppl-bench-creating-a-standard-for-benchmarking-probabilistic-programming-languages/"}],"blocks":[{"type":"rich_text","block_id":"Xe/2w","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://ai.facebook.com/blog/ppl-bench-creating-a-standard-for-benchmarking-probabilistic-programming-languages/","text":"https://ai.facebook.com/blog/ppl-bench-creating-a-standard-for-benchmarking-probabilistic-programming-languages/"}]}]}]},{"client_msg_id":"7e3ac5c3-de82-4035-8502-2899a61c2dfa","type":"message","text":"I am now convinced that the Markov kernel is the fundamental conceptional unit of functional probabilistic programming. So what is it‚Äôs equivalent in imperative code? A randomized instruction?","user":"U6C937ENB","ts":"1608125575.406900","team":"T68168MUP","edited":{"user":"U6C937ENB","ts":"1608125590.000000"},"blocks":[{"type":"rich_text","block_id":"ybkW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am now convinced that the Markov kernel is the fundamental conceptional unit of functional probabilistic programming. So what is it‚Äôs equivalent in imperative code? A randomized instruction?"}]}]}]},{"client_msg_id":"bdfb95a4-0475-4a04-a45e-557f5a179263","type":"message","text":"I think form this perspective functional and imperative are not so different. FP is a way of programming that makes code very easy to reason about, but it's not the only way. Imperative code that takes `f(x)` and returns `y` is like functional code that takes `f*(x, world1)` and returns `(y, world2)`. The biggest difference is a given \"world\" can only be referenced once, but this can be put in terms of linear logic. The Clean programming language, for example, is functional but handled IO in this way.","user":"U81PB6N77","ts":"1608129436.410800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"avQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think form this perspective functional and imperative are not so different. FP is a way of programming that makes code very easy to reason about, but it's not the only way. Imperative code that takes "},{"type":"text","text":"f(x)","style":{"code":true}},{"type":"text","text":" and returns "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" is like functional code that takes "},{"type":"text","text":"f*(x, world1)","style":{"code":true}},{"type":"text","text":" and returns "},{"type":"text","text":"(y, world2)","style":{"code":true}},{"type":"text","text":". The biggest difference is a given \"world\" can only be referenced once, but this can be put in terms of linear logic. The Clean programming language, for example, is functional but handled IO in this way."}]}]}]},{"client_msg_id":"85cedea3-c527-4218-8561-21aed007df8b","type":"message","text":"Some imperative code is very easy to reason about. For example, check out the reversible computing paradigm in  NiLang. Even SSA can be really nice :slightly_smiling_face:","user":"U81PB6N77","ts":"1608129507.412000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sdf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Some imperative code is very easy to reason about. For example, check out the reversible computing paradigm in  NiLang. Even SSA can be really nice "},{"type":"emoji","name":"slightly_smiling_face"}]}]}]},{"client_msg_id":"32637c8c-3ee8-461d-af17-baba43c149dc","type":"message","text":"So I guess my answer is, with the right abstraction it's probably still a Markov kernel","user":"U81PB6N77","ts":"1608129556.412600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1YM2G","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So I guess my answer is, with the right abstraction it's probably still a Markov kernel"}]}]}]},{"client_msg_id":"1f35521d-5819-4328-a28b-31771f30ed51","type":"message","text":"Oops I mean uniqueness typing, not quite the same as linear logic:\n<https://en.wikipedia.org/wiki/Uniqueness_type>","user":"U81PB6N77","ts":"1608129696.413100","team":"T68168MUP","attachments":[{"title":"Uniqueness type","title_link":"https://en.wikipedia.org/wiki/Uniqueness_type","from_url":"https://en.wikipedia.org/wiki/Uniqueness_type","author_name":"Wikipedia","author_link":"https://en.wikipedia.org/","text":"In computing, a unique type guarantees that an object is used in a single-threaded way, with at most a single reference to it. If a value has a unique type, a function applied to it can be optimized to update the value in-place in the object code. Such in-place updates improve the efficiency of functional languages while maintaining referential transparency. Unique types can also be used to integrate functional and imperative programming.","fallback":"wikipedia: Uniqueness type","service_icon":"https://a.slack-edge.com/80588/img/unfurl_icons/wikipedia.png","id":1,"original_url":"https://en.wikipedia.org/wiki/Uniqueness_type"}],"blocks":[{"type":"rich_text","block_id":"wdoJg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oops I mean uniqueness typing, not quite the same as linear logic:\n"},{"type":"link","url":"https://en.wikipedia.org/wiki/Uniqueness_type"}]}]}]},{"client_msg_id":"c7b6ace4-02a7-4159-93ef-fe43c66a9f71","type":"message","text":"<@U6C937ENB> how does a markov kernel handle a transition like `P(x) -&gt; P(z, y | x)` ?","user":"UKA81L34J","ts":"1608131212.413800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WMQ","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U6C937ENB"},{"type":"text","text":" how does a markov kernel handle a transition like "},{"type":"text","text":"P(x) -> P(z, y | x)","style":{"code":true}},{"type":"text","text":" ?"}]}]}]},{"client_msg_id":"e380c247-6f4a-40cb-b675-19f7fd86580a","type":"message","text":"Unless you‚Äôre describing a Markov kernel as any pure transformation from distribution to distribution. In which case I basically agree, but it‚Äôs sort of a tautology no ? If you have a transformation which is pure and measure preserving - that almost by definition describes functional PP.","user":"UKA81L34J","ts":"1608131317.415500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pB7Y4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Unless you‚Äôre describing a Markov kernel as any pure transformation from distribution to distribution. In which case I basically agree, but it‚Äôs sort of a tautology no ? If you have a transformation which is pure and measure preserving - that almost by definition describes functional PP."}]}]}]},{"client_msg_id":"4b18f51d-81e6-4304-a1c0-ae11618cf04f","type":"message","text":"My understanding of it (<@U6C937ENB> correct me if your take is different):\nProbability distributions form a monad, so for any type `a` we can form `Prob a` (haskell notation), \"probability distributions over `a`).\n\nMonads can be described in terms of *Kleisli arrows*, which are just functions `a -&gt; Prob b`.\n\nAs I understand it, a Markov kernel is just this, a function that takes a value and returns a distribution over values of some (generally different) type.","user":"U81PB6N77","ts":"1608132224.419100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MoeBD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"My understanding of it ("},{"type":"user","user_id":"U6C937ENB"},{"type":"text","text":" correct me if your take is different):\nProbability distributions form a monad, so for any type "},{"type":"text","text":"a","style":{"code":true}},{"type":"text","text":" we can form "},{"type":"text","text":"Prob a","style":{"code":true}},{"type":"text","text":" (haskell notation), \"probability distributions over "},{"type":"text","text":"a","style":{"code":true}},{"type":"text","text":").\n\nMonads can be described in terms of "},{"type":"text","text":"Kleisli arrows","style":{"bold":true}},{"type":"text","text":", which are just functions "},{"type":"text","text":"a -> Prob b","style":{"code":true}},{"type":"text","text":".\n\nAs I understand it, a Markov kernel is just this, a function that takes a value and returns a distribution over values of some (generally different) type."}]}]}],"thread_ts":"1608132224.419100","reply_count":5,"reply_users_count":3,"latest_reply":"1608298989.431300","reply_users":["UN45LV5K6","U81PB6N77","U6C937ENB"],"subscribed":false,"reactions":[{"name":"point_up","users":["UCNCMAZ6E"],"count":1}]},{"client_msg_id":"9ca3f5da-585c-44a9-83fb-97813b3d6560","type":"message","text":"Yes, and if you want you can write `P(x | üåí)` instead of `P(x)` for some formal element :waxing_crescent_moon: of a 1-element set {:waxing_crescent_moon:}","user":"U6C937ENB","ts":"1608133728.420300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rbg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes, and if you want you can write "},{"type":"text","text":"P(x | üåí)","style":{"code":true}},{"type":"text","text":" instead of "},{"type":"text","text":"P(x)","style":{"code":true}},{"type":"text","text":" for some formal element "},{"type":"emoji","name":"waxing_crescent_moon"},{"type":"text","text":" of a 1-element set {"},{"type":"emoji","name":"waxing_crescent_moon"},{"type":"text","text":"}"}]}]}],"reactions":[{"name":"+1","users":["UCNCMAZ6E"],"count":1}]},{"client_msg_id":"b659c660-8ad4-456b-8584-c2338c908564","type":"message","text":"Just like we usually consider `f()` a function","user":"U6C937ENB","ts":"1608133863.420600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wpVT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Just like we usually consider "},{"type":"text","text":"f()","style":{"code":true}},{"type":"text","text":" a function"}]}]}]},{"client_msg_id":"8dfe1ae2-cad2-4bd4-be8c-f89a4bdb3222","type":"message","text":"Normally my latents don‚Äôt depend on the phases of the moon.","user":"UKA81L34J","ts":"1608136922.421000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9TM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Normally my latents don‚Äôt depend on the phases of the moon."}]}]}],"thread_ts":"1608136922.421000","reply_count":1,"reply_users_count":1,"latest_reply":"1608141794.429900","reply_users":["U81PB6N77"],"subscribed":false,"reactions":[{"name":"joy","users":["UN97XTLCV","UPUBAM63X"],"count":2}]},{"client_msg_id":"4e203595-0651-43a2-9629-5c3f60e87cac","type":"message","text":"{:waxing_crescent_moon:}  is unique, but only up to unique isomorphism :P","user":"UN45LV5K6","ts":"1608139325.424400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lmiPz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"{"},{"type":"emoji","name":"waxing_crescent_moon"},{"type":"text","text":"}  is unique, but only up to unique isomorphism :P"}]}]}]},{"client_msg_id":"5EBE4665-F77C-46F5-BE04-09E606B5577D","type":"message","text":"We have to be careful now, I have a formal set theory book where the author bootstraps the theory setting {} = :waxing_crescent_moon: but I can‚Äôt remember why ","user":"U6C937ENB","ts":"1608140273.428800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GOiC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We have to be careful now, I have a formal set theory book where the author bootstraps the theory setting {} = "},{"type":"emoji","name":"waxing_crescent_moon"},{"type":"text","text":" but I can‚Äôt remember why "}]}]}]},{"client_msg_id":"f21fdbee-4008-4dd2-90e0-57c3e3856b38","type":"message","text":"Got this recursive keysort for nested named tuples working pretty well:\n```julia&gt; nt\n(p = (m = (d = :d, u = :u, h = :h), j = (p = :p, j = :j, b = :b), k = (y = :y, s = :s, c = :c)), j = (l = (a = :a, f = :f, e = :e), f = (o = :o, y = :y, q = :q), p = (g = :g, k = :k, p = :p)), a = (s = (n = :n, h = :h, i = :i), m = (i = :i, w = :w, e = :e), o = (x = :x, t = :t, r = :r)))\n\njulia&gt; @btime keysort($nt)\n  5.520 ns (0 allocations: 0 bytes)\n(a = (m = (e = :e, i = :i, w = :w), o = (r = :r, t = :t, x = :x), s = (h = :h, i = :i, n = :n)), j = (f = (o = :o, q = :q, y = :y), l = (a = :a, e = :e, f = :f), p = (g = :g, k = :k, p = :p)), p = (j = (b = :b, j = :j, p = :p), k = (c = :c, s = :s, y = :y), m = (d = :d, h = :h, u = :u)))```","user":"U81PB6N77","ts":"1608313780.433000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YBO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Got this recursive keysort for nested named tuples working pretty well:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> nt\n(p = (m = (d = :d, u = :u, h = :h), j = (p = :p, j = :j, b = :b), k = (y = :y, s = :s, c = :c)), j = (l = (a = :a, f = :f, e = :e), f = (o = :o, y = :y, q = :q), p = (g = :g, k = :k, p = :p)), a = (s = (n = :n, h = :h, i = :i), m = (i = :i, w = :w, e = :e), o = (x = :x, t = :t, r = :r)))\n\njulia> @btime keysort($nt)\n  5.520 ns (0 allocations: 0 bytes)\n(a = (m = (e = :e, i = :i, w = :w), o = (r = :r, t = :t, x = :x), s = (h = :h, i = :i, n = :n)), j = (f = (o = :o, q = :q, y = :y), l = (a = :a, e = :e, f = :f), p = (g = :g, k = :k, p = :p)), p = (j = (b = :b, j = :j, p = :p), k = (c = :c, s = :s, y = :y), m = (d = :d, h = :h, u = :u)))"}]}]}],"thread_ts":"1608313780.433000","reply_count":8,"reply_users_count":2,"latest_reply":"1608435203.438300","reply_users":["U81PB6N77","U9JNHB83X"],"subscribed":false},{"client_msg_id":"a34fdc15-dccf-4b3b-8617-bc108ae4f545","type":"message","text":"I vaguely recall an announcement on discourse a couple of months ago, of a package that attempted to provide a posterior distribution over the optimal solution to an optimization problem after it had been solved, using only the loss function and the optimal solution. I can't seem to find the post announcing it, and wonder if anyone here might know what I'm talking about?","user":"UJ7DVTVQ8","ts":"1608374014.436300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ynsp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I vaguely recall an announcement on discourse a couple of months ago, of a package that attempted to provide a posterior distribution over the optimal solution to an optimization problem after it had been solved, using only the loss function and the optimal solution. I can't seem to find the post announcing it, and wonder if anyone here might know what I'm talking about?"}]}]}],"thread_ts":"1608374014.436300","reply_count":2,"reply_users_count":2,"latest_reply":"1608399128.437200","reply_users":["UJ7DVTVQ8","U81PB6N77"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"I've had some weirdness come up in Soss once in a while that traces back to named tuples being ordered.\n\n<https://github.com/simonbyrne/KeywordDispatch.jl> has a nice trick here, basically sort the named tuple before passing to the call. For small named tuples that works great, but with a few levels of nesting there's a fair amount of overhead.\n\nSo the idea here is to instead just sort once, recursively, and then use that in all calls. I expect this will also allow a recursive merge to be faster, since we can walk linearly through both arguments.","user":"U81PB6N77","ts":"1608435203.438300","thread_ts":"1608313780.433000","root":{"client_msg_id":"f21fdbee-4008-4dd2-90e0-57c3e3856b38","type":"message","text":"Got this recursive keysort for nested named tuples working pretty well:\n```julia&gt; nt\n(p = (m = (d = :d, u = :u, h = :h), j = (p = :p, j = :j, b = :b), k = (y = :y, s = :s, c = :c)), j = (l = (a = :a, f = :f, e = :e), f = (o = :o, y = :y, q = :q), p = (g = :g, k = :k, p = :p)), a = (s = (n = :n, h = :h, i = :i), m = (i = :i, w = :w, e = :e), o = (x = :x, t = :t, r = :r)))\n\njulia&gt; @btime keysort($nt)\n  5.520 ns (0 allocations: 0 bytes)\n(a = (m = (e = :e, i = :i, w = :w), o = (r = :r, t = :t, x = :x), s = (h = :h, i = :i, n = :n)), j = (f = (o = :o, q = :q, y = :y), l = (a = :a, e = :e, f = :f), p = (g = :g, k = :k, p = :p)), p = (j = (b = :b, j = :j, p = :p), k = (c = :c, s = :s, y = :y), m = (d = :d, h = :h, u = :u)))```","user":"U81PB6N77","ts":"1608313780.433000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YBO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Got this recursive keysort for nested named tuples working pretty well:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> nt\n(p = (m = (d = :d, u = :u, h = :h), j = (p = :p, j = :j, b = :b), k = (y = :y, s = :s, c = :c)), j = (l = (a = :a, f = :f, e = :e), f = (o = :o, y = :y, q = :q), p = (g = :g, k = :k, p = :p)), a = (s = (n = :n, h = :h, i = :i), m = (i = :i, w = :w, e = :e), o = (x = :x, t = :t, r = :r)))\n\njulia> @btime keysort($nt)\n  5.520 ns (0 allocations: 0 bytes)\n(a = (m = (e = :e, i = :i, w = :w), o = (r = :r, t = :t, x = :x), s = (h = :h, i = :i, n = :n)), j = (f = (o = :o, q = :q, y = :y), l = (a = :a, e = :e, f = :f), p = (g = :g, k = :k, p = :p)), p = (j = (b = :b, j = :j, p = :p), k = (c = :c, s = :s, y = :y), m = (d = :d, h = :h, u = :u)))"}]}]}],"thread_ts":"1608313780.433000","reply_count":8,"reply_users_count":2,"latest_reply":"1608435203.438300","reply_users":["U81PB6N77","U9JNHB83X"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"o=4a0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I've had some weirdness come up in Soss once in a while that traces back to named tuples being ordered.\n\n"},{"type":"link","url":"https://github.com/simonbyrne/KeywordDispatch.jl"},{"type":"text","text":" has a nice trick here, basically sort the named tuple before passing to the call. For small named tuples that works great, but with a few levels of nesting there's a fair amount of overhead.\n\nSo the idea here is to instead just sort once, recursively, and then use that in all calls. I expect this will also allow a recursive merge to be faster, since we can walk linearly through both arguments."}]}]}],"client_msg_id":"5a9bb67a-f820-4d5d-a2ae-92108b87dee5"},{"client_msg_id":"a4c42033-d468-43d3-95ff-2493b3dbe25e","type":"message","text":"<@U66G4838Q> I'm reading through your paper: <http://www.zenna.org/publications/rcd.pdf>\n\nVery interesting. I had vaguely similar thoughts for dealing with higher order uncertainty. What do you think about possibility theory/fuzzysetz/ p-boxes for dealing with the same? <https://en.wikipedia.org/wiki/Probability_box>","user":"UDGT4PM41","ts":"1608750122.440100","team":"T68168MUP","edited":{"user":"UDGT4PM41","ts":"1608750129.000000"},"blocks":[{"type":"rich_text","block_id":"TWGk","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U66G4838Q"},{"type":"text","text":" I'm reading through your paper: "},{"type":"link","url":"http://www.zenna.org/publications/rcd.pdf"},{"type":"text","text":"\n\nVery interesting. I had vaguely similar thoughts for dealing with higher order uncertainty. What do you think about possibility theory/fuzzysetz/ p-boxes for dealing with the same? "},{"type":"link","url":"https://en.wikipedia.org/wiki/Probability_box"}]}]}]},{"client_msg_id":"3715e32d-b49a-45e7-a99b-271ba33cb41b","type":"message","text":"<https://github.com/AnderGray/ProbabilityBoundsAnalysis.jl>","user":"UDGT4PM41","ts":"1608750153.440600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oa0","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/AnderGray/ProbabilityBoundsAnalysis.jl"}]}]}]},{"client_msg_id":"59628e66-2984-4e12-a858-2987af003650","type":"message","text":"<@U01BZJ2JTML> what do you think about zenna's approach to epistemic uncertainty? Do you think it could provide a fully bayesian solution to prob dilution or is additivity still a problem? (<https://arxiv.org/pdf/1706.08565.pdf#:~:text=Over%20the%20past%2015%20years,probability%20of%20collision%20eventually%20decreases><https://arxiv.org/pdf/1706.08565.pdf#:~:text=Over%20the%20past%2015%20years,probability%20of%20collision%20eventually%20decreases.|.>)","user":"UDGT4PM41","ts":"1608750177.441100","team":"T68168MUP","edited":{"user":"UDGT4PM41","ts":"1608750266.000000"},"blocks":[{"type":"rich_text","block_id":"Lff","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U01BZJ2JTML"},{"type":"text","text":" what do you think about zenna's approach to epistemic uncertainty? Do you think it could provide a fully bayesian solution to prob dilution or is additivity still a problem? ("},{"type":"link","url":"https://arxiv.org/pdf/1706.08565.pdf#:~:text=Over%20the%20past%2015%20years,probability%20of%20collision%20eventually%20decreases"},{"type":"link","url":"https://arxiv.org/pdf/1706.08565.pdf#:~:text=Over%20the%20past%2015%20years,probability%20of%20collision%20eventually%20decreases.","text":"."},{"type":"text","text":")"}]}]}]},{"client_msg_id":"c2bf6027-f4c3-493a-be29-42462113d801","type":"message","text":"cc: <@U01FUDH45LN>","user":"UDGT4PM41","ts":"1608750533.441800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"thJj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"cc: "},{"type":"user","user_id":"U01FUDH45LN"}]}]}]},{"client_msg_id":"5D97E390-435F-4AD1-9713-44911A27C9AF","type":"message","text":"I have an updated version of that paper in progress which is better written; I‚Äùll upload it soon..  Do you mean to extract p-boxes from a model or define a model in terms of p-boxes?. ","user":"U66G4838Q","ts":"1608759129.460100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dx1n","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have an updated version of that paper in progress which is better written; I‚Äùll upload it soon..  Do you mean to extract p-boxes from a model or define a model in terms of p-boxes?. "}]}]}]},{"client_msg_id":"9c03060f-9073-4987-9236-4405675d4047","type":"message","text":"<@UDGT4PM41> <@U66G4838Q>  to really understand this i need a toy example of a classifier that doesn't satisfy expression (1) and a demonstration of what the ||-operator does to it to make expression (2) hold\n\na typo i noticed on page 3 right hand column two thirds of the way down:\n&gt; with P being uniform a uniform measure over that hypercube\npage 4:\n&gt; However, this is a special case; for most distributional properties, most models do not possess a corresponding variable.\nif a model is parameterized then fixing a distributional property is tantamount to constraining the parameters to take values in some subset _S_, right? and if the parameters themselves have Bayesian-style prior distributions then conditioning on the distributional property taking that fixed value is tantamount to truncating the support of the prior to just _S_, right? what am i missing? i mean, this is a pain to do in particular cases so if all Omega does is make it easy in general that's fantastic, but i don't think this is going beyond what can already be expressed in Bayesian probability theory/data analysis\n\nif i'm right about that, then p-boxes are strictly more expressive since possibility theory is a generalization of probability theory (but i have yet to find myself reaching for that extra expressive power and am so far content being a Bayesian)","user":"U01FUDH45LN","ts":"1608759431.462600","team":"T68168MUP","edited":{"user":"U01FUDH45LN","ts":"1608759569.000000"},"blocks":[{"type":"rich_text","block_id":"kvFZ5","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UDGT4PM41"},{"type":"text","text":" "},{"type":"user","user_id":"U66G4838Q"},{"type":"text","text":"  to really understand this i need a toy example of a classifier that doesn't satisfy expression (1) and a demonstration of what the ||-operator does to it to make expression (2) hold\n\na typo i noticed on page 3 right hand column two thirds of the way down:\n"}]},{"type":"rich_text_quote","elements":[{"type":"text","text":"with P being uniform a uniform measure over that hypercube"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\npage 4:\n"}]},{"type":"rich_text_quote","elements":[{"type":"text","text":"However, this is a special case; for most distributional properties, most models do not possess a corresponding variable."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"if a model is parameterized then fixing a distributional property is tantamount to constraining the parameters to take values in some subset "},{"type":"text","text":"S","style":{"italic":true}},{"type":"text","text":", right? and if the parameters themselves have Bayesian-style prior distributions then conditioning on the distributional property taking that fixed value is tantamount to truncating the support of the prior to just "},{"type":"text","text":"S","style":{"italic":true}},{"type":"text","text":", right? what am i missing? i mean, this is a pain to do in particular cases so if all Omega does is make it easy in general that's fantastic, but i don't think this is going beyond what can already be expressed in Bayesian probability theory/data analysis\n\nif i'm right about that, then p-boxes are strictly more expressive since possibility theory is a generalization of probability theory (but i have yet to find myself reaching for that extra expressive power and am so far content being a Bayesian)"}]}]}]},{"client_msg_id":"376933AA-64DF-47D9-993B-B798897DFBEF","type":"message","text":"Most classifiers are not fair.  This is the field of algorithmic fairness.  It would be good to show a toy example of that.  That said, I use a different example in a new version in the introduction of the paper","user":"U66G4838Q","ts":"1608759735.465000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7g1+t","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Most classifiers are not fair.  This is the field of algorithmic fairness.  It would be good to show a toy example of that.  That said, I use a different example in a new version in the introduction of the paper"}]}]}]},{"client_msg_id":"3DB349B4-E6FA-4908-9724-C9EBF069606E","type":"message","text":"Thanks for the typo","user":"U66G4838Q","ts":"1608759760.465200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hd6mW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for the typo"}]}]}]},{"client_msg_id":"813D6C56-05EE-4EC4-8973-F6B5B55DFB9E","type":"message","text":"I‚Äôm not sure how I feel about possibility theory","user":"U66G4838Q","ts":"1608760340.467200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"s50","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I‚Äôm not sure how I feel about possibility theory"}]}]}]},{"client_msg_id":"FC4815EC-3FB8-4E94-991E-197F9F034EDB","type":"message","text":"In some ways, the random conditional distribution is the modern realisation of a bunch of arguments people had in the 80s about the necessity of possibilistic reasoning","user":"U66G4838Q","ts":"1608760632.469400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"64xm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In some ways, the random conditional distribution is the modern realisation of a bunch of arguments people had in the 80s about the necessity of possibilistic reasoning"}]}]}]},{"client_msg_id":"4e130987-1e5e-4e32-9a25-4d9657c43750","type":"message","text":"I like the approach. It‚Äôs 2nd order probability right? In UQ we call them 2nd order distributions (some people meta-distributions), and simulating them 2nd order Monte Carlo. They should definitely be considered in Probabilistic Programming, and also does fit into the Imprecise Probabilities picture, although some of my colleges would disagree and usually they‚Äôre not discussed in IP. Computations with them can be quite expensive, and correlations/dependencies can be troublesome. But they‚Äôre nonetheless a very flexible way to express uncertainty about a probability measure.\n\nI would say:\n‚Ä¢ Random sets: generic sets probability measures\n‚Ä¢ P-boxes: cdf shaped random sets\n‚Ä¢ Possibility distributions: consonant random sets\n‚Ä¢ 2nd order distributions: probability measures which are distributions\nI would consider it as an extension of Bayesian inference. You can calibrate for them with bayesian updating but you need to modify the likelihood using a stochastic distance metric:\n<https://www.rpsonline.com.sg/proceedings/esrel2020/pdf/5520.pdf>\n<https://www.sciencedirect.com/science/article/pii/S0888327018304837?casa_token=88aISmlFIcwAAAAA:8oNovWKbMr4RDirEQ8zamR30gU5iAIHt6HKN34QXOfNOAJgrBzM4fAsu7KStGO5XM-iR1X608Y9D|https://www.sciencedirect.com/science/article/pii/S0888327018304837?casa_token=88aISmlF[‚Ä¶]zamR30gU5iAIHt6HKN34QXOfNOAJgrBzM4fAsu7KStGO5XM-iR1X608Y9D>","user":"U01BZJ2JTML","ts":"1610047970.489100","team":"T68168MUP","attachments":[{"title":"The role of the Bhattacharyya distance in stochastic model updating","title_link":"https://www.sciencedirect.com/science/article/pii/S0888327018304837?casa_token=88aISmlFIcwAAAAA:8oNovWKbMr4RDirEQ8zamR30gU5iAIHt6HKN34QXOfNOAJgrBzM4fAsu7KStGO5XM-iR1X608Y9D","text":"The Bhattacharyya distance is a stochastic measurement between two samples and taking into account their probability distributions. The objective of t‚Ä¶","fallback":"The role of the Bhattacharyya distance in stochastic model updating","from_url":"https://www.sciencedirect.com/science/article/pii/S0888327018304837?casa_token=88aISmlFIcwAAAAA:8oNovWKbMr4RDirEQ8zamR30gU5iAIHt6HKN34QXOfNOAJgrBzM4fAsu7KStGO5XM-iR1X608Y9D","thumb_url":"https://ars.els-cdn.com/content/image/1-s2.0-S0888327018X00132-cov150h.gif","thumb_width":109,"thumb_height":150,"service_icon":"https://www.sciencedirect.com/favicon.ico","service_name":"sciencedirect.com","id":1,"original_url":"https://www.sciencedirect.com/science/article/pii/S0888327018304837?casa_token=88aISmlFIcwAAAAA:8oNovWKbMr4RDirEQ8zamR30gU5iAIHt6HKN34QXOfNOAJgrBzM4fAsu7KStGO5XM-iR1X608Y9D"}],"blocks":[{"type":"rich_text","block_id":"7=9E","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I like the approach. It‚Äôs 2nd order probability right? In UQ we call them 2nd order distributions (some people meta-distributions), and simulating them 2nd order Monte Carlo. They should definitely be considered in Probabilistic Programming, and also does fit into the Imprecise Probabilities picture, although some of my colleges would disagree and usually they‚Äôre not discussed in IP. Computations with them can be quite expensive, and correlations/dependencies can be troublesome. But they‚Äôre nonetheless a very flexible way to express uncertainty about a probability measure.\n\nI would say:\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Random sets: generic sets probability measures"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"P-boxes: cdf shaped random sets"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Possibility distributions: consonant random sets"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"2nd order distributions: probability measures which are distributions"}]}],"style":"bullet","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"\nI would consider it as an extension of Bayesian inference. You can calibrate for them with bayesian updating but you need to modify the likelihood using a stochastic distance metric:\n"},{"type":"link","url":"https://www.rpsonline.com.sg/proceedings/esrel2020/pdf/5520.pdf"},{"type":"text","text":"\n"},{"type":"link","url":"https://www.sciencedirect.com/science/article/pii/S0888327018304837?casa_token=88aISmlFIcwAAAAA:8oNovWKbMr4RDirEQ8zamR30gU5iAIHt6HKN34QXOfNOAJgrBzM4fAsu7KStGO5XM-iR1X608Y9D","text":"https://www.sciencedirect.com/science/article/pii/S0888327018304837?casa_token=88aISmlF[‚Ä¶]zamR30gU5iAIHt6HKN34QXOfNOAJgrBzM4fAsu7KStGO5XM-iR1X608Y9D"}]}]}]},{"client_msg_id":"ae50fb87-b334-4122-ba6e-1ea699726529","type":"message","text":"I don‚Äôt think they‚Äôd suffer from probability dilation in the satellite problem, as long as you increase the dispersion of the upper level the more epistemic uncertainty you have","user":"U01BZJ2JTML","ts":"1610048129.490400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xU+S","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I don‚Äôt think they‚Äôd suffer from probability dilation in the satellite problem, as long as you increase the dispersion of the upper level the more epistemic uncertainty you have"}]}]}]},{"client_msg_id":"e86cdbdf-f52d-499d-a8db-8206f24a77b7","type":"message","text":"We usually have the lower level being stochastic and the upper level being epistemic, but they could be both. I guess in a fully bayesian version would have both levels as epistemic","user":"U01BZJ2JTML","ts":"1610048179.491400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"D=E","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We usually have the lower level being stochastic and the upper level being epistemic, but they could be both. I guess in a fully bayesian version would have both levels as epistemic"}]}]}]},{"client_msg_id":"c8cbb012-03d7-48bb-bdad-72f605cf2c39","type":"message","text":"<@U01FN74CNCX> What do you think of the above as a bayesian solution to the false confidence theorem^ (<@U01BZJ2JTML> Michael is the author of the satellite paper )","user":"UDGT4PM41","ts":"1610063679.494900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UYqf","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U01FN74CNCX"},{"type":"text","text":" What do you think of the above as a bayesian solution to the false confidence theorem^ ("},{"type":"user","user_id":"U01BZJ2JTML"},{"type":"text","text":" Michael is the author of the satellite paper )"}]}]}]},{"type":"message","subtype":"thread_broadcast","text":"<@U66G4838Q> use this link: <https://drive.google.com/drive/folders/1iFXo6tq5JWbG6HpJWXyLNupX4J30TaMq?usp=sharing>  I extracted even more history from my phone","user":"UDGT4PM41","ts":"1610063891.495200","thread_ts":"1610048179.491400","root":{"client_msg_id":"e86cdbdf-f52d-499d-a8db-8206f24a77b7","type":"message","text":"We usually have the lower level being stochastic and the upper level being epistemic, but they could be both epistemic. I guess in a fully bayesian version would be like this","user":"U01BZJ2JTML","ts":"1610048179.491400","team":"T68168MUP","edited":{"user":"U01BZJ2JTML","ts":"1610048273.000000"},"blocks":[{"type":"rich_text","block_id":"f++b8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We usually have the lower level being stochastic and the upper level being epistemic, but they could be both epistemic. I guess in a fully bayesian version would be like this"}]}]}],"thread_ts":"1610048179.491400","reply_count":28,"reply_users_count":4,"latest_reply":"1610066869.499600","reply_users":["UDGT4PM41","U01BZJ2JTML","U66G4838Q","U81PB6N77"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"3Go","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U66G4838Q"},{"type":"text","text":" use this link: "},{"type":"link","url":"https://drive.google.com/drive/folders/1iFXo6tq5JWbG6HpJWXyLNupX4J30TaMq?usp=sharing"},{"type":"text","text":"  I extracted even more history from my phone"}]}]}],"client_msg_id":"cd05583e-66fe-4e0f-8cb4-b8ee201c04a2"},{"client_msg_id":"6f28d1f5-ca44-4e35-aa44-68a489554eae","type":"message","text":"I have no idea how visible the discussions tab of a repo is, so just to be sure the number of people who see it isn't zero...\n<https://github.com/cscherrer/MeasureTheory.jl/discussions/37>","user":"U81PB6N77","ts":"1610076514.001100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DCLj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have no idea how visible the discussions tab of a repo is, so just to be sure the number of people who see it isn't zero...\n"},{"type":"link","url":"https://github.com/cscherrer/MeasureTheory.jl/discussions/37"}]}]}]},{"type":"message","subtype":"thread_broadcast","text":"<@U81PB6N77> does Soss support interventions?","user":"UKA81L34J","ts":"1610081974.007900","thread_ts":"1610048179.491400","root":{"client_msg_id":"e86cdbdf-f52d-499d-a8db-8206f24a77b7","type":"message","text":"We usually have the lower level being stochastic and the upper level being epistemic, but they could be both epistemic. I guess in a fully bayesian version would be like this","user":"U01BZJ2JTML","ts":"1610048179.491400","team":"T68168MUP","edited":{"user":"U01BZJ2JTML","ts":"1610048273.000000"},"blocks":[{"type":"rich_text","block_id":"f++b8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We usually have the lower level being stochastic and the upper level being epistemic, but they could be both epistemic. I guess in a fully bayesian version would be like this"}]}]}],"thread_ts":"1610048179.491400","reply_count":47,"reply_users_count":5,"latest_reply":"1610083153.008200","reply_users":["UDGT4PM41","U01BZJ2JTML","U66G4838Q","U81PB6N77","UKA81L34J"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"x7w","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U81PB6N77"},{"type":"text","text":" does Soss support interventions?"}]}]}],"client_msg_id":"2E63D9D2-7353-4C2B-9BBC-CEE3A986C81E"}]}