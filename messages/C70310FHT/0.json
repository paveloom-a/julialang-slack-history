{"cursor": 0, "messages": [{"client_msg_id":"e97557b6-2fc2-4abe-ba0c-a311f4890519","type":"message","text":"Hey <@U679VPJ8L> - out of curiosity, what would be needed to bolster the Julia NLP ecosystem? I instead had to go over to Spacy in python for some work that I had to do with tokenization but am curious what it would take to continue building the Julia NLP ecosystem. Thoughts?","user":"US64J0NPQ","ts":"1610944993.007700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"brt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey "},{"type":"user","user_id":"U679VPJ8L"},{"type":"text","text":" - out of curiosity, what would be needed to bolster the Julia NLP ecosystem? I instead had to go over to Spacy in python for some work that I had to do with tokenization but am curious what it would take to continue building the Julia NLP ecosystem. Thoughts?"}]}]}],"thread_ts":"1610944993.007700","reply_count":10,"reply_users_count":3,"latest_reply":"1610991877.009700","reply_users":["U6A936746","US64J0NPQ","U679VPJ8L"],"subscribed":false},{"client_msg_id":"523a6a68-6fd3-4ae4-b261-8bd74e7a7817","type":"message","text":"<https://twitter.com/spacy_io/status/1356238587545575427?s=19|https://twitter.com/spacy_io/status/1356238587545575427?s=19>","user":"UDGT4PM41","ts":"1612208497.001500","team":"T68168MUP","attachments":[{"fallback":"<https://twitter.com/spacy_io|@spacy_io>: Today we're releasing spaCy v3.0!\n\n:flying_saucer: Transformer-based pipelines for SOTA models\n:gear: New training and config system\nüß¨ Models using any framework\nü™ê Manage end-to-end workflows\n:fire: New and improved APIs, components &amp; more\n\n<https://github.com/explosion/spaCy/releases/tag/v3.0.0/>","ts":1612187451,"author_name":"spaCy","author_link":"https://twitter.com/spacy_io/status/1356238587545575427","author_icon":"https://pbs.twimg.com/profile_images/699256981287100416/7-7zis8f_normal.png","author_subname":"@spacy_io","text":"Today we're releasing spaCy v3.0!\n\n:flying_saucer: Transformer-based pipelines for SOTA models\n:gear: New training and config system\nüß¨ Models using any framework\nü™ê Manage end-to-end workflows\n:fire: New and improved APIs, components &amp; more\n\n<https://github.com/explosion/spaCy/releases/tag/v3.0.0/>","service_name":"twitter","service_url":"https://twitter.com/","from_url":"https://twitter.com/spacy_io/status/1356238587545575427?s=19","id":1,"original_url":"https://twitter.com/spacy_io/status/1356238587545575427?s=19","footer":"Twitter","footer_icon":"https://a.slack-edge.com/80588/img/services/twitter_pixel_snapped_32.png"}],"blocks":[{"type":"rich_text","block_id":"x69oz","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://twitter.com/spacy_io/status/1356238587545575427?s=19","text":"https://twitter.com/spacy_io/status/1356238587545575427?s=19"}]}]}],"thread_ts":"1612208497.001500","reply_count":3,"reply_users_count":2,"latest_reply":"1612292467.002400","reply_users":["U013KHU2XC1","U01CCE4QQV9"],"subscribed":false},{"client_msg_id":"06eb9cb7-9e0a-4e88-a072-d9dc3b005b18","type":"message","text":"Posted potentially interesting question related to NLP in Julia here: <https://discourse.julialang.org/t/effective-text-extraction-from-documents-pdfs/54889>","user":"US64J0NPQ","ts":"1612837508.009800","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"Effective Text Extraction from Documents (PDFs)","title_link":"https://discourse.julialang.org/t/effective-text-extraction-from-documents-pdfs/54889","text":"Hi all, Question for those involved with text extraction pipelines (or ETL pipelines for that matter): In Julia, what works best for you when doing text extraction from PDFs? In my case, I am simplifying the problem to look at PDFs that are single column in form and are written in English with either none or minimal images. Currently trying out both Taro.jl and PDFIO.jl - so far I have found Taro.jl a bit easier to work with to extract content. However, both packages struggle with white space...","fallback":"JuliaLang: Effective Text Extraction from Documents (PDFs)","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","ts":1612837475,"from_url":"https://discourse.julialang.org/t/effective-text-extraction-from-documents-pdfs/54889","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/effective-text-extraction-from-documents-pdfs/54889"}],"blocks":[{"type":"rich_text","block_id":"LuN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Posted potentially interesting question related to NLP in Julia here: "},{"type":"link","url":"https://discourse.julialang.org/t/effective-text-extraction-from-documents-pdfs/54889"}]}]}]},{"client_msg_id":"5f100a6f-c37d-4e3f-ac85-d3e400e461c4","type":"message","text":"Would love to hear additional thoughts on it! Please post on Discourse though so we have a record. :hugging_face: Thanks!","user":"US64J0NPQ","ts":"1612837535.010500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0Au8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Would love to hear additional thoughts on it! Please post on Discourse though so we have a record. "},{"type":"emoji","name":"hugging_face"},{"type":"text","text":" Thanks!"}]}]}],"thread_ts":"1612837535.010500","reply_count":2,"reply_users_count":1,"latest_reply":"1612839804.012200","reply_users":["U7V6YNG04"],"subscribed":false},{"client_msg_id":"92b72674-d618-408d-8e35-efa358914cb5","type":"message","text":"Hi.\nSaw an issue in Embeddings.jl\n'Lazily loading Embeddings'\n\nIt had a suggestion that rather than storing the array, some lazy array could be stored(instantiated only when accessed), might help in reducing time in packages like\nFastText since it's in text format(non-binary). \n\nI think it might be of help when using `load_embeddings`, but not in `using Embeddings`\nI would like to work on this issue. Can someone guide me?\nThanks. \n:blush:","user":"U0183KL21GS","ts":"1612965318.018400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+ScT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi.\nSaw an issue in Embeddings.jl\n'Lazily loading Embeddings'\n\nIt had a suggestion that rather than storing the array, some lazy array could be stored(instantiated only when accessed), might help in reducing time in packages like\nFastText since it's in text format(non-binary). \n\nI think it might be of help when using "},{"type":"text","text":"load_embeddings","style":{"code":true}},{"type":"text","text":", but not in "},{"type":"text","text":"using Embeddings\n","style":{"code":true}},{"type":"text","text":"I would like to work on this issue. Can someone guide me?\nThanks. \n"},{"type":"emoji","name":"blush"}]}]}]},{"client_msg_id":"22c4866c-95a1-40e2-b2b3-362112b80c80","type":"message","text":"Here's a strange post but; NLP is an area near and dear to my heart, and I know that Julia will overtake Python eventually, so if anyone is wanting to dig into expanding NLP in Julia - maybe continue on the work of integrating HuggingFace models and encoders - please give me a ping. Hopefully I will be in a position to provide some renumeration/$$. It's the one thing I always have to drop back to python for, and it jst .. irks me :slightly_smiling_face:\n\nif this is the wrong channel then feel free to scream at me, I won't take it personally :slightly_smiling_face: I don't know how this slack deals with cross postings, and I think this is a very directed bounty","user":"U01ER903N3Y","ts":"1613493367.023200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OrL6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Here's a strange post but; NLP is an area near and dear to my heart, and I know that Julia will overtake Python eventually, so if anyone is wanting to dig into expanding NLP in Julia - maybe continue on the work of integrating HuggingFace models and encoders - please give me a ping. Hopefully I will be in a position to provide some renumeration/$$. It's the one thing I always have to drop back to python for, and it jst .. irks me "},{"type":"emoji","name":"slightly_smiling_face"},{"type":"text","text":"\n\nif this is the wrong channel then feel free to scream at me, I won't take it personally "},{"type":"emoji","name":"slightly_smiling_face"},{"type":"text","text":" I don't know how this slack deals with cross postings, and I think this is a very directed bounty"}]}]}]},{"client_msg_id":"85f7eab8-fb4a-40f4-acb8-5c86e6d9fb59","type":"message","text":"Thanks, this is certainly the right channel for this.","user":"U679VPJ8L","ts":"1613493484.023600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"e/Po","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks, this is certainly the right channel for this."}]}]}],"reactions":[{"name":"heart","users":["U01ER903N3Y"],"count":1}]},{"client_msg_id":"45428eae-fd42-4816-ad74-c59ad91d352b","type":"message","text":"NLP in Julia is relatively sparse (no pun intended) right now, in that there are few active contributors. There is certainly a lot to do.","user":"U679VPJ8L","ts":"1613493556.024600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"D9/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"NLP in Julia is relatively sparse (no pun intended) right now, in that there are few active contributors. There is certainly a lot to do."}]}]}]},{"client_msg_id":"d6c50bd7-522c-4b90-bb3c-ab6c05798375","type":"message","text":"I wish I had more time and Julia knowledge, then I would be happy to contribute. Alas I only have enough knowledge to be a user, not a contributor.","user":"UPSSPPBFV","ts":"1613493740.025300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"n5I","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I wish I had more time and Julia knowledge, then I would be happy to contribute. Alas I only have enough knowledge to be a user, not a contributor."}]}]}]},{"client_msg_id":"58926720-e29b-401f-bc2d-2e519b58610b","type":"message","text":"<@UPSSPPBFV> I hear that. I am swamped with my normal day to day job here, I am jst grinding my teeth that I have to change from Julia to Python for using the HuggingFace part","user":"U01ER903N3Y","ts":"1613493966.026600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MMC5r","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UPSSPPBFV"},{"type":"text","text":" I hear that. I am swamped with my normal day to day job here, I am jst grinding my teeth that I have to change from Julia to Python for using the HuggingFace part"}]}]}]},{"client_msg_id":"82b4448d-2b15-4010-8ac3-05773640678b","type":"message","text":"The way I can try to drive growth at this point is to try and incentivize others to pitch in on the NLP side :slightly_smiling_face:","user":"U01ER903N3Y","ts":"1613494002.027400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kc+uO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The way I can try to drive growth at this point is to try and incentivize others to pitch in on the NLP side "},{"type":"emoji","name":"slightly_smiling_face"}]}]}]},{"client_msg_id":"a62ce862-30c0-4c87-9f80-3b0b24e75a59","type":"message","text":"I‚Äôm also personally interested in this, and would be down to try to help out a little (for free) over the summer if there are more experienced folks who would be willing to guide me then","user":"US8V7JSKB","ts":"1613544286.028300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2RB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I‚Äôm also personally interested in this, and would be down to try to help out a little (for free) over the summer if there are more experienced folks who would be willing to guide me then"}]}]}]},{"client_msg_id":"fe22c9ff-2dc3-4f2c-b8fe-078ab524557d","type":"message","text":"Last month I tried to convert one of the FairSeq NMT models (en-fr), but only finished the tokenizing/embedding part. Hoping to return to it soon. The main bottleneck seems to be peeling back all the layers of PyTorch code. I probably don't have the expertise, but would love to hear more about what you have in mind.","user":"U01G14RTVM2","ts":"1614102452.004100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MpX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Last month I tried to convert one of the FairSeq NMT models (en-fr), but only finished the tokenizing/embedding part. Hoping to return to it soon. The main bottleneck seems to be peeling back all the layers of PyTorch code. I probably don't have the expertise, but would love to hear more about what you have in mind."}]}]}]},{"client_msg_id":"4570a657-8b16-4640-a105-96ded9640d37","type":"message","text":"Which NLP tools do we have in Julia as it is?","user":"U018LQSMVHT","ts":"1614332709.004500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PLNyl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Which NLP tools do we have in Julia as it is?"}]}]}],"thread_ts":"1614332709.004500","reply_count":2,"reply_users_count":1,"latest_reply":"1614333177.004900","reply_users":["U6A936746"],"subscribed":false},{"client_msg_id":"e9b4ead2-5901-417c-a9f8-99b1879d1e47","type":"message","text":"Hi, I was going through a few issues in CorpusLoaders.jl, one of them was regarding the addition of NER Datasets namely GMB, WiNER, EWNERTC, WNUT17, etc. \nI feel that the EWNERTC dataset could be a very good addition. Should I start working on it?","user":"U0183KL21GS","ts":"1615331844.011500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MRo7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I was going through a few issues in CorpusLoaders.jl, one of them was regarding the addition of NER Datasets namely GMB, WiNER, EWNERTC, WNUT17, etc. \nI feel that the EWNERTC dataset could be a very good addition. Should I start working on it?"}]}]}]},{"client_msg_id":"0dd5c356-6b3e-4708-a3d1-b3489553bde8","type":"message","text":"Hi everyone,\nI was watching NLP video of JuliaCon2018, in which the speaker used matchall function, but I didn't find function in Cascadia package. Is matchall function of Cascadia is replaced by eachmatch function?","user":"U01NN88UYLE","ts":"1615526230.013200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Pw1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi everyone,\nI was watching NLP video of JuliaCon2018, in which the speaker used matchall function, but I didn't find function in Cascadia package. Is matchall function of Cascadia is replaced by eachmatch function?"}]}]}]},{"client_msg_id":"fdf4913c-8c28-47a2-80a7-0213662897ee","type":"message","text":"Hi, everyone!\nI have a question about the word embedding layer.\nWhat is the standard way to deal with the words out of vocabulary when using the embedding layer? Are all words out of vocabulary mapping all-zero vector? Or there is a special key in the vocabulary to record these unknown words?","user":"USAH2P9E1","ts":"1615732900.014200","team":"T68168MUP","edited":{"user":"USAH2P9E1","ts":"1615733112.000000"},"blocks":[{"type":"rich_text","block_id":"gyB3A","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, everyone!\nI have a question about the word embedding layer.\nWhat is the standard way to deal with the words out of vocabulary when using the embedding layer? Are all words out of vocabulary mapping all-zero vector? Or there is a special key in the vocabulary to record these unknown words?"}]}]}]},{"client_msg_id":"4d9ba6de-ce53-45ef-99c7-6fd079380e81","type":"message","text":"Hi.\nI've added the *installation* instructions for the package *`CorpusLoaders.jl`* in the README.md and put up a PR.\nPlease go through it. :smile:","user":"U0183KL21GS","ts":"1615903174.003400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"X0C","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi.\nI've added the "},{"type":"text","text":"installation","style":{"bold":true}},{"type":"text","text":" instructions for the package "},{"type":"text","text":"CorpusLoaders.jl","style":{"bold":true,"code":true}},{"type":"text","text":" ","style":{"bold":true}},{"type":"text","text":"in the README.md and put up a PR.\nPlease go through it. "},{"type":"emoji","name":"smile"}]}]}]},{"client_msg_id":"BDDD31E9-22DB-4C6F-B9CB-F5BC4C72D569","type":"message","text":"Hi,\nWordTokenizers currently has one statistical tokenizer for ALBERT model. Could more tokenizers be added to it?\n I was working on adding GPT2 tokenizer for Julia. Though it is a wrapper on ByterPairEncoding, I feel it would be helpful to have an end to end tokenizer for later stages just like ALBERT","user":"U01HPCV8GTW","ts":"1615992169.012700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"foyS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi,\n"},{"type":"text","text":"WordTokenizers currently has one statistical tokenizer for ALBERT model. Could more tokenizers be added to it?\n I was working on adding GPT2 tokenizer for Julia. Though it is a wrapper on ByterPairEncoding, I feel it would be helpful to have an end to end tokenizer for later stages just like ALBERT"}]}]}]},{"client_msg_id":"3291b02c-7502-4edd-b028-13659f235b4e","type":"message","text":"Does anyone has working implementation of reformer?","user":"U6YRZ18GZ","ts":"1616050666.013800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zG6dF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does anyone has working implementation of reformer?"}]}]}]},{"client_msg_id":"e8d19123-0a9f-46a8-8517-8eb01a7581c5","type":"message","text":"Hi everyone,\nThis is Sambhaw.\nI have been contributing in JuliaText and Flux for the past few months.\nI feel that XLNET model can be a great addition to NLP in Julia. It's a state of the art model and also outperforms BERT in several instances. Moreover, it is based on the Transformer-XL structure which is one of hottest topics in NLP these days. \nThis is a brief idea of the project that I have been looking forward to for GSOC-2021. \nPlease suggest me how to proceed next. \nI am looking forward to discussing it with the mentors. \nHave a great day. :smile:","user":"U0183KL21GS","ts":"1616320357.017000","team":"T68168MUP","edited":{"user":"U0183KL21GS","ts":"1616321166.000000"},"blocks":[{"type":"rich_text","block_id":"dR=tx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi everyone,\nThis is Sambhaw.\nI have been contributing in JuliaText and Flux for the past few months.\nI feel that XLNET model can be a great addition to NLP in Julia. It's a state of the art model and also outperforms BERT in several instances. Moreover, it is based on the Transformer-XL structure which is one of hottest topics in NLP these days. \nThis is a brief idea of the project that I have been looking forward to for GSOC-2021. \nPlease suggest me how to proceed next. \nI am looking forward to discussing it with the mentors. \nHave a great day. "},{"type":"emoji","name":"smile"}]}]}]},{"client_msg_id":"808490f6-b0b2-4a02-b052-ba82a86de3c3","type":"message","text":"Hi,\nI have created PR in WordTokenizers.jl 5 days ago, please go through it and give your valuable suggestion.","user":"U01NN88UYLE","ts":"1616556304.019900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pkF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi,\nI have created PR in WordTokenizers.jl 5 days ago, please go through it and give your valuable suggestion."}]}]}]},{"client_msg_id":"ee11666e-5a80-4b90-95e7-dd6744487365","type":"message","text":"Hey <@U69F2VCFJ>, I am looking for some wayout to replicate the following behaviour with Transformers.jl:\n\n```output, new_past = model(embeddings, past=past) # model -&gt; gpt2 LMhead model\n```\nCan you suggest how this can be done? I was trying to do it with Transformers.HuggingFace gpt2 model, but was facing some errors.","user":"US6KB42UW","ts":"1616616369.029100","team":"T68168MUP","edited":{"user":"US6KB42UW","ts":"1616616512.000000"},"blocks":[{"type":"rich_text","block_id":"aTLsL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey "},{"type":"user","user_id":"U69F2VCFJ"},{"type":"text","text":", I am looking for some wayout to replicate the following behaviour with Transformers.jl:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"output, new_past = model(embeddings, past=past) # model -> gpt2 LMhead model\n"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nCan you suggest how this can be done? I was trying to do it with Transformers.HuggingFace gpt2 model, but was facing some errors."}]}]}]},{"client_msg_id":"8614CCE9-16BE-479D-8196-5BE84F4C2041","type":"message","text":"Is there any resource of getting intuition/basics of converting .ckpt to .bson and how things work under the hood? I searched on the internet but couldn‚Äôt find any. ","user":"U01HPCV8GTW","ts":"1616746185.040600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"igGi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there any resource of getting intuition/basics of converting .ckpt to .bson and how things work under the hood? I searched on the internet but couldn‚Äôt find any. "}]}]}]},{"client_msg_id":"a4c0c3f1-3707-496a-99a5-281b9ec66603","type":"message","text":"Hello, has anyone faced this error while loading ‚Äúbert-uncased_L-12_H-768_A-12‚Äù model from Transformers.jl?\nDeleting the compiled directory ( ~/.julia/compiled ) didn‚Äôt work.\n`julia&gt; bert_model, wordpiece, tokenizer = pretrain\"bert-uncased_L-24_H-1024_A-16\"`\n```[ Info: loading pretrain bert model: uncased_L-24_H-1024_A-16.tfbson \nERROR: EOFError: read end of file```","user":"U01SMV1TTCL","ts":"1616748147.043300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ESNk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello, has anyone faced this error while loading ‚Äúbert-uncased_L-12_H-768_A-12‚Äù model from Transformers.jl?\nDeleting the compiled directory ( ~/.julia/compiled ) didn‚Äôt work.\n"},{"type":"text","text":"julia> bert_model, wordpiece, tokenizer = pretrain\"bert-uncased_L-24_H-1024_A-16\"","style":{"code":true}},{"type":"text","text":"\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"[ Info: loading pretrain bert model: uncased_L-24_H-1024_A-16.tfbson \nERROR: EOFError: read end of file"}]}]}],"thread_ts":"1616748147.043300","reply_count":1,"reply_users_count":1,"latest_reply":"1616749428.043800","reply_users":["US6KB42UW"],"is_locked":false,"subscribed":false},{"client_msg_id":"94d44450-2f53-47a5-a675-01da8e0bb65f","type":"message","text":"Using <https://github.com/JuliaText/TextAnalysis.jl|TextAnalysis.jl>, I have trained an LDA model on my corpus, and now I have `œï` and `Œ∏` as defined <https://juliahub.com/docs/TextAnalysis/5Mwet/0.7.2/APIReference/#TextAnalysis.lda-Tuple{DocumentTermMatrix,Int64,Int64,Float64,Float64}|here>. How can I calculate the perplexity of my model?","user":"U7THT3TM3","ts":"1617230673.050300","team":"T68168MUP","edited":{"user":"U7THT3TM3","ts":"1617230798.000000"},"blocks":[{"type":"rich_text","block_id":"sX4ha","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Using "},{"type":"link","url":"https://github.com/JuliaText/TextAnalysis.jl","text":"TextAnalysis.jl"},{"type":"text","text":", I have trained an LDA model on my corpus, and now I have `œï` and `Œ∏` as defined "},{"type":"link","url":"https://juliahub.com/docs/TextAnalysis/5Mwet/0.7.2/APIReference/#TextAnalysis.lda-Tuple{DocumentTermMatrix,Int64,Int64,Float64,Float64}","text":"here"},{"type":"text","text":". How can I calculate the perplexity of my model?"}]}]}]},{"client_msg_id":"4bf3438c-59b4-4b9f-b5ca-bc2457e8b9de","type":"message","text":"Hi,\nI think GPT-2 and ELMo are not implemented in julia and ALBERT and GPT are in julia. Correct me, if I am wrong.\nIf so, I can write GSOC proposal to implement GPT-2 and ELMo.","user":"U01NN88UYLE","ts":"1617340277.052400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"P=gQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi,\nI think GPT-2 and ELMo are not implemented in julia and ALBERT and GPT are in julia. Correct me, if I am wrong.\nIf so, I can write GSOC proposal to implement GPT-2 and ELMo."}]}]}],"reactions":[{"name":"pray","users":["US8V7JSKB"],"count":1}]},{"client_msg_id":"E0F0BA48-7E9C-4908-8BB0-2CB043854AA9","type":"message","text":"Could a TextAnalysis.jl member take a look at <https://github.com/JuliaText/TextAnalysis.jl/pull/250|https://github.com/JuliaText/TextAnalysis.jl/pull/250> (Latent Dirichlet allocation: display a progress bar during Gibbs sampling)? Gibbs sampling can take a really long time if your corpus is large, so I figured a progress bar would be nice.","user":"U7THT3TM3","ts":"1617579185.055500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Pscn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Could a TextAnalysis.jl member take a look at "},{"type":"link","url":"https://github.com/JuliaText/TextAnalysis.jl/pull/250","text":"https://github.com/JuliaText/TextAnalysis.jl/pull/250"},{"type":"text","text":" (Latent Dirichlet allocation: display a progress bar during Gibbs sampling)? Gibbs sampling can take a really long time if your corpus is large, so I figured a progress bar would be nice."}]}]}]},{"client_msg_id":"a2737dcc-35d0-4855-bdb2-65d90f4fb37c","type":"message","text":"While displaying the document-term matrix from TextAnalysis.jl, how can I show the actual terms as the column names ?","user":"U019X0KMT6Y","ts":"1617741963.056500","team":"T68168MUP","edited":{"user":"U019X0KMT6Y","ts":"1617742256.000000"},"blocks":[{"type":"rich_text","block_id":"uKf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"While displaying the document-term matrix from TextAnalysis.jl, how can I show the actual terms as the column names ?"}]}]}]},{"client_msg_id":"57939c17-3f5c-4493-b6ca-cd0e8e647cee","type":"message","text":"```m = DocumentTermMatrix(crps)```\nCan we use m.terms to be the column names?","user":"U019X0KMT6Y","ts":"1617741983.056900","team":"T68168MUP","edited":{"user":"U019X0KMT6Y","ts":"1617743099.000000"},"blocks":[{"type":"rich_text","block_id":"Y9y","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"m = DocumentTermMatrix(crps)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Can we use m.terms to be the column names?"}]}]}]},{"client_msg_id":"91581828-eeee-4180-a624-eddae80e60c7","type":"message","text":"Is there an implementation of neural machine translation available in julia right now?","user":"US8V7JSKB","ts":"1617867358.058000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UT3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there an implementation of neural machine translation available in julia right now?"}]}]}]},{"client_msg_id":"ad864799-cd40-4363-8516-0817ddc7bc0a","type":"message","text":"Hi, all the asked changes are done in <https://github.com/JuliaText/WordTokenizers.jl/pull/61>. Could someone give it a final review?","user":"U01HPCV8GTW","ts":"1617884881.059700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KRv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, all the asked changes are done in "},{"type":"link","url":"https://github.com/JuliaText/WordTokenizers.jl/pull/61"},{"type":"text","text":". Could someone give it a final review?"}]}]}]},{"client_msg_id":"721db591-0919-4db1-b21e-9977dc8e3deb","type":"message","text":"Not sure if anyone is actually using this, but I made some rewrite of my old bpe package recently. Many things are changed, you should be able to learn a more customized bpe map with it now. <https://github.com/chengchingwen/BytePairEncoding.jl>","user":"U69F2VCFJ","ts":"1617984171.063900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tNDN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not sure if anyone is actually using this, but I made some rewrite of my old bpe package recently. Many things are changed, you should be able to learn a more customized bpe map with it now. "},{"type":"link","url":"https://github.com/chengchingwen/BytePairEncoding.jl"}]}]}]},{"client_msg_id":"630e41b5-fc5b-4183-bf64-e5b87985d107","type":"message","text":"Hello, I'm struggling to use the PoSTagger in TextAnalysis, I suspect that is because I don't have a gpu, I'm using WSL, and I can't install an appropriate version of TextModels can anyone tell me if this is the case or else what am I doing wrong","user":"U01T00C2E7P","ts":"1618237308.071800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tiV+O","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello, I'm struggling to use the PoSTagger in TextAnalysis, I suspect that is because I don't have a gpu, I'm using WSL, and I can't install an appropriate version of TextModels can anyone tell me if this is the case or else what am I doing wrong"}]}]}]},{"client_msg_id":"64a80fe1-3993-4928-a073-f75e479072c5","type":"message","text":"Hello! When I try to use PoSTagger from TextAnalysis it says there is no such function, I suspect that is because I can't install an appropriate version of TextModels because I don't have a gpu, I'm using WSL, does anybody know if this is the case or what am I doing wrong? Thank you!!\nWhat I do is\n```julia&gt; using TextAnalysis\njulia&gt; pos=PoSTagger() # same with TextAnalysis.PoSTagger()\nERROR: UndefVarError: PoSTagger not defined```\nIt turns out TextModels can't be installed in julia 1.6.0 for a dependency with old version of Flux.jl","user":"U01T00C2E7P","ts":"1618237683.074900","team":"T68168MUP","edited":{"user":"U01T00C2E7P","ts":"1618239057.000000"},"blocks":[{"type":"rich_text","block_id":"y+6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello! When I try to use PoSTagger from TextAnalysis it says there is no such function, I suspect that is because I can't install an appropriate version of TextModels because I don't have a gpu, I'm using WSL, does anybody know if this is the case or what am I doing wrong? Thank you!!\nWhat I do is\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> using TextAnalysis\njulia> pos=PoSTagger() # same with TextAnalysis.PoSTagger()\nERROR: UndefVarError: PoSTagger not defined"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"It turns out TextModels can't be installed in julia 1.6.0 for a dependency with old version of Flux.jl"}]}]}]}]}