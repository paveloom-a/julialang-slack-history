{"cursor": 0, "messages": [{"client_msg_id":"0ed6ba5c-fb4b-4644-a513-a484e69a8e76","type":"message","text":"Anyone around who maintains the webpage <http://fluxml.ai|fluxml.ai> ? It throws a 404 when hitting the \"Try it out\" button pointing to this address: <https://fluxml.ai/getting_started.html> .","user":"ULL3KSGBS","ts":"1609223150.010900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SmO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Anyone around who maintains the webpage "},{"type":"link","url":"http://fluxml.ai","text":"fluxml.ai"},{"type":"text","text":" ? It throws a 404 when hitting the \"Try it out\" button pointing to this address: "},{"type":"link","url":"https://fluxml.ai/getting_started.html"},{"type":"text","text":" ."}]}]}]},{"client_msg_id":"67367ce6-6ec8-44c2-a5dd-5c9564d98cc5","type":"message","text":"Hi, I have a question considering the gradient of a convolution in Flux/NNlib\nI have defined my own `rrule` for `NNlib.conv` and if I check it by hand `rrule(conv, args…)` then it does what is expected, however, when I uses the `gradient` wrapper `gradient( () -&gt; norm(C(x)), params(C))` then it ignores my `rrule` and falls back to the default behavior. So I have two questions\n• How can I force it to use my `rrule`\n• Where is the behavior defined, because I can’t seem to find anywhere in Zygote or Flux where the gradient for the convolution layer is defind.\nThank you for any help or answer you may have","user":"U010WA4SZK5","ts":"1609349876.014100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j15tr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I have a question considering the gradient of a convolution in Flux/NNlib\nI have defined my own "},{"type":"text","text":"rrule","style":{"code":true}},{"type":"text","text":" for "},{"type":"text","text":"NNlib.conv","style":{"code":true}},{"type":"text","text":" and if I check it by hand "},{"type":"text","text":"rrule(conv, args…)","style":{"code":true}},{"type":"text","text":" then it does what is expected, however, when I uses the "},{"type":"text","text":"gradient","style":{"code":true}},{"type":"text","text":" wrapper "},{"type":"text","text":"gradient( () -> norm(C(x)), params(C))","style":{"code":true}},{"type":"text","text":" then it ignores my "},{"type":"text","text":"rrule","style":{"code":true}},{"type":"text","text":" and falls back to the default behavior. So I have two questions\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I force it to use my "},{"type":"text","text":"rrule","style":{"code":true}}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Where is the behavior defined, because I can’t seem to find anywhere in Zygote or Flux where the gradient for the convolution layer is defind."}]}],"style":"bullet","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"Thank you for any help or answer you may have"}]}]}]},{"client_msg_id":"a4b50e3d-48d7-42d0-bd08-5dd6ceeb6502","type":"message","text":"Hey everyone, is there any tutorial or specific documentation on implementing a loss function in Flux? I mean, with best practices and such, so that the function is efficient when using Flux.","user":"U01CMBH4MQE","ts":"1611774853.000700","team":"T68168MUP","edited":{"user":"U01CMBH4MQE","ts":"1611774897.000000"},"blocks":[{"type":"rich_text","block_id":"aUL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey everyone, is there any tutorial or specific documentation on implementing a loss function in Flux? I mean, with best practices and such, so that the function is efficient when using Flux."}]}]}]},{"client_msg_id":"d1d2708f-ec7c-41d8-87a8-2a9281400d73","type":"message","text":"Hello, it seems that the new LSTM, doesn't allow to put the activation function within the call, as the source code says :`LSTM(in::Integer, out::Integer, σ = tanh),`  any idea how could I achieve the same result. `Chain(LSTM(3,10), Flux.relu, Dense(10,1))` gives problems also because of broadcasting.","user":"UM70NJEER","ts":"1612217592.004700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2gsT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello, it seems that the new LSTM, doesn't allow to put the activation function within the call, as the source code says :"},{"type":"text","text":"LSTM(in::Integer, out::Integer, σ = tanh),","style":{"code":true}},{"type":"text","text":"  any idea how could I achieve the same result. "},{"type":"text","text":"Chain(LSTM(3,10), Flux.relu, Dense(10,1))","style":{"code":true}},{"type":"text","text":" gives problems also because of broadcasting."}]}]}]}]}