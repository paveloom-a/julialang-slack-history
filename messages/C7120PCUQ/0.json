{"cursor": 0, "messages": [{"client_msg_id":"0ed6ba5c-fb4b-4644-a513-a484e69a8e76","type":"message","text":"Anyone around who maintains the webpage <http://fluxml.ai|fluxml.ai> ? It throws a 404 when hitting the \"Try it out\" button pointing to this address: <https://fluxml.ai/getting_started.html> .","user":"ULL3KSGBS","ts":"1609223150.010900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SmO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Anyone around who maintains the webpage "},{"type":"link","url":"http://fluxml.ai","text":"fluxml.ai"},{"type":"text","text":" ? It throws a 404 when hitting the \"Try it out\" button pointing to this address: "},{"type":"link","url":"https://fluxml.ai/getting_started.html"},{"type":"text","text":" ."}]}]}]},{"client_msg_id":"67367ce6-6ec8-44c2-a5dd-5c9564d98cc5","type":"message","text":"Hi, I have a question considering the gradient of a convolution in Flux/NNlib\nI have defined my own `rrule` for `NNlib.conv` and if I check it by hand `rrule(conv, args…)` then it does what is expected, however, when I uses the `gradient` wrapper `gradient( () -&gt; norm(C(x)), params(C))` then it ignores my `rrule` and falls back to the default behavior. So I have two questions\n• How can I force it to use my `rrule`\n• Where is the behavior defined, because I can’t seem to find anywhere in Zygote or Flux where the gradient for the convolution layer is defind.\nThank you for any help or answer you may have","user":"U010WA4SZK5","ts":"1609349876.014100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j15tr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I have a question considering the gradient of a convolution in Flux/NNlib\nI have defined my own "},{"type":"text","text":"rrule","style":{"code":true}},{"type":"text","text":" for "},{"type":"text","text":"NNlib.conv","style":{"code":true}},{"type":"text","text":" and if I check it by hand "},{"type":"text","text":"rrule(conv, args…)","style":{"code":true}},{"type":"text","text":" then it does what is expected, however, when I uses the "},{"type":"text","text":"gradient","style":{"code":true}},{"type":"text","text":" wrapper "},{"type":"text","text":"gradient( () -> norm(C(x)), params(C))","style":{"code":true}},{"type":"text","text":" then it ignores my "},{"type":"text","text":"rrule","style":{"code":true}},{"type":"text","text":" and falls back to the default behavior. So I have two questions\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I force it to use my "},{"type":"text","text":"rrule","style":{"code":true}}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Where is the behavior defined, because I can’t seem to find anywhere in Zygote or Flux where the gradient for the convolution layer is defind."}]}],"style":"bullet","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"Thank you for any help or answer you may have"}]}]}]},{"client_msg_id":"a4b50e3d-48d7-42d0-bd08-5dd6ceeb6502","type":"message","text":"Hey everyone, is there any tutorial or specific documentation on implementing a loss function in Flux? I mean, with best practices and such, so that the function is efficient when using Flux.","user":"U01CMBH4MQE","ts":"1611774853.000700","team":"T68168MUP","edited":{"user":"U01CMBH4MQE","ts":"1611774897.000000"},"blocks":[{"type":"rich_text","block_id":"aUL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey everyone, is there any tutorial or specific documentation on implementing a loss function in Flux? I mean, with best practices and such, so that the function is efficient when using Flux."}]}]}]},{"client_msg_id":"d1d2708f-ec7c-41d8-87a8-2a9281400d73","type":"message","text":"Hello, it seems that the new LSTM, doesn't allow to put the activation function within the call, as the source code says :`LSTM(in::Integer, out::Integer, σ = tanh),`  any idea how could I achieve the same result. `Chain(LSTM(3,10), Flux.relu, Dense(10,1))` gives problems also because of broadcasting.","user":"UM70NJEER","ts":"1612217592.004700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2gsT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello, it seems that the new LSTM, doesn't allow to put the activation function within the call, as the source code says :"},{"type":"text","text":"LSTM(in::Integer, out::Integer, σ = tanh),","style":{"code":true}},{"type":"text","text":"  any idea how could I achieve the same result. "},{"type":"text","text":"Chain(LSTM(3,10), Flux.relu, Dense(10,1))","style":{"code":true}},{"type":"text","text":" gives problems also because of broadcasting."}]}]}]},{"client_msg_id":"5d57b011-bb53-401d-8cf2-52659f5a3bd4","type":"message","text":"Can anyone please help with my sequence-to-one RNN. Here is where I am:\nEach of my sequences consists of `seq_length` vectors (with zero padding if necessary for shorter sequences), each of size `n_features`, and if it makes sense for better performance, I want to pack several sequence into single tensor (to transfer to GPU with CUDA.jl as described in Flux doc) . So I’m having 3 dimensions (n_feature, seq_length, n_samples).\nHaving this data setup I want to design Layer (which is very similar to Flux.Recur), which also iterates though all time steps and outputs n_samples results. It all comes down to the following code:\n```mutable struct StepRNN{C, T}\n    cell::C     # RNN cell\n    state::T   # tensor of states for each of n_samples in batch\nend\n\nfunction (r::StepRNN)(x) \n   n_feature, seq_length, n_samples = size(x) # extract tensor size\n   reset!(r)                # reset state of RNNCell to zeros, since we start new sequence processing\n   r.state, y = r.cell(r.state, x[:,1,:])     # step through time steps for all n_samples simultaneously \n   for i in 2:seq_length\n       r.state, y = r.cell(r.state, x[:,i,:])\n   end\n   return y                                         # return results for each of n_samples \nend```\nDoes this design/code makes sense or can anyone please point me to better implementation of sequence-to-one with Flux? Or how can I fix/improve this code?\nThank you very much in advance,","user":"U01G49KC1K6","ts":"1614717402.001200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ddF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can anyone please help with my sequence-to-one RNN. Here is where I am:\nEach of my sequences consists of "},{"type":"text","text":"seq_length","style":{"code":true}},{"type":"text","text":" vectors (with zero padding if necessary for shorter sequences), each of size "},{"type":"text","text":"n_features","style":{"code":true}},{"type":"text","text":", and if it makes sense for better performance, I want to pack several sequence into single tensor (to transfer to GPU with CUDA.jl as described in Flux doc) . So I’m having 3 dimensions (n_feature, seq_length, n_samples).\nHaving this data setup I want to design Layer (which is very similar to Flux.Recur), which also iterates though all time steps and outputs n_samples results. It all comes down to the following code:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"mutable struct StepRNN{C, T}\n    cell::C     # RNN cell\n    state::T   # tensor of states for each of n_samples in batch\nend\n\nfunction (r::StepRNN)(x) \n   n_feature, seq_length, n_samples = size(x) # extract tensor size\n   reset!(r)                # reset state of RNNCell to zeros, since we start new sequence processing\n   r.state, y = r.cell(r.state, x[:,1,:])     # step through time steps for all n_samples simultaneously \n   for i in 2:seq_length\n       r.state, y = r.cell(r.state, x[:,i,:])\n   end\n   return y                                         # return results for each of n_samples \nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Does this design/code makes sense or can anyone please point me to better implementation of sequence-to-one with Flux? Or how can I fix/improve this code?\nThank you very much in advance,"}]}]}]}]}