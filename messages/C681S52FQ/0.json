{"cursor": 0, "messages": [{"client_msg_id":"93950008-5b30-441c-aef2-bd62e193694a","type":"message","text":"Do we have something like Optim but supporting `CuArray`s? I have a problem where L-BFGS outperforms Flux optimizers, but I would like to move the model computation to the GPU. Transferring the data from GPU to CPU at every iteration seems expensive, but I couldn't find a GPU friendly quasi newton solver.","user":"U6BJ9E351","ts":"1607982881.353800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ypZBE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do we have something like Optim but supporting "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":"s? I have a problem where L-BFGS outperforms Flux optimizers, but I would like to move the model computation to the GPU. Transferring the data from GPU to CPU at every iteration seems expensive, but I couldn't find a GPU friendly quasi newton solver."}]}]}],"thread_ts":"1607982881.353800","reply_count":30,"reply_users_count":3,"latest_reply":"1608044342.360600","reply_users":["U6CJRSR63","U6BJ9E351","U67G3QRJM"],"subscribed":false,"reactions":[{"name":"+1","users":["UN2U72Q3F"],"count":1}]},{"client_msg_id":"a848af70-d82c-4352-9cf8-6c80845ab575","type":"message","text":"Are there any packages for finding saddle-points (e.g. min-max) of a function?","user":"U7YD3DKL2","ts":"1608129306.361800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QhgF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Are there any packages for finding saddle-points (e.g. min-max) of a function?"}]}]}],"thread_ts":"1608129306.361800","reply_count":6,"reply_users_count":2,"latest_reply":"1608438612.364900","reply_users":["U67G3QRJM","U7YD3DKL2"],"subscribed":false},{"client_msg_id":"5edce411-dc98-4a38-9d92-d1843a56a570","type":"message","text":"Good question. Are you looking for any specific methods?","user":"U6CJRSR63","ts":"1608132188.362200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=LVn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Good question. Are you looking for any specific methods?"}]}]}],"thread_ts":"1608132188.362200","reply_count":1,"reply_users_count":1,"latest_reply":"1608441112.365100","reply_users":["U7YD3DKL2"],"subscribed":false},{"client_msg_id":"e44aebf7-c018-4772-b33f-e9c3726f6b2f","type":"message","text":"I just joined this channel to ask that question haha. :see_no_evil:","user":"U01A0S07875","ts":"1608397927.364400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Prj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I just joined this channel to ask that question haha. "},{"type":"emoji","name":"see_no_evil"}]}]}],"thread_ts":"1608397927.364400","reply_count":1,"reply_users_count":1,"latest_reply":"1608410390.364700","reply_users":["U9MD78Z9N"],"subscribed":false},{"type":"message","text":"Assuming i want to minimize a third degree taylor expansion (which is non convex) of an sufficently nice non-linear function.\nI can express the minimization of a multinomial of third degree as the root finding of a second degree multi variate polynomial, with some  conditions to not find maximas/saddle points and maybe some trust region style constraint. \nAre you aware of algorithm that do that and strategely construct roots to jump in to different local minima in order to find the best one?","user":"U9MD78Z9N","ts":"1609636138.367100","team":"T68168MUP"},{"type":"message","text":"(duplicate from other channel(s), sorry if you are flooded)","user":"U01FR2HFJ7M","ts":"1609830788.368700","team":"T68168MUP","attachments":[{"fallback":"[January 5th, 2021 2:27 PM] jf: Any idea where I can chat/contact people working/using Evolutionary.jl (other than GH issues)?","ts":"1609824439.191600","author_id":"U01FR2HFJ7M","author_subname":"Azzaare","channel_id":"CAKKFNYLD","channel_name":"biology","is_msg_unfurl":true,"is_thread_root_unfurl":true,"text":"Any idea where I can chat/contact people working/using Evolutionary.jl (other than GH issues)?","author_name":"Azzaare","author_link":"https://julialang.slack.com/team/U01FR2HFJ7M","author_icon":"https://avatars.slack-edge.com/2020-11-30/1535756085650_d8d021e24a800bfb16b8_48.jpg","mrkdwn_in":["text"],"color":"D0D0D0","from_url":"https://julialang.slack.com/archives/CAKKFNYLD/p1609824439191600?thread_ts=1609824439191600&cid=CAKKFNYLD","is_share":true,"footer":"Thread in #biology"}],"blocks":[{"type":"rich_text","block_id":"ZXyur","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(duplicate from other channel(s), sorry if you are flooded)"}]}]}]},{"client_msg_id":"30820e1e-8a1b-454d-8bf9-91effb0fa0f0","type":"message","text":"We just posted a complete draft of our new book, Algorithms for Decision Making:\n<http://algorithmsbook.com/>\nIt uses JuMP.jl for many of the key algorithms. If you have comments on how it can be improved, please let us know!","user":"UBP8QFLBY","ts":"1609909142.001100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"whU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We just posted a complete draft of our new book, Algorithms for Decision Making:\n"},{"type":"link","url":"http://algorithmsbook.com/"},{"type":"text","text":"\nIt uses JuMP.jl for many of the key algorithms. If you have comments on how it can be improved, please let us know!"}]}]}]},{"client_msg_id":"62e44dd3-ff46-45f2-97f8-44e6b3767e79","type":"message","text":"<@UBP8QFLBY> Is it going to be typeset in the same beautiful way as the first book (Algorithms for Opt)? The first book is literally pretty to look at. I keep it on my coffee table at all times.","user":"UKA81L34J","ts":"1609947992.002600","team":"T68168MUP","edited":{"user":"UKA81L34J","ts":"1609948033.000000"},"blocks":[{"type":"rich_text","block_id":"DNYZ","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UBP8QFLBY"},{"type":"text","text":" Is it going to be typeset in the same beautiful way as the first book (Algorithms for Opt)? The first book is literally pretty to look at. I keep it on my coffee table at all times."}]}]}]},{"type":"message","text":"Is someone aware of global optimization methods using local optimizers which work by subdividing the space along first order optimilaty points and their connections along pseudo manifolds(?) where the the gradient is zero except in moving along the manifold. (Root finding on the total derivative, expect in the direction of the step.)\nThis would divide a x^2 - y^2 type sadle point in to 4 seperate regions.\nIf one had such a diving pseudo manifold one would have the basins of attraction for local optimizers.\nI am not convinced that the approach as i described it would be performant but maybe the principle is used in some global optimizers.","user":"U9MD78Z9N","ts":"1610113685.005300","team":"T68168MUP"},{"client_msg_id":"b4bd71a1-55d6-4dd1-bfef-c52758e6059c","type":"message","text":"multistart methods kind of try to do that","user":"U69BL50BF","ts":"1610113777.005600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"a6OC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"multistart methods kind of try to do that"}]}]}]},{"type":"message","text":"Multi start to try start in different basins of attraction but not rigerously identify the basis. (Atleast the multi start methods i learned about in class)","user":"U9MD78Z9N","ts":"1610113836.005700","team":"T68168MUP"},{"client_msg_id":"83be9a4a-64ec-4231-8cf6-2cc124d73400","type":"message","text":"I mean, to try and start in different basis is to heuristically identify a basin and choose another start in the basin.","user":"U69BL50BF","ts":"1610114025.006300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KsSD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I mean, to try and start in different basis is to heuristically identify a basin and choose another start in the basin."}]}]}]},{"type":"message","text":"Mhh, Where can i learn more about those heuristics?","user":"U9MD78Z9N","ts":"1610114051.006400","team":"T68168MUP"},{"client_msg_id":"1a5f09b7-3016-4f94-9278-f4df2c0b5612","type":"message","text":"<https://fguvenendotcom.files.wordpress.com/2019/09/agk2019-september-nber-submit.pdf>","user":"U69BL50BF","ts":"1610114182.006600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"I94","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://fguvenendotcom.files.wordpress.com/2019/09/agk2019-september-nber-submit.pdf"}]}]}]},{"client_msg_id":"6c4ed35a-004a-4887-874b-0e582011f541","type":"message","text":"TikTak is the one people talk about these days","user":"U69BL50BF","ts":"1610114189.006900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yvv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"TikTak is the one people talk about these days"}]}]}]},{"client_msg_id":"81d611a9-10b3-4597-901f-e494a702b9a9","type":"message","text":"it does a quasi-random sample but then uses the local search information","user":"U69BL50BF","ts":"1610114198.007200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"N05vr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it does a quasi-random sample but then uses the local search information"}]}]}]},{"type":"message","text":"In particular like lyapunov stability proofs for an ODE formulation of a local minimizer (like gradient flow) to show that the complete basin indeed converges to some minimum would be epic.","user":"U9MD78Z9N","ts":"1610114210.007300","team":"T68168MUP"},{"client_msg_id":"97b1ef17-9adb-4436-af4e-d6faf6a0c284","type":"message","text":"to me the best bet is to make guesses from a quasi-random sequence.","user":"U69BL50BF","ts":"1610114308.007600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"csd6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"to me the best bet is to make guesses from a quasi-random sequence."}]}]}]},{"type":"message","text":"If understand that correctly it would be guaranteed to find the true solution but it would not able able to certify that any found solution is the optimal one","user":"U9MD78Z9N","ts":"1610114400.007700","team":"T68168MUP"},{"client_msg_id":"654c1c74-292e-42b3-b6ff-d88f694a5ce5","type":"message","text":"yes","user":"U69BL50BF","ts":"1610114552.007900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mB2YI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yes"}]}]}]},{"client_msg_id":"479add28-6003-4983-aa27-24b750fb10b2","type":"message","text":"well","user":"U69BL50BF","ts":"1610114560.008200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7y6tc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"well"}]}]}]},{"client_msg_id":"9ea48a1f-b8b7-44ed-b906-bfbc0eaede26","type":"message","text":"it couldn't guarantee a true solution unless you take infinitely many starts.","user":"U69BL50BF","ts":"1610114578.008600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"drfZX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it couldn't guarantee a true solution unless you take infinitely many starts."}]}]}]},{"type":"message","text":"That paper looks awesome, i add it to my must read list before i write my own NLP solver.","user":"U9MD78Z9N","ts":"1610114611.008700","team":"T68168MUP"},{"client_msg_id":"f5371bae-deb3-42dd-ad90-3a8c232cb06d","type":"message","text":"<@U9MD78Z9N> That sounds similar to <https://arxiv.org/pdf/2011.06505.pdf>","user":"U67G3QRJM","ts":"1610149316.009200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sNOO","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U9MD78Z9N"},{"type":"text","text":" That sounds similar to "},{"type":"link","url":"https://arxiv.org/pdf/2011.06505.pdf"}]}]}]},{"client_msg_id":"cba45e62-a56b-4348-834c-b8cd017ce0d1","type":"message","text":"Also similar to the nudged elastic band method maybe?","user":"U67G3QRJM","ts":"1610149354.009500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eeZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Also similar to the nudged elastic band method maybe?"}]}]}]},{"client_msg_id":"ea070cc8-d1de-45aa-ac42-7afb46fab2ae","type":"message","text":"e.g. <https://arxiv.org/abs/cond-mat/0402209>","user":"U67G3QRJM","ts":"1610149395.009700","team":"T68168MUP","attachments":[{"service_name":"arXiv.org","title":"A Doubly Nudged Elastic Band Method for Finding Transition States","title_link":"https://arxiv.org/abs/cond-mat/0402209","text":"A modification of the nudged elastic band (NEB) method is presented that enables stable optimisations to be run using both the limited-memory quasi-Newton (L-BFGS) and slow-response quenched...","fallback":"arXiv.org: A Doubly Nudged Elastic Band Method for Finding Transition States","from_url":"https://arxiv.org/abs/cond-mat/0402209","service_icon":"https://static.arxiv.org/static/browse/0.3.2.6/images/icons/favicon.ico","id":1,"original_url":"https://arxiv.org/abs/cond-mat/0402209"}],"blocks":[{"type":"rich_text","block_id":"0=R3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"e.g. "},{"type":"link","url":"https://arxiv.org/abs/cond-mat/0402209"}]}]}]},{"client_msg_id":"b83314c0-c1c7-4723-96a4-f77f0281d523","type":"message","text":"What does this tell me?\n```┌ Warning: Failed to achieve finite new evaluation point, using alpha=0\n└ @ LineSearches C:\\Users\\ngudat\\.julia\\packages\\LineSearches\\Ki4c5\\src\\hagerzhang.jl:148```","user":"U7JQGPGCQ","ts":"1610478129.010700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"09CRL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What does this tell me?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"┌ Warning: Failed to achieve finite new evaluation point, using alpha=0\n└ @ LineSearches C:\\Users\\ngudat\\.julia\\packages\\LineSearches\\Ki4c5\\src\\hagerzhang.jl:148"}]}]}]},{"client_msg_id":"b831e653-0c23-43ee-9a1a-8820fd2a1480","type":"message","text":"You objective seems to have returned only non-finite values","user":"U6CJRSR63","ts":"1610487361.011200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OU9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You objective seems to have returned only non-finite values"}]}]}]},{"client_msg_id":"ee43a8f6-47b7-4648-a593-29f7a71ac53d","type":"message","text":"(+/- Inf, NaN)","user":"U6CJRSR63","ts":"1610487371.011500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uKU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(+/- Inf, NaN)"}]}]}]},{"client_msg_id":"8fc758d9-718c-43c5-9d44-49f629f7a8de","type":"message","text":"Is your objective stochastic?","user":"U6CJRSR63","ts":"1610487383.011700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oEB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is your objective stochastic?"}]}]}]},{"client_msg_id":"63d52e5b-6072-45d8-a0ab-ecb1d9ec2c62","type":"message","text":"No, deterministic and (I thought) not very complicated - I have a 60 element target vector and want to find a vector of weights for a 60-by-x matrix such that the matrix*weights product gives a vector that is as close as possible to the target. Weirdly enough it seems to work just fine if I remove some columns in the matrix (which are quite far off of the target), but I would have thought that one could find the same solution if all columns are included (as the ones that are far off should just get zero weights). Anyway for a change the data I'm working on is public so I'll post a reproducer later on.","user":"U7JQGPGCQ","ts":"1610518547.018700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gWr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"No, deterministic and (I thought) not very complicated - I have a 60 element target vector and want to find a vector of weights for a 60-by-x matrix such that the matrix*weights product gives a vector that is as close as possible to the target. Weirdly enough it seems to work just fine if I remove some columns in the matrix (which are quite far off of the target), but I would have thought that one could find the same solution if all columns are included (as the ones that are far off should just get zero weights). Anyway for a change the data I'm working on is public so I'll post a reproducer later on."}]}]}]},{"type":"message","text":"Okay I think I have narrowed it down to the MWE below.\n\nFor context (feel free to skip), this occurs as part of my (still woefully incomplete) SynthControl package, which implements synthetic control estimation. The core of the method is to take a number of potential \"donor\" time series and weight them in such a way that they get as close as possible to a target time series. The full code is here: <https://github.com/nilshg/SynthControl.jl/blob/master/src/SynthControl.jl>, but I've extracted out the relevant bits below:\n\n```using Optim, DelimitedFiles, LinearAlgebra\n\nfunction get_weights(y, xs)\n\n    obj(w) = norm(y .- xs*w) + 1e6*abs(1.0 - sum(w))\n\n    # Initial condition and bounds\n    initial = [1e-5 for _ in 1:size(xs, 2)]\n    lower = zeros(size(xs, 2))\n    upper = ones(size(xs, 2))\n\n    # Get weights through optimization\n    res = optimize(obj, lower, upper, initial)\n\n    return res.minimizer\nend\n\nyᵂ = [100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.7, 100.7, 100.7, 100.7, \n      100.2, 100.2, 100.2, 100.1, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, \n      99.4, 99.4, 99.4, 99.4, 99.3, 99.2, 99.2, 99.2, 99.2, 98.9, 98.9, 98.9, 98.9, 98.9, \n      98.9, 98.9, 98.9, 98.9, 98.9, 98.8, 98.8, 98.8, 98.7, 98.7, 98.6, 98.6, 98.6, 98.3, \n      97.9, 97.9, 97.9, 97.9, 97.8, 97.7, 97.7, 97.7]\n\nxs = readdlm(\"xs.txt\")\n\nget_weights(yᵂ, xs) # Fails - some weights are NaN\nw₁ = get_weights(yᵂ, xs[:, [1:4; 6:end]])\n\nsum(w₁) # ≈ 1.00\nnorm(yᵂ .- xs[:, [1:4; 6:end]]*w₁) # ≈ 0.897```\n","files":[{"id":"F01JZDH0YP3","created":1610533648,"timestamp":1610533648,"name":"xs.txt","title":"xs.txt","mimetype":"text/plain","filetype":"text","pretty_type":"Plain Text","user":"U7JQGPGCQ","editable":true,"size":5899,"mode":"snippet","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01JZDH0YP3/xs.txt","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01JZDH0YP3/download/xs.txt","permalink":"https://julialang.slack.com/files/U7JQGPGCQ/F01JZDH0YP3/xs.txt","permalink_public":"https://slack-files.com/T68168MUP-F01JZDH0YP3-9eb35dde6d","edit_link":"https://julialang.slack.com/files/U7JQGPGCQ/F01JZDH0YP3/xs.txt/edit","preview":"100.0\t100.7\t101.7\t102.1\t96.2\t100.8\t100.3\t99.6\t97.6\t99.8\t99.5\t100.9\t99.0\t101.4\t101.2\t100.9\t102.5\t102.4\n100.0\t101.3\t101.7\t102.1\t95.7\t101.0\t100.3\t99.6\t97.4\t99.7\t99.5\t101.0\t98.2\t101.4\t101.2\t100.9\t102.5\t102.4\n99.9\t101.1\t101.7\t102.1\t97.1\t101.1\t100.3\t99.6\t97.4\t99.8\t99.5\t100.7\t97.9\t101.4\t101.2\t100.9\t102.5\t102.4\n100.0\t101.1\t101.7\t101.9\t97.4\t101.1\t100.7\t99.6\t97.6\t99.8\t99.6\t100.9\t98.0\t101.3\t101.2\t100.6\t102.5\t102.3\n100.0\t101.1\t101.7\t101.9\t97.6\t101.1\t100.7\t99.5\t97.7\t99.9\t99.6\t100.8\t97.9\t100.2\t101.3\t100.6\t102.5\t102.3","preview_highlight":"<div class=\"CodeMirror cm-s-default CodeMirrorServer\" oncopy=\"if(event.clipboardData){event.clipboardData.setData('text/plain',window.getSelection().toString().replace(/\\u200b/g,''));event.preventDefault();event.stopPropagation();}\">\n<div class=\"CodeMirror-code\">\n<div><pre>100.0   100.7   101.7   102.1   96.2    100.8   100.3   99.6    97.6    99.8    99.5    100.9   99.0    101.4   101.2   100.9   102.5   102.4</pre></div>\n<div><pre>100.0   101.3   101.7   102.1   95.7    101.0   100.3   99.6    97.4    99.7    99.5    101.0   98.2    101.4   101.2   100.9   102.5   102.4</pre></div>\n<div><pre>99.9    101.1   101.7   102.1   97.1    101.1   100.3   99.6    97.4    99.8    99.5    100.7   97.9    101.4   101.2   100.9   102.5   102.4</pre></div>\n<div><pre>100.0   101.1   101.7   101.9   97.4    101.1   100.7   99.6    97.6    99.8    99.6    100.9   98.0    101.3   101.2   100.6   102.5   102.3</pre></div>\n<div><pre>100.0   101.1   101.7   101.9   97.6    101.1   100.7   99.5    97.7    99.9    99.6    100.8   97.9    100.2   101.3   100.6   102.5   102.3</pre></div>\n</div>\n</div>\n","lines":61,"lines_more":56,"preview_is_truncated":true,"is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"pMLF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Okay I think I have narrowed it down to the MWE below.\n\nFor context (feel free to skip), this occurs as part of my (still woefully incomplete) SynthControl package, which implements synthetic control estimation. The core of the method is to take a number of potential \"donor\" time series and weight them in such a way that they get as close as possible to a target time series. The full code is here: "},{"type":"link","url":"https://github.com/nilshg/SynthControl.jl/blob/master/src/SynthControl.jl"},{"type":"text","text":", but I've extracted out the relevant bits below:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Optim, DelimitedFiles, LinearAlgebra\n\nfunction get_weights(y, xs)\n\n    obj(w) = norm(y .- xs*w) + 1e6*abs(1.0 - sum(w))\n\n    # Initial condition and bounds\n    initial = [1e-5 for _ in 1:size(xs, 2)]\n    lower = zeros(size(xs, 2))\n    upper = ones(size(xs, 2))\n\n    # Get weights through optimization\n    res = optimize(obj, lower, upper, initial)\n\n    return res.minimizer\nend\n\nyᵂ = [100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.7, 100.7, 100.7, 100.7, \n      100.2, 100.2, 100.2, 100.1, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, \n      99.4, 99.4, 99.4, 99.4, 99.3, 99.2, 99.2, 99.2, 99.2, 98.9, 98.9, 98.9, 98.9, 98.9, \n      98.9, 98.9, 98.9, 98.9, 98.9, 98.8, 98.8, 98.8, 98.7, 98.7, 98.6, 98.6, 98.6, 98.3, \n      97.9, 97.9, 97.9, 97.9, 97.8, 97.7, 97.7, 97.7]\n\nxs = readdlm(\"xs.txt\")\n\nget_weights(yᵂ, xs) # Fails - some weights are NaN\nw₁ = get_weights(yᵂ, xs[:, [1:4; 6:end]])\n\nsum(w₁) # ≈ 1.00\nnorm(yᵂ .- xs[:, [1:4; 6:end]]*w₁) # ≈ 0.897"}]},{"type":"rich_text_section","elements":[]}]}],"user":"U7JQGPGCQ","display_as_bot":false,"ts":"1610533782.021300"},{"client_msg_id":"457b7f4d-83ab-4060-8556-e4fa4db49c4d","type":"message","text":"When using the full `xs` matrix with 18 columns, some of the weights end up being `NaN`. Removing one of the columns makes the whole thing work. I would have thought that adding a column shouldn't make the fit worse (as in the worst case one could just set that weight to zero), but it seems that here it's throwing everything off. Any ideas how this is happening, and how to deal with it?","user":"U7JQGPGCQ","ts":"1610533931.023700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"r8zK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"When using the full "},{"type":"text","text":"xs","style":{"code":true}},{"type":"text","text":" matrix with 18 columns, some of the weights end up being "},{"type":"text","text":"NaN","style":{"code":true}},{"type":"text","text":". Removing one of the columns makes the whole thing work. I would have thought that adding a column shouldn't make the fit worse (as in the worst case one could just set that weight to zero), but it seems that here it's throwing everything off. Any ideas how this is happening, and how to deal with it?"}]}]}]},{"client_msg_id":"de6496ce-313b-46b2-94bf-800426b1921f","type":"message","text":"Hi all. I have a non-linear least-squares problem where I have two functions: one that computes the residuals; and another that computes both the residuals and the Jacobian (that is being wrapped to only return the Jacobian). I'm currently using `LsqFit.jl`. My question is: for performance, is there a way to take advantage of the fact that when the Jacobian function is called, the residuals are also computed?","user":"U6CCK2SCV","ts":"1610627374.005100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JJCI2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi all. I have a non-linear least-squares problem where I have two functions: one that computes the residuals; and another that computes both the residuals and the Jacobian (that is being wrapped to only return the Jacobian). I'm currently using "},{"type":"text","text":"LsqFit.jl","style":{"code":true}},{"type":"text","text":". My question is: for performance, is there a way to take advantage of the fact that when the Jacobian function is called, the residuals are also computed?"}]}]}]},{"client_msg_id":"f109e998-b448-4a49-bb76-7b6bd111da4f","type":"message","text":"David Sanders is giving a GERAD seminar on global optimization with interval arithmetic tomorrow at 11AM EST:\n<https://www.gerad.ca/fr/events/1843>\nAll welcome!","user":"UB4KR33H9","ts":"1610669094.005700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ut2Jr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"David Sanders is giving a GERAD seminar on global optimization with interval arithmetic tomorrow at 11AM EST:\n"},{"type":"link","url":"https://www.gerad.ca/fr/events/1843"},{"type":"text","text":"\nAll welcome!"}]}]}],"thread_ts":"1610669094.005700","reply_count":1,"reply_users_count":1,"latest_reply":"1610669861.006000","reply_users":["U01FR2HFJ7M"],"subscribed":false,"reactions":[{"name":"+1","users":["U01FR2HFJ7M","UCZ7VBGUD"],"count":2}]}]}