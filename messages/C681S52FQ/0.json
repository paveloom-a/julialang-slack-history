{"cursor": 0, "messages": [{"client_msg_id":"93950008-5b30-441c-aef2-bd62e193694a","type":"message","text":"Do we have something like Optim but supporting `CuArray`s? I have a problem where L-BFGS outperforms Flux optimizers, but I would like to move the model computation to the GPU. Transferring the data from GPU to CPU at every iteration seems expensive, but I couldn't find a GPU friendly quasi newton solver.","user":"U6BJ9E351","ts":"1607982881.353800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ypZBE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do we have something like Optim but supporting "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":"s? I have a problem where L-BFGS outperforms Flux optimizers, but I would like to move the model computation to the GPU. Transferring the data from GPU to CPU at every iteration seems expensive, but I couldn't find a GPU friendly quasi newton solver."}]}]}],"thread_ts":"1607982881.353800","reply_count":30,"reply_users_count":3,"latest_reply":"1608044342.360600","reply_users":["U6CJRSR63","U6BJ9E351","U67G3QRJM"],"subscribed":false,"reactions":[{"name":"+1","users":["UN2U72Q3F"],"count":1}]},{"client_msg_id":"a848af70-d82c-4352-9cf8-6c80845ab575","type":"message","text":"Are there any packages for finding saddle-points (e.g. min-max) of a function?","user":"U7YD3DKL2","ts":"1608129306.361800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QhgF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Are there any packages for finding saddle-points (e.g. min-max) of a function?"}]}]}],"thread_ts":"1608129306.361800","reply_count":6,"reply_users_count":2,"latest_reply":"1608438612.364900","reply_users":["U67G3QRJM","U7YD3DKL2"],"subscribed":false},{"client_msg_id":"5edce411-dc98-4a38-9d92-d1843a56a570","type":"message","text":"Good question. Are you looking for any specific methods?","user":"U6CJRSR63","ts":"1608132188.362200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=LVn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Good question. Are you looking for any specific methods?"}]}]}],"thread_ts":"1608132188.362200","reply_count":1,"reply_users_count":1,"latest_reply":"1608441112.365100","reply_users":["U7YD3DKL2"],"subscribed":false},{"client_msg_id":"e44aebf7-c018-4772-b33f-e9c3726f6b2f","type":"message","text":"I just joined this channel to ask that question haha. :see_no_evil:","user":"U01A0S07875","ts":"1608397927.364400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Prj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I just joined this channel to ask that question haha. "},{"type":"emoji","name":"see_no_evil"}]}]}],"thread_ts":"1608397927.364400","reply_count":1,"reply_users_count":1,"latest_reply":"1608410390.364700","reply_users":["U9MD78Z9N"],"subscribed":false},{"type":"message","text":"Assuming i want to minimize a third degree taylor expansion (which is non convex) of an sufficently nice non-linear function.\nI can express the minimization of a multinomial of third degree as the root finding of a second degree multi variate polynomial, with some  conditions to not find maximas/saddle points and maybe some trust region style constraint. \nAre you aware of algorithm that do that and strategely construct roots to jump in to different local minima in order to find the best one?","user":"U9MD78Z9N","ts":"1609636138.367100","team":"T68168MUP"},{"type":"message","text":"(duplicate from other channel(s), sorry if you are flooded)","user":"U01FR2HFJ7M","ts":"1609830788.368700","team":"T68168MUP","attachments":[{"fallback":"[January 5th, 2021 2:27 PM] jf: Any idea where I can chat/contact people working/using Evolutionary.jl (other than GH issues)?","ts":"1609824439.191600","author_id":"U01FR2HFJ7M","author_subname":"Azzaare","channel_id":"CAKKFNYLD","channel_name":"biology","is_msg_unfurl":true,"is_thread_root_unfurl":true,"text":"Any idea where I can chat/contact people working/using Evolutionary.jl (other than GH issues)?","author_name":"Azzaare","author_link":"https://julialang.slack.com/team/U01FR2HFJ7M","author_icon":"https://avatars.slack-edge.com/2020-11-30/1535756085650_d8d021e24a800bfb16b8_48.jpg","mrkdwn_in":["text"],"color":"D0D0D0","from_url":"https://julialang.slack.com/archives/CAKKFNYLD/p1609824439191600?thread_ts=1609824439191600&cid=CAKKFNYLD","is_share":true,"footer":"Thread in #biology"}],"blocks":[{"type":"rich_text","block_id":"ZXyur","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(duplicate from other channel(s), sorry if you are flooded)"}]}]}]},{"client_msg_id":"30820e1e-8a1b-454d-8bf9-91effb0fa0f0","type":"message","text":"We just posted a complete draft of our new book, Algorithms for Decision Making:\n<http://algorithmsbook.com/>\nIt uses JuMP.jl for many of the key algorithms. If you have comments on how it can be improved, please let us know!","user":"UBP8QFLBY","ts":"1609909142.001100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"whU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We just posted a complete draft of our new book, Algorithms for Decision Making:\n"},{"type":"link","url":"http://algorithmsbook.com/"},{"type":"text","text":"\nIt uses JuMP.jl for many of the key algorithms. If you have comments on how it can be improved, please let us know!"}]}]}]},{"client_msg_id":"62e44dd3-ff46-45f2-97f8-44e6b3767e79","type":"message","text":"<@UBP8QFLBY> Is it going to be typeset in the same beautiful way as the first book (Algorithms for Opt)? The first book is literally pretty to look at. I keep it on my coffee table at all times.","user":"UKA81L34J","ts":"1609947992.002600","team":"T68168MUP","edited":{"user":"UKA81L34J","ts":"1609948033.000000"},"blocks":[{"type":"rich_text","block_id":"DNYZ","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UBP8QFLBY"},{"type":"text","text":" Is it going to be typeset in the same beautiful way as the first book (Algorithms for Opt)? The first book is literally pretty to look at. I keep it on my coffee table at all times."}]}]}]},{"type":"message","text":"Is someone aware of global optimization methods using local optimizers which work by subdividing the space along first order optimilaty points and their connections along pseudo manifolds(?) where the the gradient is zero except in moving along the manifold. (Root finding on the total derivative, expect in the direction of the step.)\nThis would divide a x^2 - y^2 type sadle point in to 4 seperate regions.\nIf one had such a diving pseudo manifold one would have the basins of attraction for local optimizers.\nI am not convinced that the approach as i described it would be performant but maybe the principle is used in some global optimizers.","user":"U9MD78Z9N","ts":"1610113685.005300","team":"T68168MUP"},{"client_msg_id":"b4bd71a1-55d6-4dd1-bfef-c52758e6059c","type":"message","text":"multistart methods kind of try to do that","user":"U69BL50BF","ts":"1610113777.005600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"a6OC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"multistart methods kind of try to do that"}]}]}]},{"type":"message","text":"Multi start to try start in different basins of attraction but not rigerously identify the basis. (Atleast the multi start methods i learned about in class)","user":"U9MD78Z9N","ts":"1610113836.005700","team":"T68168MUP"},{"client_msg_id":"83be9a4a-64ec-4231-8cf6-2cc124d73400","type":"message","text":"I mean, to try and start in different basis is to heuristically identify a basin and choose another start in the basin.","user":"U69BL50BF","ts":"1610114025.006300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KsSD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I mean, to try and start in different basis is to heuristically identify a basin and choose another start in the basin."}]}]}]},{"type":"message","text":"Mhh, Where can i learn more about those heuristics?","user":"U9MD78Z9N","ts":"1610114051.006400","team":"T68168MUP"},{"client_msg_id":"1a5f09b7-3016-4f94-9278-f4df2c0b5612","type":"message","text":"<https://fguvenendotcom.files.wordpress.com/2019/09/agk2019-september-nber-submit.pdf>","user":"U69BL50BF","ts":"1610114182.006600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"I94","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://fguvenendotcom.files.wordpress.com/2019/09/agk2019-september-nber-submit.pdf"}]}]}]},{"client_msg_id":"6c4ed35a-004a-4887-874b-0e582011f541","type":"message","text":"TikTak is the one people talk about these days","user":"U69BL50BF","ts":"1610114189.006900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yvv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"TikTak is the one people talk about these days"}]}]}]},{"client_msg_id":"81d611a9-10b3-4597-901f-e494a702b9a9","type":"message","text":"it does a quasi-random sample but then uses the local search information","user":"U69BL50BF","ts":"1610114198.007200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"N05vr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it does a quasi-random sample but then uses the local search information"}]}]}]},{"type":"message","text":"In particular like lyapunov stability proofs for an ODE formulation of a local minimizer (like gradient flow) to show that the complete basin indeed converges to some minimum would be epic.","user":"U9MD78Z9N","ts":"1610114210.007300","team":"T68168MUP"},{"client_msg_id":"97b1ef17-9adb-4436-af4e-d6faf6a0c284","type":"message","text":"to me the best bet is to make guesses from a quasi-random sequence.","user":"U69BL50BF","ts":"1610114308.007600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"csd6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"to me the best bet is to make guesses from a quasi-random sequence."}]}]}]},{"type":"message","text":"If understand that correctly it would be guaranteed to find the true solution but it would not able able to certify that any found solution is the optimal one","user":"U9MD78Z9N","ts":"1610114400.007700","team":"T68168MUP"},{"client_msg_id":"654c1c74-292e-42b3-b6ff-d88f694a5ce5","type":"message","text":"yes","user":"U69BL50BF","ts":"1610114552.007900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mB2YI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yes"}]}]}]},{"client_msg_id":"479add28-6003-4983-aa27-24b750fb10b2","type":"message","text":"well","user":"U69BL50BF","ts":"1610114560.008200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7y6tc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"well"}]}]}]},{"client_msg_id":"9ea48a1f-b8b7-44ed-b906-bfbc0eaede26","type":"message","text":"it couldn't guarantee a true solution unless you take infinitely many starts.","user":"U69BL50BF","ts":"1610114578.008600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"drfZX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it couldn't guarantee a true solution unless you take infinitely many starts."}]}]}]},{"type":"message","text":"That paper looks awesome, i add it to my must read list before i write my own NLP solver.","user":"U9MD78Z9N","ts":"1610114611.008700","team":"T68168MUP"},{"client_msg_id":"f5371bae-deb3-42dd-ad90-3a8c232cb06d","type":"message","text":"<@U9MD78Z9N> That sounds similar to <https://arxiv.org/pdf/2011.06505.pdf>","user":"U67G3QRJM","ts":"1610149316.009200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sNOO","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U9MD78Z9N"},{"type":"text","text":" That sounds similar to "},{"type":"link","url":"https://arxiv.org/pdf/2011.06505.pdf"}]}]}]},{"client_msg_id":"cba45e62-a56b-4348-834c-b8cd017ce0d1","type":"message","text":"Also similar to the nudged elastic band method maybe?","user":"U67G3QRJM","ts":"1610149354.009500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eeZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Also similar to the nudged elastic band method maybe?"}]}]}]},{"client_msg_id":"ea070cc8-d1de-45aa-ac42-7afb46fab2ae","type":"message","text":"e.g. <https://arxiv.org/abs/cond-mat/0402209>","user":"U67G3QRJM","ts":"1610149395.009700","team":"T68168MUP","attachments":[{"service_name":"arXiv.org","title":"A Doubly Nudged Elastic Band Method for Finding Transition States","title_link":"https://arxiv.org/abs/cond-mat/0402209","text":"A modification of the nudged elastic band (NEB) method is presented that enables stable optimisations to be run using both the limited-memory quasi-Newton (L-BFGS) and slow-response quenched...","fallback":"arXiv.org: A Doubly Nudged Elastic Band Method for Finding Transition States","from_url":"https://arxiv.org/abs/cond-mat/0402209","service_icon":"https://static.arxiv.org/static/browse/0.3.2.6/images/icons/favicon.ico","id":1,"original_url":"https://arxiv.org/abs/cond-mat/0402209"}],"blocks":[{"type":"rich_text","block_id":"0=R3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"e.g. "},{"type":"link","url":"https://arxiv.org/abs/cond-mat/0402209"}]}]}]},{"client_msg_id":"b83314c0-c1c7-4723-96a4-f77f0281d523","type":"message","text":"What does this tell me?\n```┌ Warning: Failed to achieve finite new evaluation point, using alpha=0\n└ @ LineSearches C:\\Users\\ngudat\\.julia\\packages\\LineSearches\\Ki4c5\\src\\hagerzhang.jl:148```","user":"U7JQGPGCQ","ts":"1610478129.010700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"09CRL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What does this tell me?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"┌ Warning: Failed to achieve finite new evaluation point, using alpha=0\n└ @ LineSearches C:\\Users\\ngudat\\.julia\\packages\\LineSearches\\Ki4c5\\src\\hagerzhang.jl:148"}]}]}]},{"client_msg_id":"b831e653-0c23-43ee-9a1a-8820fd2a1480","type":"message","text":"You objective seems to have returned only non-finite values","user":"U6CJRSR63","ts":"1610487361.011200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OU9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You objective seems to have returned only non-finite values"}]}]}]},{"client_msg_id":"ee43a8f6-47b7-4648-a593-29f7a71ac53d","type":"message","text":"(+/- Inf, NaN)","user":"U6CJRSR63","ts":"1610487371.011500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uKU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(+/- Inf, NaN)"}]}]}]},{"client_msg_id":"8fc758d9-718c-43c5-9d44-49f629f7a8de","type":"message","text":"Is your objective stochastic?","user":"U6CJRSR63","ts":"1610487383.011700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oEB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is your objective stochastic?"}]}]}]},{"client_msg_id":"63d52e5b-6072-45d8-a0ab-ecb1d9ec2c62","type":"message","text":"No, deterministic and (I thought) not very complicated - I have a 60 element target vector and want to find a vector of weights for a 60-by-x matrix such that the matrix*weights product gives a vector that is as close as possible to the target. Weirdly enough it seems to work just fine if I remove some columns in the matrix (which are quite far off of the target), but I would have thought that one could find the same solution if all columns are included (as the ones that are far off should just get zero weights). Anyway for a change the data I'm working on is public so I'll post a reproducer later on.","user":"U7JQGPGCQ","ts":"1610518547.018700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gWr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"No, deterministic and (I thought) not very complicated - I have a 60 element target vector and want to find a vector of weights for a 60-by-x matrix such that the matrix*weights product gives a vector that is as close as possible to the target. Weirdly enough it seems to work just fine if I remove some columns in the matrix (which are quite far off of the target), but I would have thought that one could find the same solution if all columns are included (as the ones that are far off should just get zero weights). Anyway for a change the data I'm working on is public so I'll post a reproducer later on."}]}]}]},{"type":"message","text":"Okay I think I have narrowed it down to the MWE below.\n\nFor context (feel free to skip), this occurs as part of my (still woefully incomplete) SynthControl package, which implements synthetic control estimation. The core of the method is to take a number of potential \"donor\" time series and weight them in such a way that they get as close as possible to a target time series. The full code is here: <https://github.com/nilshg/SynthControl.jl/blob/master/src/SynthControl.jl>, but I've extracted out the relevant bits below:\n\n```using Optim, DelimitedFiles, LinearAlgebra\n\nfunction get_weights(y, xs)\n\n    obj(w) = norm(y .- xs*w) + 1e6*abs(1.0 - sum(w))\n\n    # Initial condition and bounds\n    initial = [1e-5 for _ in 1:size(xs, 2)]\n    lower = zeros(size(xs, 2))\n    upper = ones(size(xs, 2))\n\n    # Get weights through optimization\n    res = optimize(obj, lower, upper, initial)\n\n    return res.minimizer\nend\n\nyᵂ = [100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.7, 100.7, 100.7, 100.7, \n      100.2, 100.2, 100.2, 100.1, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, \n      99.4, 99.4, 99.4, 99.4, 99.3, 99.2, 99.2, 99.2, 99.2, 98.9, 98.9, 98.9, 98.9, 98.9, \n      98.9, 98.9, 98.9, 98.9, 98.9, 98.8, 98.8, 98.8, 98.7, 98.7, 98.6, 98.6, 98.6, 98.3, \n      97.9, 97.9, 97.9, 97.9, 97.8, 97.7, 97.7, 97.7]\n\nxs = readdlm(\"xs.txt\")\n\nget_weights(yᵂ, xs) # Fails - some weights are NaN\nw₁ = get_weights(yᵂ, xs[:, [1:4; 6:end]])\n\nsum(w₁) # ≈ 1.00\nnorm(yᵂ .- xs[:, [1:4; 6:end]]*w₁) # ≈ 0.897```\n","files":[{"id":"F01JZDH0YP3","created":1610533648,"timestamp":1610533648,"name":"xs.txt","title":"xs.txt","mimetype":"text/plain","filetype":"text","pretty_type":"Plain Text","user":"U7JQGPGCQ","editable":true,"size":5899,"mode":"snippet","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01JZDH0YP3/xs.txt","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01JZDH0YP3/download/xs.txt","permalink":"https://julialang.slack.com/files/U7JQGPGCQ/F01JZDH0YP3/xs.txt","permalink_public":"https://slack-files.com/T68168MUP-F01JZDH0YP3-9eb35dde6d","edit_link":"https://julialang.slack.com/files/U7JQGPGCQ/F01JZDH0YP3/xs.txt/edit","preview":"100.0\t100.7\t101.7\t102.1\t96.2\t100.8\t100.3\t99.6\t97.6\t99.8\t99.5\t100.9\t99.0\t101.4\t101.2\t100.9\t102.5\t102.4\n100.0\t101.3\t101.7\t102.1\t95.7\t101.0\t100.3\t99.6\t97.4\t99.7\t99.5\t101.0\t98.2\t101.4\t101.2\t100.9\t102.5\t102.4\n99.9\t101.1\t101.7\t102.1\t97.1\t101.1\t100.3\t99.6\t97.4\t99.8\t99.5\t100.7\t97.9\t101.4\t101.2\t100.9\t102.5\t102.4\n100.0\t101.1\t101.7\t101.9\t97.4\t101.1\t100.7\t99.6\t97.6\t99.8\t99.6\t100.9\t98.0\t101.3\t101.2\t100.6\t102.5\t102.3\n100.0\t101.1\t101.7\t101.9\t97.6\t101.1\t100.7\t99.5\t97.7\t99.9\t99.6\t100.8\t97.9\t100.2\t101.3\t100.6\t102.5\t102.3","preview_highlight":"<div class=\"CodeMirror cm-s-default CodeMirrorServer\" oncopy=\"if(event.clipboardData){event.clipboardData.setData('text/plain',window.getSelection().toString().replace(/\\u200b/g,''));event.preventDefault();event.stopPropagation();}\">\n<div class=\"CodeMirror-code\">\n<div><pre>100.0   100.7   101.7   102.1   96.2    100.8   100.3   99.6    97.6    99.8    99.5    100.9   99.0    101.4   101.2   100.9   102.5   102.4</pre></div>\n<div><pre>100.0   101.3   101.7   102.1   95.7    101.0   100.3   99.6    97.4    99.7    99.5    101.0   98.2    101.4   101.2   100.9   102.5   102.4</pre></div>\n<div><pre>99.9    101.1   101.7   102.1   97.1    101.1   100.3   99.6    97.4    99.8    99.5    100.7   97.9    101.4   101.2   100.9   102.5   102.4</pre></div>\n<div><pre>100.0   101.1   101.7   101.9   97.4    101.1   100.7   99.6    97.6    99.8    99.6    100.9   98.0    101.3   101.2   100.6   102.5   102.3</pre></div>\n<div><pre>100.0   101.1   101.7   101.9   97.6    101.1   100.7   99.5    97.7    99.9    99.6    100.8   97.9    100.2   101.3   100.6   102.5   102.3</pre></div>\n</div>\n</div>\n","lines":61,"lines_more":56,"preview_is_truncated":true,"is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"pMLF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Okay I think I have narrowed it down to the MWE below.\n\nFor context (feel free to skip), this occurs as part of my (still woefully incomplete) SynthControl package, which implements synthetic control estimation. The core of the method is to take a number of potential \"donor\" time series and weight them in such a way that they get as close as possible to a target time series. The full code is here: "},{"type":"link","url":"https://github.com/nilshg/SynthControl.jl/blob/master/src/SynthControl.jl"},{"type":"text","text":", but I've extracted out the relevant bits below:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Optim, DelimitedFiles, LinearAlgebra\n\nfunction get_weights(y, xs)\n\n    obj(w) = norm(y .- xs*w) + 1e6*abs(1.0 - sum(w))\n\n    # Initial condition and bounds\n    initial = [1e-5 for _ in 1:size(xs, 2)]\n    lower = zeros(size(xs, 2))\n    upper = ones(size(xs, 2))\n\n    # Get weights through optimization\n    res = optimize(obj, lower, upper, initial)\n\n    return res.minimizer\nend\n\nyᵂ = [100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.7, 100.7, 100.7, 100.7, \n      100.2, 100.2, 100.2, 100.1, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, \n      99.4, 99.4, 99.4, 99.4, 99.3, 99.2, 99.2, 99.2, 99.2, 98.9, 98.9, 98.9, 98.9, 98.9, \n      98.9, 98.9, 98.9, 98.9, 98.9, 98.8, 98.8, 98.8, 98.7, 98.7, 98.6, 98.6, 98.6, 98.3, \n      97.9, 97.9, 97.9, 97.9, 97.8, 97.7, 97.7, 97.7]\n\nxs = readdlm(\"xs.txt\")\n\nget_weights(yᵂ, xs) # Fails - some weights are NaN\nw₁ = get_weights(yᵂ, xs[:, [1:4; 6:end]])\n\nsum(w₁) # ≈ 1.00\nnorm(yᵂ .- xs[:, [1:4; 6:end]]*w₁) # ≈ 0.897"}]},{"type":"rich_text_section","elements":[]}]}],"user":"U7JQGPGCQ","display_as_bot":false,"ts":"1610533782.021300"},{"client_msg_id":"457b7f4d-83ab-4060-8556-e4fa4db49c4d","type":"message","text":"When using the full `xs` matrix with 18 columns, some of the weights end up being `NaN`. Removing one of the columns makes the whole thing work. I would have thought that adding a column shouldn't make the fit worse (as in the worst case one could just set that weight to zero), but it seems that here it's throwing everything off. Any ideas how this is happening, and how to deal with it?","user":"U7JQGPGCQ","ts":"1610533931.023700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"r8zK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"When using the full "},{"type":"text","text":"xs","style":{"code":true}},{"type":"text","text":" matrix with 18 columns, some of the weights end up being "},{"type":"text","text":"NaN","style":{"code":true}},{"type":"text","text":". Removing one of the columns makes the whole thing work. I would have thought that adding a column shouldn't make the fit worse (as in the worst case one could just set that weight to zero), but it seems that here it's throwing everything off. Any ideas how this is happening, and how to deal with it?"}]}]}]},{"client_msg_id":"de6496ce-313b-46b2-94bf-800426b1921f","type":"message","text":"Hi all. I have a non-linear least-squares problem where I have two functions: one that computes the residuals; and another that computes both the residuals and the Jacobian (that is being wrapped to only return the Jacobian). I'm currently using `LsqFit.jl`. My question is: for performance, is there a way to take advantage of the fact that when the Jacobian function is called, the residuals are also computed?","user":"U6CCK2SCV","ts":"1610627374.005100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JJCI2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi all. I have a non-linear least-squares problem where I have two functions: one that computes the residuals; and another that computes both the residuals and the Jacobian (that is being wrapped to only return the Jacobian). I'm currently using "},{"type":"text","text":"LsqFit.jl","style":{"code":true}},{"type":"text","text":". My question is: for performance, is there a way to take advantage of the fact that when the Jacobian function is called, the residuals are also computed?"}]}]}]},{"client_msg_id":"f109e998-b448-4a49-bb76-7b6bd111da4f","type":"message","text":"David Sanders is giving a GERAD seminar on global optimization with interval arithmetic tomorrow at 11AM EST:\n<https://www.gerad.ca/fr/events/1843>\nAll welcome!","user":"UB4KR33H9","ts":"1610669094.005700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ut2Jr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"David Sanders is giving a GERAD seminar on global optimization with interval arithmetic tomorrow at 11AM EST:\n"},{"type":"link","url":"https://www.gerad.ca/fr/events/1843"},{"type":"text","text":"\nAll welcome!"}]}]}],"thread_ts":"1610669094.005700","reply_count":1,"reply_users_count":1,"latest_reply":"1610669861.006000","reply_users":["U01FR2HFJ7M"],"subscribed":false,"reactions":[{"name":"+1","users":["U01FR2HFJ7M","UCZ7VBGUD"],"count":2}]},{"type":"message","text":"","user":"U01FR2HFJ7M","ts":"1610708908.006600","team":"T68168MUP","attachments":[{"fallback":"[January 15th, 2021 8:06 PM] jf: Hi there! I am very proud to announce the first beta release of LocalSearchSolvers.jl, a framework for CBLS (Constraint-Based Local Search) solvers in pure Julia, hosted on the GitHub org JuliaConstraints.\nIts goal is to solve satisfaction and optimization problems where models are formulated as a simple triplet of variables domains, constraints, and optional objectives.\nDue to high scalability, CBLS can have more than linear speedup with parallelization (though not yet implemented here), and many other options are coming such as dynamic models, DSL for JuMP and Minizinc, automatic parameter tuning and such.","ts":"1610708786.007900","author_id":"U01FR2HFJ7M","author_subname":"Azzaare","channel_id":"CDPQVTD4L","channel_name":"pkg-announcements","is_msg_unfurl":true,"text":"Hi there! I am very proud to announce the first beta release of LocalSearchSolvers.jl, a framework for CBLS (Constraint-Based Local Search) solvers in pure Julia, hosted on the GitHub org JuliaConstraints.\nIts goal is to solve satisfaction and optimization problems where models are formulated as a simple triplet of variables domains, constraints, and optional objectives.\nDue to high scalability, CBLS can have more than linear speedup with parallelization (though not yet implemented here), and many other options are coming such as dynamic models, DSL for JuMP and Minizinc, automatic parameter tuning and such.","author_name":"Azzaare","author_link":"https://julialang.slack.com/team/U01FR2HFJ7M","author_icon":"https://avatars.slack-edge.com/2020-11-30/1535756085650_d8d021e24a800bfb16b8_48.jpg","mrkdwn_in":["text"],"files":[{"id":"F01JDB0FXRD","created":1610708490,"timestamp":1610708490,"name":"sudoku3x3.png","title":"sudoku3x3.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01FR2HFJ7M","editable":false,"size":54164,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01JDB0FXRD/sudoku3x3.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01JDB0FXRD/download/sudoku3x3.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01JDB0FXRD-04bd60c462/sudoku3x3_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01JDB0FXRD-04bd60c462/sudoku3x3_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01JDB0FXRD-04bd60c462/sudoku3x3_360.png","thumb_360_w":360,"thumb_360_h":131,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01JDB0FXRD-04bd60c462/sudoku3x3_480.png","thumb_480_w":480,"thumb_480_h":174,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01JDB0FXRD-04bd60c462/sudoku3x3_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01JDB0FXRD-04bd60c462/sudoku3x3_720.png","thumb_720_w":720,"thumb_720_h":262,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01JDB0FXRD-04bd60c462/sudoku3x3_800.png","thumb_800_w":800,"thumb_800_h":291,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01JDB0FXRD-04bd60c462/sudoku3x3_960.png","thumb_960_w":960,"thumb_960_h":349,"thumb_1024":"https://files.slack.com/files-tmb/T68168MUP-F01JDB0FXRD-04bd60c462/sudoku3x3_1024.png","thumb_1024_w":1024,"thumb_1024_h":372,"original_w":1717,"original_h":624,"thumb_tiny":"AwARADCh26UHgdKKD0oAbn2FGfYUlFAC5ozSUUAPpD0paQ9KAG0UUUAFFFFAH//Z","permalink":"https://julialang.slack.com/files/U01FR2HFJ7M/F01JDB0FXRD/sudoku3x3.png","permalink_public":"https://slack-files.com/T68168MUP-F01JDB0FXRD-3746801d05","is_starred":false,"has_rich_preview":false}],"color":"D0D0D0","from_url":"https://julialang.slack.com/archives/CDPQVTD4L/p1610708786007900","is_share":true,"footer":"Posted in #pkg-announcements"}],"blocks":[{"type":"rich_text","block_id":"1Gx","elements":[{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"ebded6dd-9285-41c2-b64d-555de5d51359","type":"message","text":"I may be missing something simple, but is there a way to save the intermediate values of the parameters during each iteration with Optim.jl?\n\nFor example, I'm doing 100 iterations of LBFGS and would like to save 100 arrays with the values of parameters being optimized. The `callback` option seems to only have access to the objective function value. If I change `f` by hand to add a callback there, I would get more \"timestamps\" than iterations.","user":"U6BJ9E351","ts":"1611661662.003800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IJAC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I may be missing something simple, but is there a way to save the intermediate values of the parameters during each iteration with Optim.jl?\n\nFor example, I'm doing 100 iterations of LBFGS and would like to save 100 arrays with the values of parameters being optimized. The "},{"type":"text","text":"callback","style":{"code":true}},{"type":"text","text":" option seems to only have access to the objective function value. If I change "},{"type":"text","text":"f","style":{"code":true}},{"type":"text","text":" by hand to add a callback there, I would get more \"timestamps\" than iterations."}]}]}]},{"client_msg_id":"e0df1788-feb9-493c-ba98-044d1493498d","type":"message","text":"(I understand doing this is costly in general, but I just need it for demo purposes)","user":"U6BJ9E351","ts":"1611661688.004300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kRC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(I understand doing this is costly in general, but I just need it for demo purposes)"}]}]}]},{"client_msg_id":"829869aa-18fb-49f2-9c23-9d69880fa3f0","type":"message","text":"You have to turn on the extended trace","user":"U6CJRSR63","ts":"1611670611.004500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"T24p","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You have to turn on the extended trace"}]}]}]},{"client_msg_id":"08d440a2-bd84-40e0-922e-992249479f07","type":"message","text":"Then you will have `\"x\"` in the dictionary passed to the callback","user":"U6CJRSR63","ts":"1611670625.004800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UWb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Then you will have "},{"type":"text","text":"\"x\"","style":{"code":true}},{"type":"text","text":" in the dictionary passed to the callback"}]}]}]},{"client_msg_id":"ae87e240-1d33-4e61-b257-041cd78e2a99","type":"message","text":"```using OptimTestProblems, Optim\nproblem = MultivariateProblems.UnconstrainedProblems.examples[\"Rosenbrock\"]\n\nf = MultivariateProblems.objective(problem)\ng! = MultivariateProblems.gradient(problem)\ninitial_x = problem.initial_x\nd2 = OnceDifferentiable(f, g!, initial_x)\n\nmethod = LBFGS()\n\nxs = []\ncb = tr -&gt; begin\n            push!(xs, tr[end].metadata[\"x\"])\n            false\n        end\n\noptions = Optim.Options(callback = cb, show_every=3, store_trace=true, extended_trace=true)\n\noptimize(d2, initial_x, method, options)\n\nxs```","user":"U6CJRSR63","ts":"1611671338.005000","team":"T68168MUP","edited":{"user":"U6CJRSR63","ts":"1611671403.000000"},"blocks":[{"type":"rich_text","block_id":"Lsqh","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using OptimTestProblems, Optim\nproblem = MultivariateProblems.UnconstrainedProblems.examples[\"Rosenbrock\"]\n\nf = MultivariateProblems.objective(problem)\ng! = MultivariateProblems.gradient(problem)\ninitial_x = problem.initial_x\nd2 = OnceDifferentiable(f, g!, initial_x)\n\nmethod = LBFGS()\n\nxs = []\ncb = tr -> begin\n            push!(xs, tr[end].metadata[\"x\"])\n            false\n        end\n\noptions = Optim.Options(callback = cb, show_every=3, store_trace=true, extended_trace=true)\n\noptimize(d2, initial_x, method, options)\n\nxs"}]}]}]},{"client_msg_id":"a5593bba-c28b-4893-8d16-9f289f0b6ab3","type":"message","text":"```julia&gt; xs\n10-element Array{Any,1}:\n [-1.2, 1.0]\n [-1.0273245872918702, 1.0519171772443916]\n [-0.5778727732836022, 0.30910840092006675]\n [-0.18592337280782262, 0.024501100158156375]\n [0.08298376142192398, -0.02506995929967343]\n [0.541793804618421, 0.2806815781121066]\n [0.7742123596652257, 0.5990640879627914]\n [0.9310195762135645, 0.8620343536910203]\n [0.9938739225073766, 0.9864589201403947]\n [0.9999989497058278, 0.9999979677375264]```","user":"U6CJRSR63","ts":"1611671434.005600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1Rt","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> xs\n10-element Array{Any,1}:\n [-1.2, 1.0]\n [-1.0273245872918702, 1.0519171772443916]\n [-0.5778727732836022, 0.30910840092006675]\n [-0.18592337280782262, 0.024501100158156375]\n [0.08298376142192398, -0.02506995929967343]\n [0.541793804618421, 0.2806815781121066]\n [0.7742123596652257, 0.5990640879627914]\n [0.9310195762135645, 0.8620343536910203]\n [0.9938739225073766, 0.9864589201403947]\n [0.9999989497058278, 0.9999979677375264]"}]}]}]},{"client_msg_id":"638f5425-19bc-4f7c-9810-97329450efeb","type":"message","text":"Thank you, that worked!","user":"U6BJ9E351","ts":"1611671531.005700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1Yj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thank you, that worked!"}]}]}]},{"client_msg_id":"1178921f-3090-4156-848c-c4ed49157801","type":"message","text":"can I copy paste it and post it here <https://discourse.julialang.org/t/optim-jl-callback/52625> (and here <https://discourse.julialang.org/t/optim-callback-function-how-to-use-optimization-variable/8807>) as an answer? Looks like somebody had asked the same question on discourse","user":"U6BJ9E351","ts":"1611671568.006500","team":"T68168MUP","edited":{"user":"U6BJ9E351","ts":"1611671643.000000"},"blocks":[{"type":"rich_text","block_id":"qzXHt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"can I copy paste it and post it here "},{"type":"link","url":"https://discourse.julialang.org/t/optim-jl-callback/52625"},{"type":"text","text":" (and here "},{"type":"link","url":"https://discourse.julialang.org/t/optim-callback-function-how-to-use-optimization-variable/8807"},{"type":"text","text":") as an answer? Looks like somebody had asked the same question on discourse"}]}]}]},{"client_msg_id":"78aa1d7d-fe05-490a-8c0c-7227ccbf6d57","type":"message","text":"yeah","user":"U6CJRSR63","ts":"1611674529.007200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ta=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yeah"}]}]}]},{"client_msg_id":"30a261ef-c362-49a0-8f1c-fbe8ddc35bb4","type":"message","text":"hmm... what is the recommended package for `erf()` ? `SpecialFunctions.jl` ?","user":"U013V2CFZAN","ts":"1611834428.008800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eZ5Ok","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hmm... what is the recommended package for "},{"type":"text","text":"erf()","style":{"code":true}},{"type":"text","text":" ? "},{"type":"text","text":"SpecialFunctions.jl","style":{"code":true}},{"type":"text","text":" ?"}]}]}]},{"client_msg_id":"ae904fff-be15-4cdf-a9b3-c99cd7e49b44","type":"message","text":"Might any of you know if there's a standard formulation for approximating `exp(x)` that avoids overflow issues by switching to the Taylor approximation or something like that for `x` big enough?","user":"U91Q3595Y","ts":"1611935792.010600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Jsush","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Might any of you know if there's a standard formulation for approximating "},{"type":"text","text":"exp(x)","style":{"code":true}},{"type":"text","text":" that avoids overflow issues by switching to the Taylor approximation or something like that for "},{"type":"text","text":"x","style":{"code":true}},{"type":"text","text":" big enough?"}]}]}],"thread_ts":"1611935792.010600","reply_count":15,"reply_users_count":4,"latest_reply":"1612024909.019500","reply_users":["U677NAWV8","U91Q3595Y","U6CJRSR63","U67G3QRJM"],"subscribed":false},{"type":"message","text":"","user":"U82RE6STE","ts":"1612105963.019700","team":"T68168MUP","attachments":[{"fallback":"[January 31st, 2021 3:27 PM] mathieu.besancon: hi folks, if some are interested, I'll do a stream in a bit more than an hour\n<https://twitter.com/matbesancon/status/1355830167059496971>\nprobably on optimization","ts":"1612103252.000800","author_id":"U82RE6STE","author_subname":"Mathieu Besançon","channel_id":"C01123T3NLT","channel_name":"streaming","is_msg_unfurl":true,"text":"hi folks, if some are interested, I'll do a stream in a bit more than an hour\n<https://twitter.com/matbesancon/status/1355830167059496971>\nprobably on optimization","author_name":"Mathieu Besançon","author_link":"https://julialang.slack.com/team/U82RE6STE","author_icon":"https://avatars.slack-edge.com/2018-01-24/305197567655_508578aa5910bca4b959_48.jpg","mrkdwn_in":["text"],"color":"D0D0D0","from_url":"https://julialang.slack.com/archives/C01123T3NLT/p1612103252000800","is_share":true,"footer":"Posted in #streaming"}],"thread_ts":"1612105963.019700","reply_count":1,"reply_users_count":1,"latest_reply":"1612134675.019800","reply_users":["U0138UTB7A4"],"subscribed":false,"reactions":[{"name":"+1","users":["U019078LDND"],"count":1}]},{"client_msg_id":"0cacdfb3-f079-4244-93ae-55162efcec46","type":"message","text":"What do we think the quickest way to find roots of bessel functions of first kind zero order that would satisfy `J₀(A*sₙ) = 0` where A is some varying positive constant? I see there is FunctionZeros.jl that will find the roots of bessels functions but unfortunately I am using this as an inverse model and calculating sₙ for each iteration is very slow (as the constant A depends on the fitting parameters in the fitting model)....","user":"U01FSUY7YES","ts":"1612198572.024700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"M23I","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What do we think the quickest way to find roots of bessel functions of first kind zero order that would satisfy `J₀(A*sₙ) = 0` where A is some varying positive constant? I see there is FunctionZeros.jl that will find the roots of bessels functions but unfortunately I am using this as an inverse model and calculating sₙ for each iteration is very slow (as the constant A depends on the fitting parameters in the fitting model)...."}]}]}],"thread_ts":"1612198572.024700","reply_count":9,"reply_users_count":3,"latest_reply":"1612324646.033300","reply_users":["U01FSUY7YES","U67G3QRJM","UHDQQ4GN6"],"subscribed":false},{"type":"message","text":"","user":"U6A936746","ts":"1612217833.025300","team":"T68168MUP","attachments":[{"fallback":"[February 1st, 2021 8:58 PM] rmsrosa: Hi. I didn’t find a specific channel for questions about JuMP. Please, advise me if there is a better place for this. Anyway. It is my first time using JuMP and SumOfSquares. I would like to use it to get bounds over time-averages of quantities of solutions of differential equations. I started following the examples in <https://jump.dev/SumOfSquares.jl/stable/>, for bounds of functions and for searching for Lyapunov functions. The only solver I saw being used there is `CSDP`. But I would like to use `SDPA-GMP` since it was the one that I saw working in the MATLAB ecosystem. My Julia code with `CSDP` does not converge properly. But I get an error when replacing `CSDP`with `SDPAFamily`. Is there something else I should do? The error I get is the same error that appears when I replace `CSDP.Optimizer` by `SDPAFamily.Optimizer` in the Lyapunov example <https://jump.dev/SumOfSquares.jl/stable/generated/Lyapunov%20Function%20Search/>","ts":"1612213105.482100","author_id":"U01CR62LAAD","author_subname":"Ricardo M. S. Rosa","channel_id":"C67910KEH","channel_name":"general","is_msg_unfurl":true,"is_thread_root_unfurl":true,"text":"Hi. I didn’t find a specific channel for questions about JuMP. Please, advise me if there is a better place for this. Anyway. It is my first time using JuMP and SumOfSquares. I would like to use it to get bounds over time-averages of quantities of solutions of differential equations. I started following the examples in <https://jump.dev/SumOfSquares.jl/stable/>, for bounds of functions and for searching for Lyapunov functions. The only solver I saw being used there is `CSDP`. But I would like to use `SDPA-GMP` since it was the one that I saw working in the MATLAB ecosystem. My Julia code with `CSDP` does not converge properly. But I get an error when replacing `CSDP`with `SDPAFamily`. Is there something else I should do? The error I get is the same error that appears when I replace `CSDP.Optimizer` by `SDPAFamily.Optimizer` in the Lyapunov example <https://jump.dev/SumOfSquares.jl/stable/generated/Lyapunov%20Function%20Search/>","author_name":"Ricardo M. S. Rosa","author_link":"https://julialang.slack.com/team/U01CR62LAAD","author_icon":"https://secure.gravatar.com/avatar/93ad793225cfbb515a9d3c603081edaf.jpg?s=48&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0017-48.png","mrkdwn_in":["text"],"color":"D0D0D0","from_url":"https://julialang.slack.com/archives/C67910KEH/p1612213105482100?thread_ts=1612213105482100&cid=C67910KEH","is_share":true,"footer":"Thread in #general"}]},{"client_msg_id":"16370124-792b-4f84-b2a3-457f445c2d01","type":"message","text":"hi <@U01CR62LAAD> you might have more luck in <#CNEGS2YF5|jump-dev-bridged> or even more specific in the sum-of-square gitter here: <https://gitter.im/JuliaOpt/SumOfSquares.jl>","user":"U82RE6STE","ts":"1612258193.030600","team":"T68168MUP","attachments":[{"title":"JuliaOpt/SumOfSquares.jl","title_link":"https://gitter.im/JuliaOpt/SumOfSquares.jl","text":"Where developers come to talk.","fallback":"JuliaOpt/SumOfSquares.jl","thumb_url":"https://avatars-02.gitter.im/group/iv/4/57542cbbc43b8c601977603c","from_url":"https://gitter.im/JuliaOpt/SumOfSquares.jl","thumb_width":128,"thumb_height":128,"service_icon":"http://cdn03.gitter.im/_s/0240288c4/images/favicon-normal.ico","service_name":"gitter.im","id":1,"original_url":"https://gitter.im/JuliaOpt/SumOfSquares.jl"}],"blocks":[{"type":"rich_text","block_id":"ZGU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hi "},{"type":"user","user_id":"U01CR62LAAD"},{"type":"text","text":" you might have more luck in "},{"type":"channel","channel_id":"CNEGS2YF5"},{"type":"text","text":" or even more specific in the sum-of-square gitter here: "},{"type":"link","url":"https://gitter.im/JuliaOpt/SumOfSquares.jl"}]}]}],"thread_ts":"1612258193.030600","reply_count":1,"reply_users_count":1,"latest_reply":"1612268857.031100","reply_users":["U01CR62LAAD"],"subscribed":false},{"type":"message","subtype":"channel_join","ts":"1612258197.030900","user":"U01CR62LAAD","text":"<@U01CR62LAAD> has joined the channel","inviter":"U82RE6STE"},{"type":"message","subtype":"thread_broadcast","text":"Just to follow up on numerically evaluating the inverse Laplace transform. It didn't look like InverseLaplace.jl had implemented the Post-Widder formula as described <https://arxiv.org/abs/1204.4754|here>. I implemented it below if anyone is interested. It's actually really impressive how many less evaluations in the frequency domain you need to reconstruct the time domain signal (compared to FT). If anyone has any optimization tips for the algorithm (PWcoeffs can be precomputed, LT_postwid is very performance critical) let me know.\n```# Compute Post-Widder coefficients Vk\nfunction _PWcoeffs(N)\n    v = zeros(N)\n    aux = 0.0\n    for k in 1:N\n        for j in floor(Int, (k + 1)/2):minimum(Int, [k, N/2])\n            aux = big(j)^(N/2)*factorial(big(2*j))\n            aux /= factorial(big(N/2 - j))*factorial(big(j))*factorial(big(j-1))\n            aux /= factorial(big(k - j))*factorial(big(2*j - k))\n            v[k] += aux\n        end\n        v[k] *= (-1)^(k + N/2) \n    end\n    return v\nend\n\n# compute LT for an array of t using multithreading\nfunction LT_postwid(f::Function, v, t::AbstractArray)\n    N = length(v)\n    a = zeros(length(t))\n    Threads.@threads for ind in eachindex(t)\n        for k in 1:N\n            a[ind] += v[k]*f(k*log(2)/t[ind])\n        end\n        a[ind] *= log(2)/t[ind]\n    end\n    return a\nend\n\n### example\nv = _PWcoeffs(18)\nlt2 = LT_postwid(s -&gt; exp(-sqrt(3 + s/3))/(4*pi*0.1),v, 0.01:0.01:4)```","user":"U01FSUY7YES","ts":"1612536580.034300","thread_ts":"1612198572.024700","root":{"client_msg_id":"0cacdfb3-f079-4244-93ae-55162efcec46","type":"message","text":"What do we think the quickest way to find roots of bessel functions of first kind zero order that would satisfy `J₀(A*sₙ) = 0` where A is some varying positive constant? I see there is FunctionZeros.jl that will find the roots of bessels functions but unfortunately I am using this as an inverse model and calculating sₙ for each iteration is very slow (as the constant A depends on the fitting parameters in the fitting model)....","user":"U01FSUY7YES","ts":"1612198572.024700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"M23I","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What do we think the quickest way to find roots of bessel functions of first kind zero order that would satisfy `J₀(A*sₙ) = 0` where A is some varying positive constant? I see there is FunctionZeros.jl that will find the roots of bessels functions but unfortunately I am using this as an inverse model and calculating sₙ for each iteration is very slow (as the constant A depends on the fitting parameters in the fitting model)...."}]}]}],"thread_ts":"1612198572.024700","reply_count":11,"reply_users_count":3,"latest_reply":"1612537111.034900","reply_users":["U01FSUY7YES","U67G3QRJM","UHDQQ4GN6"],"subscribed":false},"attachments":[{"service_name":"arXiv.org","title":"Review of Inverse Laplace Transform Algorithms for Laplace-Space...","title_link":"https://arxiv.org/abs/1204.4754","text":"A boundary element method (BEM) simulation is used to compare the efficiency of numerical inverse Laplace transform strategies, considering general requirements of Laplace-space numerical...","fallback":"arXiv.org: Review of Inverse Laplace Transform Algorithms for Laplace-Space...","from_url":"https://arxiv.org/abs/1204.4754","service_icon":"https://static.arxiv.org/static/browse/0.3.2.6/images/icons/favicon.ico","id":1,"original_url":"https://arxiv.org/abs/1204.4754"}],"blocks":[{"type":"rich_text","block_id":"K42","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Just to follow up on numerically evaluating the inverse Laplace transform. It didn't look like InverseLaplace.jl had implemented the Post-Widder formula as described "},{"type":"link","url":"https://arxiv.org/abs/1204.4754","text":"here"},{"type":"text","text":". I implemented it below if anyone is interested. It's actually really impressive how many less evaluations in the frequency domain you need to reconstruct the time domain signal (compared to FT). If anyone has any optimization tips for the algorithm (PWcoeffs can be precomputed, LT_postwid is very performance critical) let me know.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"# Compute Post-Widder coefficients Vk\nfunction _PWcoeffs(N)\n    v = zeros(N)\n    aux = 0.0\n    for k in 1:N\n        for j in floor(Int, (k + 1)/2):minimum(Int, [k, N/2])\n            aux = big(j)^(N/2)*factorial(big(2*j))\n            aux /= factorial(big(N/2 - j))*factorial(big(j))*factorial(big(j-1))\n            aux /= factorial(big(k - j))*factorial(big(2*j - k))\n            v[k] += aux\n        end\n        v[k] *= (-1)^(k + N/2) \n    end\n    return v\nend\n\n# compute LT for an array of t using multithreading\nfunction LT_postwid(f::Function, v, t::AbstractArray)\n    N = length(v)\n    a = zeros(length(t))\n    Threads.@threads for ind in eachindex(t)\n        for k in 1:N\n            a[ind] += v[k]*f(k*log(2)/t[ind])\n        end\n        a[ind] *= log(2)/t[ind]\n    end\n    return a\nend\n\n### example\nv = _PWcoeffs(18)\nlt2 = LT_postwid(s -> exp(-sqrt(3 + s/3))/(4*pi*0.1),v, 0.01:0.01:4)"}]}]}],"client_msg_id":"9842bd63-098d-4b72-b449-25c3ae7e99f3","edited":{"user":"U01FSUY7YES","ts":"1612536710.000000"}},{"client_msg_id":"025DA368-A5C2-425A-9224-AC7FF8D66426","type":"message","text":"Is there a way to get the time stamps and current objective function value for a MILP in JuMP using CBC? I’d like to plot this to see how much better the solution is over time.","user":"USSNH7BGT","ts":"1612798973.038500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4QCK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to get the time stamps and current objective function value for a MILP in JuMP using CBC? I’d like to plot this to see how much better the solution is over time."}]}]}],"thread_ts":"1612798973.038500","reply_count":2,"reply_users_count":2,"latest_reply":"1612799302.039200","reply_users":["UCT7E536E","USSNH7BGT"],"subscribed":false},{"client_msg_id":"ca245530-efc1-487d-8f2f-8b7c934ef144","type":"message","text":"I'm trying to solve an optimization problem with an equality constraint with NLopt. The basic syntax asks for separate functions to compute the objective and the constraint, but this requires redoing the same calculation from the inputs twice. Is there any way to avoid that?","user":"U91Q3595Y","ts":"1613005424.046100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bfl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm trying to solve an optimization problem with an equality constraint with NLopt. The basic syntax asks for separate functions to compute the objective and the constraint, but this requires redoing the same calculation from the inputs twice. Is there any way to avoid that?"}]}]}]},{"client_msg_id":"96c91c17-3cd0-43e5-bbf2-4d4c513787a9","type":"message","text":"Hi people! I have an optimization problem which is convex (I know that) but not DCP-compliant. That means I can’t use `Convex.jl` but I would still like to benefit from algorithms specifically taylored for convex optimization. Is there any alternative besides nonlinear modelling with `JuMP.jl` ?","user":"U01GMP3HF9C","ts":"1613028825.050000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"cSH4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi people! I have an optimization problem which is convex (I know that) but not DCP-compliant. That means I can’t use "},{"type":"text","text":"Convex.jl","style":{"code":true}},{"type":"text","text":" but I would still like to benefit from algorithms specifically taylored for convex optimization. Is there any alternative besides nonlinear modelling with "},{"type":"text","text":"JuMP.jl","style":{"code":true}},{"type":"text","text":" ?"}]}]}]},{"client_msg_id":"968451d2-9fe5-403d-8ae4-b3baf7fe5232","type":"message","text":"`Optim`’s `IPNewton` or use `Ipopt`/`NLopt` directly.","user":"U85JBUGGP","ts":"1613032087.050400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ARpb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Optim","style":{"code":true}},{"type":"text","text":"’s "},{"type":"text","text":"IPNewton","style":{"code":true}},{"type":"text","text":" or use "},{"type":"text","text":"Ipopt","style":{"code":true}},{"type":"text","text":"/"},{"type":"text","text":"NLopt","style":{"code":true}},{"type":"text","text":" directly."}]}]}]}]}