{"cursor": 0, "messages": [{"client_msg_id":"93950008-5b30-441c-aef2-bd62e193694a","type":"message","text":"Do we have something like Optim but supporting `CuArray`s? I have a problem where L-BFGS outperforms Flux optimizers, but I would like to move the model computation to the GPU. Transferring the data from GPU to CPU at every iteration seems expensive, but I couldn't find a GPU friendly quasi newton solver.","user":"U6BJ9E351","ts":"1607982881.353800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ypZBE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do we have something like Optim but supporting "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":"s? I have a problem where L-BFGS outperforms Flux optimizers, but I would like to move the model computation to the GPU. Transferring the data from GPU to CPU at every iteration seems expensive, but I couldn't find a GPU friendly quasi newton solver."}]}]}],"thread_ts":"1607982881.353800","reply_count":30,"reply_users_count":3,"latest_reply":"1608044342.360600","reply_users":["U6CJRSR63","U6BJ9E351","U67G3QRJM"],"subscribed":false,"reactions":[{"name":"+1","users":["UN2U72Q3F"],"count":1}]},{"client_msg_id":"a848af70-d82c-4352-9cf8-6c80845ab575","type":"message","text":"Are there any packages for finding saddle-points (e.g. min-max) of a function?","user":"U7YD3DKL2","ts":"1608129306.361800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QhgF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Are there any packages for finding saddle-points (e.g. min-max) of a function?"}]}]}],"thread_ts":"1608129306.361800","reply_count":6,"reply_users_count":2,"latest_reply":"1608438612.364900","reply_users":["U67G3QRJM","U7YD3DKL2"],"subscribed":false},{"client_msg_id":"5edce411-dc98-4a38-9d92-d1843a56a570","type":"message","text":"Good question. Are you looking for any specific methods?","user":"U6CJRSR63","ts":"1608132188.362200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=LVn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Good question. Are you looking for any specific methods?"}]}]}],"thread_ts":"1608132188.362200","reply_count":1,"reply_users_count":1,"latest_reply":"1608441112.365100","reply_users":["U7YD3DKL2"],"subscribed":false},{"client_msg_id":"e44aebf7-c018-4772-b33f-e9c3726f6b2f","type":"message","text":"I just joined this channel to ask that question haha. :see_no_evil:","user":"U01A0S07875","ts":"1608397927.364400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Prj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I just joined this channel to ask that question haha. "},{"type":"emoji","name":"see_no_evil"}]}]}],"thread_ts":"1608397927.364400","reply_count":1,"reply_users_count":1,"latest_reply":"1608410390.364700","reply_users":["U9MD78Z9N"],"subscribed":false},{"type":"message","text":"Assuming i want to minimize a third degree taylor expansion (which is non convex) of an sufficently nice non-linear function.\nI can express the minimization of a multinomial of third degree as the root finding of a second degree multi variate polynomial, with some  conditions to not find maximas/saddle points and maybe some trust region style constraint. \nAre you aware of algorithm that do that and strategely construct roots to jump in to different local minima in order to find the best one?","user":"U9MD78Z9N","ts":"1609636138.367100","team":"T68168MUP"},{"type":"message","text":"(duplicate from other channel(s), sorry if you are flooded)","user":"U01FR2HFJ7M","ts":"1609830788.368700","team":"T68168MUP","attachments":[{"fallback":"[January 5th, 2021 2:27 PM] jf: Any idea where I can chat/contact people working/using Evolutionary.jl (other than GH issues)?","ts":"1609824439.191600","author_id":"U01FR2HFJ7M","author_subname":"Azzaare","channel_id":"CAKKFNYLD","channel_name":"biology","is_msg_unfurl":true,"is_thread_root_unfurl":true,"text":"Any idea where I can chat/contact people working/using Evolutionary.jl (other than GH issues)?","author_name":"Azzaare","author_link":"https://julialang.slack.com/team/U01FR2HFJ7M","author_icon":"https://avatars.slack-edge.com/2020-11-30/1535756085650_d8d021e24a800bfb16b8_48.jpg","mrkdwn_in":["text"],"color":"D0D0D0","from_url":"https://julialang.slack.com/archives/CAKKFNYLD/p1609824439191600?thread_ts=1609824439191600&cid=CAKKFNYLD","is_share":true,"footer":"Thread in #biology"}],"blocks":[{"type":"rich_text","block_id":"ZXyur","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(duplicate from other channel(s), sorry if you are flooded)"}]}]}]},{"client_msg_id":"30820e1e-8a1b-454d-8bf9-91effb0fa0f0","type":"message","text":"We just posted a complete draft of our new book, Algorithms for Decision Making:\n<http://algorithmsbook.com/>\nIt uses JuMP.jl for many of the key algorithms. If you have comments on how it can be improved, please let us know!","user":"UBP8QFLBY","ts":"1609909142.001100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"whU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We just posted a complete draft of our new book, Algorithms for Decision Making:\n"},{"type":"link","url":"http://algorithmsbook.com/"},{"type":"text","text":"\nIt uses JuMP.jl for many of the key algorithms. If you have comments on how it can be improved, please let us know!"}]}]}]},{"client_msg_id":"62e44dd3-ff46-45f2-97f8-44e6b3767e79","type":"message","text":"<@UBP8QFLBY> Is it going to be typeset in the same beautiful way as the first book (Algorithms for Opt)? The first book is literally pretty to look at. I keep it on my coffee table at all times.","user":"UKA81L34J","ts":"1609947992.002600","team":"T68168MUP","edited":{"user":"UKA81L34J","ts":"1609948033.000000"},"blocks":[{"type":"rich_text","block_id":"DNYZ","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UBP8QFLBY"},{"type":"text","text":" Is it going to be typeset in the same beautiful way as the first book (Algorithms for Opt)? The first book is literally pretty to look at. I keep it on my coffee table at all times."}]}]}]},{"type":"message","text":"Is someone aware of global optimization methods using local optimizers which work by subdividing the space along first order optimilaty points and their connections along pseudo manifolds(?) where the the gradient is zero except in moving along the manifold. (Root finding on the total derivative, expect in the direction of the step.)\nThis would divide a x^2 - y^2 type sadle point in to 4 seperate regions.\nIf one had such a diving pseudo manifold one would have the basins of attraction for local optimizers.\nI am not convinced that the approach as i described it would be performant but maybe the principle is used in some global optimizers.","user":"U9MD78Z9N","ts":"1610113685.005300","team":"T68168MUP"},{"client_msg_id":"b4bd71a1-55d6-4dd1-bfef-c52758e6059c","type":"message","text":"multistart methods kind of try to do that","user":"U69BL50BF","ts":"1610113777.005600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"a6OC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"multistart methods kind of try to do that"}]}]}]},{"type":"message","text":"Multi start to try start in different basins of attraction but not rigerously identify the basis. (Atleast the multi start methods i learned about in class)","user":"U9MD78Z9N","ts":"1610113836.005700","team":"T68168MUP"},{"client_msg_id":"83be9a4a-64ec-4231-8cf6-2cc124d73400","type":"message","text":"I mean, to try and start in different basis is to heuristically identify a basin and choose another start in the basin.","user":"U69BL50BF","ts":"1610114025.006300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KsSD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I mean, to try and start in different basis is to heuristically identify a basin and choose another start in the basin."}]}]}]},{"type":"message","text":"Mhh, Where can i learn more about those heuristics?","user":"U9MD78Z9N","ts":"1610114051.006400","team":"T68168MUP"},{"client_msg_id":"1a5f09b7-3016-4f94-9278-f4df2c0b5612","type":"message","text":"<https://fguvenendotcom.files.wordpress.com/2019/09/agk2019-september-nber-submit.pdf>","user":"U69BL50BF","ts":"1610114182.006600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"I94","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://fguvenendotcom.files.wordpress.com/2019/09/agk2019-september-nber-submit.pdf"}]}]}]},{"client_msg_id":"6c4ed35a-004a-4887-874b-0e582011f541","type":"message","text":"TikTak is the one people talk about these days","user":"U69BL50BF","ts":"1610114189.006900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yvv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"TikTak is the one people talk about these days"}]}]}]},{"client_msg_id":"81d611a9-10b3-4597-901f-e494a702b9a9","type":"message","text":"it does a quasi-random sample but then uses the local search information","user":"U69BL50BF","ts":"1610114198.007200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"N05vr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it does a quasi-random sample but then uses the local search information"}]}]}]},{"type":"message","text":"In particular like lyapunov stability proofs for an ODE formulation of a local minimizer (like gradient flow) to show that the complete basin indeed converges to some minimum would be epic.","user":"U9MD78Z9N","ts":"1610114210.007300","team":"T68168MUP"},{"client_msg_id":"97b1ef17-9adb-4436-af4e-d6faf6a0c284","type":"message","text":"to me the best bet is to make guesses from a quasi-random sequence.","user":"U69BL50BF","ts":"1610114308.007600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"csd6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"to me the best bet is to make guesses from a quasi-random sequence."}]}]}]},{"type":"message","text":"If understand that correctly it would be guaranteed to find the true solution but it would not able able to certify that any found solution is the optimal one","user":"U9MD78Z9N","ts":"1610114400.007700","team":"T68168MUP"},{"client_msg_id":"654c1c74-292e-42b3-b6ff-d88f694a5ce5","type":"message","text":"yes","user":"U69BL50BF","ts":"1610114552.007900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mB2YI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yes"}]}]}]},{"client_msg_id":"479add28-6003-4983-aa27-24b750fb10b2","type":"message","text":"well","user":"U69BL50BF","ts":"1610114560.008200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7y6tc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"well"}]}]}]},{"client_msg_id":"9ea48a1f-b8b7-44ed-b906-bfbc0eaede26","type":"message","text":"it couldn't guarantee a true solution unless you take infinitely many starts.","user":"U69BL50BF","ts":"1610114578.008600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"drfZX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it couldn't guarantee a true solution unless you take infinitely many starts."}]}]}]},{"type":"message","text":"That paper looks awesome, i add it to my must read list before i write my own NLP solver.","user":"U9MD78Z9N","ts":"1610114611.008700","team":"T68168MUP"},{"client_msg_id":"f5371bae-deb3-42dd-ad90-3a8c232cb06d","type":"message","text":"<@U9MD78Z9N> That sounds similar to <https://arxiv.org/pdf/2011.06505.pdf>","user":"U67G3QRJM","ts":"1610149316.009200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sNOO","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U9MD78Z9N"},{"type":"text","text":" That sounds similar to "},{"type":"link","url":"https://arxiv.org/pdf/2011.06505.pdf"}]}]}]},{"client_msg_id":"cba45e62-a56b-4348-834c-b8cd017ce0d1","type":"message","text":"Also similar to the nudged elastic band method maybe?","user":"U67G3QRJM","ts":"1610149354.009500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eeZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Also similar to the nudged elastic band method maybe?"}]}]}]},{"client_msg_id":"ea070cc8-d1de-45aa-ac42-7afb46fab2ae","type":"message","text":"e.g. <https://arxiv.org/abs/cond-mat/0402209>","user":"U67G3QRJM","ts":"1610149395.009700","team":"T68168MUP","attachments":[{"service_name":"arXiv.org","title":"A Doubly Nudged Elastic Band Method for Finding Transition States","title_link":"https://arxiv.org/abs/cond-mat/0402209","text":"A modification of the nudged elastic band (NEB) method is presented that enables stable optimisations to be run using both the limited-memory quasi-Newton (L-BFGS) and slow-response quenched...","fallback":"arXiv.org: A Doubly Nudged Elastic Band Method for Finding Transition States","from_url":"https://arxiv.org/abs/cond-mat/0402209","service_icon":"https://static.arxiv.org/static/browse/0.3.2.6/images/icons/favicon.ico","id":1,"original_url":"https://arxiv.org/abs/cond-mat/0402209"}],"blocks":[{"type":"rich_text","block_id":"0=R3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"e.g. "},{"type":"link","url":"https://arxiv.org/abs/cond-mat/0402209"}]}]}]},{"client_msg_id":"b83314c0-c1c7-4723-96a4-f77f0281d523","type":"message","text":"What does this tell me?\n```┌ Warning: Failed to achieve finite new evaluation point, using alpha=0\n└ @ LineSearches C:\\Users\\ngudat\\.julia\\packages\\LineSearches\\Ki4c5\\src\\hagerzhang.jl:148```","user":"U7JQGPGCQ","ts":"1610478129.010700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"09CRL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What does this tell me?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"┌ Warning: Failed to achieve finite new evaluation point, using alpha=0\n└ @ LineSearches C:\\Users\\ngudat\\.julia\\packages\\LineSearches\\Ki4c5\\src\\hagerzhang.jl:148"}]}]}]},{"client_msg_id":"b831e653-0c23-43ee-9a1a-8820fd2a1480","type":"message","text":"You objective seems to have returned only non-finite values","user":"U6CJRSR63","ts":"1610487361.011200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OU9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You objective seems to have returned only non-finite values"}]}]}]},{"client_msg_id":"ee43a8f6-47b7-4648-a593-29f7a71ac53d","type":"message","text":"(+/- Inf, NaN)","user":"U6CJRSR63","ts":"1610487371.011500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uKU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(+/- Inf, NaN)"}]}]}]},{"client_msg_id":"8fc758d9-718c-43c5-9d44-49f629f7a8de","type":"message","text":"Is your objective stochastic?","user":"U6CJRSR63","ts":"1610487383.011700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oEB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is your objective stochastic?"}]}]}]},{"client_msg_id":"63d52e5b-6072-45d8-a0ab-ecb1d9ec2c62","type":"message","text":"No, deterministic and (I thought) not very complicated - I have a 60 element target vector and want to find a vector of weights for a 60-by-x matrix such that the matrix*weights product gives a vector that is as close as possible to the target. Weirdly enough it seems to work just fine if I remove some columns in the matrix (which are quite far off of the target), but I would have thought that one could find the same solution if all columns are included (as the ones that are far off should just get zero weights). Anyway for a change the data I'm working on is public so I'll post a reproducer later on.","user":"U7JQGPGCQ","ts":"1610518547.018700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gWr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"No, deterministic and (I thought) not very complicated - I have a 60 element target vector and want to find a vector of weights for a 60-by-x matrix such that the matrix*weights product gives a vector that is as close as possible to the target. Weirdly enough it seems to work just fine if I remove some columns in the matrix (which are quite far off of the target), but I would have thought that one could find the same solution if all columns are included (as the ones that are far off should just get zero weights). Anyway for a change the data I'm working on is public so I'll post a reproducer later on."}]}]}]},{"type":"message","text":"Okay I think I have narrowed it down to the MWE below.\n\nFor context (feel free to skip), this occurs as part of my (still woefully incomplete) SynthControl package, which implements synthetic control estimation. The core of the method is to take a number of potential \"donor\" time series and weight them in such a way that they get as close as possible to a target time series. The full code is here: <https://github.com/nilshg/SynthControl.jl/blob/master/src/SynthControl.jl>, but I've extracted out the relevant bits below:\n\n```using Optim, DelimitedFiles, LinearAlgebra\n\nfunction get_weights(y, xs)\n\n    obj(w) = norm(y .- xs*w) + 1e6*abs(1.0 - sum(w))\n\n    # Initial condition and bounds\n    initial = [1e-5 for _ in 1:size(xs, 2)]\n    lower = zeros(size(xs, 2))\n    upper = ones(size(xs, 2))\n\n    # Get weights through optimization\n    res = optimize(obj, lower, upper, initial)\n\n    return res.minimizer\nend\n\nyᵂ = [100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.7, 100.7, 100.7, 100.7, \n      100.2, 100.2, 100.2, 100.1, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, \n      99.4, 99.4, 99.4, 99.4, 99.3, 99.2, 99.2, 99.2, 99.2, 98.9, 98.9, 98.9, 98.9, 98.9, \n      98.9, 98.9, 98.9, 98.9, 98.9, 98.8, 98.8, 98.8, 98.7, 98.7, 98.6, 98.6, 98.6, 98.3, \n      97.9, 97.9, 97.9, 97.9, 97.8, 97.7, 97.7, 97.7]\n\nxs = readdlm(\"xs.txt\")\n\nget_weights(yᵂ, xs) # Fails - some weights are NaN\nw₁ = get_weights(yᵂ, xs[:, [1:4; 6:end]])\n\nsum(w₁) # ≈ 1.00\nnorm(yᵂ .- xs[:, [1:4; 6:end]]*w₁) # ≈ 0.897```\n","files":[{"id":"F01JZDH0YP3","created":1610533648,"timestamp":1610533648,"name":"xs.txt","title":"xs.txt","mimetype":"text/plain","filetype":"text","pretty_type":"Plain Text","user":"U7JQGPGCQ","editable":true,"size":5899,"mode":"snippet","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01JZDH0YP3/xs.txt","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01JZDH0YP3/download/xs.txt","permalink":"https://julialang.slack.com/files/U7JQGPGCQ/F01JZDH0YP3/xs.txt","permalink_public":"https://slack-files.com/T68168MUP-F01JZDH0YP3-9eb35dde6d","edit_link":"https://julialang.slack.com/files/U7JQGPGCQ/F01JZDH0YP3/xs.txt/edit","preview":"100.0\t100.7\t101.7\t102.1\t96.2\t100.8\t100.3\t99.6\t97.6\t99.8\t99.5\t100.9\t99.0\t101.4\t101.2\t100.9\t102.5\t102.4\n100.0\t101.3\t101.7\t102.1\t95.7\t101.0\t100.3\t99.6\t97.4\t99.7\t99.5\t101.0\t98.2\t101.4\t101.2\t100.9\t102.5\t102.4\n99.9\t101.1\t101.7\t102.1\t97.1\t101.1\t100.3\t99.6\t97.4\t99.8\t99.5\t100.7\t97.9\t101.4\t101.2\t100.9\t102.5\t102.4\n100.0\t101.1\t101.7\t101.9\t97.4\t101.1\t100.7\t99.6\t97.6\t99.8\t99.6\t100.9\t98.0\t101.3\t101.2\t100.6\t102.5\t102.3\n100.0\t101.1\t101.7\t101.9\t97.6\t101.1\t100.7\t99.5\t97.7\t99.9\t99.6\t100.8\t97.9\t100.2\t101.3\t100.6\t102.5\t102.3","preview_highlight":"<div class=\"CodeMirror cm-s-default CodeMirrorServer\" oncopy=\"if(event.clipboardData){event.clipboardData.setData('text/plain',window.getSelection().toString().replace(/\\u200b/g,''));event.preventDefault();event.stopPropagation();}\">\n<div class=\"CodeMirror-code\">\n<div><pre>100.0   100.7   101.7   102.1   96.2    100.8   100.3   99.6    97.6    99.8    99.5    100.9   99.0    101.4   101.2   100.9   102.5   102.4</pre></div>\n<div><pre>100.0   101.3   101.7   102.1   95.7    101.0   100.3   99.6    97.4    99.7    99.5    101.0   98.2    101.4   101.2   100.9   102.5   102.4</pre></div>\n<div><pre>99.9    101.1   101.7   102.1   97.1    101.1   100.3   99.6    97.4    99.8    99.5    100.7   97.9    101.4   101.2   100.9   102.5   102.4</pre></div>\n<div><pre>100.0   101.1   101.7   101.9   97.4    101.1   100.7   99.6    97.6    99.8    99.6    100.9   98.0    101.3   101.2   100.6   102.5   102.3</pre></div>\n<div><pre>100.0   101.1   101.7   101.9   97.6    101.1   100.7   99.5    97.7    99.9    99.6    100.8   97.9    100.2   101.3   100.6   102.5   102.3</pre></div>\n</div>\n</div>\n","lines":61,"lines_more":56,"preview_is_truncated":true,"is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"pMLF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Okay I think I have narrowed it down to the MWE below.\n\nFor context (feel free to skip), this occurs as part of my (still woefully incomplete) SynthControl package, which implements synthetic control estimation. The core of the method is to take a number of potential \"donor\" time series and weight them in such a way that they get as close as possible to a target time series. The full code is here: "},{"type":"link","url":"https://github.com/nilshg/SynthControl.jl/blob/master/src/SynthControl.jl"},{"type":"text","text":", but I've extracted out the relevant bits below:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using Optim, DelimitedFiles, LinearAlgebra\n\nfunction get_weights(y, xs)\n\n    obj(w) = norm(y .- xs*w) + 1e6*abs(1.0 - sum(w))\n\n    # Initial condition and bounds\n    initial = [1e-5 for _ in 1:size(xs, 2)]\n    lower = zeros(size(xs, 2))\n    upper = ones(size(xs, 2))\n\n    # Get weights through optimization\n    res = optimize(obj, lower, upper, initial)\n\n    return res.minimizer\nend\n\nyᵂ = [100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.9, 100.7, 100.7, 100.7, 100.7, \n      100.2, 100.2, 100.2, 100.1, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.8, \n      99.4, 99.4, 99.4, 99.4, 99.3, 99.2, 99.2, 99.2, 99.2, 98.9, 98.9, 98.9, 98.9, 98.9, \n      98.9, 98.9, 98.9, 98.9, 98.9, 98.8, 98.8, 98.8, 98.7, 98.7, 98.6, 98.6, 98.6, 98.3, \n      97.9, 97.9, 97.9, 97.9, 97.8, 97.7, 97.7, 97.7]\n\nxs = readdlm(\"xs.txt\")\n\nget_weights(yᵂ, xs) # Fails - some weights are NaN\nw₁ = get_weights(yᵂ, xs[:, [1:4; 6:end]])\n\nsum(w₁) # ≈ 1.00\nnorm(yᵂ .- xs[:, [1:4; 6:end]]*w₁) # ≈ 0.897"}]},{"type":"rich_text_section","elements":[]}]}],"user":"U7JQGPGCQ","display_as_bot":false,"ts":"1610533782.021300"},{"client_msg_id":"457b7f4d-83ab-4060-8556-e4fa4db49c4d","type":"message","text":"When using the full `xs` matrix with 18 columns, some of the weights end up being `NaN`. Removing one of the columns makes the whole thing work. I would have thought that adding a column shouldn't make the fit worse (as in the worst case one could just set that weight to zero), but it seems that here it's throwing everything off. Any ideas how this is happening, and how to deal with it?","user":"U7JQGPGCQ","ts":"1610533931.023700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"r8zK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"When using the full "},{"type":"text","text":"xs","style":{"code":true}},{"type":"text","text":" matrix with 18 columns, some of the weights end up being "},{"type":"text","text":"NaN","style":{"code":true}},{"type":"text","text":". Removing one of the columns makes the whole thing work. I would have thought that adding a column shouldn't make the fit worse (as in the worst case one could just set that weight to zero), but it seems that here it's throwing everything off. Any ideas how this is happening, and how to deal with it?"}]}]}]},{"client_msg_id":"de6496ce-313b-46b2-94bf-800426b1921f","type":"message","text":"Hi all. I have a non-linear least-squares problem where I have two functions: one that computes the residuals; and another that computes both the residuals and the Jacobian (that is being wrapped to only return the Jacobian). I'm currently using `LsqFit.jl`. My question is: for performance, is there a way to take advantage of the fact that when the Jacobian function is called, the residuals are also computed?","user":"U6CCK2SCV","ts":"1610627374.005100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JJCI2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi all. I have a non-linear least-squares problem where I have two functions: one that computes the residuals; and another that computes both the residuals and the Jacobian (that is being wrapped to only return the Jacobian). I'm currently using "},{"type":"text","text":"LsqFit.jl","style":{"code":true}},{"type":"text","text":". My question is: for performance, is there a way to take advantage of the fact that when the Jacobian function is called, the residuals are also computed?"}]}]}]},{"client_msg_id":"f109e998-b448-4a49-bb76-7b6bd111da4f","type":"message","text":"David Sanders is giving a GERAD seminar on global optimization with interval arithmetic tomorrow at 11AM EST:\n<https://www.gerad.ca/fr/events/1843>\nAll welcome!","user":"UB4KR33H9","ts":"1610669094.005700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ut2Jr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"David Sanders is giving a GERAD seminar on global optimization with interval arithmetic tomorrow at 11AM EST:\n"},{"type":"link","url":"https://www.gerad.ca/fr/events/1843"},{"type":"text","text":"\nAll welcome!"}]}]}],"thread_ts":"1610669094.005700","reply_count":1,"reply_users_count":1,"latest_reply":"1610669861.006000","reply_users":["U01FR2HFJ7M"],"subscribed":false,"reactions":[{"name":"+1","users":["U01FR2HFJ7M","UCZ7VBGUD"],"count":2}]},{"type":"message","text":"","user":"U01FR2HFJ7M","ts":"1610708908.006600","team":"T68168MUP","attachments":[{"fallback":"[January 15th, 2021 8:06 PM] jf: Hi there! I am very proud to announce the first beta release of LocalSearchSolvers.jl, a framework for CBLS (Constraint-Based Local Search) solvers in pure Julia, hosted on the GitHub org JuliaConstraints.\nIts goal is to solve satisfaction and optimization problems where models are formulated as a simple triplet of variables domains, constraints, and optional objectives.\nDue to high scalability, CBLS can have more than linear speedup with parallelization (though not yet implemented here), and many other options are coming such as dynamic models, DSL for JuMP and Minizinc, automatic parameter tuning and such.","ts":"1610708786.007900","author_id":"U01FR2HFJ7M","author_subname":"Azzaare","channel_id":"CDPQVTD4L","channel_name":"pkg-announcements","is_msg_unfurl":true,"text":"Hi there! I am very proud to announce the first beta release of LocalSearchSolvers.jl, a framework for CBLS (Constraint-Based Local Search) solvers in pure Julia, hosted on the GitHub org JuliaConstraints.\nIts goal is to solve satisfaction and optimization problems where models are formulated as a simple triplet of variables domains, constraints, and optional objectives.\nDue to high scalability, CBLS can have more than linear speedup with parallelization (though not yet implemented here), and many other options are coming such as dynamic models, DSL for JuMP and Minizinc, automatic parameter tuning and such.","author_name":"Azzaare","author_link":"https://julialang.slack.com/team/U01FR2HFJ7M","author_icon":"https://avatars.slack-edge.com/2020-11-30/1535756085650_d8d021e24a800bfb16b8_48.jpg","mrkdwn_in":["text"],"files":[{"id":"F01JDB0FXRD","created":1610708490,"timestamp":1610708490,"name":"sudoku3x3.png","title":"sudoku3x3.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01FR2HFJ7M","editable":false,"size":54164,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01JDB0FXRD/sudoku3x3.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01JDB0FXRD/download/sudoku3x3.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01JDB0FXRD-04bd60c462/sudoku3x3_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01JDB0FXRD-04bd60c462/sudoku3x3_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01JDB0FXRD-04bd60c462/sudoku3x3_360.png","thumb_360_w":360,"thumb_360_h":131,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01JDB0FXRD-04bd60c462/sudoku3x3_480.png","thumb_480_w":480,"thumb_480_h":174,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01JDB0FXRD-04bd60c462/sudoku3x3_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01JDB0FXRD-04bd60c462/sudoku3x3_720.png","thumb_720_w":720,"thumb_720_h":262,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01JDB0FXRD-04bd60c462/sudoku3x3_800.png","thumb_800_w":800,"thumb_800_h":291,"thumb_960":"https://files.slack.com/files-tmb/T68168MUP-F01JDB0FXRD-04bd60c462/sudoku3x3_960.png","thumb_960_w":960,"thumb_960_h":349,"thumb_1024":"https://files.slack.com/files-tmb/T68168MUP-F01JDB0FXRD-04bd60c462/sudoku3x3_1024.png","thumb_1024_w":1024,"thumb_1024_h":372,"original_w":1717,"original_h":624,"thumb_tiny":"AwARADCh26UHgdKKD0oAbn2FGfYUlFAC5ozSUUAPpD0paQ9KAG0UUUAFFFFAH//Z","permalink":"https://julialang.slack.com/files/U01FR2HFJ7M/F01JDB0FXRD/sudoku3x3.png","permalink_public":"https://slack-files.com/T68168MUP-F01JDB0FXRD-3746801d05","is_starred":false,"has_rich_preview":false}],"color":"D0D0D0","from_url":"https://julialang.slack.com/archives/CDPQVTD4L/p1610708786007900","is_share":true,"footer":"Posted in #pkg-announcements"}],"blocks":[{"type":"rich_text","block_id":"1Gx","elements":[{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"ebded6dd-9285-41c2-b64d-555de5d51359","type":"message","text":"I may be missing something simple, but is there a way to save the intermediate values of the parameters during each iteration with Optim.jl?\n\nFor example, I'm doing 100 iterations of LBFGS and would like to save 100 arrays with the values of parameters being optimized. The `callback` option seems to only have access to the objective function value. If I change `f` by hand to add a callback there, I would get more \"timestamps\" than iterations.","user":"U6BJ9E351","ts":"1611661662.003800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IJAC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I may be missing something simple, but is there a way to save the intermediate values of the parameters during each iteration with Optim.jl?\n\nFor example, I'm doing 100 iterations of LBFGS and would like to save 100 arrays with the values of parameters being optimized. The "},{"type":"text","text":"callback","style":{"code":true}},{"type":"text","text":" option seems to only have access to the objective function value. If I change "},{"type":"text","text":"f","style":{"code":true}},{"type":"text","text":" by hand to add a callback there, I would get more \"timestamps\" than iterations."}]}]}]},{"client_msg_id":"e0df1788-feb9-493c-ba98-044d1493498d","type":"message","text":"(I understand doing this is costly in general, but I just need it for demo purposes)","user":"U6BJ9E351","ts":"1611661688.004300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kRC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(I understand doing this is costly in general, but I just need it for demo purposes)"}]}]}]},{"client_msg_id":"829869aa-18fb-49f2-9c23-9d69880fa3f0","type":"message","text":"You have to turn on the extended trace","user":"U6CJRSR63","ts":"1611670611.004500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"T24p","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You have to turn on the extended trace"}]}]}]},{"client_msg_id":"08d440a2-bd84-40e0-922e-992249479f07","type":"message","text":"Then you will have `\"x\"` in the dictionary passed to the callback","user":"U6CJRSR63","ts":"1611670625.004800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UWb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Then you will have "},{"type":"text","text":"\"x\"","style":{"code":true}},{"type":"text","text":" in the dictionary passed to the callback"}]}]}]},{"client_msg_id":"ae87e240-1d33-4e61-b257-041cd78e2a99","type":"message","text":"```using OptimTestProblems, Optim\nproblem = MultivariateProblems.UnconstrainedProblems.examples[\"Rosenbrock\"]\n\nf = MultivariateProblems.objective(problem)\ng! = MultivariateProblems.gradient(problem)\ninitial_x = problem.initial_x\nd2 = OnceDifferentiable(f, g!, initial_x)\n\nmethod = LBFGS()\n\nxs = []\ncb = tr -&gt; begin\n            push!(xs, tr[end].metadata[\"x\"])\n            false\n        end\n\noptions = Optim.Options(callback = cb, show_every=3, store_trace=true, extended_trace=true)\n\noptimize(d2, initial_x, method, options)\n\nxs```","user":"U6CJRSR63","ts":"1611671338.005000","team":"T68168MUP","edited":{"user":"U6CJRSR63","ts":"1611671403.000000"},"blocks":[{"type":"rich_text","block_id":"Lsqh","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using OptimTestProblems, Optim\nproblem = MultivariateProblems.UnconstrainedProblems.examples[\"Rosenbrock\"]\n\nf = MultivariateProblems.objective(problem)\ng! = MultivariateProblems.gradient(problem)\ninitial_x = problem.initial_x\nd2 = OnceDifferentiable(f, g!, initial_x)\n\nmethod = LBFGS()\n\nxs = []\ncb = tr -> begin\n            push!(xs, tr[end].metadata[\"x\"])\n            false\n        end\n\noptions = Optim.Options(callback = cb, show_every=3, store_trace=true, extended_trace=true)\n\noptimize(d2, initial_x, method, options)\n\nxs"}]}]}]},{"client_msg_id":"a5593bba-c28b-4893-8d16-9f289f0b6ab3","type":"message","text":"```julia&gt; xs\n10-element Array{Any,1}:\n [-1.2, 1.0]\n [-1.0273245872918702, 1.0519171772443916]\n [-0.5778727732836022, 0.30910840092006675]\n [-0.18592337280782262, 0.024501100158156375]\n [0.08298376142192398, -0.02506995929967343]\n [0.541793804618421, 0.2806815781121066]\n [0.7742123596652257, 0.5990640879627914]\n [0.9310195762135645, 0.8620343536910203]\n [0.9938739225073766, 0.9864589201403947]\n [0.9999989497058278, 0.9999979677375264]```","user":"U6CJRSR63","ts":"1611671434.005600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1Rt","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> xs\n10-element Array{Any,1}:\n [-1.2, 1.0]\n [-1.0273245872918702, 1.0519171772443916]\n [-0.5778727732836022, 0.30910840092006675]\n [-0.18592337280782262, 0.024501100158156375]\n [0.08298376142192398, -0.02506995929967343]\n [0.541793804618421, 0.2806815781121066]\n [0.7742123596652257, 0.5990640879627914]\n [0.9310195762135645, 0.8620343536910203]\n [0.9938739225073766, 0.9864589201403947]\n [0.9999989497058278, 0.9999979677375264]"}]}]}]},{"client_msg_id":"638f5425-19bc-4f7c-9810-97329450efeb","type":"message","text":"Thank you, that worked!","user":"U6BJ9E351","ts":"1611671531.005700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1Yj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thank you, that worked!"}]}]}]},{"client_msg_id":"1178921f-3090-4156-848c-c4ed49157801","type":"message","text":"can I copy paste it and post it here <https://discourse.julialang.org/t/optim-jl-callback/52625> (and here <https://discourse.julialang.org/t/optim-callback-function-how-to-use-optimization-variable/8807>) as an answer? Looks like somebody had asked the same question on discourse","user":"U6BJ9E351","ts":"1611671568.006500","team":"T68168MUP","edited":{"user":"U6BJ9E351","ts":"1611671643.000000"},"blocks":[{"type":"rich_text","block_id":"qzXHt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"can I copy paste it and post it here "},{"type":"link","url":"https://discourse.julialang.org/t/optim-jl-callback/52625"},{"type":"text","text":" (and here "},{"type":"link","url":"https://discourse.julialang.org/t/optim-callback-function-how-to-use-optimization-variable/8807"},{"type":"text","text":") as an answer? Looks like somebody had asked the same question on discourse"}]}]}]},{"client_msg_id":"78aa1d7d-fe05-490a-8c0c-7227ccbf6d57","type":"message","text":"yeah","user":"U6CJRSR63","ts":"1611674529.007200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ta=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yeah"}]}]}]},{"client_msg_id":"30a261ef-c362-49a0-8f1c-fbe8ddc35bb4","type":"message","text":"hmm... what is the recommended package for `erf()` ? `SpecialFunctions.jl` ?","user":"U013V2CFZAN","ts":"1611834428.008800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eZ5Ok","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hmm... what is the recommended package for "},{"type":"text","text":"erf()","style":{"code":true}},{"type":"text","text":" ? "},{"type":"text","text":"SpecialFunctions.jl","style":{"code":true}},{"type":"text","text":" ?"}]}]}]},{"client_msg_id":"ae904fff-be15-4cdf-a9b3-c99cd7e49b44","type":"message","text":"Might any of you know if there's a standard formulation for approximating `exp(x)` that avoids overflow issues by switching to the Taylor approximation or something like that for `x` big enough?","user":"U91Q3595Y","ts":"1611935792.010600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Jsush","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Might any of you know if there's a standard formulation for approximating "},{"type":"text","text":"exp(x)","style":{"code":true}},{"type":"text","text":" that avoids overflow issues by switching to the Taylor approximation or something like that for "},{"type":"text","text":"x","style":{"code":true}},{"type":"text","text":" big enough?"}]}]}],"thread_ts":"1611935792.010600","reply_count":15,"reply_users_count":4,"latest_reply":"1612024909.019500","reply_users":["U677NAWV8","U91Q3595Y","U6CJRSR63","U67G3QRJM"],"subscribed":false},{"type":"message","text":"","user":"U82RE6STE","ts":"1612105963.019700","team":"T68168MUP","attachments":[{"fallback":"[January 31st, 2021 3:27 PM] mathieu.besancon: hi folks, if some are interested, I'll do a stream in a bit more than an hour\n<https://twitter.com/matbesancon/status/1355830167059496971>\nprobably on optimization","ts":"1612103252.000800","author_id":"U82RE6STE","author_subname":"Mathieu Besançon","channel_id":"C01123T3NLT","channel_name":"streaming","is_msg_unfurl":true,"text":"hi folks, if some are interested, I'll do a stream in a bit more than an hour\n<https://twitter.com/matbesancon/status/1355830167059496971>\nprobably on optimization","author_name":"Mathieu Besançon","author_link":"https://julialang.slack.com/team/U82RE6STE","author_icon":"https://avatars.slack-edge.com/2018-01-24/305197567655_508578aa5910bca4b959_48.jpg","mrkdwn_in":["text"],"color":"D0D0D0","from_url":"https://julialang.slack.com/archives/C01123T3NLT/p1612103252000800","is_share":true,"footer":"Posted in #streaming"}],"thread_ts":"1612105963.019700","reply_count":1,"reply_users_count":1,"latest_reply":"1612134675.019800","reply_users":["U0138UTB7A4"],"subscribed":false,"reactions":[{"name":"+1","users":["U019078LDND"],"count":1}]},{"client_msg_id":"0cacdfb3-f079-4244-93ae-55162efcec46","type":"message","text":"What do we think the quickest way to find roots of bessel functions of first kind zero order that would satisfy `J₀(A*sₙ) = 0` where A is some varying positive constant? I see there is FunctionZeros.jl that will find the roots of bessels functions but unfortunately I am using this as an inverse model and calculating sₙ for each iteration is very slow (as the constant A depends on the fitting parameters in the fitting model)....","user":"U01FSUY7YES","ts":"1612198572.024700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"M23I","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What do we think the quickest way to find roots of bessel functions of first kind zero order that would satisfy `J₀(A*sₙ) = 0` where A is some varying positive constant? I see there is FunctionZeros.jl that will find the roots of bessels functions but unfortunately I am using this as an inverse model and calculating sₙ for each iteration is very slow (as the constant A depends on the fitting parameters in the fitting model)...."}]}]}],"thread_ts":"1612198572.024700","reply_count":9,"reply_users_count":3,"latest_reply":"1612324646.033300","reply_users":["U01FSUY7YES","U67G3QRJM","UHDQQ4GN6"],"subscribed":false},{"type":"message","text":"","user":"U6A936746","ts":"1612217833.025300","team":"T68168MUP","attachments":[{"fallback":"[February 1st, 2021 8:58 PM] rmsrosa: Hi. I didn’t find a specific channel for questions about JuMP. Please, advise me if there is a better place for this. Anyway. It is my first time using JuMP and SumOfSquares. I would like to use it to get bounds over time-averages of quantities of solutions of differential equations. I started following the examples in <https://jump.dev/SumOfSquares.jl/stable/>, for bounds of functions and for searching for Lyapunov functions. The only solver I saw being used there is `CSDP`. But I would like to use `SDPA-GMP` since it was the one that I saw working in the MATLAB ecosystem. My Julia code with `CSDP` does not converge properly. But I get an error when replacing `CSDP`with `SDPAFamily`. Is there something else I should do? The error I get is the same error that appears when I replace `CSDP.Optimizer` by `SDPAFamily.Optimizer` in the Lyapunov example <https://jump.dev/SumOfSquares.jl/stable/generated/Lyapunov%20Function%20Search/>","ts":"1612213105.482100","author_id":"U01CR62LAAD","author_subname":"Ricardo M. S. Rosa","channel_id":"C67910KEH","channel_name":"general","is_msg_unfurl":true,"is_thread_root_unfurl":true,"text":"Hi. I didn’t find a specific channel for questions about JuMP. Please, advise me if there is a better place for this. Anyway. It is my first time using JuMP and SumOfSquares. I would like to use it to get bounds over time-averages of quantities of solutions of differential equations. I started following the examples in <https://jump.dev/SumOfSquares.jl/stable/>, for bounds of functions and for searching for Lyapunov functions. The only solver I saw being used there is `CSDP`. But I would like to use `SDPA-GMP` since it was the one that I saw working in the MATLAB ecosystem. My Julia code with `CSDP` does not converge properly. But I get an error when replacing `CSDP`with `SDPAFamily`. Is there something else I should do? The error I get is the same error that appears when I replace `CSDP.Optimizer` by `SDPAFamily.Optimizer` in the Lyapunov example <https://jump.dev/SumOfSquares.jl/stable/generated/Lyapunov%20Function%20Search/>","author_name":"Ricardo M. S. Rosa","author_link":"https://julialang.slack.com/team/U01CR62LAAD","author_icon":"https://secure.gravatar.com/avatar/93ad793225cfbb515a9d3c603081edaf.jpg?s=48&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0017-48.png","mrkdwn_in":["text"],"color":"D0D0D0","from_url":"https://julialang.slack.com/archives/C67910KEH/p1612213105482100?thread_ts=1612213105482100&cid=C67910KEH","is_share":true,"footer":"Thread in #general"}]},{"client_msg_id":"16370124-792b-4f84-b2a3-457f445c2d01","type":"message","text":"hi <@U01CR62LAAD> you might have more luck in <#CNEGS2YF5|jump-dev-bridged> or even more specific in the sum-of-square gitter here: <https://gitter.im/JuliaOpt/SumOfSquares.jl>","user":"U82RE6STE","ts":"1612258193.030600","team":"T68168MUP","attachments":[{"title":"JuliaOpt/SumOfSquares.jl","title_link":"https://gitter.im/JuliaOpt/SumOfSquares.jl","text":"Where developers come to talk.","fallback":"JuliaOpt/SumOfSquares.jl","thumb_url":"https://avatars-02.gitter.im/group/iv/4/57542cbbc43b8c601977603c","from_url":"https://gitter.im/JuliaOpt/SumOfSquares.jl","thumb_width":128,"thumb_height":128,"service_icon":"http://cdn03.gitter.im/_s/0240288c4/images/favicon-normal.ico","service_name":"gitter.im","id":1,"original_url":"https://gitter.im/JuliaOpt/SumOfSquares.jl"}],"blocks":[{"type":"rich_text","block_id":"ZGU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hi "},{"type":"user","user_id":"U01CR62LAAD"},{"type":"text","text":" you might have more luck in "},{"type":"channel","channel_id":"CNEGS2YF5"},{"type":"text","text":" or even more specific in the sum-of-square gitter here: "},{"type":"link","url":"https://gitter.im/JuliaOpt/SumOfSquares.jl"}]}]}],"thread_ts":"1612258193.030600","reply_count":1,"reply_users_count":1,"latest_reply":"1612268857.031100","reply_users":["U01CR62LAAD"],"subscribed":false},{"type":"message","subtype":"channel_join","ts":"1612258197.030900","user":"U01CR62LAAD","text":"<@U01CR62LAAD> has joined the channel","inviter":"U82RE6STE"},{"type":"message","subtype":"thread_broadcast","text":"Just to follow up on numerically evaluating the inverse Laplace transform. It didn't look like InverseLaplace.jl had implemented the Post-Widder formula as described <https://arxiv.org/abs/1204.4754|here>. I implemented it below if anyone is interested. It's actually really impressive how many less evaluations in the frequency domain you need to reconstruct the time domain signal (compared to FT). If anyone has any optimization tips for the algorithm (PWcoeffs can be precomputed, LT_postwid is very performance critical) let me know.\n```# Compute Post-Widder coefficients Vk\nfunction _PWcoeffs(N)\n    v = zeros(N)\n    aux = 0.0\n    for k in 1:N\n        for j in floor(Int, (k + 1)/2):minimum(Int, [k, N/2])\n            aux = big(j)^(N/2)*factorial(big(2*j))\n            aux /= factorial(big(N/2 - j))*factorial(big(j))*factorial(big(j-1))\n            aux /= factorial(big(k - j))*factorial(big(2*j - k))\n            v[k] += aux\n        end\n        v[k] *= (-1)^(k + N/2) \n    end\n    return v\nend\n\n# compute LT for an array of t using multithreading\nfunction LT_postwid(f::Function, v, t::AbstractArray)\n    N = length(v)\n    a = zeros(length(t))\n    Threads.@threads for ind in eachindex(t)\n        for k in 1:N\n            a[ind] += v[k]*f(k*log(2)/t[ind])\n        end\n        a[ind] *= log(2)/t[ind]\n    end\n    return a\nend\n\n### example\nv = _PWcoeffs(18)\nlt2 = LT_postwid(s -&gt; exp(-sqrt(3 + s/3))/(4*pi*0.1),v, 0.01:0.01:4)```","user":"U01FSUY7YES","ts":"1612536580.034300","thread_ts":"1612198572.024700","root":{"client_msg_id":"0cacdfb3-f079-4244-93ae-55162efcec46","type":"message","text":"What do we think the quickest way to find roots of bessel functions of first kind zero order that would satisfy `J₀(A*sₙ) = 0` where A is some varying positive constant? I see there is FunctionZeros.jl that will find the roots of bessels functions but unfortunately I am using this as an inverse model and calculating sₙ for each iteration is very slow (as the constant A depends on the fitting parameters in the fitting model)....","user":"U01FSUY7YES","ts":"1612198572.024700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"M23I","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What do we think the quickest way to find roots of bessel functions of first kind zero order that would satisfy `J₀(A*sₙ) = 0` where A is some varying positive constant? I see there is FunctionZeros.jl that will find the roots of bessels functions but unfortunately I am using this as an inverse model and calculating sₙ for each iteration is very slow (as the constant A depends on the fitting parameters in the fitting model)...."}]}]}],"thread_ts":"1612198572.024700","reply_count":11,"reply_users_count":3,"latest_reply":"1612537111.034900","reply_users":["U01FSUY7YES","U67G3QRJM","UHDQQ4GN6"],"subscribed":false},"attachments":[{"service_name":"arXiv.org","title":"Review of Inverse Laplace Transform Algorithms for Laplace-Space...","title_link":"https://arxiv.org/abs/1204.4754","text":"A boundary element method (BEM) simulation is used to compare the efficiency of numerical inverse Laplace transform strategies, considering general requirements of Laplace-space numerical...","fallback":"arXiv.org: Review of Inverse Laplace Transform Algorithms for Laplace-Space...","from_url":"https://arxiv.org/abs/1204.4754","service_icon":"https://static.arxiv.org/static/browse/0.3.2.6/images/icons/favicon.ico","id":1,"original_url":"https://arxiv.org/abs/1204.4754"}],"blocks":[{"type":"rich_text","block_id":"K42","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Just to follow up on numerically evaluating the inverse Laplace transform. It didn't look like InverseLaplace.jl had implemented the Post-Widder formula as described "},{"type":"link","url":"https://arxiv.org/abs/1204.4754","text":"here"},{"type":"text","text":". I implemented it below if anyone is interested. It's actually really impressive how many less evaluations in the frequency domain you need to reconstruct the time domain signal (compared to FT). If anyone has any optimization tips for the algorithm (PWcoeffs can be precomputed, LT_postwid is very performance critical) let me know.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"# Compute Post-Widder coefficients Vk\nfunction _PWcoeffs(N)\n    v = zeros(N)\n    aux = 0.0\n    for k in 1:N\n        for j in floor(Int, (k + 1)/2):minimum(Int, [k, N/2])\n            aux = big(j)^(N/2)*factorial(big(2*j))\n            aux /= factorial(big(N/2 - j))*factorial(big(j))*factorial(big(j-1))\n            aux /= factorial(big(k - j))*factorial(big(2*j - k))\n            v[k] += aux\n        end\n        v[k] *= (-1)^(k + N/2) \n    end\n    return v\nend\n\n# compute LT for an array of t using multithreading\nfunction LT_postwid(f::Function, v, t::AbstractArray)\n    N = length(v)\n    a = zeros(length(t))\n    Threads.@threads for ind in eachindex(t)\n        for k in 1:N\n            a[ind] += v[k]*f(k*log(2)/t[ind])\n        end\n        a[ind] *= log(2)/t[ind]\n    end\n    return a\nend\n\n### example\nv = _PWcoeffs(18)\nlt2 = LT_postwid(s -> exp(-sqrt(3 + s/3))/(4*pi*0.1),v, 0.01:0.01:4)"}]}]}],"client_msg_id":"9842bd63-098d-4b72-b449-25c3ae7e99f3","edited":{"user":"U01FSUY7YES","ts":"1612536710.000000"}},{"client_msg_id":"025DA368-A5C2-425A-9224-AC7FF8D66426","type":"message","text":"Is there a way to get the time stamps and current objective function value for a MILP in JuMP using CBC? I’d like to plot this to see how much better the solution is over time.","user":"USSNH7BGT","ts":"1612798973.038500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4QCK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to get the time stamps and current objective function value for a MILP in JuMP using CBC? I’d like to plot this to see how much better the solution is over time."}]}]}],"thread_ts":"1612798973.038500","reply_count":2,"reply_users_count":2,"latest_reply":"1612799302.039200","reply_users":["UCT7E536E","USSNH7BGT"],"subscribed":false},{"client_msg_id":"ca245530-efc1-487d-8f2f-8b7c934ef144","type":"message","text":"I'm trying to solve an optimization problem with an equality constraint with NLopt. The basic syntax asks for separate functions to compute the objective and the constraint, but this requires redoing the same calculation from the inputs twice. Is there any way to avoid that?","user":"U91Q3595Y","ts":"1613005424.046100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bfl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm trying to solve an optimization problem with an equality constraint with NLopt. The basic syntax asks for separate functions to compute the objective and the constraint, but this requires redoing the same calculation from the inputs twice. Is there any way to avoid that?"}]}]}]},{"client_msg_id":"96c91c17-3cd0-43e5-bbf2-4d4c513787a9","type":"message","text":"Hi people! I have an optimization problem which is convex (I know that) but not DCP-compliant. That means I can’t use `Convex.jl` but I would still like to benefit from algorithms specifically taylored for convex optimization. Is there any alternative besides nonlinear modelling with `JuMP.jl` ?","user":"U01GMP3HF9C","ts":"1613028825.050000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"cSH4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi people! I have an optimization problem which is convex (I know that) but not DCP-compliant. That means I can’t use "},{"type":"text","text":"Convex.jl","style":{"code":true}},{"type":"text","text":" but I would still like to benefit from algorithms specifically taylored for convex optimization. Is there any alternative besides nonlinear modelling with "},{"type":"text","text":"JuMP.jl","style":{"code":true}},{"type":"text","text":" ?"}]}]}]},{"client_msg_id":"968451d2-9fe5-403d-8ae4-b3baf7fe5232","type":"message","text":"`Optim`’s `IPNewton` or use `Ipopt`/`NLopt` directly.","user":"U85JBUGGP","ts":"1613032087.050400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ARpb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Optim","style":{"code":true}},{"type":"text","text":"’s "},{"type":"text","text":"IPNewton","style":{"code":true}},{"type":"text","text":" or use "},{"type":"text","text":"Ipopt","style":{"code":true}},{"type":"text","text":"/"},{"type":"text","text":"NLopt","style":{"code":true}},{"type":"text","text":" directly."}]}]}]},{"client_msg_id":"3adb3583-bd99-48b3-8c19-2367b638e7d3","type":"message","text":"You say convex, but is it constrained or just a convex objective?","user":"U6CJRSR63","ts":"1613035301.051300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bQs6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You say convex, but is it constrained or just a convex objective?"}]}]}]},{"client_msg_id":"aa2a8fc0-e1ef-49c5-8953-ba32370942eb","type":"message","text":"~Convex constraints~ Linear constraints, convex objective","user":"U01GMP3HF9C","ts":"1613035338.051500","team":"T68168MUP","edited":{"user":"U01GMP3HF9C","ts":"1613035413.000000"},"blocks":[{"type":"rich_text","block_id":"Dd1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Convex constraints","style":{"strike":true}},{"type":"text","text":" Linear constraints, convex objective"}]}]}]},{"client_msg_id":"59bcc833-4144-4aa7-9b63-e301824663ce","type":"message","text":"And basically I have to use a reformulation that is not DCP","user":"U01GMP3HF9C","ts":"1613035348.051800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bbo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And basically I have to use a reformulation that is not DCP"}]}]}]},{"client_msg_id":"3c01c0dd-a548-4b4c-9f5b-4dafc2692d68","type":"message","text":"Okay. I agree with Mohamed's suggestions above, I was just curious. The last two probably have good handling of the linear constraints (I say that as the maintainer of Optim...). The first does quite well compared to the time put into it, but I have to admit that it's not exactly as battle tested as the other options.","user":"U6CJRSR63","ts":"1613035582.054500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nT3y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Okay. I agree with Mohamed's suggestions above, I was just curious. The last two probably have good handling of the linear constraints (I say that as the maintainer of Optim...). The first does quite well compared to the time put into it, but I have to admit that it's not exactly as battle tested as the other options."}]}]}],"reactions":[{"name":"thumbsup_all","users":["U01GMP3HF9C"],"count":1}]},{"client_msg_id":"9dbb2ddd-f47e-4b9a-856a-d34791de4f2d","type":"message","text":"Would you suggest going through JuMP or using a solver wrapper directly with a solver that is specific to convex problems?","user":"U01GMP3HF9C","ts":"1613035617.055000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"m9=G","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Would you suggest going through JuMP or using a solver wrapper directly with a solver that is specific to convex problems?"}]}]}]},{"client_msg_id":"c5185bb3-fc96-470f-8b36-a43ac4469883","type":"message","text":"I'd not go through JuMP, but that's mostly because I'm not really a JuMP user. I would probably just pick an algorithm that could solve my problem (linear constraints) and not worry about the convexity of the objective beyond using it to not worry about having found a non-global optimum.","user":"U6CJRSR63","ts":"1613035823.057000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=YE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'd not go through JuMP, but that's mostly because I'm not really a JuMP user. I would probably just pick an algorithm that could solve my problem (linear constraints) and not worry about the convexity of the objective beyond using it to not worry about having found a non-global optimum."}]}]}]},{"client_msg_id":"4a744037-ed50-4503-8fbc-d078f52050f9","type":"message","text":"Does setting a start value to a decision variable provide any upside in terms of performance in the solving phase?","user":"U01EZ6VN118","ts":"1613059286.058000","team":"T68168MUP","edited":{"user":"U01EZ6VN118","ts":"1613059299.000000"},"blocks":[{"type":"rich_text","block_id":"2yU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does setting a start value to a decision variable provide any upside in terms of performance in the solving phase?"}]}]}],"thread_ts":"1613059286.058000","reply_count":18,"reply_users_count":2,"latest_reply":"1613060963.061700","reply_users":["UGD4K0Z25","U01EZ6VN118"],"subscribed":false},{"client_msg_id":"5fe9ea77-8fea-4b46-9cf5-26ac4adc51ce","type":"message","text":"Taken from the JuMP documentation — what’s the upside of using `SparseAxisArray` instead of a  `Dict` when dealing with `VariableRef` objects? Does it provide a faster querying time?\n```julia&gt; dict = Dict((:a, 2) =&gt; 1.0, (:a, 3) =&gt; 2.0, (:b, 3) =&gt; 3.0)\nDict{Tuple{Symbol,Int64},Float64} with 3 entries:\n  (:b, 3) =&gt; 3.0\n  (:a, 2) =&gt; 1.0\n  (:a, 3) =&gt; 2.0\njulia&gt; array = JuMP.Containers.SparseAxisArray(dict)\nJuMP.Containers.SparseAxisArray{Float64,2,Tuple{Symbol,Int64}} with 3 entries:\n  [b, 3]  =  3.0\n  [a, 2]  =  1.0\n  [a, 3]  =  2.0\njulia&gt; array[:b, 3]\n3.0```","user":"U01EZ6VN118","ts":"1613073545.064000","team":"T68168MUP","edited":{"user":"U01EZ6VN118","ts":"1613073592.000000"},"blocks":[{"type":"rich_text","block_id":"qF1k","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Taken from the JuMP documentation — what’s the upside of using "},{"type":"text","text":"SparseAxisArray","style":{"code":true}},{"type":"text","text":" instead of a  `Dict` when dealing with "},{"type":"text","text":"VariableRef","style":{"code":true}},{"type":"text","text":" objects? Does it provide a faster querying time?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> dict = Dict((:a, 2) => 1.0, (:a, 3) => 2.0, (:b, 3) => 3.0)\nDict{Tuple{Symbol,Int64},Float64} with 3 entries:\n  (:b, 3) => 3.0\n  (:a, 2) => 1.0\n  (:a, 3) => 2.0\njulia> array = JuMP.Containers.SparseAxisArray(dict)\nJuMP.Containers.SparseAxisArray{Float64,2,Tuple{Symbol,Int64}} with 3 entries:\n  [b, 3]  =  3.0\n  [a, 2]  =  1.0\n  [a, 3]  =  2.0\njulia> array[:b, 3]\n3.0"}]}]}]},{"client_msg_id":"a473ee69-3830-4afd-be1b-c89cc8843ece","type":"message","text":"SparseAxisArray has less overhead, it behaves a bit like the collections from SparseArrays","user":"U82RE6STE","ts":"1613074807.064900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Cja","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"SparseAxisArray has less overhead, it behaves a bit like the collections from SparseArrays"}]}]}]},{"client_msg_id":"d2346159-e59e-43f4-b1bf-6fce1f4ee3c2","type":"message","text":"more importantly, I believe SparseAxisArray is actually an array, thoguh it looks like it also supports some bizarre indexing schemes","user":"U9VG1AYSG","ts":"1613078354.065400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"N4H","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"more importantly, I believe SparseAxisArray is actually an array, thoguh it looks like it also supports some bizarre indexing schemes"}]}]}],"thread_ts":"1613078354.065400","reply_count":2,"reply_users_count":2,"latest_reply":"1613079409.065700","reply_users":["U82RE6STE","U9VG1AYSG"],"subscribed":false},{"client_msg_id":"f8bbc81f-dc84-44e8-b990-686cf530677a","type":"message","text":"Cross-posting: the open-source HiGHS solver is now accessible from Julia!\n<https://discourse.julialang.org/t/highs-jl-v0-1-0-released/55114>","user":"UCT7E536E","ts":"1613144435.067700","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"HiGHS.jl v0.1.0 released","title_link":"https://discourse.julialang.org/t/highs-jl-v0-1-0-released/55114","text":"Dear all, We are happy to announce the initial v0.1 release of HiGHS.jl, a wrapper for HiGHS. HiGHS is a new high-performance open-source linear programming solver being developed by Julian Hall and colleagues at the University of Edinburgh, Scotland. You can try it out as follows (note that it requires Julia 1.3 or later): import Pkg; Pkg.add(\"HiGHS\") using JuMP, HiGHS model = Model(HiGHS.Optimizer) # set_silent(model) # Uncomment to turn off printing While HiGHS is ready to use today, yo...","fallback":"JuliaLang: HiGHS.jl v0.1.0 released","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","fields":[{"title":"Reading time","value":"1 mins :clock2:","short":true},{"title":"Likes","value":"10 :heart:","short":true}],"ts":1613100904,"from_url":"https://discourse.julialang.org/t/highs-jl-v0-1-0-released/55114","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/highs-jl-v0-1-0-released/55114"}],"blocks":[{"type":"rich_text","block_id":"urjvT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Cross-posting: the open-source HiGHS solver is now accessible from Julia!\n"},{"type":"link","url":"https://discourse.julialang.org/t/highs-jl-v0-1-0-released/55114"}]}]}],"reactions":[{"name":"party_wizard","users":["U82RE6STE","U01GMP3HF9C","ULG5V164A","UCZ7VBGUD"],"count":4},{"name":"tada","users":["UN2U72Q3F","UCZ7VBGUD"],"count":2}]},{"client_msg_id":"1863ba85-c3aa-4b66-bc0e-83900cd46914","type":"message","text":"Am I reading correctly that it supports both LP and MIP? That might push me to update LongestPaths to MOI.","user":"UBVE598BC","ts":"1613148183.069800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rJy64","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Am I reading correctly that it supports both LP and MIP? That might push me to update LongestPaths to MOI."}]}]}],"thread_ts":"1613148183.069800","reply_count":1,"reply_users_count":1,"latest_reply":"1613148455.070500","reply_users":["UCT7E536E"],"subscribed":false},{"client_msg_id":"39c9f82e-102f-4e43-9cdc-81e8d10d6cb3","type":"message","text":"I have a `SparseAxisArray` defined as\n```activityparticipants::JuMP.Containers.SparseAxisArray{String, 3, Tuple{String, String, String}}```\nbut when I try to access a slice as `activityparticipants[:, :, \"FormalWorkMeeting\"]` I get the following error\n```ArgumentError: Indexing with `:` is not supported by Containers.SparseAxisArray```\nAny idea why?\nEDIT: OK  looks like the slice operator is not supported for any kind of indices. I’ll revert to a `DenseAxisArray`  then","user":"U01EZ6VN118","ts":"1613148493.071200","team":"T68168MUP","edited":{"user":"U01EZ6VN118","ts":"1613151005.000000"},"blocks":[{"type":"rich_text","block_id":"3S8Y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a "},{"type":"text","text":"SparseAxisArray","style":{"code":true}},{"type":"text","text":" defined as\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"activityparticipants::JuMP.Containers.SparseAxisArray{String, 3, Tuple{String, String, String}}"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"but when I try to access a slice as "},{"type":"text","text":"activityparticipants[:, :, \"FormalWorkMeeting\"]","style":{"code":true}},{"type":"text","text":" I get the following error\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ArgumentError: Indexing with `:` is not supported by Containers.SparseAxisArray"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Any idea why?\nEDIT: OK  looks like the slice operator is not supported for any kind of indices. I’ll revert to a "},{"type":"text","text":"DenseAxisArray","style":{"code":true}},{"type":"text","text":"  then"}]}]}]},{"client_msg_id":"f8149310-0f7f-4968-a8da-79e99106e42c","type":"message","text":"Hi! I'd like to ask for a suggestion of what approach and package to use for an opt problem. Basically, I need to fit data with a heavily nonlinear formula, so: the objective is fast to compute, analytical/autodiff derivatives are easily available, but it is known that the objective \"surface\" is very complex with local minima. There are about 10-20 parameters. Is there any go-to method for such cases?","user":"UGTUKUHLN","ts":"1613173835.076800","team":"T68168MUP","edited":{"user":"UGTUKUHLN","ts":"1613173885.000000"},"blocks":[{"type":"rich_text","block_id":"QXh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi! I'd like to ask for a suggestion of what approach and package to use for an opt problem. Basically, I need to fit data with a heavily nonlinear formula, so: the objective is fast to compute, analytical/autodiff derivatives are easily available, but it is known that the objective \"surface\" is very complex with local minima. There are about 10-20 parameters. Is there any go-to method for such cases?"}]}]}],"thread_ts":"1613173835.076800","reply_count":2,"reply_users_count":2,"latest_reply":"1613174480.077300","reply_users":["UCZ7VBGUD","UGTUKUHLN"],"subscribed":false},{"type":"message","text":"Hey guys. How do you test random generation and optimization functionality in packages?\nThe tests always work on my PC, but fail in github/ci :disappointed:\nSome randomness is involved when <https://github.com/mmikhasenko/AlgebraPDF.jl/blob/master/test/runtests.jl#L282|data set is generated>","files":[{"id":"F01MZE0RRFF","created":1613333129,"timestamp":1613333129,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"U01087W46CF","editable":false,"size":24533,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01MZE0RRFF/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01MZE0RRFF/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01MZE0RRFF-0d9abd5f69/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01MZE0RRFF-0d9abd5f69/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01MZE0RRFF-0d9abd5f69/image_360.png","thumb_360_w":304,"thumb_360_h":360,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01MZE0RRFF-0d9abd5f69/image_160.png","original_w":394,"original_h":466,"thumb_tiny":"AwAwACjQHVvnzz+XtTv+BU0dW+Tv+dOP+7QAnf71H/AqP+AUv/AaAFooHTpiigBgxlvnPX8qU4/vYo+bJ+UdeOaXn0H50AJkZ+/Rkf3qX5vQfnRz6CgBR0ooHvRQAwbctyevNKMZ6mj58nOOvFL82e1ADfl9TS8ccml+b2o+bHagAXHYk0tJzntiloA//9k=","permalink":"https://julialang.slack.com/files/U01087W46CF/F01MZE0RRFF/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01MZE0RRFF-2ed8d76b3a","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"GZq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey guys. How do you test random generation and optimization functionality in packages?\nThe tests always work on my PC, but fail in github/ci "},{"type":"emoji","name":"disappointed"},{"type":"text","text":"\nSome randomness is involved when "},{"type":"link","url":"https://github.com/mmikhasenko/AlgebraPDF.jl/blob/master/test/runtests.jl#L282","text":"data set is generated"}]}]}],"user":"U01087W46CF","display_as_bot":false,"ts":"1613333543.087600","thread_ts":"1613333543.087600","reply_count":2,"reply_users_count":1,"latest_reply":"1613333769.088100","reply_users":["U01087W46CF"],"subscribed":false},{"client_msg_id":"5bbb0851-00ea-47ed-b071-7c06aec9143a","type":"message","text":"Is there a good (as in numerically robust, and fast) SQP solver for NLP other than Knitro?","user":"UCT7E536E","ts":"1613607871.090400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Q=Z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a good (as in numerically robust, and fast) SQP solver for NLP other than Knitro?"}]}]}]},{"client_msg_id":"c2f44594-ed2b-424e-ab4f-2e1b08fd9fd9","type":"message","text":"If I have defined a Convex.jl problem, is there a way to ask Convex if the problem is representable on a particular form, e.g., `is_qp(problem)::Bool`?","user":"UJ7DVTVQ8","ts":"1613637568.092000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0GQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If I have defined a Convex.jl problem, is there a way to ask Convex if the problem is representable on a particular form, e.g., "},{"type":"text","text":"is_qp(problem)::Bool","style":{"code":true}},{"type":"text","text":"?"}]}]}],"thread_ts":"1613637568.092000","reply_count":1,"reply_users_count":1,"latest_reply":"1613639788.093800","reply_users":["UCZ7VBGUD"],"subscribed":false},{"client_msg_id":"61102f44-48aa-4267-bdd9-05eb375f9690","type":"message","text":"If I have a nonlinear discontinuous function that I would like to minimize, which is pointwise limit of smooth functions, is there a way to use this information to help the solver (I'm thinking of using Optim solvers)?\n\nI was imagining something like the barrier trick, where the barrier gets steeper over time. Can I somehow instruct Optim to use very smooth but inaccurate approximations in the early iterations and less smooth but more accurate approximations later on?","user":"U6BJ9E351","ts":"1614006291.007500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MEOR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If I have a nonlinear discontinuous function that I would like to minimize, which is pointwise limit of smooth functions, is there a way to use this information to help the solver (I'm thinking of using Optim solvers)?\n\nI was imagining something like the barrier trick, where the barrier gets steeper over time. Can I somehow instruct Optim to use very smooth but inaccurate approximations in the early iterations and less smooth but more accurate approximations later on?"}]}]}]},{"client_msg_id":"70b10283-3666-4280-94e4-05f7b72f0468","type":"message","text":"use a loop with many Optim solves?","user":"U85JBUGGP","ts":"1614006397.007800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"r=b","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"use a loop with many Optim solves?"}]}]}]},{"client_msg_id":"f774cc6a-6ff4-42d9-8688-48b94f06d94a","type":"message","text":"Ah, I see, I could just constrain the number of iterations to be very low and iterate manually with a for loop. I was wondering if there was a built-in way, but this is probably good enough","user":"U6BJ9E351","ts":"1614007905.010100","team":"T68168MUP","edited":{"user":"U6BJ9E351","ts":"1614007924.000000"},"blocks":[{"type":"rich_text","block_id":"30CU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah, I see, I could just constrain the number of iterations to be very low and iterate manually with a for loop. I was wondering if there was a built-in way, but this is probably good enough"}]}]}]},{"client_msg_id":"3a0a2ca2-05e9-4093-8bba-b51eadc10178","type":"message","text":"or use a stopping criteria that starts loose and becomes stricter and stricter","user":"U85JBUGGP","ts":"1614009390.011000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3Zf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"or use a stopping criteria that starts loose and becomes stricter and stricter"}]}]}]},{"client_msg_id":"9dca6ed2-d752-4518-90cd-f5c143012713","type":"message","text":"<@U6BJ9E351> How many variables?","user":"U67G3QRJM","ts":"1614020810.011300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0JS","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U6BJ9E351"},{"type":"text","text":" How many variables?"}]}]}],"thread_ts":"1614020810.011300","reply_count":1,"reply_users_count":1,"latest_reply":"1614021115.011400","reply_users":["U6BJ9E351"],"subscribed":false},{"client_msg_id":"f0dfd861-113c-4cc9-883d-f2537eca96f5","type":"message","text":"Hey folks, and perhaps <@UJ7DVTVQ8> might be the right person to ask. Does anyone know if the optimization libraries in Julia are fully differentiable? So say I have a linear program--which is not differentiable. But then I read about an approach to smoothing the solution to a linear program, which would make it differentiable. So i was trying to figure out how to implement something like this. I can write the smoother, but I am am not sure about what else needs to be setup in a library to allow for differentiating though that library? Sorry, I am not sure if I am phrasing these things correct.","user":"UDDSTBX19","ts":"1614187835.016500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9Eg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey folks, and perhaps "},{"type":"user","user_id":"UJ7DVTVQ8"},{"type":"text","text":" might be the right person to ask. Does anyone know if the optimization libraries in Julia are fully differentiable? So say I have a linear program--which is not differentiable. But then I read about an approach to smoothing the solution to a linear program, which would make it differentiable. So i was trying to figure out how to implement something like this. I can write the smoother, but I am am not sure about what else needs to be setup in a library to allow for differentiating though that library? Sorry, I am not sure if I am phrasing these things correct."}]}]}],"thread_ts":"1614187835.016500","reply_count":6,"reply_users_count":3,"latest_reply":"1614188292.017600","reply_users":["UCT7E536E","UJ7DVTVQ8","UDDSTBX19"],"subscribed":false},{"client_msg_id":"e4938993-74b9-4213-99af-f1c052f6a959","type":"message","text":"How can I use reverse-mode autodiff with Optim? It says in the docs it's not fully supported","user":"U7YD3DKL2","ts":"1614343704.023100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QGA+S","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I use reverse-mode autodiff with Optim? It says in the docs it's not fully supported"}]}]}]},{"client_msg_id":"fd7b9339-2b1f-4a24-8914-b6b784ffdc61","type":"message","text":"L-BFGS is prone to getting stuck on saddle-points (like all Newton-like second-order methods). Are there alternative methods in Optim / NLopt that one can use that avoid this problem?","user":"U7YD3DKL2","ts":"1614447717.028100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jUh8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"L-BFGS is prone to getting stuck on saddle-points (like all Newton-like second-order methods). Are there alternative methods in Optim / NLopt that one can use that avoid this problem?"}]}]}],"thread_ts":"1614447717.028100","reply_count":3,"reply_users_count":2,"latest_reply":"1614449448.028700","reply_users":["U67G3QRJM","U9MD78Z9N"],"subscribed":false},{"client_msg_id":"72a267b9-6164-46d1-a8fa-95b54beff966","type":"message","text":"Not directly a julia question, but:\nAre methods like SGD and minibatch optimization useful for fitting traditional analytical models? I only see them mentioned in context of neutral networks.\nMaybe someone has experience with this, or pointers...","user":"UGTUKUHLN","ts":"1614554676.035700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=Iar","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not directly a julia question, but:\nAre methods like SGD and minibatch optimization useful for fitting traditional analytical models? I only see them mentioned in context of neutral networks.\nMaybe someone has experience with this, or pointers..."}]}]}],"thread_ts":"1614554676.035700","reply_count":6,"reply_users_count":2,"latest_reply":"1614555387.039800","reply_users":["U9MD78Z9N","UGTUKUHLN"],"subscribed":false},{"client_msg_id":"8cc67e4b-f3bd-4c2e-8858-956392af0960","type":"message","text":"Depends on the landscape","user":"U69BL50BF","ts":"1614554783.036200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oLr99","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Depends on the landscape"}]}]}],"thread_ts":"1614554783.036200","reply_count":2,"reply_users_count":2,"latest_reply":"1614556903.042000","reply_users":["UGTUKUHLN","U69BL50BF"],"subscribed":false},{"client_msg_id":"8a4a1e1d-2ada-4232-90d2-d3ced4200da7","type":"message","text":"If it's smooth enough, no BFGS is a lot faster","user":"U69BL50BF","ts":"1614554800.036800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8Ojss","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If it's smooth enough, no BFGS is a lot faster"}]}]}]},{"client_msg_id":"aa4f60a8-f849-4c53-9641-2c8e5914451e","type":"message","text":"And depends on the amount of calculations for the loss","user":"U69BL50BF","ts":"1614554815.037400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FlQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And depends on the amount of calculations for the loss"}]}]}]},{"client_msg_id":"5a341186-f7f0-4094-968c-009329994979","type":"message","text":"I'm mostly interested in escaping local minima","user":"UGTUKUHLN","ts":"1614554826.037700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Hry2i","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm mostly interested in escaping local minima"}]}]}],"thread_ts":"1614554826.037700","reply_count":19,"reply_users_count":2,"latest_reply":"1614558294.043800","reply_users":["U9MD78Z9N","UGTUKUHLN"],"subscribed":false},{"client_msg_id":"d73009ea-bb97-4f9d-a4b2-f5ee93b138f8","type":"message","text":"Decreasing amount of computation is great of course, but more of a side effect for my cases","user":"UGTUKUHLN","ts":"1614554881.038600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wbRFc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Decreasing amount of computation is great of course, but more of a side effect for my cases"}]}]}]},{"client_msg_id":"0965d46b-7854-4b36-ae21-5a952d83d173","type":"message","text":"I'm interested in using Ipopt via JuMP. My objective function is user defined, about 1000 variables. I can manually provide efficient first derivatives. Because the Hessian matrix is a) almost certainly dense; b) I don't know how to manually and efficiently calculate it; c) `ForwardDiff.Hessian(my_objective, x)` takes about an hour, I'd like to use Ipopt's L-BFGS Hessian approximation, described in the <https://coin-or.github.io/Ipopt/SPECIALS.html#QUASI_NEWTON|Ipopt docs here>.\n\nIn that case, what should I do for the last (Hessian-specifying) argument to `JuMP.register`?","user":"U73ACR3TQ","ts":"1614787456.049300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"quDc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm interested in using Ipopt via JuMP. My objective function is user defined, about 1000 variables. I can manually provide efficient first derivatives. Because the Hessian matrix is a) almost certainly dense; b) I don't know how to manually and efficiently calculate it; c) "},{"type":"text","text":"ForwardDiff.Hessian(my_objective, x)","style":{"code":true}},{"type":"text","text":" takes about an hour, I'd like to use Ipopt's L-BFGS Hessian approximation, described in the "},{"type":"link","url":"https://coin-or.github.io/Ipopt/SPECIALS.html#QUASI_NEWTON","text":"Ipopt docs here"},{"type":"text","text":".\n\nIn that case, what should I do for the last (Hessian-specifying) argument to "},{"type":"text","text":"JuMP.register","style":{"code":true}},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"d7091899-25a9-474c-ae7d-8ad82ba20fff","type":"message","text":"I don't think you need to call `JuMP.register` if you want to use L-BFGS for the Hessians. You should be able to set it with an optimiser attribute:\n```model = Model(Ipopt.Optimizer)\nset_optimizer_attribute(model, \"hessian_approximation\", \"limited-memory\")```","user":"UGHS7LC64","ts":"1614799759.051900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sFBa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I don't think you need to call "},{"type":"text","text":"JuMP.register","style":{"code":true}},{"type":"text","text":" if you want to use L-BFGS for the Hessians. You should be able to set it with an optimiser attribute:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"model = Model(Ipopt.Optimizer)\nset_optimizer_attribute(model, \"hessian_approximation\", \"limited-memory\")"}]}]}]},{"client_msg_id":"f6fea334-8a5c-4e31-a04a-fb5106e3340d","type":"message","text":"We don't support hessians for multivariate user-defined functions","user":"U014UPHHPM0","ts":"1614802824.052200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Iz70","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We don't support hessians for multivariate user-defined functions"}]}]}]},{"client_msg_id":"b56b5bab-d8c8-4ba0-919d-bf421fa5db0f","type":"message","text":"The new NLP docs make this clearer: <https://jump.dev/JuMP.jl/dev/manual/nlp/#Multivariate-functions>","user":"U014UPHHPM0","ts":"1614802889.052500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PXlhE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The new NLP docs make this clearer: "},{"type":"link","url":"https://jump.dev/JuMP.jl/dev/manual/nlp/#Multivariate-functions"}]}]}]},{"client_msg_id":"0e8bc3e3-4727-4683-af63-76419a1ce79b","type":"message","text":"<@U014UPHHPM0> Hmm the new docs are nice but I am actually pretty confused by this:\n```Register a function, gradient, and hessian\nWarning\nThis feature is only available for univariate functions.```\nIt seems like the immediately following content about splatting and Vector vs scalar inputs is describing how to handle multivariate user defined functions?","user":"U73ACR3TQ","ts":"1614804539.054000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"M7cI","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U014UPHHPM0"},{"type":"text","text":" Hmm the new docs are nice but I am actually pretty confused by this:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Register a function, gradient, and hessian\nWarning\nThis feature is only available for univariate functions."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"It seems like the immediately following content about splatting and Vector vs scalar inputs is describing how to handle multivariate user defined functions?"}]}]}]},{"client_msg_id":"a87b6708-1e31-4bbb-ae5e-969dface2fe3","type":"message","text":"Is there a Julia package that allows exploiting the function space structure of the problem (using the right inner products, instead of Euclidian l^2)?","user":"UFMH09DAR","ts":"1614809224.055300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"r4ts","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a Julia package that allows exploiting the function space structure of the problem (using the right inner products, instead of Euclidian l^2)?"}]}]}],"thread_ts":"1614809224.055300","reply_count":1,"reply_users_count":1,"latest_reply":"1614809944.055400","reply_users":["UCT7E536E"],"subscribed":false},{"client_msg_id":"5701e34d-0867-46a7-9ceb-27137279b31f","type":"message","text":"Would \"The ability to explicitly register a hessian is only available for univariate functions.\" be more explicit? Multivariate user-defined functions are supported.","user":"U014UPHHPM0","ts":"1614812988.057400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YAb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Would \"The ability to explicitly register a hessian is only available for univariate functions.\" be more explicit? Multivariate user-defined functions are supported."}]}]}]},{"client_msg_id":"b1eeeac4-6d4f-4015-b290-c8bb02255948","type":"message","text":"&gt; We don't support hessians for multivariate user-defined functions\nBut Ipopt and other solvers can still approximate those with L-BFGS, right?","user":"UGHS7LC64","ts":"1614814928.059100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"K41","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"We don't support hessians for multivariate user-defined functions"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"But Ipopt and other solvers can still approximate those with L-BFGS, right?"}]}]}]},{"client_msg_id":"45b836c4-8a29-448d-b100-045a1da6e7b3","type":"message","text":"Yes, they could still approximate it from first-order information. But that's a solver-feature not a JuMP feature.","user":"U014UPHHPM0","ts":"1614825106.060100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Yad","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes, they could still approximate it from first-order information. But that's a solver-feature not a JuMP feature."}]}]}]},{"client_msg_id":"dd40d9de-d26c-41e3-a176-91073403c4d8","type":"message","text":"Also having access to the hessian or the hessian parity can be helpful","user":"U9MD78Z9N","ts":"1614825253.060900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4YrFt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Also having access to the hessian or the hessian parity can be helpful"}]}]}]},{"client_msg_id":"17d792b8-eb74-4c70-9ddb-e74f7fc819db","type":"message","text":"Hey. I'm thinking of implementing Fourier-Motzkin by hand in julia to decide whether a set of linear inequalities (with coefficients in some number field, e.g. Q(\\sqrt 2) ) have a solution. Technically, not all inequalities are \"lax\" (i.e. ≤), some are strict (i.e. &lt;), but I think since all inequalities are linear, the solution set is a cone, so that I can make everything lax. Thus, I'm wondering:\n1. Is there a good reference on F-M and the ways to speed it up? I see wikipedia talks about some speedups.\n2. Can F-M be made into an \"incremental\" algorithm, in the sense that you start with a given set of inequalities, solve them, and *then* can add a new inequality to the lot and not start from zero?\n3. Is there a standard way to deal with strict inequalities?\n4. Is there any chance that a handcrafted \"naive\" implementation has comparable speed as just using some LP solver and testing for feasibility?\nAnd about LP solvers: If I am to actually use a LP solver to decide infeasibility, since I'll have to approximate my field elements, I think the best would be to work with one that can deal with high precision, e.g. `tulip` through `convex.jl` ; does it always output a certificate of infeasibility in case the problem is infeasible?\n\nThanks!\n*edit* I found this <http://www.cs.man.ac.uk/~tsiskarn/pdf/CRA.pdf> which might be better than F-M?","user":"U01MG0TN079","ts":"1615181190.070800","team":"T68168MUP","edited":{"user":"U01MG0TN079","ts":"1615182276.000000"},"blocks":[{"type":"rich_text","block_id":"L=no","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey. I'm thinking of implementing Fourier-Motzkin by hand in julia to decide whether a set of linear inequalities (with coefficients in some number field, e.g. Q(\\sqrt 2) ) have a solution. Technically, not all inequalities are \"lax\" (i.e. ≤), some are strict (i.e. <), but I think since all inequalities are linear, the solution set is a cone, so that I can make everything lax. Thus, I'm wondering:\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a good reference on F-M and the ways to speed it up? I see wikipedia talks about some speedups."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Can F-M be made into an \"incremental\" algorithm, in the sense that you start with a given set of inequalities, solve them, and "},{"type":"text","text":"then","style":{"bold":true}},{"type":"text","text":" can add a new inequality to the lot and not start from zero?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a standard way to deal with strict inequalities?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Is there any chance that a handcrafted \"naive\" implementation has comparable speed as just using some LP solver and testing for feasibility?"}]}],"style":"ordered","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"And about LP solvers: If I am to actually use a LP solver to decide infeasibility, since I'll have to approximate my field elements, I think the best would be to work with one that can deal with high precision, e.g. "},{"type":"text","text":"tulip","style":{"code":true}},{"type":"text","text":" through "},{"type":"text","text":"convex.jl","style":{"code":true}},{"type":"text","text":" ; does it always output a certificate of infeasibility in case the problem is infeasible?\n\nThanks!\n"},{"type":"text","text":"edit","style":{"bold":true}},{"type":"text","text":" I found this "},{"type":"link","url":"http://www.cs.man.ac.uk/~tsiskarn/pdf/CRA.pdf"},{"type":"text","text":" which might be better than F-M?"}]}]}]},{"client_msg_id":"3d6e6855-9b02-40cc-a616-3d01519c520f","type":"message","text":"I've found these two papers (<https://arxiv.org/abs/1609.00061> and <https://arxiv.org/abs/1808.01724>) that define a Pixel Array method for solving systems of NL equations. The idea sounds crazy (plot each equation as a n-dim matrix of 1s and 0s, then multiply them to find where they \"intersect\"),  but somewhat intuitive. Would anyone be interested in discussing it in a reading group(/meeting)?","user":"U01PWULG2JU","ts":"1615475594.078200","team":"T68168MUP","attachments":[{"service_name":"arXiv.org","title":"Pixel Arrays: A fast and elementary method for solving nonlinear systems","title_link":"https://arxiv.org/abs/1609.00061","text":"We present a new method, called the pixel array method, for approximating all solutions in a bounding box for an arbitrary nonlinear system of relations. In contrast with other solvers, our...","fallback":"arXiv.org: Pixel Arrays: A fast and elementary method for solving nonlinear systems","from_url":"https://arxiv.org/abs/1609.00061","service_icon":"https://static.arxiv.org/static/browse/0.3.2.6/images/icons/favicon.ico","id":1,"original_url":"https://arxiv.org/abs/1609.00061"},{"service_name":"arXiv.org","title":"Evaluating the Pixel Array Method as Applied to Partial...","title_link":"https://arxiv.org/abs/1808.01724","text":"The Pixel Array (PA) Method, originally introduced by Spivak et. al., is a fast method for solving nonlinear or linear systems. One of its distinguishing features is that it presents all solutions...","fallback":"arXiv.org: Evaluating the Pixel Array Method as Applied to Partial...","from_url":"https://arxiv.org/abs/1808.01724","service_icon":"https://static.arxiv.org/static/browse/0.3.2.6/images/icons/favicon.ico","id":2,"original_url":"https://arxiv.org/abs/1808.01724"}],"blocks":[{"type":"rich_text","block_id":"mu/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I've found these two papers ("},{"type":"link","url":"https://arxiv.org/abs/1609.00061"},{"type":"text","text":" and "},{"type":"link","url":"https://arxiv.org/abs/1808.01724"},{"type":"text","text":") that define a Pixel Array method for solving systems of NL equations. The idea sounds crazy (plot each equation as a n-dim matrix of 1s and 0s, then multiply them to find where they \"intersect\"),  but somewhat intuitive. Would anyone be interested in discussing it in a reading group(/meeting)?"}]}]}]},{"client_msg_id":"4ea72659-2b58-4e38-bb5a-14c52913f295","type":"message","text":"How can I find some Julia code examples of feasible set convex approximation (like Markov, Bernstein)?","user":"U01QSKYD4DU","ts":"1615489844.080000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PPyc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How can I find some Julia code examples of feasible set convex approximation (like Markov, Bernstein)?"}]}]}]},{"type":"message","subtype":"channel_join","ts":"1615491639.080400","user":"U9Z3H4H5F","text":"<@U9Z3H4H5F> has joined the channel","inviter":"U67G3QRJM"},{"client_msg_id":"c633bdd5-41f8-497a-81d4-fbb809cec26f","type":"message","text":"What is the state of constrained optimization in Optim.jl ?\nLast time I checked sparse jacobians of the constraints were not possible. Compatibility with AD systems?","user":"UQEDP1Q5V","ts":"1615757649.087500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"W2tnL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What is the state of constrained optimization in Optim.jl ?\nLast time I checked sparse jacobians of the constraints were not possible. Compatibility with AD systems?"}]}]}]},{"client_msg_id":"916ab2e7-b5f0-4e80-8688-49a4e5a497c3","type":"message","text":"Mmm, maybe I can take a shot on that, I implemented ad on constraints","user":"UKG4WF8PJ","ts":"1615761499.088500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oM1N","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Mmm, maybe I can take a shot on that, I implemented ad on constraints"}]}]}]},{"client_msg_id":"edad2e70-2a14-4fb3-a697-6cfd20f33e0a","type":"message","text":"Ad is supported on constraints, but only in dense form, so an sparse representation  will be faster","user":"UKG4WF8PJ","ts":"1615761554.089700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4OQTE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ad is supported on constraints, but only in dense form, so an sparse representation  will be faster"}]}]}]},{"client_msg_id":"099f2785-7ef0-4357-8e08-fb6a62afe93c","type":"message","text":"Might anyone have a recommendation for a package for finding the concave hull of a function (or set of points)?","user":"U91Q3595Y","ts":"1615764549.090400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/4k1W","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Might anyone have a recommendation for a package for finding the concave hull of a function (or set of points)?"}]}]}]},{"client_msg_id":"2234cb4e-2ff1-4040-a1e8-0dd0664f25ab","type":"message","text":"To be precise, I have a 1D function `y(x)` (say `y(x) = cos(x)` and I want the smallest curve that is (weakly) above every point of `y`","user":"U91Q3595Y","ts":"1615765690.091600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"78Bok","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"To be precise, I have a 1D function "},{"type":"text","text":"y(x)","style":{"code":true}},{"type":"text","text":" (say "},{"type":"text","text":"y(x) = cos(x)","style":{"code":true}},{"type":"text","text":" and I want the smallest curve that is (weakly) above every point of "},{"type":"text","text":"y","style":{"code":true}}]}]}],"thread_ts":"1615765690.091600","reply_count":1,"reply_users_count":1,"latest_reply":"1615767678.091700","reply_users":["UKG4WF8PJ"],"subscribed":false},{"client_msg_id":"1c6b55c2-d612-4b61-8561-3cd2609322f7","type":"message","text":"Hi, I'm using `NelderMead` from Optim.jl. The function calls are very slow so I'd like to do the optimization in multiple parts; is there a way to save a simplex and continue from there?","user":"UCB7L9W49","ts":"1615806082.093200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qrSq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I'm using "},{"type":"text","text":"NelderMead","style":{"code":true}},{"type":"text","text":" from Optim.jl. The function calls are very slow so I'd like to do the optimization in multiple parts; is there a way to save a simplex and continue from there?"}]}]}]},{"client_msg_id":"304929f2-65cf-4475-b38a-7e1273238e66","type":"message","text":"Is there a name for optimization but you don't have an objective but you have an Oracle which you query to compare solutions and get the best of the bunch?\nThe Oracle doesn't have to impose a total and strict ordering but it is transitive.\nThe variables making up the objects can be real valued, integer valued or categorical.\nConstraints might be (partially) know without querying an oracle.\n\nOr something like that.","user":"U9MD78Z9N","ts":"1615848932.102200","team":"T68168MUP","edited":{"user":"U9MD78Z9N","ts":"1615849116.000000"},"blocks":[{"type":"rich_text","block_id":"Ptf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a name for optimization but you don't have an objective but you have an Oracle which you query to compare solutions and get the best of the bunch?\nThe Oracle doesn't have to impose a total and strict ordering but it is transitive.\nThe variables making up the objects can be real valued, integer valued or categorical.\nConstraints might be (partially) know without querying an oracle.\n\nOr something like that."}]}]}],"thread_ts":"1615848932.102200","reply_count":2,"reply_users_count":2,"latest_reply":"1615850455.105200","reply_users":["UCT7E536E","U9MD78Z9N"],"subscribed":false},{"client_msg_id":"f9b57f82-3855-4e45-b9c9-633bf670409a","type":"message","text":"Is this the right channel to ask some basic fitting/optimization questions?","user":"U01FKQQ7J0J","ts":"1616006829.001300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Hu0vp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is this the right channel to ask some basic fitting/optimization questions?"}]}]}],"thread_ts":"1616006829.001300","reply_count":1,"reply_users_count":1,"latest_reply":"1616008127.001500","reply_users":["UGD4K0Z25"],"subscribed":false},{"client_msg_id":"6d20a49e-fc09-457a-9c82-f4c13544dddf","type":"message","text":"Hello everyone! I have a question regarding parallelization of a combinatorial operation, let me know if my question is better suited for another channel.\n\nI am using the `Combinatorics.jl` package. I have to perform an operation on two matrices for each `permutations(1:N, 4)` where `N` should go up to 100.  For now I wrote it like this,\n\n```function Δfe(M::Matrix, i, j, k, l)\n    first = M[i, j] - M[i, k]\n    second = M[l, j] - M[l, k]\n\n    return first - second\nend\n\nfunction computeU(X, U, tetrads)\n    u = 0.\n\n    for t in tetrads\n        u += Δfe(X, t...) * Δfe(U, t...)\n    end\n\n    return u / length(tetrads)\n\nend\n\nN = 100\n\nX, U = # NxN matrices\ntetrads = permutations(1:N, 4)\ncomputeU(X, U, tetrads)```\nThis is currently very slow. I would like to know if there is any way to parallelize the `for t in tetrads` or a way of restructuring this to make it faster.","user":"U01FPHR7WKB","ts":"1616060236.007800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pVK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello everyone! I have a question regarding parallelization of a combinatorial operation, let me know if my question is better suited for another channel.\n\nI am using the "},{"type":"text","text":"Combinatorics.jl","style":{"code":true}},{"type":"text","text":" package. I have to perform an operation on two matrices for each "},{"type":"text","text":"permutations(1:N, 4)","style":{"code":true}},{"type":"text","text":" where "},{"type":"text","text":"N","style":{"code":true}},{"type":"text","text":" should go up to 100.  For now I wrote it like this,\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function Δfe(M::Matrix, i, j, k, l)\n    first = M[i, j] - M[i, k]\n    second = M[l, j] - M[l, k]\n\n    return first - second\nend\n\nfunction computeU(X, U, tetrads)\n    u = 0.\n\n    for t in tetrads\n        u += Δfe(X, t...) * Δfe(U, t...)\n    end\n\n    return u / length(tetrads)\n\nend\n\nN = 100\n\nX, U = # NxN matrices\ntetrads = permutations(1:N, 4)\ncomputeU(X, U, tetrads)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nThis is currently very slow. I would like to know if there is any way to parallelize the "},{"type":"text","text":"for t in tetrads","style":{"code":true}},{"type":"text","text":" or a way of restructuring this to make it faster."}]}]}],"thread_ts":"1616060236.007800","reply_count":3,"reply_users_count":2,"latest_reply":"1616061714.008300","reply_users":["U01FPHR7WKB","U01MG0TN079"],"subscribed":false},{"client_msg_id":"518e348a-f5bf-4022-8d79-01c946115107","type":"message","text":"I'm using `Convex.jl` for some simple feasibility test of a set of linear constraints (<https://github.com/bottine/VinbergsAlgorithmNF/blob/cd4aeccfe37e84b7990e344fed698ef4e82f53f0/src/util.jl#L135>). Right now I set the problem as `p = satisfy()` and then do `p.status == MathOptInterface.INFEASIBLE &amp;&amp; return false` and `p.status == MathOptInterface.OPTIMAL` . Does this cover all possible statuses here? Is it guaranteed that I get either feasibility or infeasibility? thanks!","user":"U01MG0TN079","ts":"1616241716.014800","team":"T68168MUP","edited":{"user":"U01MG0TN079","ts":"1616241727.000000"},"blocks":[{"type":"rich_text","block_id":"0lE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm using "},{"type":"text","text":"Convex.jl","style":{"code":true}},{"type":"text","text":" for some simple feasibility test of a set of linear constraints ("},{"type":"link","url":"https://github.com/bottine/VinbergsAlgorithmNF/blob/cd4aeccfe37e84b7990e344fed698ef4e82f53f0/src/util.jl#L135"},{"type":"text","text":"). Right now I set the problem as "},{"type":"text","text":"p = satisfy()","style":{"code":true}},{"type":"text","text":" and then do "},{"type":"text","text":"p.status == MathOptInterface.INFEASIBLE && return false","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"p.status == MathOptInterface.OPTIMAL","style":{"code":true}},{"type":"text","text":" . Does this cover all possible statuses here? Is it guaranteed that I get either feasibility or infeasibility? thanks!"}]}]}]},{"client_msg_id":"60740d17-fb0d-428e-9838-590309f79f99","type":"message","text":"Does anyone know of good references on stochastic root finding? I.e., when there is some underlying deterministic function corrupted by (small-ish) levels of noise. I've got a situation when a standard Newton iteration usually works (with some averaging) but I'd like to be able to handle higher levels of noise.","user":"U73KWSUGN","ts":"1616245893.018400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"muwG7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does anyone know of good references on stochastic root finding? I.e., when there is some underlying deterministic function corrupted by (small-ish) levels of noise. I've got a situation when a standard Newton iteration usually works (with some averaging) but I'd like to be able to handle higher levels of noise."}]}]}]},{"client_msg_id":"3a72c679-75dd-41e6-ac75-e290b7298a5e","type":"message","text":"&gt;  Is it guaranteed that I get either feasibility or infeasibility?\nNo. See <https://jump.dev/MathOptInterface.jl/stable/apireference/#MathOptInterface.TerminationStatusCode> for a list of possible returns","user":"U014UPHHPM0","ts":"1616280693.018800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"H9/g","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":" Is it guaranteed that I get either feasibility or infeasibility?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"No. See "},{"type":"link","url":"https://jump.dev/MathOptInterface.jl/stable/apireference/#MathOptInterface.TerminationStatusCode"},{"type":"text","text":" for a list of possible returns"}]}]}]},{"client_msg_id":"b47c6b3c-8bb3-4d4c-9065-3c27b5ead3aa","type":"message","text":"maybe a silly question, but I was wondering: does the initial value (ie, `init` in `Optim.optimize(f, init, method)`) play a role in the `Optim.ParticleSwarm()` algorithm if I explicitly pass upper and lower bound? That is to say, are particles distributed around it initially (so I should be careful what value I put) or uniformly within the bounds?","user":"U6BJ9E351","ts":"1616439788.021100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iNAN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"maybe a silly question, but I was wondering: does the initial value (ie, "},{"type":"text","text":"init","style":{"code":true}},{"type":"text","text":" in "},{"type":"text","text":"Optim.optimize(f, init, method)","style":{"code":true}},{"type":"text","text":") play a role in the "},{"type":"text","text":"Optim.ParticleSwarm()","style":{"code":true}},{"type":"text","text":" algorithm if I explicitly pass upper and lower bound? That is to say, are particles distributed around it initially (so I should be careful what value I put) or uniformly within the bounds?"}]}]}]},{"client_msg_id":"3c7b1d94-8295-4098-aa61-d1e851bc71fd","type":"message","text":"somebody knows of a package that performs 1d fixpoint methods and fixpoint convergence acceleration methods?","user":"UKG4WF8PJ","ts":"1616531937.024000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JeKF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"somebody knows of a package that performs 1d fixpoint methods and fixpoint convergence acceleration methods?"}]}]}],"thread_ts":"1616531937.024000","reply_count":1,"reply_users_count":1,"latest_reply":"1616532414.025600","reply_users":["UJ7DVTVQ8"],"subscribed":false},{"client_msg_id":"31c05572-a249-420d-9ecc-0b7c1089662c","type":"message","text":"for example, Aitken's delta-squared process","user":"UKG4WF8PJ","ts":"1616532049.024300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"BU4et","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"for example, Aitken's delta-squared process"}]}]}]},{"client_msg_id":"8598d784-4d5d-4528-9d2d-57401abace01","type":"message","text":"i know that i can transform to newton, but a newton root finding jumps around the solution, where as a fixed point iteration (domain specific) always aproaches from the stable side","user":"UKG4WF8PJ","ts":"1616532127.025500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ofLri","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"i know that i can transform to newton, but a newton root finding jumps around the solution, where as a fixed point iteration (domain specific) always aproaches from the stable side"}]}]}]},{"client_msg_id":"9ea7a938-f85e-4a69-8a3f-70ed350a58da","type":"message","text":"Hello! Using `Convex.jl` on a simple LP problem, I'd like to:\n• If feasible, get a certificate.\n• If infeasible, get a certificate, and ideally get which equations are broken by the certificate.\nIs that possible? how should I extract those? Thanks!","user":"U01MG0TN079","ts":"1616652732.029700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bCsin","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hello! Using "},{"type":"text","text":"Convex.jl","style":{"code":true}},{"type":"text","text":" on a simple LP problem, I'd like to:\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If feasible, get a certificate."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"If infeasible, get a certificate, and ideally get which equations are broken by the certificate."}]}],"style":"bullet","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"Is that possible? how should I extract those? Thanks!"}]}]}]},{"client_msg_id":"6effdfd6-2be9-4a16-966a-feb3f77d7ce6","type":"message","text":"Essentially, I'd like to get an answer and a way to check that the answer given is the right one. Obviously, if I get a feasibility certificate, I can plug it in my original constraints and check that it's good, but if it's unfeasible, I'm not sure how to check the unfeasibility, i.e. what equations prove that my problem is unfeasible.","user":"U01MG0TN079","ts":"1616657433.031300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hSEGJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Essentially, I'd like to get an answer and a way to check that the answer given is the right one. Obviously, if I get a feasibility certificate, I can plug it in my original constraints and check that it's good, but if it's unfeasible, I'm not sure how to check the unfeasibility, i.e. what equations prove that my problem is unfeasible."}]}]}]},{"client_msg_id":"db2e46b2-a0b0-4760-86c4-f608714dd480","type":"message","text":"Read about Farkas lemma","user":"U85JBUGGP","ts":"1616658075.031800","team":"T68168MUP","edited":{"user":"U85JBUGGP","ts":"1616658088.000000"},"blocks":[{"type":"rich_text","block_id":"RMn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Read about Farkas lemma"}]}]}]},{"client_msg_id":"8a9019b5-0bf3-42c7-9e04-0e91c6939d09","type":"message","text":"In simple terms, if the dual solution is not optimal in the dual problem then the primal solution is not feasible.","user":"U85JBUGGP","ts":"1616658220.033500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zFsqw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In simple terms, if the dual solution is not optimal in the dual problem then the primal solution is not feasible."}]}]}]},{"client_msg_id":"68f8e63c-0bcb-4abf-82ad-f93e54117994","type":"message","text":"but in the docs, it seems there is both `PrimalInfeasible` and `DualInfeasible` so is there two kinds of infeasibility \"options\"?","user":"U01MG0TN079","ts":"1616658305.034300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Tyber","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"but in the docs, it seems there is both "},{"type":"text","text":"PrimalInfeasible","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"DualInfeasible","style":{"code":true}},{"type":"text","text":" so is there two kinds of infeasibility \"options\"?"}]}]}]},{"client_msg_id":"e6226858-0fac-4a5d-bea2-f2eb9b1cb803","type":"message","text":"I think in rare cases you can have both infeasible. But in general if one is unbounded the other is infeasible.","user":"U85JBUGGP","ts":"1616658654.035600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sAT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think in rare cases you can have both infeasible. But in general if one is unbounded the other is infeasible."}]}]}]},{"client_msg_id":"d61259e3-9acb-4442-bb95-fb74b5bcb6b1","type":"message","text":"So to prove that your problem is infeasible, find a feasible dual direction that always improves dual objective. Therefore, it's unbounded therefore the primal is not feasible.","user":"U85JBUGGP","ts":"1616658756.037700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=iBgH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So to prove that your problem is infeasible, find a feasible dual direction that always improves dual objective. Therefore, it's unbounded therefore the primal is not feasible."}]}]}]},{"client_msg_id":"d7f47f27-673c-43ca-bc07-40810f51c9b2","type":"message","text":"OK, thanks!","user":"U01MG0TN079","ts":"1616659290.037900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"G/Q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"OK, thanks!"}]}]}]},{"client_msg_id":"f1a87f9e-0e61-4b63-8373-fcb5834dce4d","type":"message","text":"Dear, <#C681S52FQ|math-optimization> community. I would really appreciate some help with solving constrained nonlinear optimization problems (in particular, collocated optimal control problems). First off, I am aware of `NLOptControl.jl` and, for now, am just focusing on interacting more directly with the NLP solver. [This gist](<https://gist.github.com/avonmoll/a18b1d77e418a0eb98f854d143424a73>) describes the problems I'm having in solving very simple collocated optimal control problems. The biggest issue seems to be that `GalacticOptim`'s solvers aren't respecting the constraints I specify. As for `JuMP`, it seems difficult to express even these simple dynamics. I've had some success with `NLopt`, but it's very brittle (I get `:FORCED_STOP` without any additional info on what the actual error/problem is).","user":"U01N4PQ663W","ts":"1617044160.042900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YqZgH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Dear, "},{"type":"channel","channel_id":"C681S52FQ"},{"type":"text","text":" community. I would really appreciate some help with solving constrained nonlinear optimization problems (in particular, collocated optimal control problems). First off, I am aware of "},{"type":"text","text":"NLOptControl.jl","style":{"code":true}},{"type":"text","text":" and, for now, am just focusing on interacting more directly with the NLP solver. [This gist]("},{"type":"link","url":"https://gist.github.com/avonmoll/a18b1d77e418a0eb98f854d143424a73"},{"type":"text","text":") describes the problems I'm having in solving very simple collocated optimal control problems. The biggest issue seems to be that "},{"type":"text","text":"GalacticOptim","style":{"code":true}},{"type":"text","text":"'s solvers aren't respecting the constraints I specify. As for "},{"type":"text","text":"JuMP","style":{"code":true}},{"type":"text","text":", it seems difficult to express even these simple dynamics. I've had some success with "},{"type":"text","text":"NLopt","style":{"code":true}},{"type":"text","text":", but it's very brittle (I get "},{"type":"text","text":":FORCED_STOP","style":{"code":true}},{"type":"text","text":" without any additional info on what the actual error/problem is)."}]}]}],"thread_ts":"1617044160.042900","reply_count":2,"reply_users_count":1,"latest_reply":"1617044467.043200","reply_users":["U85JBUGGP"],"is_locked":false,"subscribed":false},{"client_msg_id":"952ab841-c725-4e23-8595-6f92259b1d66","type":"message","text":"I'm trying to double check some (hand written) derivatives for use in optimization, using FiniteDifferences. My objective function has has ~600 dimensional input and real output, and takes about .1 seconds to evaluate. I know the derivatives are not zero, because if I perturb an input dimension by a little (eg .01) the output changes. But with FiniteDifferences.jl, I'm getting all zeros: `grad(forward_fdm(3,1), objective, x_0)` returns all zeros (well, strictly speaking a 1-tuple with first element an array of all zeros).\n\nI thought maybe this has to do with conversions between Float64 and Float32; internally, my objective function (which is a simulation) converts some inputs to F32 and does GPU computation. So I tried `wrapper_obj(x::Vector{Float32}) = obj(Float64.(x))` and took the gradient of `wrapper_obj`, trying to force FiniteDifferences to take bigger (float32 compatible) steps. But while the resulting gradient isn't _all_ zeros now, it's mostly zeros and definitely wrong.\n\nAnything else I can try?","user":"U73ACR3TQ","ts":"1617134898.048500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7/4e5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm trying to double check some (hand written) derivatives for use in optimization, using FiniteDifferences. My objective function has has ~600 dimensional input and real output, and takes about .1 seconds to evaluate. I know the derivatives are not zero, because if I perturb an input dimension by a little (eg .01) the output changes. But with FiniteDifferences.jl, I'm getting all zeros: "},{"type":"text","text":"grad(forward_fdm(3,1), objective, x_0)","style":{"code":true}},{"type":"text","text":" returns all zeros (well, strictly speaking a 1-tuple with first element an array of all zeros).\n\nI thought maybe this has to do with conversions between Float64 and Float32; internally, my objective function (which is a simulation) converts some inputs to F32 and does GPU computation. So I tried "},{"type":"text","text":"wrapper_obj(x::Vector{Float32}) = obj(Float64.(x))","style":{"code":true}},{"type":"text","text":" and took the gradient of "},{"type":"text","text":"wrapper_obj","style":{"code":true}},{"type":"text","text":", trying to force FiniteDifferences to take bigger (float32 compatible) steps. But while the resulting gradient isn't "},{"type":"text","text":"all","style":{"italic":true}},{"type":"text","text":" zeros now, it's mostly zeros and definitely wrong.\n\nAnything else I can try?"}]}]}],"thread_ts":"1617134898.048500","reply_count":1,"reply_users_count":1,"latest_reply":"1617135413.048600","reply_users":["U9MD78Z9N"],"is_locked":false,"subscribed":false},{"client_msg_id":"fad709bc-5aaf-45b3-9e4b-c1307c754151","type":"message","text":"Hi everyone, I'm trying to figure out this mysterious \"instability error\" that I'm getting from DifferentialEquations' Tsit5() solver. This is part of a program I'm working on for estimating the parameters of ODE systems. The ODE solver is called inside an objective function, called `objective()` and I'd like to print out the values that were passed to the objective function which caused the error. But when I try to print the input value, I get this huge mess:\n```The x values that were passed to objective(): \nForwardDiff.Dual{ForwardDiff.Tag{var\"#objective#59\"{typeof(OneAgeModel!), \nVector{Symbol}, Vector{Float64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, \nVector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64, \ntypeof(mean_square_rel_error), Vector{Real}, Vector{Union{Missing, Real}}, \nVector{Union{Missing, Real}}, Tsit5, Float64, Float64, Int64, Int64, Int64, Vector{String}, \nDataFrame}, Float64}, Float64, 10}[Dual{ForwardDiff.Tag{var\"#objective#59\"\n{typeof(OneAgeModel!), Vector{Symbol}, Vector{Float64}, Vector{Int64}, Vector{Int64}, \nVector{Int64}, Vector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64, typeof(mean_square_rel_error), Vector{Real}, Vector{Union{Missing, Real}}, \nVector{Union{Missing, Real}}, Tsit5, Float64, Float64, Int64, Int64, Int64, Vector{String},\n DataFrame}, Float64}}(NaN,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0), \nDual{ForwardDiff.Tag{var\"#objective#59\"{typeof(OneAgeModel!), Vector{Symbol}, \nVector{Float64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Real}, \nVector{Real}, typeof(f_ICs), Float64, typeof(mean_square_rel_error), Vector{Real}, \nVector{Union{Missing, Real}}, Vector{Union{Missing, Real}}, Tsit5, Float64, Float64, \nInt64, Int64, Int64, Vector{String}, DataFrame}, Float64}}\n(NaN,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0), Dual{ForwardDiff.Tag{var\"#objective#59\"\n{typeof(OneAgeModel!), Vector{Symbol}, Vector{Float64}, Vector{Int64}, Vector{Int64}, \nVector{Int64}, Vector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64,\n typeof(mean_square_rel_error), Vector{Real}, Vector{Union{Missing, Real}}, \nVector{Union{Missing, Real}}, Tsit5, Float64, Float64, Int64, Int64, Int64, Vector{String}, \nDataFrame}, Float64}}(NaN,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0), \nDual{ForwardDiff.Tag{var\"#objective#59\"{typeof(OneAgeModel!), Vector{Symbol}, \nVector{Float64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Real}, \nVector{Real}, typeof(f_ICs), Float64, typeof(mean_square_rel_error), Vector{Real}, \nVector{Union{Missing, Real}}, Vector{Union{Missing, Real}}, Tsit5, Float64, Float64, \nInt64, Int64, Int64, Vector{String}, DataFrame}, Float64}}\n(NaN,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0), Dual{ForwardDiff.Tag{var\"#objective#59\"\n{typeof(OneAgeModel!), Vector{Symbol}, Vector{Float64}, Vector{Int64}, Vector{Int64}, \nVector{Int64}, Vector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64,\n typeof(mean_square_rel_error), Vector{Real}, Vector{Union{Missing, Real}}, \nVector{Union{Missing, Real}}, Tsit5, Float64, Float64, Int64, Int64, Int64, Vector{String}, \nDataFrame}, Float64}}(NaN,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0), \nDual{ForwardDiff.Tag{var\"#objective#59\"{typeof(OneAgeModel!), Vector{Symbol}, \nVector{Float64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Real}, \nVector{Real}, typeof(f_ICs), Float64, typeof(mean_square_rel_error), Vector{Real},\n Vector{Union{Missing, Real}}, Vector{Union{Missing, Real}}, Tsit5, Float64, Float64, \nInt64, Int64, Int64, Vector{String}, DataFrame}, Float64}}\n(NaN,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0), Dual{ForwardDiff.Tag{var\"#objective#59\"\n{typeof(OneAgeModel!), Vector{Symbol}, Vector{Float64}, Vector{Int64}, Vector{Int64}, \nVector{Int64}, Vector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64, \ntypeof(mean_square_rel_error), Vector{Real}, Vector{Union{Missing, Real}}, \nVector{Union{Missing, Real}}, Tsit5, Float64, Float64, Int64, Int64, Int64, Vector{String},\n DataFrame}, Float64}}(NaN,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0), \nDual{ForwardDiff.Tag{var\"#objective#59\"{typeof(OneAgeModel!), Vector{Symbol}, \nVector{Float64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Real}, \nVector{Real}, typeof(f_ICs), Float64, typeof(mean_square_rel_error), Vector{Real}, \nVector{Union{Missing, Real}}, Vector{Union{Missing, Real}}, Tsit5, Float64, Float64, \nInt64, Int64, Int64, Vector{String}, DataFrame}, Float64}}\n(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0), Dual{ForwardDiff.Tag{var\"#objective#59\"\n{typeof(OneAgeModel!), Vector{Symbol}, Vector{Float64}, Vector{Int64}, Vector{Int64},\n Vector{Int64}, Vector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64, \ntypeof(mean_square_rel_error), Vector{Real}, Vector{Union{Missing, Real}}, \nVector{Union{Missing, Real}}, Tsit5, Float64, Float64, Int64, Int64, Int64, Vector{String}, \nDataFrame}, Float64}}(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0), \nDual{ForwardDiff.Tag{var\"#objective#59\"{typeof(OneAgeModel!), Vector{Symbol}, \nVector{Float64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Real},\n Vector{Real}, typeof(f_ICs), Float64, typeof(mean_square_rel_error), Vector{Real}, \nVector{Union{Missing, Real}}, Vector{Union{Missing, Real}}, Tsit5, Float64, Float64, \nInt64, Int64, Int64, Vector{String}, DataFrame}, Float64}}\n(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0), Dual{ForwardDiff.Tag{var\"#objective#59\"\n{typeof(OneAgeModel!), Vector{Symbol}, Vector{Float64}, Vector{Int64}, Vector{Int64}, \nVector{Int64}, Vector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64, \ntypeof(mean_square_rel_error), Vector{Real}, Vector{Union{Missing, Real}}, \nVector{Union{Missing, Real}}, Tsit5, Float64, Float64, Int64, Int64, Int64, Vector{String}, \nDataFrame}, Float64}}(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0), \nDual{ForwardDiff.Tag{var\"#objective#59\"{typeof(OneAgeModel!), Vector{Symbol}, \nVector{Float64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Real}, \nVector{Real}, typeof(f_ICs), Float64, typeof(mean_square_rel_error), Vector{Real}, \nVector{Union{Missing, Real}}, Vector{Union{Missing, Real}}, Tsit5, Float64, Float64, \nInt64, Int64, Int64, Vector{String}, DataFrame}, Float64}}\n(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0), Dual{ForwardDiff.Tag{var\"#objective#59\"\n{typeof(OneAgeModel!), Vector{Symbol}, Vector{Float64}, Vector{Int64}, Vector{Int64}, \nVector{Int64}, Vector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64, \ntypeof(mean_square_rel_error), Vector{Real}, Vector{Union{Missing, Real}}, \nVector{Union{Missing, Real}}, Tsit5, Float64, Float64, Int64, Int64, Int64, Vector{String}, \nDataFrame}, Float64}}(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0), \nDual{ForwardDiff.Tag{var\"#objective#59\"{typeof(OneAgeModel!), Vector{Symbol}, \nVector{Float64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Real},\n Vector{Real}, typeof(f_ICs), Float64, typeof(mean_square_rel_error), Vector{Real}, \nVector{Union{Missing, Real}}, Vector{Union{Missing, Real}}, Tsit5, Float64, Float64, \nInt64, Int64, Int64, Vector{String}, DataFrame}, Float64}}\n(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0), Dual{ForwardDiff.Tag{var\"#objective#59\"\n{typeof(OneAgeModel!), Vector{Symbol}, Vector{Float64}, Vector{Int64}, Vector{Int64}, \nVector{Int64}, Vector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64, \ntypeof(mean_square_rel_error), Vector{Real}, Vector{Union{Missing, Real}}, \nVector{Union{Missing, Real}}, Tsit5, Float64, Float64, Int64, Int64, Int64, Vector{String},\n DataFrame}, Float64}}(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0), \nDual{ForwardDiff.Tag{var\"#objective#59\"{typeof(OneAgeModel!), Vector{Symbol}, \nVector{Float64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Real},\n Vector{Real}, typeof(f_ICs), Float64, typeof(mean_square_rel_error), Vector{Real}, \nVector{Union{Missing, Real}}, Vector{Union{Missing, Real}}, Tsit5, Float64, Float64,\n Int64, Int64, Int64, Vector{String}, DataFrame}, Float64}}\n(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0), Dual{ForwardDiff.Tag{var\"#objective#59\"\n{typeof(OneAgeModel!), Vector{Symbol}, Vector{Float64}, Vector{Int64}, Vector{Int64},\n Vector{Int64}, Vector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64, \ntypeof(mean_square_rel_error), Vector{Real}, Vector{Union{Missing, Real}}, \nVector{Union{Missing, Real}}, Tsit5, Float64, Float64, Int64, Int64, Int64, Vector{String},\n DataFrame}, Float64}}(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0), \nDual{ForwardDiff.Tag{var\"#objective#59\"{typeof(OneAgeModel!), Vector{Symbol}, \nVector{Float64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64, typeof(mean_square_rel_error), Vector{Real}, \nVector{Union{Missing, Real}}, Vector{Union{Missing, Real}}, Tsit5, Float64, Float64, \nInt64, Int64, Int64, Vector{String}, DataFrame}, Float64}}\n(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0), Dual{ForwardDiff.Tag{var\"#objective#59\"\n{typeof(OneAgeModel!), Vector{Symbol}, Vector{Float64}, Vector{Int64}, Vector{Int64},\n Vector{Int64}, Vector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64, \ntypeof(mean_square_rel_error), Vector{Real}, Vector{Union{Missing, Real}}, \nVector{Union{Missing, Real}}, Tsit5, Float64, Float64, Int64, Int64, Int64, Vector{String}, \nDataFrame}, Float64}}(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0)]```\nI think this has to do with the fact that I'm using ForwardDiff to compute gradients and hessians on the objective...? Is there a way to just print out the array of values passed to `objective()`, and not all this other junk? (It should be an array of floats of length 19).","user":"U01J2HJR17E","ts":"1617214457.059800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"BWto","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi everyone, I'm trying to figure out this mysterious \"instability error\" that I'm getting from DifferentialEquations' Tsit5() solver. This is part of a program I'm working on for estimating the parameters of ODE systems. The ODE solver is called inside an objective function, called `objective()` and I'd like to print out the values that were passed to the objective function which caused the error. But when I try to print the input value, I get this huge mess:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"The x values that were passed to objective(): \nForwardDiff.Dual{ForwardDiff.Tag{var\"#objective#59\"{typeof(OneAgeModel!), \nVector{Symbol}, Vector{Float64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, \nVector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64, \ntypeof(mean_square_rel_error), Vector{Real}, Vector{Union{Missing, Real}}, \nVector{Union{Missing, Real}}, Tsit5, Float64, Float64, Int64, Int64, Int64, Vector{String}, \nDataFrame}, Float64}, Float64, 10}[Dual{ForwardDiff.Tag{var\"#objective#59\"\n{typeof(OneAgeModel!), Vector{Symbol}, Vector{Float64}, Vector{Int64}, Vector{Int64}, \nVector{Int64}, Vector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64, typeof(mean_square_rel_error), Vector{Real}, Vector{Union{Missing, Real}}, \nVector{Union{Missing, Real}}, Tsit5, Float64, Float64, Int64, Int64, Int64, Vector{String},\n DataFrame}, Float64}}(NaN,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0), \nDual{ForwardDiff.Tag{var\"#objective#59\"{typeof(OneAgeModel!), Vector{Symbol}, \nVector{Float64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Real}, \nVector{Real}, typeof(f_ICs), Float64, typeof(mean_square_rel_error), Vector{Real}, \nVector{Union{Missing, Real}}, Vector{Union{Missing, Real}}, Tsit5, Float64, Float64, \nInt64, Int64, Int64, Vector{String}, DataFrame}, Float64}}\n(NaN,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0), Dual{ForwardDiff.Tag{var\"#objective#59\"\n{typeof(OneAgeModel!), Vector{Symbol}, Vector{Float64}, Vector{Int64}, Vector{Int64}, \nVector{Int64}, Vector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64,\n typeof(mean_square_rel_error), Vector{Real}, Vector{Union{Missing, Real}}, \nVector{Union{Missing, Real}}, Tsit5, Float64, Float64, Int64, Int64, Int64, Vector{String}, \nDataFrame}, Float64}}(NaN,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0), \nDual{ForwardDiff.Tag{var\"#objective#59\"{typeof(OneAgeModel!), Vector{Symbol}, \nVector{Float64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Real}, \nVector{Real}, typeof(f_ICs), Float64, typeof(mean_square_rel_error), Vector{Real}, \nVector{Union{Missing, Real}}, Vector{Union{Missing, Real}}, Tsit5, Float64, Float64, \nInt64, Int64, Int64, Vector{String}, DataFrame}, Float64}}\n(NaN,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0), Dual{ForwardDiff.Tag{var\"#objective#59\"\n{typeof(OneAgeModel!), Vector{Symbol}, Vector{Float64}, Vector{Int64}, Vector{Int64}, \nVector{Int64}, Vector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64,\n typeof(mean_square_rel_error), Vector{Real}, Vector{Union{Missing, Real}}, \nVector{Union{Missing, Real}}, Tsit5, Float64, Float64, Int64, Int64, Int64, Vector{String}, \nDataFrame}, Float64}}(NaN,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0), \nDual{ForwardDiff.Tag{var\"#objective#59\"{typeof(OneAgeModel!), Vector{Symbol}, \nVector{Float64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Real}, \nVector{Real}, typeof(f_ICs), Float64, typeof(mean_square_rel_error), Vector{Real},\n Vector{Union{Missing, Real}}, Vector{Union{Missing, Real}}, Tsit5, Float64, Float64, \nInt64, Int64, Int64, Vector{String}, DataFrame}, Float64}}\n(NaN,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0), Dual{ForwardDiff.Tag{var\"#objective#59\"\n{typeof(OneAgeModel!), Vector{Symbol}, Vector{Float64}, Vector{Int64}, Vector{Int64}, \nVector{Int64}, Vector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64, \ntypeof(mean_square_rel_error), Vector{Real}, Vector{Union{Missing, Real}}, \nVector{Union{Missing, Real}}, Tsit5, Float64, Float64, Int64, Int64, Int64, Vector{String},\n DataFrame}, Float64}}(NaN,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0), \nDual{ForwardDiff.Tag{var\"#objective#59\"{typeof(OneAgeModel!), Vector{Symbol}, \nVector{Float64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Real}, \nVector{Real}, typeof(f_ICs), Float64, typeof(mean_square_rel_error), Vector{Real}, \nVector{Union{Missing, Real}}, Vector{Union{Missing, Real}}, Tsit5, Float64, Float64, \nInt64, Int64, Int64, Vector{String}, DataFrame}, Float64}}\n(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0), Dual{ForwardDiff.Tag{var\"#objective#59\"\n{typeof(OneAgeModel!), Vector{Symbol}, Vector{Float64}, Vector{Int64}, Vector{Int64},\n Vector{Int64}, Vector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64, \ntypeof(mean_square_rel_error), Vector{Real}, Vector{Union{Missing, Real}}, \nVector{Union{Missing, Real}}, Tsit5, Float64, Float64, Int64, Int64, Int64, Vector{String}, \nDataFrame}, Float64}}(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0), \nDual{ForwardDiff.Tag{var\"#objective#59\"{typeof(OneAgeModel!), Vector{Symbol}, \nVector{Float64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Real},\n Vector{Real}, typeof(f_ICs), Float64, typeof(mean_square_rel_error), Vector{Real}, \nVector{Union{Missing, Real}}, Vector{Union{Missing, Real}}, Tsit5, Float64, Float64, \nInt64, Int64, Int64, Vector{String}, DataFrame}, Float64}}\n(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0), Dual{ForwardDiff.Tag{var\"#objective#59\"\n{typeof(OneAgeModel!), Vector{Symbol}, Vector{Float64}, Vector{Int64}, Vector{Int64}, \nVector{Int64}, Vector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64, \ntypeof(mean_square_rel_error), Vector{Real}, Vector{Union{Missing, Real}}, \nVector{Union{Missing, Real}}, Tsit5, Float64, Float64, Int64, Int64, Int64, Vector{String}, \nDataFrame}, Float64}}(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0), \nDual{ForwardDiff.Tag{var\"#objective#59\"{typeof(OneAgeModel!), Vector{Symbol}, \nVector{Float64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Real}, \nVector{Real}, typeof(f_ICs), Float64, typeof(mean_square_rel_error), Vector{Real}, \nVector{Union{Missing, Real}}, Vector{Union{Missing, Real}}, Tsit5, Float64, Float64, \nInt64, Int64, Int64, Vector{String}, DataFrame}, Float64}}\n(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0), Dual{ForwardDiff.Tag{var\"#objective#59\"\n{typeof(OneAgeModel!), Vector{Symbol}, Vector{Float64}, Vector{Int64}, Vector{Int64}, \nVector{Int64}, Vector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64, \ntypeof(mean_square_rel_error), Vector{Real}, Vector{Union{Missing, Real}}, \nVector{Union{Missing, Real}}, Tsit5, Float64, Float64, Int64, Int64, Int64, Vector{String}, \nDataFrame}, Float64}}(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0), \nDual{ForwardDiff.Tag{var\"#objective#59\"{typeof(OneAgeModel!), Vector{Symbol}, \nVector{Float64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Real},\n Vector{Real}, typeof(f_ICs), Float64, typeof(mean_square_rel_error), Vector{Real}, \nVector{Union{Missing, Real}}, Vector{Union{Missing, Real}}, Tsit5, Float64, Float64, \nInt64, Int64, Int64, Vector{String}, DataFrame}, Float64}}\n(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0), Dual{ForwardDiff.Tag{var\"#objective#59\"\n{typeof(OneAgeModel!), Vector{Symbol}, Vector{Float64}, Vector{Int64}, Vector{Int64}, \nVector{Int64}, Vector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64, \ntypeof(mean_square_rel_error), Vector{Real}, Vector{Union{Missing, Real}}, \nVector{Union{Missing, Real}}, Tsit5, Float64, Float64, Int64, Int64, Int64, Vector{String},\n DataFrame}, Float64}}(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0), \nDual{ForwardDiff.Tag{var\"#objective#59\"{typeof(OneAgeModel!), Vector{Symbol}, \nVector{Float64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Real},\n Vector{Real}, typeof(f_ICs), Float64, typeof(mean_square_rel_error), Vector{Real}, \nVector{Union{Missing, Real}}, Vector{Union{Missing, Real}}, Tsit5, Float64, Float64,\n Int64, Int64, Int64, Vector{String}, DataFrame}, Float64}}\n(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0), Dual{ForwardDiff.Tag{var\"#objective#59\"\n{typeof(OneAgeModel!), Vector{Symbol}, Vector{Float64}, Vector{Int64}, Vector{Int64},\n Vector{Int64}, Vector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64, \ntypeof(mean_square_rel_error), Vector{Real}, Vector{Union{Missing, Real}}, \nVector{Union{Missing, Real}}, Tsit5, Float64, Float64, Int64, Int64, Int64, Vector{String},\n DataFrame}, Float64}}(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0), \nDual{ForwardDiff.Tag{var\"#objective#59\"{typeof(OneAgeModel!), Vector{Symbol}, \nVector{Float64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64, typeof(mean_square_rel_error), Vector{Real}, \nVector{Union{Missing, Real}}, Vector{Union{Missing, Real}}, Tsit5, Float64, Float64, \nInt64, Int64, Int64, Vector{String}, DataFrame}, Float64}}\n(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0), Dual{ForwardDiff.Tag{var\"#objective#59\"\n{typeof(OneAgeModel!), Vector{Symbol}, Vector{Float64}, Vector{Int64}, Vector{Int64},\n Vector{Int64}, Vector{Int64}, Vector{Real}, Vector{Real}, typeof(f_ICs), Float64, \ntypeof(mean_square_rel_error), Vector{Real}, Vector{Union{Missing, Real}}, \nVector{Union{Missing, Real}}, Tsit5, Float64, Float64, Int64, Int64, Int64, Vector{String}, \nDataFrame}, Float64}}(NaN,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0)]"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I think this has to do with the fact that I'm using ForwardDiff to compute gradients and hessians on the objective...? Is there a way to just print out the array of values passed to "},{"type":"text","text":"objective()","style":{"code":true}},{"type":"text","text":", and not all this other junk? (It should be an array of floats of length 19)."}]}]}]},{"client_msg_id":"216709cb-7151-48ed-85ba-e151671b2ad0","type":"message","text":"print `ForwardDiff.value.(ForwardDiff.value.(x))`","user":"U85JBUGGP","ts":"1617216415.060200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9IAPe","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"print "},{"type":"text","text":"ForwardDiff.value.(ForwardDiff.value.(x))","style":{"code":true}}]}]}],"reactions":[{"name":"heart","users":["U01J2HJR17E"],"count":1}]},{"client_msg_id":"c6f28364-11a5-4482-82c1-9609fb3f51dc","type":"message","text":"Thank you! I will try this!","user":"U01J2HJR17E","ts":"1617216677.060600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"h1/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thank you! I will try this!"}]}]}]}]}