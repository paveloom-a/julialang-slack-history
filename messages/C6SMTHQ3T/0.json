{"cursor": 0, "messages": [{"client_msg_id":"f897bf92-61d3-4747-b238-5aac3b224413","type":"message","text":"I remember that being the case in former versions of Julia. Now with 1.5.3 I’m getting:\n```julia &gt; fail(x) = nonexistent(x)\n\njulia&gt; t = Threads.@spawn fail(1)\nTask (runnable) @0x0000000120b19690\n\njulia&gt; t\nTask (failed) @0x0000000120b19690\nUndefVarError: nonexistent not defined\nfail(::Int64) at /Users/paul/.julia/dev/tmp/exception.jl:1\n(::var\"#45#46\")() at ./threadingconstructs.jl:169\n\njulia&gt; t.exception\nUndefVarError(:nonexistent)\n\njulia&gt; stacktrace(t.backtrace)\n2-element Array{Base.StackTraces.StackFrame,1}:\n fail(::Int64) at exception.jl:1\n (::var\"#45#46\")() at threadingconstructs.jl:169```\nThe same now with `rethrow`:\n```function fail1(x)\n    try\n        nonexistent(x)\n    catch exc\n        rethrow(exc)\n    end\nend\n\njulia&gt; t1 = Threads.@spawn fail1(1)\nTask (runnable) @0x0000000120b19210\n\njulia&gt; t1\nTask (failed) @0x0000000120b19210\nUndefVarError: nonexistent not defined\nfail1(::Int64) at /Users/paul/.julia/dev/tmp/exception.jl:5\n(::var\"#47#48\")() at ./threadingconstructs.jl:169\n\njulia&gt; t1.exception\nUndefVarError(:nonexistent)\n\njulia&gt; stacktrace(t1.backtrace)\n2-element Array{Base.StackTraces.StackFrame,1}:\n fail1(::Int64) at exception.jl:5\n (::var\"#47#48\")() at threadingconstructs.jl:169```\nWhich version are you using?","user":"UP9P4JFNJ","ts":"1607930935.168100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Q/0T/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I remember that being the case in former versions of Julia. Now with 1.5.3 I’m getting:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia > fail(x) = nonexistent(x)\n\njulia> t = Threads.@spawn fail(1)\nTask (runnable) @0x0000000120b19690\n\njulia> t\nTask (failed) @0x0000000120b19690\nUndefVarError: nonexistent not defined\nfail(::Int64) at /Users/paul/.julia/dev/tmp/exception.jl:1\n(::var\"#45#46\")() at ./threadingconstructs.jl:169\n\njulia> t.exception\nUndefVarError(:nonexistent)\n\njulia> stacktrace(t.backtrace)\n2-element Array{Base.StackTraces.StackFrame,1}:\n fail(::Int64) at exception.jl:1\n (::var\"#45#46\")() at threadingconstructs.jl:169"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"The same now with "},{"type":"text","text":"rethrow","style":{"code":true}},{"type":"text","text":":\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function fail1(x)\n    try\n        nonexistent(x)\n    catch exc\n        rethrow(exc)\n    end\nend\n\njulia> t1 = Threads.@spawn fail1(1)\nTask (runnable) @0x0000000120b19210\n\njulia> t1\nTask (failed) @0x0000000120b19210\nUndefVarError: nonexistent not defined\nfail1(::Int64) at /Users/paul/.julia/dev/tmp/exception.jl:5\n(::var\"#47#48\")() at ./threadingconstructs.jl:169\n\njulia> t1.exception\nUndefVarError(:nonexistent)\n\njulia> stacktrace(t1.backtrace)\n2-element Array{Base.StackTraces.StackFrame,1}:\n fail1(::Int64) at exception.jl:5\n (::var\"#47#48\")() at threadingconstructs.jl:169"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Which version are you using?"}]}]}]},{"client_msg_id":"33872a18-2f12-4c0b-becc-1e5a2d8b39af","type":"message","text":"I tested 1.5 and 1.6. But my problem isn't the exception, which is correct: it's how I could rethrow it *outside of the task* without losing the original backtrace.","user":"U67431ELR","ts":"1607972444.168600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Joo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I tested 1.5 and 1.6. But my problem isn't the exception, which is correct: it's how I could rethrow it *outside of the task* without losing the original backtrace."}]}]}],"thread_ts":"1607972444.168600","reply_count":8,"reply_users_count":2,"latest_reply":"1607980415.172400","reply_users":["UP9P4JFNJ","U67431ELR"],"subscribed":false},{"client_msg_id":"81604d55-3ef2-4654-af0e-635e5776ded0","type":"message","text":"Hey, all! The <#C6SMTHQ3T|multithreading> users’ BoF for December is planned for Wednesday, at 4:30 PM UTC (11:30a EST+5). Link at <https://meet.google.com/ugr-sbmu-wts> and I’ll update minutes at <https://docs.google.com/document/d/1rDgdFH94QL4wlEAdzBbdxJJ0Wk2CkBhWuptzPETSRc0/edit?usp=sharing>","user":"U67BXBF99","ts":"1607974366.170100","team":"T68168MUP","edited":{"user":"U67BXBF99","ts":"1607974384.000000"},"blocks":[{"type":"rich_text","block_id":"LQfL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey, all! The "},{"type":"channel","channel_id":"C6SMTHQ3T"},{"type":"text","text":" users’ BoF for December is planned for Wednesday, at 4:30 PM UTC (11:30a EST+5). Link at "},{"type":"link","url":"https://meet.google.com/ugr-sbmu-wts"},{"type":"text","text":" and I’ll update minutes at "},{"type":"link","url":"https://docs.google.com/document/d/1rDgdFH94QL4wlEAdzBbdxJJ0Wk2CkBhWuptzPETSRc0/edit?usp=sharing"}]}]}],"thread_ts":"1607974366.170100","reply_count":1,"reply_users_count":1,"latest_reply":"1608133869.175300","reply_users":["U67BXBF99"],"subscribed":false},{"client_msg_id":"a9ec60bf-4bdf-48f0-b6e6-abce09c11da7","type":"message","text":"Let me know, preferably in advance, if you have any topics you’d like to have discussed!","user":"U67BXBF99","ts":"1607974394.170800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4POs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Let me know, preferably in advance, if you have any topics you’d like to have discussed!"}]}]}]},{"client_msg_id":"8cffd73b-0a4f-4b3a-a938-788be37cde45","type":"message","text":"Does julia natively use pthreads? I'm getting some profiling done of my code, and the guy helping me wants to know what threading model we use...","user":"U7C8KRZKL","ts":"1608132266.174100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FBa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does julia natively use pthreads? I'm getting some profiling done of my code, and the guy helping me wants to know what threading model we use..."}]}]}],"thread_ts":"1608132266.174100","reply_count":7,"reply_users_count":3,"latest_reply":"1608136093.181200","reply_users":["U6795JH6H","U67BXBF99","U7C8KRZKL"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"In 40 minutes","user":"U67BXBF99","ts":"1608133869.175300","thread_ts":"1607974366.170100","root":{"client_msg_id":"81604d55-3ef2-4654-af0e-635e5776ded0","type":"message","text":"Hey, all! The <#C6SMTHQ3T|multithreading> users’ BoF for December is planned for Wednesday, at 4:30 PM UTC (11:30a EST+5). Link at <https://meet.google.com/ugr-sbmu-wts> and I’ll update minutes at <https://docs.google.com/document/d/1rDgdFH94QL4wlEAdzBbdxJJ0Wk2CkBhWuptzPETSRc0/edit?usp=sharing>","user":"U67BXBF99","ts":"1607974366.170100","team":"T68168MUP","edited":{"user":"U67BXBF99","ts":"1607974384.000000"},"blocks":[{"type":"rich_text","block_id":"LQfL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey, all! The "},{"type":"channel","channel_id":"C6SMTHQ3T"},{"type":"text","text":" users’ BoF for December is planned for Wednesday, at 4:30 PM UTC (11:30a EST+5). Link at "},{"type":"link","url":"https://meet.google.com/ugr-sbmu-wts"},{"type":"text","text":" and I’ll update minutes at "},{"type":"link","url":"https://docs.google.com/document/d/1rDgdFH94QL4wlEAdzBbdxJJ0Wk2CkBhWuptzPETSRc0/edit?usp=sharing"}]}]}],"thread_ts":"1607974366.170100","reply_count":1,"reply_users_count":1,"latest_reply":"1608133869.175300","reply_users":["U67BXBF99"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"/M1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In 40 minutes"}]}]}],"client_msg_id":"8df728fd-c24e-47e3-9453-6bc0a413d6a8"},{"client_msg_id":"5f2dd458-edd5-4815-bfb2-73f7e686d6d0","type":"message","text":"I'm seeing a rather fun behaviour with tasks.  in a system with AMD EPYC 7502 32-Core Processor, I launch the script in the:thread:below, with different number of threads and get the following timings:\n```% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  285.718 ms (7020 allocations: 543.05 KiB)\n  143.495 ms (8020 allocations: 574.30 KiB)\n  71.975 ms (8020 allocations: 574.30 KiB)\n  36.460 ms (8020 allocations: 574.30 KiB)\n  18.987 ms (8020 allocations: 574.30 KiB)\n  24.721 ms (8021 allocations: 574.33 KiB)\n  80.087 ms (8024 allocations: 574.42 KiB)\n  217.695 ms (8022 allocations: 574.36 KiB)```\nit scales perfectly up to 16 threads, then it starts getting worse and worse, up to 128 threads (the number of threads available in the system), where it's close to the performance of a single thread.\n\nthe equivalent function which uses `Threads.@threads` instead of `Thread.@spawn` has a much more straightforward behaviour (but scaling is worse, because of load imbalance):\n```% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  286.673 ms (2020 allocations: 113.64 KiB)\n  222.603 ms (2026 allocations: 114.11 KiB)\n  132.780 ms (2038 allocations: 115.06 KiB)\n  72.050 ms (2060 allocations: 116.91 KiB)\n  37.192 ms (2101 allocations: 120.50 KiB)\n  20.731 ms (2193 allocations: 128.00 KiB)\n  11.464 ms (2348 allocations: 142.12 KiB)\n  9.128 ms (2659 allocations: 170.38 KiB)```\ndo you have an idea why tasks are behaving this way?","user":"UDB26738Q","ts":"1608134434.180500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"k7Gj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm seeing a rather fun behaviour with tasks.  in a system with AMD EPYC 7502 32-Core Processor, I launch the script in the"},{"type":"emoji","name":"thread"},{"type":"text","text":"below, with different number of threads and get the following timings:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  285.718 ms (7020 allocations: 543.05 KiB)\n  143.495 ms (8020 allocations: 574.30 KiB)\n  71.975 ms (8020 allocations: 574.30 KiB)\n  36.460 ms (8020 allocations: 574.30 KiB)\n  18.987 ms (8020 allocations: 574.30 KiB)\n  24.721 ms (8021 allocations: 574.33 KiB)\n  80.087 ms (8024 allocations: 574.42 KiB)\n  217.695 ms (8022 allocations: 574.36 KiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"it scales perfectly up to 16 threads, then it starts getting worse and worse, up to 128 threads (the number of threads available in the system), where it's close to the performance of a single thread.\n\nthe equivalent function which uses "},{"type":"text","text":"Threads.@threads","style":{"code":true}},{"type":"text","text":" instead of "},{"type":"text","text":"Thread.@spawn","style":{"code":true}},{"type":"text","text":" has a much more straightforward behaviour (but scaling is worse, because of load imbalance):\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  286.673 ms (2020 allocations: 113.64 KiB)\n  222.603 ms (2026 allocations: 114.11 KiB)\n  132.780 ms (2038 allocations: 115.06 KiB)\n  72.050 ms (2060 allocations: 116.91 KiB)\n  37.192 ms (2101 allocations: 120.50 KiB)\n  20.731 ms (2193 allocations: 128.00 KiB)\n  11.464 ms (2348 allocations: 142.12 KiB)\n  9.128 ms (2659 allocations: 170.38 KiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\ndo you have an idea why tasks are behaving this way?"}]}]}],"thread_ts":"1608134434.180500","reply_count":39,"reply_users_count":4,"latest_reply":"1608148411.189600","reply_users":["UDB26738Q","U67BJLYCS","U01GRS159T8","UC7AF7NSU"],"subscribed":false,"reactions":[{"name":"eyes","users":["UEP056STX"],"count":1}]},{"type":"message","subtype":"thread_broadcast","text":"it looks like that with more threads available the scheduler is overloading few threads when using the tasks?","user":"UDB26738Q","ts":"1608137594.182000","thread_ts":"1608134434.180500","root":{"client_msg_id":"5f2dd458-edd5-4815-bfb2-73f7e686d6d0","type":"message","text":"I'm seeing a rather fun behaviour with tasks.  in a system with AMD EPYC 7502 32-Core Processor, I launch the script in the:thread:below, with different number of threads and get the following timings:\n```% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  285.718 ms (7020 allocations: 543.05 KiB)\n  143.495 ms (8020 allocations: 574.30 KiB)\n  71.975 ms (8020 allocations: 574.30 KiB)\n  36.460 ms (8020 allocations: 574.30 KiB)\n  18.987 ms (8020 allocations: 574.30 KiB)\n  24.721 ms (8021 allocations: 574.33 KiB)\n  80.087 ms (8024 allocations: 574.42 KiB)\n  217.695 ms (8022 allocations: 574.36 KiB)```\nit scales perfectly up to 16 threads, then it starts getting worse and worse, up to 128 threads (the number of threads available in the system), where it's close to the performance of a single thread.\n\nthe equivalent function which uses `Threads.@threads` instead of `Thread.@spawn` has a much more straightforward behaviour (but scaling is worse, because of load imbalance):\n```% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  286.673 ms (2020 allocations: 113.64 KiB)\n  222.603 ms (2026 allocations: 114.11 KiB)\n  132.780 ms (2038 allocations: 115.06 KiB)\n  72.050 ms (2060 allocations: 116.91 KiB)\n  37.192 ms (2101 allocations: 120.50 KiB)\n  20.731 ms (2193 allocations: 128.00 KiB)\n  11.464 ms (2348 allocations: 142.12 KiB)\n  9.128 ms (2659 allocations: 170.38 KiB)```\ndo you have an idea why tasks are behaving this way?","user":"UDB26738Q","ts":"1608134434.180500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"k7Gj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm seeing a rather fun behaviour with tasks.  in a system with AMD EPYC 7502 32-Core Processor, I launch the script in the"},{"type":"emoji","name":"thread"},{"type":"text","text":"below, with different number of threads and get the following timings:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  285.718 ms (7020 allocations: 543.05 KiB)\n  143.495 ms (8020 allocations: 574.30 KiB)\n  71.975 ms (8020 allocations: 574.30 KiB)\n  36.460 ms (8020 allocations: 574.30 KiB)\n  18.987 ms (8020 allocations: 574.30 KiB)\n  24.721 ms (8021 allocations: 574.33 KiB)\n  80.087 ms (8024 allocations: 574.42 KiB)\n  217.695 ms (8022 allocations: 574.36 KiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"it scales perfectly up to 16 threads, then it starts getting worse and worse, up to 128 threads (the number of threads available in the system), where it's close to the performance of a single thread.\n\nthe equivalent function which uses "},{"type":"text","text":"Threads.@threads","style":{"code":true}},{"type":"text","text":" instead of "},{"type":"text","text":"Thread.@spawn","style":{"code":true}},{"type":"text","text":" has a much more straightforward behaviour (but scaling is worse, because of load imbalance):\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"% for t in 1 2 4 8 16 32 64 128; do julia -t ${t} pi.jl; done\n  286.673 ms (2020 allocations: 113.64 KiB)\n  222.603 ms (2026 allocations: 114.11 KiB)\n  132.780 ms (2038 allocations: 115.06 KiB)\n  72.050 ms (2060 allocations: 116.91 KiB)\n  37.192 ms (2101 allocations: 120.50 KiB)\n  20.731 ms (2193 allocations: 128.00 KiB)\n  11.464 ms (2348 allocations: 142.12 KiB)\n  9.128 ms (2659 allocations: 170.38 KiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\ndo you have an idea why tasks are behaving this way?"}]}]}],"thread_ts":"1608134434.180500","reply_count":39,"reply_users_count":4,"latest_reply":"1608148411.189600","reply_users":["UDB26738Q","U67BJLYCS","U01GRS159T8","UC7AF7NSU"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"giJ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it looks like that with more threads available the scheduler is overloading few threads when using the tasks?"}]}]}],"client_msg_id":"bea24a8a-f47f-4a0b-b9d7-1a32b26d6178"},{"client_msg_id":"4866efa3-4d0b-4e96-8eef-ad4481e3e529","type":"message","text":"I was looking at `jl_gc_enable()` this morning and it doesn't seem threadsafe- is this a known issue?","user":"UH2HTNM50","ts":"1608220740.196600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"XWl5Q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I was looking at "},{"type":"text","text":"jl_gc_enable()","style":{"code":true}},{"type":"text","text":" this morning and it doesn't seem threadsafe- is this a known issue?"}]}]}]},{"client_msg_id":"daa171b3-7ad2-4773-ad20-e922cea3fe01","type":"message","text":"I thought it was thread-local","user":"U67BXBF99","ts":"1608222567.196900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"BKwDF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I thought it was thread-local"}]}]}],"reactions":[{"name":"man-facepalming","users":["UH2HTNM50"],"count":1}]},{"client_msg_id":"fb90468a-18bc-48b6-bef7-f4735b0caed9","type":"message","text":"oh, indeed, it does appear to be thread-local. that wasn't obvious from the julia side of the code, i think (none of the comments around those callsites indicate it):\n<https://github.com/JuliaLang/julia/blob/c9923cda97064072ecf9eaf3e243bf58c140a89b/src/gc.c#L2807>","user":"U89GY9W1J","ts":"1608227472.198400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2Th85","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"oh, indeed, it does appear to be thread-local. that wasn't obvious from the julia side of the code, i think (none of the comments around those callsites indicate it):\n"},{"type":"link","url":"https://github.com/JuliaLang/julia/blob/c9923cda97064072ecf9eaf3e243bf58c140a89b/src/gc.c#L2807"}]}]}]},{"client_msg_id":"73f835fe-f00d-40fb-a87c-224fb32471c3","type":"message","text":"thanks, that makes sense.","user":"UH2HTNM50","ts":"1608227552.198800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"A0lv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"thanks, that makes sense."}]}]}]},{"client_msg_id":"dc41eb5c-93fe-46ba-90f7-3589b4146b9e","type":"message","text":"I’ve looked into the parallel computing sections of the manual. Then I see that there is JuliaFolds that has a different view on how to do this, and there might be others. How should one go about thinking this, what’s the actual difference, and which is better for what?","user":"U01CQTKB86N","ts":"1608312913.201700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DOOxu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’ve looked into the parallel computing sections of the manual. Then I see that there is JuliaFolds that has a different view on how to do this, and there might be others. How should one go about thinking this, what’s the actual difference, and which is better for what?"}]}]}],"thread_ts":"1608312913.201700","reply_count":10,"reply_users_count":2,"latest_reply":"1608494907.228200","reply_users":["U8D9768Q6","U01CQTKB86N"],"subscribed":false},{"client_msg_id":"ea211e5a-9332-4881-8c8c-671641960ef1","type":"message","text":"Crossposting my announcement of <https://github.com/JuliaActors/Actors.jl|Actors 0.2> on Discourse:\n\n<https://discourse.julialang.org/t/ann-actors-0-2-genservers-and-guards/52056>","user":"UP9P4JFNJ","ts":"1608315969.204800","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"[ANN] Actors 0.2, GenServers and Guards","title_link":"https://discourse.julialang.org/t/ann-actors-0-2-genservers-and-guards/52056","text":"I’m happy to announce the release of Actors 0.2 together with two actor infrastructure libraries GenServers and Guards. Actors: Concurrency based on the Actor Model Actors implements the classical Actor model in Julia. It builds on Julia’s concurrency primitives, provides a message based programming model for making concurrency easy to understand and reason about and integrates well with Julia’s features for multi-threading and distributed computing. Actors expresses actor behavior as a fun...","fallback":"JuliaLang: [ANN] Actors 0.2, GenServers and Guards","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","fields":[{"title":"Reading time","value":"1 mins :clock2:","short":true},{"title":"Likes","value":"3 :heart:","short":true}],"ts":1608308480,"from_url":"https://discourse.julialang.org/t/ann-actors-0-2-genservers-and-guards/52056","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/ann-actors-0-2-genservers-and-guards/52056"}],"blocks":[{"type":"rich_text","block_id":"QPA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Crossposting my announcement of "},{"type":"link","url":"https://github.com/JuliaActors/Actors.jl","text":"Actors 0.2"},{"type":"text","text":" on Discourse:\n\n"},{"type":"link","url":"https://discourse.julialang.org/t/ann-actors-0-2-genservers-and-guards/52056"}]}]}],"thread_ts":"1608315969.204800","reply_count":15,"reply_users_count":3,"latest_reply":"1608576509.230800","reply_users":["U01CQTKB86N","UP9P4JFNJ","U6QGE7S86"],"subscribed":false,"reactions":[{"name":"wow","users":["U680THK2S","U01CQTKB86N","UDGT4PM41","U6QGE7S86"],"count":4}]},{"client_msg_id":"3f0507e6-d204-4624-8e73-1fa5955a3c24","type":"message","text":"What's a nice design for a multithreaded iterator over a dataset (i.e. sampling rows, fetching them, doing some unpacking / processing)? I feel like channels could work? A channel for indices to fetch, then an output channel, and some number of workers in the middle that run in a loop? One thing I need is to able to guarantee ordering is preserved between the input and output channels - any easy way to do that?","user":"UCNPT22MQ","ts":"1608397061.221000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"olVbu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What's a nice design for a multithreaded iterator over a dataset (i.e. sampling rows, fetching them, doing some unpacking / processing)? I feel like channels could work? A channel for indices to fetch, then an output channel, and some number of workers in the middle that run in a loop? One thing I need is to able to guarantee ordering is preserved between the input and output channels - any easy way to do that?"}]}]}]},{"client_msg_id":"60e3e13c-1404-4f88-970e-afbf32a0c6fe","type":"message","text":"Yes, channels and workers are a good design. I've done exactly that for writing compressed multi-level tiff files. The output ordering I handled by having a dedicated worker receiving the output from the compressor workers. It stored the compressed tiles in an internal dict, keyed by their sequence number. Whenever it had tiles available in order it forwarded those to the next step, out of order tiles had to wait in the dict until everything before them was ready.","user":"UBVE598BC","ts":"1608398714.225900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mCm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes, channels and workers are a good design. I've done exactly that for writing compressed multi-level tiff files. The output ordering I handled by having a dedicated worker receiving the output from the compressor workers. It stored the compressed tiles in an internal dict, keyed by their sequence number. Whenever it had tiles available in order it forwarded those to the next step, out of order tiles had to wait in the dict until everything before them was ready."}]}]}]},{"client_msg_id":"24869ca3-ca49-4920-88f2-dabe7577473c","type":"message","text":"I've found this to be a useful abstraction.\n```struct TerminationMessage end\n\nstruct WorkerPool\n    channel::Channel{Any}\n    workers::Vector{Task}\nend\n\nfunction workerpool(f::Function, n::Integer)\n    channel = Channel(n)\n    workers = Task[Base.Threads.@spawn _worker(f, channel) for _ = 1:n]\n    return WorkerPool(channel, workers)\nend\n\nfunction _worker(f::Function, channel::Channel)\n    while true\n        message = take!(channel)\n        message == TerminationMessage &amp;&amp; return\n        f(message)\n    end\nend\n\nfunction finish(pool::WorkerPool)\n    for worker in pool.workers\n        put!(pool.channel, TerminationMessage)\n    end\n    for worker in pool.workers\n        wait(worker)\n    end\nend```","user":"UBVE598BC","ts":"1608399047.226500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oQ59R","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I've found this to be a useful abstraction.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"struct TerminationMessage end\n\nstruct WorkerPool\n    channel::Channel{Any}\n    workers::Vector{Task}\nend\n\nfunction workerpool(f::Function, n::Integer)\n    channel = Channel(n)\n    workers = Task[Base.Threads.@spawn _worker(f, channel) for _ = 1:n]\n    return WorkerPool(channel, workers)\nend\n\nfunction _worker(f::Function, channel::Channel)\n    while true\n        message = take!(channel)\n        message == TerminationMessage && return\n        f(message)\n    end\nend\n\nfunction finish(pool::WorkerPool)\n    for worker in pool.workers\n        put!(pool.channel, TerminationMessage)\n    end\n    for worker in pool.workers\n        wait(worker)\n    end\nend"}]}]}]},{"client_msg_id":"ec8c656a-3bd2-4c64-aa47-d147f633b1ec","type":"message","text":"Fantastic, thanks <@UBVE598BC>. I'll try the dict ordering  solution, and I like the termination message idea.","user":"UCNPT22MQ","ts":"1608403418.228100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"S2m06","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Fantastic, thanks "},{"type":"user","user_id":"UBVE598BC"},{"type":"text","text":". I'll try the dict ordering  solution, and I like the termination message idea."}]}]}]},{"type":"message","subtype":"channel_join","ts":"1608550686.229900","user":"U01GX4L17PW","text":"<@U01GX4L17PW> has joined the channel","inviter":"USU9FRPEU"},{"client_msg_id":"90e841b8-fac2-4057-a83d-7b6c672ec634","type":"message","text":"hi I'm new to multithreading. Do i need to load things on each thread or do `@everywhere using Stuff` or what?","user":"UAH43TMUN","ts":"1608926491.234000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jib","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hi I'm new to multithreading. Do i need to load things on each thread or do "},{"type":"text","text":"@everywhere using Stuff","style":{"code":true}},{"type":"text","text":" or what?"}]}]}]},{"client_msg_id":"9a8fea6d-1a4f-4026-9e60-b1914279cf77","type":"message","text":"I seem have errors when I use threads but not just doing `@async`","user":"UAH43TMUN","ts":"1608926571.235100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ABxxS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I seem have errors when I use threads but not just doing "},{"type":"text","text":"@async","style":{"code":true}}]}]}]},{"client_msg_id":"d6c12377-58c9-4070-8c02-263560d2c311","type":"message","text":"What kind of errors?","user":"USU9FRPEU","ts":"1608926598.235300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Tp9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What kind of errors?"}]}]}]},{"client_msg_id":"90ac0417-a655-4fe7-ab87-e2c9c7b60cfd","type":"message","text":"You might be trying to use something that is not necessarily thread safe.","user":"USU9FRPEU","ts":"1608926646.235600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vWXbZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You might be trying to use something that is not necessarily thread safe."}]}]}]},{"client_msg_id":"cdf7ef6d-b22f-4f4c-b66b-20e139a8051b","type":"message","text":"`Task` (which is what you use with `@async` ) can also run on another thread. Maybe just stick to that if that is working for you?","user":"USU9FRPEU","ts":"1608926689.236600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"y49T","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Task","style":{"code":true}},{"type":"text","text":" (which is what you use with "},{"type":"text","text":"@async","style":{"code":true}},{"type":"text","text":" ) can also run on another thread. Maybe just stick to that if that is working for you?"}]}]}]},{"client_msg_id":"45fe20b6-e81e-4fb6-89b9-46eebe27a934","type":"message","text":"well what I really want is to be able to send jobs to something like a thread pool or some buffer","user":"UAH43TMUN","ts":"1608927049.237700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LnRZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"well what I really want is to be able to send jobs to something like a thread pool or some buffer"}]}]}]},{"client_msg_id":"fbc4b167-c02c-4f02-b809-ed40f7ef854b","type":"message","text":"so that I can set things in motion but keep working","user":"UAH43TMUN","ts":"1608927087.238200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dQx3B","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"so that I can set things in motion but keep working"}]}]}]},{"client_msg_id":"4173d9ea-1e17-4446-a382-c41ac234d4cc","type":"message","text":"Well in Julia you have a fixed thread pool","user":"USU9FRPEU","ts":"1608927110.238800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rijS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Well in Julia you have a fixed thread pool"}]}]}]},{"client_msg_id":"bba90188-7a50-4485-876a-608044c35d2b","type":"message","text":"so far I have just been running things an different computers","user":"UAH43TMUN","ts":"1608927126.239400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LTGgC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"so far I have just been running things an different computers"}]}]}]},{"client_msg_id":"0122acb4-7dcd-4007-992d-db83d3f1044f","type":"message","text":"So `@async` seems to be the best way to do this","user":"USU9FRPEU","ts":"1608927129.239500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RuPC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So "},{"type":"text","text":"@async","style":{"code":true}},{"type":"text","text":" seems to be the best way to do this"}]}]}]},{"client_msg_id":"e6d06fe9-a92e-4fef-a457-7d7da7cc3be6","type":"message","text":"If you want to do that, maybe you should think of `Distributed` instead?","user":"USU9FRPEU","ts":"1608927146.240000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zDKT6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If you want to do that, maybe you should think of "},{"type":"text","text":"Distributed","style":{"code":true}},{"type":"text","text":" instead?"}]}]}]},{"client_msg_id":"1b37f359-fcdf-4db0-91a0-e948953d2964","type":"message","text":"What you really want are closures.","user":"USU9FRPEU","ts":"1608927168.240600","team":"T68168MUP","edited":{"user":"USU9FRPEU","ts":"1608927202.000000"},"blocks":[{"type":"rich_text","block_id":"qXp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What you really want are closures."}]}]}]},{"client_msg_id":"9b6a67f8-476c-40e3-9354-da266758b446","type":"message","text":"What kind of errors are you getting though?","user":"USU9FRPEU","ts":"1608927217.241800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fA6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What kind of errors are you getting though?"}]}]}]},{"client_msg_id":"fc3d2543-d8d5-4066-b08b-20fd5598fd4b","type":"message","text":"ok. threads are more for running big for loops quickly maybe?","user":"UAH43TMUN","ts":"1608927220.242000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Kxl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ok. threads are more for running big for loops quickly maybe?"}]}]}]},{"client_msg_id":"057a4475-00ca-41a1-ba97-f91b488e806c","type":"message","text":"Tasks are \"jobs\" that are distributed automatically to threads","user":"USU9FRPEU","ts":"1608927239.242700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gh+W","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Tasks are \"jobs\" that are distributed automatically to threads"}]}]}]},{"client_msg_id":"0ec5afb0-d376-45fb-8a36-47b14bb4f9d7","type":"message","text":"<https://julialang.org/blog/2019/07/multithreading/>","user":"USU9FRPEU","ts":"1608927275.243300","team":"T68168MUP","attachments":[{"title":"Announcing composable multi-threaded parallelism in Julia","title_link":"https://julialang.org/blog/2019/07/multithreading/","text":"Announcing composable multi-threaded parallelism in Julia | Software performance depends more and more on exploiting multiple processor cores....","fallback":"Announcing composable multi-threaded parallelism in Julia","image_url":"https://julialang.org/assets/images/julia-open-graph.png","from_url":"https://julialang.org/blog/2019/07/multithreading/","image_width":500,"image_height":250,"image_bytes":742374,"service_name":"julialang.org","id":1,"original_url":"https://julialang.org/blog/2019/07/multithreading/"}],"blocks":[{"type":"rich_text","block_id":"a97vc","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://julialang.org/blog/2019/07/multithreading/"}]}]}]},{"client_msg_id":"9203603f-08c6-49c1-8315-72d4b9429cca","type":"message","text":"packages not being loaded usually. I used to have an init script that loaded everything on several threads","user":"UAH43TMUN","ts":"1608927302.244300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Spyib","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"packages not being loaded usually. I used to have an init script that loaded everything on several threads"}]}]}]},{"client_msg_id":"5eae7293-8ec9-4927-97ac-662b4bf3a490","type":"message","text":"How are you using threads?","user":"USU9FRPEU","ts":"1608927399.244900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Wib/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How are you using threads?"}]}]}]},{"client_msg_id":"85c36ae3-1e6f-46e3-b59c-070ea8eab7ca","type":"message","text":"It sounds like you are doing `Distributed` and using multiple processes. That is distinct from threads.","user":"USU9FRPEU","ts":"1608927420.245600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eKp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It sounds like you are doing "},{"type":"text","text":"Distributed","style":{"code":true}},{"type":"text","text":" and using multiple processes. That is distinct from threads."}]}]}]},{"client_msg_id":"4f07ec87-d350-4632-9542-f200cdef90da","type":"message","text":"trying to split loops up","user":"UAH43TMUN","ts":"1608927434.245700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hY1N","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"trying to split loops up"}]}]}]},{"client_msg_id":"03814efb-b7e7-4a14-8d95-9885f8861a57","type":"message","text":"What are the actual macros or functions that you are using?","user":"USU9FRPEU","ts":"1608927454.246200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hqSG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What are the actual macros or functions that you are using?"}]}]}]},{"client_msg_id":"292dcf46-b369-4754-8278-def341b2837b","type":"message","text":"`@everywhere` is a multiprocess thing, and yes in that case you need to `@everywhere using Pkg`","user":"USU9FRPEU","ts":"1608927531.247200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uV8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"@everywhere","style":{"code":true}},{"type":"text","text":" is a multiprocess thing, and yes in that case you need to "},{"type":"text","text":"@everywhere using Pkg","style":{"code":true}}]}]}]},{"client_msg_id":"35f83960-3bfd-4cc7-9d7b-09c2557ca7a6","type":"message","text":"running a math function on a big list of inputs","user":"UAH43TMUN","ts":"1608927538.247400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"o=ZU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"running a math function on a big list of inputs"}]}]}]},{"client_msg_id":"043f38c0-cb44-4756-816f-5cb24c289748","type":"message","text":"Right, but how are you invoking that? Are you using `@threads` ?","user":"USU9FRPEU","ts":"1608927555.247900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7Xwz0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Right, but how are you invoking that? Are you using "},{"type":"text","text":"@threads","style":{"code":true}},{"type":"text","text":" ?"}]}]}]},{"client_msg_id":"fb63e6d8-42ed-4b68-a24a-22855ee1501f","type":"message","text":"I was, then I saw some stuff about threadpools and it got me excited, lol","user":"UAH43TMUN","ts":"1608927603.249200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xZii","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I was, then I saw some stuff about threadpools and it got me excited, lol"}]}]}]},{"client_msg_id":"62e798e6-6611-43b7-a200-c234552ae75a","type":"message","text":"Can you show us some code?","user":"USU9FRPEU","ts":"1608927603.249300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"O5V","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can you show us some code?"}]}]}]},{"client_msg_id":"5f92fc06-804c-40c5-9343-2e9cf77193b7","type":"message","text":"So what are you using then?","user":"USU9FRPEU","ts":"1608927655.249500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ly9Qu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So what are you using then?"}]}]}]},{"client_msg_id":"9ba90b76-7568-452d-a7ec-ccda25d0b0ce","type":"message","text":"Are you using <https://github.com/tro3/ThreadPools.jl> or something?","user":"USU9FRPEU","ts":"1608927716.250300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pWw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Are you using "},{"type":"link","url":"https://github.com/tro3/ThreadPools.jl"},{"type":"text","text":" or something?"}]}]}]},{"client_msg_id":"f26b8ba7-4176-41ef-8e2d-8a0f45a85186","type":"message","text":"I'll have to look around a bit to find an example that I can share. That's what I've been looking at","user":"UAH43TMUN","ts":"1608927746.250800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oSe","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'll have to look around a bit to find an example that I can share. That's what I've been looking at"}]}]}]},{"client_msg_id":"5b9c6bb0-a9e2-4f54-9aa2-b95c9ffe06cf","type":"message","text":"what would you say is the main difference between using multiple processes and multiple threads?","user":"UAH43TMUN","ts":"1608927831.251800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Xoijd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"what would you say is the main difference between using multiple processes and multiple threads?"}]}]}]},{"client_msg_id":"51df2bbd-e112-4eba-b722-c589e2c97576","type":"message","text":"overhead","user":"USU9FRPEU","ts":"1608927845.252000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xwGd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"overhead"}]}]}]},{"client_msg_id":"7c415489-3e53-458b-a195-0735c5ee3582","type":"message","text":"With multiple processes you spend more time moving data between them. With threads you have in-process shared memory","user":"USU9FRPEU","ts":"1608927868.252500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VT4D","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"With multiple processes you spend more time moving data between them. With threads you have in-process shared memory"}]}]}]},{"client_msg_id":"c1ac77df-d8a6-48df-b280-cfa934bdaaab","type":"message","text":"ok, that makes sense","user":"UAH43TMUN","ts":"1608927897.253300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OtM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ok, that makes sense"}]}]}]},{"client_msg_id":"4e971f8f-2127-4def-8363-896c31a30e05","type":"message","text":"Processes don't even have to live on the same computer though, so you potentially can use more resources","user":"USU9FRPEU","ts":"1608927901.253400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ws+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Processes don't even have to live on the same computer though, so you potentially can use more resources"}]}]}]},{"client_msg_id":"52435ed4-a088-4eec-95e0-6dea2297b737","type":"message","text":"If you're just starting out, I would recommend focusing on the standard `@async` and `@threads` . Once you got that working and need even more performance, come back here.","user":"USU9FRPEU","ts":"1608928042.255700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Qi4YM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If you're just starting out, I would recommend focusing on the standard "},{"type":"text","text":"@async","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"@threads","style":{"code":true}},{"type":"text","text":" . Once you got that working and need even more performance, come back here."}]}]}]},{"client_msg_id":"e21181ea-1ed8-4426-bad2-801b6ec477ff","type":"message","text":"Make sure that you are invoking `julia` with the `-t auto` or something like that so that Julia starts multiple threads.","user":"USU9FRPEU","ts":"1608928069.256500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HnZUn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Make sure that you are invoking "},{"type":"text","text":"julia","style":{"code":true}},{"type":"text","text":" with the "},{"type":"text","text":"-t auto","style":{"code":true}},{"type":"text","text":" or something like that so that Julia starts multiple threads."}]}]}]},{"client_msg_id":"7938b4dd-94c9-417b-ad9e-14114552a1cb","type":"message","text":"alright, thanks for your help! I think I have a better idea now","user":"UAH43TMUN","ts":"1608928083.256600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9HoAy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"alright, thanks for your help! I think I have a better idea now"}]}]}]},{"client_msg_id":"45b51b37-ec4c-48b0-a9c1-1c1574d2ebf3","type":"message","text":"Anyone have experience dealing with threading overhead? I'm wondering if there's some API I'm not aware of that could make it actually feasible to compete with MKL for small matrices.\nCreating tasks seems to be faster than using channels.\n<https://github.com/JuliaLinearAlgebra/Octavian.jl/issues/24#issuecomment-753420485>","user":"UAUPJLBQX","ts":"1609559185.259300","team":"T68168MUP","edited":{"user":"UAUPJLBQX","ts":"1609559225.000000"},"blocks":[{"type":"rich_text","block_id":"mKtdO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Anyone have experience dealing with threading overhead? I'm wondering if there's some API I'm not aware of that could make it actually feasible to compete with MKL for small matrices.\nCreating tasks seems to be faster than using channels.\n"},{"type":"link","url":"https://github.com/JuliaLinearAlgebra/Octavian.jl/issues/24#issuecomment-753420485"}]}]}],"reactions":[{"name":"+1","users":["U7THT3TM3"],"count":1}]},{"client_msg_id":"c1182a7a-0775-4155-a3dc-db8d88dffae9","type":"message","text":"I also added an approach using channels to mimic the task API.","user":"UAUPJLBQX","ts":"1609590903.262100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"CAK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I also added an approach using channels to mimic the task API."}]}]}]},{"client_msg_id":"a25a8016-ec85-4ed0-b3e6-6d21463024d0","type":"message","text":"What is the best way to reduce the results of a `Threads.@threads` for loop?\n```N = 100_000\nvalue_to_reduce = Vector{Float64}(undef, N)\nThreads.@threads for i = 1:N\n   value_to_reduce[i] = some_expensive_calculation( i )\nend\nsum(value_to_reduce) # result I want```","user":"USU9FRPEU","ts":"1609702767.264500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VblF3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What is the best way to reduce the results of a "},{"type":"text","text":"Threads.@threads","style":{"code":true}},{"type":"text","text":" for loop?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"N = 100_000\nvalue_to_reduce = Vector{Float64}(undef, N)\nThreads.@threads for i = 1:N\n   value_to_reduce[i] = some_expensive_calculation( i )\nend\nsum(value_to_reduce) # result I want"}]}]}],"thread_ts":"1609702767.264500","reply_count":8,"reply_users_count":4,"latest_reply":"1609704829.266300","reply_users":["UDB26738Q","USU9FRPEU","U8D9768Q6","UH8A351DJ"],"subscribed":false},{"type":"message","text":"","user":"USU9FRPEU","ts":"1609788181.267900","team":"T68168MUP","attachments":[{"fallback":"[January 4th, 2021 1:24 PM] chris847: I use a different <https://cnuernber.github.io/dtype-next/tech.v3.parallel.for.html#var-indexed-map-reduce|threading primitive> that I think hasn't been discussed and that I think outperforms transducers in a lot of situations.\n\nI am testing out a <https://github.com/cnuernber/kmeans-mnist/blob/master/resources/kmeans.jl#L4|julia implementation> with a bespoke kmeans pathway.  It is really less of a pure primitive than @threads as it takes two functions, one for iterating across a subset of the index space and one to reduce the results but that first function gets a chance to setup local state just on the stack which gives you very efficient access to it.\n\n@threads could be implemented in terms of this primitive but the other way around isn't true.  And I think it is just as efficient as @threads without requiring anything fancier than an explicit local function definition and index space iteration.","ts":"1609784692.267600","author_id":"U01GX4L17PW","author_subname":"Chris Nuernberger","channel_id":"C6SMTHQ3T","channel_name":"multithreading","is_msg_unfurl":true,"is_reply_unfurl":true,"text":"I use a different <https://cnuernber.github.io/dtype-next/tech.v3.parallel.for.html#var-indexed-map-reduce|threading primitive> that I think hasn't been discussed and that I think outperforms transducers in a lot of situations.\n\nI am testing out a <https://github.com/cnuernber/kmeans-mnist/blob/master/resources/kmeans.jl#L4|julia implementation> with a bespoke kmeans pathway.  It is really less of a pure primitive than @threads as it takes two functions, one for iterating across a subset of the index space and one to reduce the results but that first function gets a chance to setup local state just on the stack which gives you very efficient access to it.\n\n@threads could be implemented in terms of this primitive but the other way around isn't true.  And I think it is just as efficient as @threads without requiring anything fancier than an explicit local function definition and index space iteration.","author_name":"Chris Nuernberger","author_link":"https://julialang.slack.com/team/U01GX4L17PW","author_icon":"https://secure.gravatar.com/avatar/840f31aa37e991760687b31deb385333.jpg?s=48&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0016-48.png","mrkdwn_in":["text"],"color":"D0D0D0","from_url":"https://julialang.slack.com/archives/C6SMTHQ3T/p1609784692267600?thread_ts=1609702767264500&cid=C6SMTHQ3T","is_share":true,"footer":"From a thread in #multithreading"}]},{"type":"message","subtype":"thread_broadcast","text":"It appears our first <#C6SMTHQ3T|multithreading> BoF is scheduled for tomorrow at 11:30am EST (on the JuliaLang community calendar). Any are welcome to join! <https://meet.google.com/ugr-sbmu-wts>","user":"U67BXBF99","ts":"1609884218.268200","thread_ts":"1596507680.147500","root":{"client_msg_id":"a983bc33-e374-450e-a499-326eed3b14d9","type":"message","text":"For this week’s multithreading-BoF, I’m thinking of hosting at 9:30a EDT (13h30 UTC) on <https://meet.google.com/ugr-sbmu-wts>, for anyone that wants to drop by and discuss threads and atomics!","user":"U67BXBF99","ts":"1596507680.147500","team":"T68168MUP","attachments":[{"title":"Meet","title_link":"https://meet.google.com/ugr-sbmu-wts","text":"Real-time meetings by Google. Using your browser, share your video, desktop, and presentations with teammates and customers.","fallback":"Meet","thumb_url":"https://www.gstatic.com/images/branding/product/2x/meet_96dp.png","from_url":"https://meet.google.com/ugr-sbmu-wts","thumb_width":192,"thumb_height":192,"service_icon":"http://www.gstatic.com/images/branding/product/1x/meet_16dp.png","service_name":"meet.google.com","id":1,"original_url":"https://meet.google.com/ugr-sbmu-wts"}],"blocks":[{"type":"rich_text","block_id":"HrV1l","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"For this week’s multithreading-BoF, I’m thinking of hosting at 9:30a EDT (13h30 UTC) on "},{"type":"link","url":"https://meet.google.com/ugr-sbmu-wts"},{"type":"text","text":", for anyone that wants to drop by and discuss threads and atomics!"}]}]}],"thread_ts":"1596507680.147500","reply_count":49,"reply_users_count":8,"latest_reply":"1609884218.268200","reply_users":["ULY7Q1X53","U67BXBF99","UF9T3JL4D","USU9FRPEU","U6A936746","UB7JS9CHF","U67BJLYCS","U6A0PD8CR"],"subscribed":false},"attachments":[{"title":"Meet","title_link":"https://meet.google.com/ugr-sbmu-wts","text":"Real-time meetings by Google. Using your browser, share your video, desktop, and presentations with teammates and customers.","fallback":"Meet","thumb_url":"https://fonts.gstatic.com/s/i/productlogos/meet_2020q4/v1/web-96dp/logo_meet_2020q4_color_2x_web_96dp.png","from_url":"https://meet.google.com/ugr-sbmu-wts","thumb_width":192,"thumb_height":192,"service_icon":"http://fonts.gstatic.com/s/i/productlogos/meet_2020q4/v1/web-24dp/logo_meet_2020q4_color_1x_web_24dp.png","service_name":"meet.google.com","id":1,"original_url":"https://meet.google.com/ugr-sbmu-wts"}],"blocks":[{"type":"rich_text","block_id":"U0j","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It appears our first "},{"type":"channel","channel_id":"C6SMTHQ3T"},{"type":"text","text":" BoF is scheduled for tomorrow at 11:30am EST (on the JuliaLang community calendar). Any are welcome to join! "},{"type":"link","url":"https://meet.google.com/ugr-sbmu-wts"}]}]}],"client_msg_id":"389da0a4-39bf-4a1e-ba57-c7b645fe6e36"},{"client_msg_id":"36e17101-d8a5-4153-8e95-c59455707e19","type":"message","text":"As v1.6 release works wraps up, I expect we’ll ramp up multithreading work soon","user":"U67BXBF99","ts":"1609884267.269300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hOA4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"As v1.6 release works wraps up, I expect we’ll ramp up multithreading work soon"}]}]}]},{"client_msg_id":"8749D7AE-C25E-4974-A9C2-6E9454CD9F3A","type":"message","text":"Do you usually have an agenda for these BoF sessions? Or would anyone just show up and talk?","user":"U8T0YV7QC","ts":"1609949870.270900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JaDC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do you usually have an agenda for these BoF sessions? Or would anyone just show up and talk?"}]}]}]},{"client_msg_id":"15ba0dbf-6672-446d-8d86-76e8cb5b04d4","type":"message","text":"Is the meeting going on today?","user":"U01GX4L17PW","ts":"1609951378.271300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"f98xF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is the meeting going on today?"}]}]}]},{"client_msg_id":"6a9ff279-299d-4044-b35c-d4ab57e34fc5","type":"message","text":"also waiting","user":"UP9P4JFNJ","ts":"1609951458.271800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"C3DfT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"also waiting"}]}]}]},{"client_msg_id":"4c0dd8b1-e689-4fba-aba0-6c98c0f6f596","type":"message","text":"now giving up","user":"UP9P4JFNJ","ts":"1609951725.272100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4MQ3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"now giving up"}]}]}]},{"client_msg_id":"c89d4fff-f27d-432f-b94e-6f7c3efe904f","type":"message","text":"15 minute rule :slightly_smiling_face:","user":"U01GX4L17PW","ts":"1609951750.272400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PGBko","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"15 minute rule "},{"type":"emoji","name":"slightly_smiling_face"}]}]}]},{"client_msg_id":"a23dcf09-246d-43c3-a5f8-253a09101848","type":"message","text":"Ah, sorry my last meeting ran a bit long","user":"U67BXBF99","ts":"1609951815.272700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/29s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah, sorry my last meeting ran a bit long"}]}]}]},{"client_msg_id":"3966add9-ae78-46fc-a571-2fac8fa96eef","type":"message","text":"<@UP9P4JFNJ>","user":"UDGT4PM41","ts":"1609951855.273100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"z5Qr","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UP9P4JFNJ"}]}]}]},{"client_msg_id":"d8d13c63-5c19-4bf6-a495-1f19fb12ed56","type":"message","text":"Someday I’ll try to find a platform where people can join without me hosting","user":"U67BXBF99","ts":"1609951869.273500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ao+vA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Someday I’ll try to find a platform where people can join without me hosting"}]}]}],"thread_ts":"1609951869.273500","reply_count":1,"reply_users_count":1,"latest_reply":"1609953664.275500","reply_users":["U67BJLYCS"],"subscribed":false},{"client_msg_id":"fc7bf9fd-0d71-4902-b016-117c50ba1f0b","type":"message","text":"ok, I’m in","user":"UP9P4JFNJ","ts":"1609951904.273800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fLs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ok, I’m in"}]}]}]},{"client_msg_id":"9512d32d-2c2e-4f1c-9a11-c44b8874bdc1","type":"message","text":"The agenda is maintained at <https://docs.google.com/document/d/1rDgdFH94QL4wlEAdzBbdxJJ0Wk2CkBhWuptzPETSRc0/edit>","user":"U67BXBF99","ts":"1609951925.274300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0zX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The agenda is maintained at "},{"type":"link","url":"https://docs.google.com/document/d/1rDgdFH94QL4wlEAdzBbdxJJ0Wk2CkBhWuptzPETSRc0/edit"}]}]}]},{"client_msg_id":"879dbaed-8662-44ab-be82-a722a33324ca","type":"message","text":"<@U01GX4L17PW>","user":"UDGT4PM41","ts":"1609952008.275000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xZ=LG","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U01GX4L17PW"}]}]}]},{"client_msg_id":"b8ed703e-b637-4177-9963-e8720dcd9c16","type":"message","text":"Started if you still want to join","user":"UDGT4PM41","ts":"1609952019.275400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"97D","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Started if you still want to join"}]}]}]},{"client_msg_id":"4FBCA9B9-F17A-470A-BB83-9FD54B168630","type":"message","text":"Darn, missed it! Dealing with teenagers going to school remotely (from my home office!) can be a pain!","user":"UB7JS9CHF","ts":"1610020228.277500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5XZE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Darn, missed it! Dealing with teenagers going to school remotely (from my home office!) can be a pain!"}]}]}]},{"type":"message","subtype":"thread_broadcast","text":"I’m quite interested in this approach but I’m wondering what’s the difference between `@spawn` and this approach, should one always call that C API directly to get minimal threading overhead?","user":"UC6SUUPRC","ts":"1610055133.278000","thread_ts":"1609559185.259300","root":{"client_msg_id":"45b51b37-ec4c-48b0-a9c1-1c1574d2ebf3","type":"message","text":"Anyone have experience dealing with threading overhead? I'm wondering if there's some API I'm not aware of that could make it actually feasible to compete with MKL for small matrices.\nCreating tasks seems to be faster than using channels.\n<https://github.com/JuliaLinearAlgebra/Octavian.jl/issues/24#issuecomment-753420485>","user":"UAUPJLBQX","ts":"1609559185.259300","team":"T68168MUP","edited":{"user":"UAUPJLBQX","ts":"1609559225.000000"},"blocks":[{"type":"rich_text","block_id":"mKtdO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Anyone have experience dealing with threading overhead? I'm wondering if there's some API I'm not aware of that could make it actually feasible to compete with MKL for small matrices.\nCreating tasks seems to be faster than using channels.\n"},{"type":"link","url":"https://github.com/JuliaLinearAlgebra/Octavian.jl/issues/24#issuecomment-753420485"}]}]}],"thread_ts":"1609559185.259300","reply_count":1,"reply_users_count":1,"latest_reply":"1610055133.278000","reply_users":["UC6SUUPRC"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"Mk3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I’m quite interested in this approach but I’m wondering what’s the difference between "},{"type":"text","text":"@spawn","style":{"code":true}},{"type":"text","text":" and this approach, should one always call that C API directly to get minimal threading overhead?"}]}]}],"client_msg_id":"130e3d74-e79f-4f2b-887f-680b920318a6"},{"client_msg_id":"5c9e79a5-c951-44de-834b-c3bf3c3f071f","type":"message","text":"I've got some multithreaded code that continuously allocates and frees large matrices. The tasks never terminate, they just write their outputs to a channel that is consumed.\nIt appears that garbage collection never runs, and the process keeps on allocating until it gets OOM killed. If I stick a GC.gc() in the main loop, memory usage stays bounded.\nI'm going to try and produce an MWE, but it might be a bit tricky.\nNonetheless, has anyone seen behaviour like this? It is driving me a bit nuts.","user":"UCNPT22MQ","ts":"1610061964.283000","team":"T68168MUP","edited":{"user":"UCNPT22MQ","ts":"1610061977.000000"},"blocks":[{"type":"rich_text","block_id":"11n","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I've got some multithreaded code that continuously allocates and frees large matrices. The tasks never terminate, they just write their outputs to a channel that is consumed.\nIt appears that garbage collection never runs, and the process keeps on allocating until it gets OOM killed. If I stick a GC.gc() in the main loop, memory usage stays bounded.\nI'm going to try and produce an MWE, but it might be a bit tricky.\nNonetheless, has anyone seen behaviour like this? It is driving me a bit nuts."}]}]}],"thread_ts":"1610061964.283000","reply_count":2,"reply_users_count":2,"latest_reply":"1610062335.283500","reply_users":["U680THK2S","UCNPT22MQ"],"subscribed":false},{"client_msg_id":"3b061f39-51ef-4754-8cef-bf7551db5b5b","type":"message","text":"Can I ask whether anyone knows what library provides the underlying pthreads implementation for Julia, and whether it is statically linked? We're doing some profiling of our code (using extrae), and we're successfully hooking into the MPI libraries and profiling them, but we're not succeeding with our threading code (yet!)...","user":"U7C8KRZKL","ts":"1610626715.001800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6LcUy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can I ask whether anyone knows what library provides the underlying pthreads implementation for Julia, and whether it is statically linked? We're doing some profiling of our code (using extrae), and we're successfully hooking into the MPI libraries and profiling them, but we're not succeeding with our threading code (yet!)..."}]}]}]},{"client_msg_id":"ea002995-cabe-488f-bd29-79a44ac33d9d","type":"message","text":"<@U67431ELR> is working on adding multi-threading to DataFrames.jl. When testing his code I am hitting the following issue. It seems that when we run several operations that heavily allocate (only temporarily and small chunks of memory) we have the following situation:\n1. with no threading I never have a problem\n2. as I increase the number of threads it becomes possible that the Julia process gets Killed (on Linux) because it runs out of memory before GC happens (in single-threaded mode it is not a problem because we never allocate a lot of memory)\nIs there a way to work around this issue. (this is a working hypothesis about the reason, the sure thing is that I get \"Killed\" result)","user":"U8JAMQGQY","ts":"1610644311.002000","team":"T68168MUP","edited":{"user":"U8JAMQGQY","ts":"1610644539.000000"},"blocks":[{"type":"rich_text","block_id":"pKE","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U67431ELR"},{"type":"text","text":" is working on adding multi-threading to DataFrames.jl. When testing his code I am hitting the following issue. It seems that when we run several operations that heavily allocate (only temporarily and small chunks of memory) we have the following situation:\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"with no threading I never have a problem"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"as I increase the number of threads it becomes possible that the Julia process gets Killed (on Linux) because it runs out of memory before GC happens (in single-threaded mode it is not a problem because we never allocate a lot of memory)"}]}],"style":"ordered","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to work around this issue. (this is a working hypothesis about the reason, the sure thing is that I get \"Killed\" result)"}]}]}],"thread_ts":"1610644311.002000","reply_count":10,"reply_users_count":2,"latest_reply":"1610647340.004400","reply_users":["U67431ELR","U8JAMQGQY"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"I would assume it is improved a bit on 1.6 because of lower RAM usage. Actually I hope that in this thread someone who knows exactly (maybe you do) how and when GC gets invoked in multi-threaded code could comment so I could better understand this aspect of how Julia works.","user":"U8JAMQGQY","ts":"1610646432.003500","thread_ts":"1610644311.002000","root":{"client_msg_id":"ea002995-cabe-488f-bd29-79a44ac33d9d","type":"message","text":"<@U67431ELR> is working on adding multi-threading to DataFrames.jl. When testing his code I am hitting the following issue. It seems that when we run several operations that heavily allocate (only temporarily and small chunks of memory) we have the following situation:\n1. with no threading I never have a problem\n2. as I increase the number of threads it becomes possible that the Julia process gets Killed (on Linux) because it runs out of memory before GC happens (in single-threaded mode it is not a problem because we never allocate a lot of memory)\nIs there a way to work around this issue. (this is a working hypothesis about the reason, the sure thing is that I get \"Killed\" result)","user":"U8JAMQGQY","ts":"1610644311.002000","team":"T68168MUP","edited":{"user":"U8JAMQGQY","ts":"1610644539.000000"},"blocks":[{"type":"rich_text","block_id":"pKE","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U67431ELR"},{"type":"text","text":" is working on adding multi-threading to DataFrames.jl. When testing his code I am hitting the following issue. It seems that when we run several operations that heavily allocate (only temporarily and small chunks of memory) we have the following situation:\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"with no threading I never have a problem"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"as I increase the number of threads it becomes possible that the Julia process gets Killed (on Linux) because it runs out of memory before GC happens (in single-threaded mode it is not a problem because we never allocate a lot of memory)"}]}],"style":"ordered","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to work around this issue. (this is a working hypothesis about the reason, the sure thing is that I get \"Killed\" result)"}]}]}],"thread_ts":"1610644311.002000","reply_count":10,"reply_users_count":2,"latest_reply":"1610647340.004400","reply_users":["U67431ELR","U8JAMQGQY"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"mad","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I would assume it is improved a bit on 1.6 because of lower RAM usage. Actually I hope that in this thread someone who knows exactly (maybe you do) how and when GC gets invoked in multi-threaded code could comment so I could better understand this aspect of how Julia works."}]}]}],"client_msg_id":"35af4205-ac67-41bc-ae41-d3e62978aefa"},{"type":"message","subtype":"thread_broadcast","text":"It's disappointing that the speed tests so far shows that multiprocess MPI code runs faster than multithreaded using julia threads - it would be great to be able to understand why, so help with profiling the pthreads code would be appreciated...","user":"U7C8KRZKL","ts":"1610665497.006700","thread_ts":"1610626715.001800","root":{"client_msg_id":"3b061f39-51ef-4754-8cef-bf7551db5b5b","type":"message","text":"Can I ask whether anyone knows what library provides the underlying pthreads implementation for Julia, and whether it is statically linked? We're doing some profiling of our code (using extrae), and we're successfully hooking into the MPI libraries and profiling them, but we're not succeeding with our threading code (yet!)...","user":"U7C8KRZKL","ts":"1610626715.001800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6LcUy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can I ask whether anyone knows what library provides the underlying pthreads implementation for Julia, and whether it is statically linked? We're doing some profiling of our code (using extrae), and we're successfully hooking into the MPI libraries and profiling them, but we're not succeeding with our threading code (yet!)..."}]}]}],"thread_ts":"1610626715.001800","reply_count":3,"reply_users_count":2,"latest_reply":"1610667781.007200","reply_users":["U7C8KRZKL","U67BJLYCS"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"FVdd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It's disappointing that the speed tests so far shows that multiprocess MPI code runs faster than multithreaded using julia threads - it would be great to be able to understand why, so help with profiling the pthreads code would be appreciated..."}]}]}],"client_msg_id":"db3c939d-9cff-48fa-86fe-e235a34f55cd"},{"client_msg_id":"f12290be-464c-4235-b58c-677707050fe6","type":"message","text":"Is there an easy way to add timer to functions called inside threads? AFAIK, `TimerOutputs.jl` does not work for threaded parts.","user":"U01G219K685","ts":"1610708213.015400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lMHQV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there an easy way to add timer to functions called inside threads? AFAIK, "},{"type":"text","text":"TimerOutputs.jl","style":{"code":true}},{"type":"text","text":" does not work for threaded parts."}]}]}]},{"client_msg_id":"780d2a20-b8ff-4298-a105-22bf9d90d123","type":"message","text":"I'm quite a few weeks behind, but i was watching a couple talks from GopherCon, and this one was really neat! :slightly_smiling_face:\n<https://discourse.julialang.org/t/external-talk-gophercon-2020-austin-clements-pardon-the-interruption-loop-preemption-in-go-1-14/53555>","user":"U89GY9W1J","ts":"1611000032.017000","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"External talk: \"GopherCon 2020: Austin Clements - Pardon the Interruption: Loop Preemption in Go 1.14\"","title_link":"https://discourse.julialang.org/t/external-talk-gophercon-2020-austin-clements-pardon-the-interruption-loop-preemption-in-go-1-14/53555","text":"I really enjoyed watching this talk about how Go has added preemption to their coroutines to avoid stop-the-world GC latency caused by long-running tight-loops in tasks: GopherCon 2020: Austin Clements - Pardon the Interruption: Loop Preemption in Go 1.14 Very interesting and inspiring to see how far they’re able to push this concurrency model! Interesting to see where they’ve been and what they’ve done and imagine how it might or might not relate to julia’s future in this area. :...","fallback":"JuliaLang: External talk: \"GopherCon 2020: Austin Clements - Pardon the Interruption: Loop Preemption in Go 1.14\"","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","ts":1610999983,"from_url":"https://discourse.julialang.org/t/external-talk-gophercon-2020-austin-clements-pardon-the-interruption-loop-preemption-in-go-1-14/53555","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/external-talk-gophercon-2020-austin-clements-pardon-the-interruption-loop-preemption-in-go-1-14/53555"}],"blocks":[{"type":"rich_text","block_id":"7qdC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm quite a few weeks behind, but i was watching a couple talks from GopherCon, and this one was really neat! "},{"type":"emoji","name":"slightly_smiling_face"},{"type":"text","text":"\n"},{"type":"link","url":"https://discourse.julialang.org/t/external-talk-gophercon-2020-austin-clements-pardon-the-interruption-loop-preemption-in-go-1-14/53555"}]}]}],"reactions":[{"name":"gopher_dance","users":["UDB26738Q","U89GY9W1J"],"count":2}]},{"client_msg_id":"6a96b588-4569-4891-be91-fe1441e8d4b2","type":"message","text":"Whoah, did not see that conclusion coming","user":"U67BXBF99","ts":"1611003832.017400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iR4ja","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Whoah, did not see that conclusion coming"}]}]}]},{"client_msg_id":"78288a70-68cd-46cb-95bf-d5a2df7f5682","type":"message","text":"Didn't watch the talk yet, but adding safepoints into backeges of loops is a fairly common thing for people  to do","user":"U674T3KB3","ts":"1611003952.018100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2y/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Didn't watch the talk yet, but adding safepoints into backeges of loops is a fairly common thing for people  to do"}]}]}]},{"client_msg_id":"0075a279-a990-4514-ba1b-705615489070","type":"message","text":"Yeah, that came with an extravagant performance penalty for them (okay, they got it down to under 5%, but still not great)","user":"U67BXBF99","ts":"1611004004.019000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ej1Rg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah, that came with an extravagant performance penalty for them (okay, they got it down to under 5%, but still not great)"}]}]}],"thread_ts":"1611004004.019000","reply_count":13,"reply_users_count":2,"latest_reply":"1611077615.038200","reply_users":["U89GY9W1J","U674T3KB3"],"subscribed":false},{"client_msg_id":"f090305f-ddda-42d6-a977-3858d3bef8c5","type":"message","text":"not surprised","user":"U674T3KB3","ts":"1611004070.019200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Lu8f","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"not surprised"}]}]}]},{"client_msg_id":"582ed514-df63-4644-8c51-42d9df4f73a6","type":"message","text":"I was expecting they’d go with Java’s approach (like ours)","user":"U67BXBF99","ts":"1611004100.019600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5i4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I was expecting they’d go with Java’s approach (like ours)"}]}]}],"thread_ts":"1611004100.019600","reply_count":10,"reply_users_count":4,"latest_reply":"1611070566.034600","reply_users":["U67BJLYCS","U6795JH6H","U67BXBF99","U67431ELR"],"subscribed":false},{"client_msg_id":"12f126b9-b29a-4a1f-8f78-9226e3839ad4","type":"message","text":"Interesting talk, I was vaguely aware of all the things they had explored, but I didn't know what they ended up with","user":"U67BJLYCS","ts":"1611008647.021200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tWWL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Interesting talk, I was vaguely aware of all the things they had explored, but I didn't know what they ended up with"}]}]}],"reactions":[{"name":"+1","users":["U89GY9W1J"],"count":1}]},{"client_msg_id":"862c2027-157d-46e2-bad6-8d0320e3e1cf","type":"message","text":"Hi there. I am doing my first package with multi-threading, and I was wondering how (and if possible) to start a github CI action with two (or more threads)?","user":"U01FR2HFJ7M","ts":"1611027228.025400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"n06w","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi there. I am doing my first package with multi-threading, and I was wondering how (and if possible) to start a github CI action with two (or more threads)?"}]}]}]},{"client_msg_id":"859935DA-3AA9-46D4-8AA7-8BACB56C8DF7","type":"message","text":"Yeah","user":"U7THT3TM3","ts":"1611027713.025700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sS2/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah"}]}]}]},{"client_msg_id":"508E2872-98E5-4171-875C-58B6943E16B8","type":"message","text":"For example, take a look at <https://github.com/mcabbott/Tullio.jl/blob/master/.github/workflows/ci.yml|https://github.com/mcabbott/Tullio.jl/blob/master/.github/workflows/ci.yml>","user":"U7THT3TM3","ts":"1611027757.026400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lz/gY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"For example, take a look at "},{"type":"link","url":"https://github.com/mcabbott/Tullio.jl/blob/master/.github/workflows/ci.yml","text":"https://github.com/mcabbott/Tullio.jl/blob/master/.github/workflows/ci.yml"}]}]}]},{"client_msg_id":"C4B82024-4095-49C8-8513-96212A9AE612","type":"message","text":"Where we do one job with one thread, and one job with two threads","user":"U7THT3TM3","ts":"1611027770.026800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QJI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Where we do one job with one thread, and one job with two threads"}]}]}]},{"client_msg_id":"7a599e48-a1b8-4a77-bbc2-f1849dfc02da","type":"message","text":"Oh great example, so we can make threads one of the matrix argument. Perfect!","user":"U01FR2HFJ7M","ts":"1611027939.027700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RPWA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oh great example, so we can make threads one of the matrix argument. Perfect!"}]}]}]},{"client_msg_id":"37991b23-f3ec-44d5-96db-dabe9b0c7aae","type":"message","text":"I have another question! I want to share some info between my main structure `A` (that create sub structures `Bi` each associated to a different thread). Info is stored as a field `info` of `A`, and it would be nice to have an access to `info` from each `Bi` .\n`A` can update the info from time to time, so I would need to lock the `info` (is it possible to lock a field? If not I guess I can make a copy of the `info` field that I pass as a argument of all the involved functions ... and lock it when `A` modifies it)\nAlso, if I refer to `A` in `Bi`s and the opposite, I will end up with circular references. Is that OK  in julia or should I try to avoid it?","user":"U01FR2HFJ7M","ts":"1611032477.033800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IDeu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have another question! I want to share some info between my main structure "},{"type":"text","text":"A","style":{"code":true}},{"type":"text","text":" (that create sub structures "},{"type":"text","text":"Bi","style":{"code":true}},{"type":"text","text":" each associated to a different thread). Info is stored as a field "},{"type":"text","text":"info","style":{"code":true}},{"type":"text","text":" of "},{"type":"text","text":"A","style":{"code":true}},{"type":"text","text":", and it would be nice to have an access to "},{"type":"text","text":"info","style":{"code":true}},{"type":"text","text":" from each "},{"type":"text","text":"Bi","style":{"code":true}},{"type":"text","text":" .\n"},{"type":"text","text":"A","style":{"code":true}},{"type":"text","text":" can update the info from time to time, so I would need to lock the "},{"type":"text","text":"info","style":{"code":true}},{"type":"text","text":" (is it possible to lock a field? If not I guess I can make a copy of the "},{"type":"text","text":"info","style":{"code":true}},{"type":"text","text":" field that I pass as a argument of all the involved functions ... and lock it when "},{"type":"text","text":"A","style":{"code":true}},{"type":"text","text":" modifies it)\nAlso, if I refer to "},{"type":"text","text":"A","style":{"code":true}},{"type":"text","text":" in "},{"type":"text","text":"Bi","style":{"code":true}},{"type":"text","text":"s and the opposite, I will end up with circular references. Is that OK  in julia or should I try to avoid it?"}]}]}],"thread_ts":"1611032477.033800","reply_count":1,"reply_users_count":1,"latest_reply":"1611035369.033900","reply_users":["U01FR2HFJ7M"],"subscribed":false},{"client_msg_id":"e07ff1b1-f4e9-4fd0-a2ba-dff8ded051dc","type":"message","text":"Just to say thanks for the help these last days. My LocalSearchSolvers.jl is updated to multithreading without any syntax changes outside of starting Julia with threads. The improvements are quite good even on very small instances!\nI guess next will be a distributed version (that is gonna be harder haha)","user":"U01FR2HFJ7M","ts":"1611239344.002100","team":"T68168MUP","edited":{"user":"U01FR2HFJ7M","ts":"1611239401.000000"},"blocks":[{"type":"rich_text","block_id":"a1I+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Just to say thanks for the help these last days. My LocalSearchSolvers.jl is updated to multithreading without any syntax changes outside of starting Julia with threads. The improvements are quite good even on very small instances!\nI guess next will be a distributed version (that is gonna be harder haha)"}]}]}],"reactions":[{"name":"100","users":["U6795JH6H","UKG4WF8PJ","U969CNQU9","U68QW0PUZ","U9MED583T"],"count":5},{"name":"tada","users":["U6795JH6H","UCZ7VBGUD","U8T0YV7QC"],"count":3},{"name":"heart","users":["U6795JH6H","UAZP7LJLU","U8T0YV7QC"],"count":3},{"name":"juliaspinner","users":["U68QW0PUZ"],"count":1}]},{"client_msg_id":"ddb68cf2-9149-42a9-a50c-f6d50535a8bc","type":"message","text":"Is `tempname` threadsafe? I consistently get `IOError: open: no such file or directory (ENOENT)` \n\nWhen using tempname + mv to atomically write a file. I'm doing heavy multithreading. When I switch to just generating my own tempnames (random number suffix, or whatever), the errors go away.","user":"UCNPT22MQ","ts":"1611583125.007600","team":"T68168MUP","edited":{"user":"UCNPT22MQ","ts":"1611583152.000000"},"blocks":[{"type":"rich_text","block_id":"wcT92","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is "},{"type":"text","text":"tempname","style":{"code":true}},{"type":"text","text":" threadsafe? I consistently get "},{"type":"text","text":"IOError: open: no such file or directory (ENOENT) \n","style":{"code":true}},{"type":"text","text":"\nWhen using tempname + mv to atomically write a file. I'm doing heavy multithreading. When I switch to just generating my own tempnames (random number suffix, or whatever), the errors go away."}]}]}]},{"client_msg_id":"58a99d70-7b3e-403a-81d6-7667ebc78bf5","type":"message","text":"The error suggests that two threads are being given the same tempname, and one is mving it first, with the other losing","user":"UCNPT22MQ","ts":"1611583200.009500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/xA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The error suggests that two threads are being given the same tempname, and one is mving it first, with the other losing"}]}]}],"thread_ts":"1611583200.009500","reply_count":4,"reply_users_count":3,"latest_reply":"1611602585.010300","reply_users":["U67SCG4HG","UCNPT22MQ","U67431ELR"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"<#C6SMTHQ3T|multithreading> meeting this week on the usual link is starting in 10 minutes. All are welcome to join!","user":"U67BXBF99","ts":"1611764448.010600","thread_ts":"1596507680.147500","root":{"client_msg_id":"a983bc33-e374-450e-a499-326eed3b14d9","type":"message","text":"For this week’s multithreading-BoF, I’m thinking of hosting at 9:30a EDT (13h30 UTC) on <https://meet.google.com/ugr-sbmu-wts>, for anyone that wants to drop by and discuss threads and atomics!","user":"U67BXBF99","ts":"1596507680.147500","team":"T68168MUP","attachments":[{"title":"Meet","title_link":"https://meet.google.com/ugr-sbmu-wts","text":"Real-time meetings by Google. Using your browser, share your video, desktop, and presentations with teammates and customers.","fallback":"Meet","thumb_url":"https://www.gstatic.com/images/branding/product/2x/meet_96dp.png","from_url":"https://meet.google.com/ugr-sbmu-wts","thumb_width":192,"thumb_height":192,"service_icon":"http://www.gstatic.com/images/branding/product/1x/meet_16dp.png","service_name":"meet.google.com","id":1,"original_url":"https://meet.google.com/ugr-sbmu-wts"}],"blocks":[{"type":"rich_text","block_id":"HrV1l","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"For this week’s multithreading-BoF, I’m thinking of hosting at 9:30a EDT (13h30 UTC) on "},{"type":"link","url":"https://meet.google.com/ugr-sbmu-wts"},{"type":"text","text":", for anyone that wants to drop by and discuss threads and atomics!"}]}]}],"thread_ts":"1596507680.147500","reply_count":61,"reply_users_count":10,"latest_reply":"1611771392.012900","reply_users":["ULY7Q1X53","U67BXBF99","UF9T3JL4D","USU9FRPEU","U6A936746","UB7JS9CHF","U67BJLYCS","U6A0PD8CR","U6QGE7S86","UP9P4JFNJ"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"VtYjd","elements":[{"type":"rich_text_section","elements":[{"type":"channel","channel_id":"C6SMTHQ3T"},{"type":"text","text":" meeting this week on the usual link is starting in 10 minutes. All are welcome to join!"}]}]}],"client_msg_id":"f80a4efc-0100-4b02-8a96-97090d12b64d"}]}