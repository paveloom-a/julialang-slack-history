{"cursor": 1, "messages": [{"client_msg_id":"30a84e08-61c6-4a22-8e66-7103fc5379ed","type":"message","text":"Has anyone looked into using the newly added `reinterpret(reshape, ...)` on `CuArrays`? I am running into issues with KernelAbstractions and am wondering whether that might be a CUDA issue: <https://github.com/JuliaGPU/KernelAbstractions.jl/issues/179#issuecomment-756714700>","user":"UM30MT6RF","ts":"1610111815.194200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uy2oO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Has anyone looked into using the newly added "},{"type":"text","text":"reinterpret(reshape, ...)","style":{"code":true}},{"type":"text","text":" on "},{"type":"text","text":"CuArrays","style":{"code":true}},{"type":"text","text":"? I am running into issues with KernelAbstractions and am wondering whether that might be a CUDA issue: "},{"type":"link","url":"https://github.com/JuliaGPU/KernelAbstractions.jl/issues/179#issuecomment-756714700"}]}]}]},{"client_msg_id":"1f1b622d-db78-4124-9069-6491bee77213","type":"message","text":"Maybe the best solution here would be to produce a `CuArray` as well instead of a wrapper?","user":"UM30MT6RF","ts":"1610111935.195300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"l/Lm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Maybe the best solution here would be to produce a "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":" as well instead of a wrapper?"}]}]}]},{"client_msg_id":"2e074ac1-d506-47c8-9e28-a673743020c5","type":"message","text":"Hi all,\nDoes anyone have any ideas on how to fix this code so that the GPU output matches the CPU CG + CPU `\\`? On Julia 1.5 with CUDA 2.0.2\n```using CUDA, CUDA.CUSPARSE, CUDA.CUSOLVER\nusing LinearAlgebra\nusing SparseArrays\nusing IncompleteLU\nusing IterativeSolvers\nCUDA.allowscalar(false)\n\nval = sprand(200,200,0.05);\nA_cpu = val*val'\nb_cpu = rand(200)\n\nA_gpu = CuSparseMatrixCSC(A_cpu)\nb_gpu = CuArray(b_cpu)\n\nPrecilu = ilu(A_cpu, τ = 3.0)\n\nimport Base: ldiv!\nfunction LinearAlgebra.ldiv!(y::CuArray, P::LUgpu, x::CuArray)\n  copyto!(y, x)\n  sv2!('N', 'L', 1.0, P, y, 'O')\n  sv2!('N', 'U', 1.0, P, y, 'O')\n  return y\nend\n\nfunction LinearAlgebra.ldiv!(P::LUgpu, x::CuArray)\n  sv2!('N', 'L', 1.0, P, x, 'O')\n  sv2!('N', 'U', 1.0, P, x, 'O')\n  return x\nend\n\n# function LinearAlgebra.ldiv!(_lu::LUperso, rhs::CUDA.CuArray)\n# \t_x = UpperTriangular(_lu.Ut) \\ (LowerTriangular(_lu.L) \\ rhs)\n# \trhs .= vec(_x)\n# \t# CUDA.unsafe_free!(_x)\n# \trhs\n# end\n#\n# function LinearAlgebra.ldiv!(yrhs::CuArray,_lu::LUperso, rhs::CuArray)\n# \tcopyto!(yrhs,rhs)\n# \t_x = UpperTriangular(_lu.Ut) \\ (LowerTriangular(_lu.L) \\ rhs)\n# \trhs .= vec(_x)\n# \t# CUDA.unsafe_free!(_x)\n# \trhs\n# end\n\nstruct LUgpu\n\tL\n\tUt\t# transpose of U in LU decomposition\nend\n\n\nP = LUperso(LowerTriangular(CuSparseMatrixCSC(I+Precilu.L)), UpperTriangular(CuSparseMatrixCSC(sparse(Precilu.U'))));\nval = cg(A_gpu,b_gpu,verbose=true,Pl=P,tol=10^-7,maxiter=1000)\n\nP_cpu = ilu(A_cpu,τ=3.0)\nval = cg(A_cpu,b_cpu;verbose=true,Pl=P_cpu,tol=10^-7,maxiter=1000)\nA_cpu\\b_cpu ```","user":"U01CZ5M383E","ts":"1610153168.199800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"p9E","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi all,\nDoes anyone have any ideas on how to fix this code so that the GPU output matches the CPU CG + CPU "},{"type":"text","text":"\\","style":{"code":true}},{"type":"text","text":"? On Julia 1.5 with CUDA 2.0.2\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using CUDA, CUDA.CUSPARSE, CUDA.CUSOLVER\nusing LinearAlgebra\nusing SparseArrays\nusing IncompleteLU\nusing IterativeSolvers\nCUDA.allowscalar(false)\n\nval = sprand(200,200,0.05);\nA_cpu = val*val'\nb_cpu = rand(200)\n\nA_gpu = CuSparseMatrixCSC(A_cpu)\nb_gpu = CuArray(b_cpu)\n\nPrecilu = ilu(A_cpu, τ = 3.0)\n\nimport Base: ldiv!\nfunction LinearAlgebra.ldiv!(y::CuArray, P::LUgpu, x::CuArray)\n  copyto!(y, x)\n  sv2!('N', 'L', 1.0, P, y, 'O')\n  sv2!('N', 'U', 1.0, P, y, 'O')\n  return y\nend\n\nfunction LinearAlgebra.ldiv!(P::LUgpu, x::CuArray)\n  sv2!('N', 'L', 1.0, P, x, 'O')\n  sv2!('N', 'U', 1.0, P, x, 'O')\n  return x\nend\n\n# function LinearAlgebra.ldiv!(_lu::LUperso, rhs::CUDA.CuArray)\n# \t_x = UpperTriangular(_lu.Ut) \\ (LowerTriangular(_lu.L) \\ rhs)\n# \trhs .= vec(_x)\n# \t# CUDA.unsafe_free!(_x)\n# \trhs\n# end\n#\n# function LinearAlgebra.ldiv!(yrhs::CuArray,_lu::LUperso, rhs::CuArray)\n# \tcopyto!(yrhs,rhs)\n# \t_x = UpperTriangular(_lu.Ut) \\ (LowerTriangular(_lu.L) \\ rhs)\n# \trhs .= vec(_x)\n# \t# CUDA.unsafe_free!(_x)\n# \trhs\n# end\n\nstruct LUgpu\n\tL\n\tUt\t# transpose of U in LU decomposition\nend\n\n\nP = LUperso(LowerTriangular(CuSparseMatrixCSC(I+Precilu.L)), UpperTriangular(CuSparseMatrixCSC(sparse(Precilu.U'))));\nval = cg(A_gpu,b_gpu,verbose=true,Pl=P,tol=10^-7,maxiter=1000)\n\nP_cpu = ilu(A_cpu,τ=3.0)\nval = cg(A_cpu,b_cpu;verbose=true,Pl=P_cpu,tol=10^-7,maxiter=1000)\nA_cpu\\b_cpu "}]}]}]},{"client_msg_id":"94898089-1969-4F52-B660-A03E7B41E7AA","type":"message","text":"Is the newly released CUDA.jl version 2.4.0 meant to be compatible with Julia 1.6?\n\nThe compat entry is `julia = \"~1.5\"`, which unfortunately excludes Julia 1.6.","user":"U7THT3TM3","ts":"1610241890.201500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KiE6B","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is the newly released CUDA.jl version 2.4.0 meant to be compatible with Julia 1.6?\n"},{"type":"text","text":"\n"},{"type":"text","text":"The compat entry is "},{"type":"text","text":"julia = \"~1.5\"","style":{"code":true}},{"type":"text","text":", which unfortunately excludes Julia 1.6."}]}]}]},{"client_msg_id":"6862B773-446C-48C4-BB16-6E8EA985263E","type":"message","text":"Oh I see... CUDA.jl 2.4.0 was the release for Julia 1.5","user":"U7THT3TM3","ts":"1610241927.202100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"s7ZHW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oh I see... CUDA.jl 2.4.0 was the release for Julia 1.5"}]}]}]},{"client_msg_id":"2B8E2C0A-E732-4363-8188-21F63C1FCE88","type":"message","text":"CUDA.jl 2.5.0 will be for Julia 1.6, but we are having trouble registering in because of Registrator","user":"U7THT3TM3","ts":"1610241947.202800","team":"T68168MUP","edited":{"user":"U7THT3TM3","ts":"1610241951.000000"},"blocks":[{"type":"rich_text","block_id":"WXQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CUDA.jl 2.5.0 will be for Julia 1.6, but we are having trouble registering in because of Registrator"}]}]}]},{"client_msg_id":"ab803024-8cdb-439c-bbe8-a6d28bbb6447","type":"message","text":"what docker or singularity container should I use? is there a 1.5 image?","user":"UAH43TMUN","ts":"1610346131.204800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wMl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"what docker or singularity container should I use? is there a 1.5 image?"}]}]}]},{"client_msg_id":"f07fd6a4-3821-4e5c-b009-4a2635a35489","type":"message","text":"Hi, I am trying to use `CUDA.cudaDeviceGetAttribute()` on `CUDA.cudaDevAttrMaxThreadsPerBlock` but how do I feed the first argument?\n\n```function cudaDeviceGetAttribute(value, attr, device)\n    ccall(\"extern cudaDeviceGetAttribute\", llvmcall, cudaError_t,\n          (Ptr{Cint}, cudaDeviceAttr, Cint),\n          value, attr, device)\nend```\ni.e. I'm wondering how to get a pointer for the first argument:\n`CUDA.cudaDeviceGetAttribute(????, CUDA.cudaDevAttrMaxThreadsPerBlock, Cint(0))`","user":"UG1KB4NLD","ts":"1610384752.207300","team":"T68168MUP","edited":{"user":"UG1KB4NLD","ts":"1610384973.000000"},"blocks":[{"type":"rich_text","block_id":"Us7n","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I am trying to use "},{"type":"text","text":"CUDA.cudaDeviceGetAttribute()","style":{"code":true}},{"type":"text","text":" on "},{"type":"text","text":"CUDA.cudaDevAttrMaxThreadsPerBlock","style":{"code":true}},{"type":"text","text":" but how do I feed the first argument?\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function cudaDeviceGetAttribute(value, attr, device)\n    ccall(\"extern cudaDeviceGetAttribute\", llvmcall, cudaError_t,\n          (Ptr{Cint}, cudaDeviceAttr, Cint),\n          value, attr, device)\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"i.e. I'm wondering how to get a pointer for the first argument:\n"},{"type":"text","text":"CUDA.cudaDeviceGetAttribute(????, CUDA.cudaDevAttrMaxThreadsPerBlock, Cint(0))","style":{"code":true}}]}]}],"thread_ts":"1610384752.207300","reply_count":15,"reply_users_count":3,"latest_reply":"1610387814.210400","reply_users":["U68A3ASP9","UG1KB4NLD","U67BJLYCS"],"subscribed":false},{"client_msg_id":"e8f569e6-af81-4788-9719-f86178c06ec4","type":"message","text":"To update the `CUdevice_attribute_enum`, I\n1. `cd /res/wrap`\n2. `julia`\n3. `] activated .`\n4. `include(\"wrap.jl\")`\n\nbut the output feels like it's not installing the latest version of everything, since it installs according to the local `Project.toml` and `Manifest.toml` files. Should I modify these two files (feels tidious) or update the packages later by `] up` later (but it is taking too much time)? Are there other methods?\n\nThe output now (BTW the downloads for both `CUDA` and `CUDA_full` were stuck at 17.5%. The first ctrl+c makes the download continue to run, the second ctrl+c makes it abort):\n```(@v1.7) pkg&gt; activate .\n  Activating environment at `~/.julia/dev/CUDA/res/wrap/Project.toml`\n\n\nStacktrace:\n [1] _require(pkg::Base.PkgId)\n   @ Base ./loading.jl:986\n [2] require(uuidkey::Base.PkgId)\n   @ Base ./loading.jl:910\n [3] require(into::Module, mod::Symbol)\n   @ Base ./loading.jl:897\n [4] include(fname::String)\n   @ Base.MainInclude ./client.jl:451\n [5] top-level scope\n   @ REPL[3]:1\nin expression starting at /home/qyu/.julia/dev/CUDA/res/wrap/wrap.jl:370\n\n(wrap) pkg&gt; instantiate\n   Installed Clang_jll ──────────── v9.0.1+4\n   Installed OrderedCollections ─── v1.3.0\n   Installed CUDNN_CUDA110_jll ──── v8.0.2+0\n   Installed CUDA_full_jll ──────── v11.0.3+0\n   Installed CUTENSOR_CUDA110_jll ─ v1.2.0+0\n   Installed CUDA_jll ───────────── v11.0.2+1\n   Installed Clang ──────────────── v0.12.0\n Downloading artifact: CUDA\n^C┌ Error: curl_multi_remove_handle: 1                    ]  17.5 %\n└ @ Downloads.Curl /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.7/Do  Downloaded artifact: CUDA\n  Downloaded artifact: CUDA\n  Downloaded artifact: CUTENSOR_CUDA110\n  Downloaded artifact: CUDNN_CUDA110\n  Downloaded artifact: Clang\n Downloading artifact: CUDA_full\n^C┌ Error: curl_multi_remove_handle: 1                    ]  17.5 %\n  Downloaded artifact: CUDA_full\n Downloading artifact: CUDA_full\njulia: src/unix/poll.c:117: uv_poll_start: Assertion `!(((handle)-&gt;flags &amp; (UV_HANDLE_CLOSING | UV_HANDLE_CLOSED)) != 0)' failed.\n\nsignal (6): Aborted```","user":"UG1KB4NLD","ts":"1610402295.215100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SXd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"To update the "},{"type":"text","text":"CUdevice_attribute_enum","style":{"code":true}},{"type":"text","text":", I\n1. "},{"type":"text","text":"cd /res/wrap","style":{"code":true}},{"type":"text","text":"\n2. "},{"type":"text","text":"julia","style":{"code":true}},{"type":"text","text":"\n3. "},{"type":"text","text":"] activated .","style":{"code":true}},{"type":"text","text":"\n4. "},{"type":"text","text":"include(\"wrap.jl\")","style":{"code":true}},{"type":"text","text":"\n\nbut the output feels like it's not installing the latest version of everything, since it installs according to the local "},{"type":"text","text":"Project.toml","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"Manifest.toml","style":{"code":true}},{"type":"text","text":" files. Should I modify these two files (feels tidious) or update the packages later by "},{"type":"text","text":"] up","style":{"code":true}},{"type":"text","text":" later (but it is taking too much time)? Are there other methods?\n\nThe output now (BTW the downloads for both "},{"type":"text","text":"CUDA","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"CUDA_full","style":{"code":true}},{"type":"text","text":" were stuck at 17.5%. The first ctrl+c makes the download continue to run, the second ctrl+c makes it abort):\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"(@v1.7) pkg> activate .\n  Activating environment at `~/.julia/dev/CUDA/res/wrap/Project.toml`\n\n\nStacktrace:\n [1] _require(pkg::Base.PkgId)\n   @ Base ./loading.jl:986\n [2] require(uuidkey::Base.PkgId)\n   @ Base ./loading.jl:910\n [3] require(into::Module, mod::Symbol)\n   @ Base ./loading.jl:897\n [4] include(fname::String)\n   @ Base.MainInclude ./client.jl:451\n [5] top-level scope\n   @ REPL[3]:1\nin expression starting at /home/qyu/.julia/dev/CUDA/res/wrap/wrap.jl:370\n\n(wrap) pkg> instantiate\n   Installed Clang_jll ──────────── v9.0.1+4\n   Installed OrderedCollections ─── v1.3.0\n   Installed CUDNN_CUDA110_jll ──── v8.0.2+0\n   Installed CUDA_full_jll ──────── v11.0.3+0\n   Installed CUTENSOR_CUDA110_jll ─ v1.2.0+0\n   Installed CUDA_jll ───────────── v11.0.2+1\n   Installed Clang ──────────────── v0.12.0\n Downloading artifact: CUDA\n^C┌ Error: curl_multi_remove_handle: 1                    ]  17.5 %\n└ @ Downloads.Curl /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.7/Do  Downloaded artifact: CUDA\n  Downloaded artifact: CUDA\n  Downloaded artifact: CUTENSOR_CUDA110\n  Downloaded artifact: CUDNN_CUDA110\n  Downloaded artifact: Clang\n Downloading artifact: CUDA_full\n^C┌ Error: curl_multi_remove_handle: 1                    ]  17.5 %\n  Downloaded artifact: CUDA_full\n Downloading artifact: CUDA_full\njulia: src/unix/poll.c:117: uv_poll_start: Assertion `!(((handle)->flags & (UV_HANDLE_CLOSING | UV_HANDLE_CLOSED)) != 0)' failed.\n\nsignal (6): Aborted"}]}]}]},{"client_msg_id":"84ac4e82-eb38-498c-ab50-a6942e8e31c5","type":"message","text":"Is there a way to get the element out of a dimension-0 CuArray without triggering a scalar indexing warning/error? Eg\n```julia&gt; CUDA.allowscalar(false)\n\njulia&gt; x = CuArray{Float64,0}(undef)\n 0-dimensional CuArray{Float64,0}:\n0.0\n\njulia&gt; x[1] # error\n\njulia&gt; x[] # error```\n","user":"UUMJUCYRK","ts":"1610413181.218700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"R/t=I","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to get the element out of a dimension-0 CuArray without triggering a scalar indexing warning/error? Eg\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> CUDA.allowscalar(false)\n\njulia> x = CuArray{Float64,0}(undef)\n 0-dimensional CuArray{Float64,0}:\n0.0\n\njulia> x[1] # error\n\njulia> x[] # error"}]},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"3ab42ffd-4275-4298-ac9e-987d906760ee","type":"message","text":"Alternatively, some way to sum over specified dims of an array and get back a scalar if they happen to be _all_ of the dims? I was trying,\n```julia&gt; x = cu(rand(2,2));\n\njulia&gt; dropdims(sum(x, dims=(1,2)), dims=(1,2))\n 0-dimensional CuArray{Float32,0}:\n2.363763```\nObviously you can do `sum(x)` but here I need to specify the dims, just that sometimes its all of them and in those cases I want the scalar","user":"UUMJUCYRK","ts":"1610413442.220300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"urx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Alternatively, some way to sum over specified dims of an array and get back a scalar if they happen to be "},{"type":"text","text":"all ","style":{"italic":true}},{"type":"text","text":"of the dims? I was trying,\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> x = cu(rand(2,2));\n\njulia> dropdims(sum(x, dims=(1,2)), dims=(1,2))\n 0-dimensional CuArray{Float32,0}:\n2.363763"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Obviously you can do "},{"type":"text","text":"sum(x)","style":{"code":true}},{"type":"text","text":" but here I need to specify the dims, just that sometimes its all of them and in those cases I want the scalar"}]}]}]},{"client_msg_id":"33edf21d-639a-4dd4-a417-ced4ea2fbfed","type":"message","text":"Ah.. `CUDA.@allowscalar`","user":"UUMJUCYRK","ts":"1610416650.220600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ErB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah.. "},{"type":"text","text":"CUDA.@allowscalar","style":{"code":true}}]}]}]},{"client_msg_id":"7dcee73d-6e53-4b1e-9b7e-f865b855ba01","type":"message","text":"Is there a quick way to disable the GPU, so I can test my code without it? Perhaps through an environment variable?","user":"U01EK81V5GF","ts":"1610454806.225000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Uyw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a quick way to disable the GPU, so I can test my code without it? Perhaps through an environment variable?"}]}]}]},{"client_msg_id":"bd2837af-2faf-4aad-8565-73939879e754","type":"message","text":"`CUDA_VISIBLE_DEVICES=-1`","user":"U68A3ASP9","ts":"1610455600.225300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oAk6p","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CUDA_VISIBLE_DEVICES=-1","style":{"code":true}}]}]}]},{"client_msg_id":"c074bd3f-07b3-4011-accb-355371ff96a2","type":"message","text":"Weird question: do you need to have the GPU available when you download the CUDA artifacts?","user":"U7THT3TM3","ts":"1610522895.226600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"f7Vd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Weird question: do you need to have the GPU available when you download the CUDA artifacts?"}]}]}]},{"client_msg_id":"976610be-5bb3-44b8-8216-b386595ebc12","type":"message","text":"I'm on a cluster, so if the answer is no, I can just do `CUDA.versioninfo()` on a login node","user":"U7THT3TM3","ts":"1610522920.227100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Lrf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm on a cluster, so if the answer is no, I can just do "},{"type":"text","text":"CUDA.versioninfo()","style":{"code":true}},{"type":"text","text":" on a login node"}]}]}]},{"client_msg_id":"e076f612-d4d7-4403-b854-b70d38baf1aa","type":"message","text":"But if the answer is yes, I need to `salloc` to get on a GPU node first","user":"U7THT3TM3","ts":"1610522943.227800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"heXc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But if the answer is yes, I need to "},{"type":"text","text":"salloc","style":{"code":true}},{"type":"text","text":" to get on a GPU node first"}]}]}]},{"client_msg_id":"cb7a2005-c50e-457b-b7a8-80db601296b6","type":"message","text":"yes, as it needs to know which version to download, which depends on some results aquired from the CUDA driver","user":"U68A3ASP9","ts":"1610523137.228600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9Rqm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yes, as it needs to know which version to download, which depends on some results aquired from the CUDA driver"}]}]}],"reactions":[{"name":"+1","users":["U7THT3TM3"],"count":1}]},{"client_msg_id":"46ef9c50-ba21-49ea-bf1e-1dd0b7bdf81f","type":"message","text":"technically that needs `libcuda` and not strictly a GPU, but the difference isn't implemented right now (and `libcuda` might fail to init if you don't have a GPU)","user":"U68A3ASP9","ts":"1610523171.229800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ETC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"technically that needs "},{"type":"text","text":"libcuda","style":{"code":true}},{"type":"text","text":" and not strictly a GPU, but the difference isn't implemented right now (and "},{"type":"text","text":"libcuda","style":{"code":true}},{"type":"text","text":" might fail to init if you don't have a GPU)"}]}]}]},{"client_msg_id":"f9ba2c69-61ec-4ee2-b704-357477ec7dad","type":"message","text":"note that on a cluster you may need to use the local CUDA installation, in case it is specifically optimized or tested for your platform","user":"U68A3ASP9","ts":"1610523219.231400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ENtag","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"note that on a cluster you may need to use the local CUDA installation, in case it is specifically optimized or tested for your platform"}]}]}]},{"client_msg_id":"04cf7810-1280-40e0-a44f-2056b161dd83","type":"message","text":"Follow-ip question: I have a different system (not the cluster) in which I have to build a Docker or Singularity image on a separate build server, and then transfer the image to the main system. The build server has Internet access; the main system does not. So I can't do the artifact downloading on the main system. Is there a way to just download \"all of the artifacts\" when building the Docker image on the build server? I don't mind if the image is really large; disk space is not an issue.","user":"U7THT3TM3","ts":"1610523590.234100","team":"T68168MUP","edited":{"user":"U7THT3TM3","ts":"1610523599.000000"},"blocks":[{"type":"rich_text","block_id":"qg/x","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Follow-ip question: I have a different system (not the cluster) in which I have to build a Docker or Singularity image on a separate build server, and then transfer the image to the main system. The build server has Internet access; the main system does not. So I can't do the artifact downloading on the main system. Is there a way to just download \"all of the artifacts\" when building the Docker image on the build server? I don't mind if the image is really large; disk space is not an issue."}]}]}]},{"client_msg_id":"b127e781-3b8c-4e68-8ea6-cf102ca5d9e7","type":"message","text":"You could do what PackageCompiler does -- download them all","user":"U68A3ASP9","ts":"1610523942.234900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8mJZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You could do what PackageCompiler does -- download them all"}]}]}]},{"client_msg_id":"09565ee6-1776-4745-9bc4-3181317fae72","type":"message","text":"of course, if you know which driver version your main system has, you can download the appropriate artifact, but CUDA.jl doesn't have functionality to control that (it does have `JULIA_CUDA_VERSION` to force the CUDA toolkit version, and thus the artifact that will be downloaded, but for it to successfully initialize you might still need a working `libcuda` on your build system)","user":"U68A3ASP9","ts":"1610524020.236400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dni+T","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"of course, if you know which driver version your main system has, you can download the appropriate artifact, but CUDA.jl doesn't have functionality to control that (it does have "},{"type":"text","text":"JULIA_CUDA_VERSION","style":{"code":true}},{"type":"text","text":" to force the CUDA toolkit version, and thus the artifact that will be downloaded, but for it to successfully initialize you might still need a working "},{"type":"text","text":"libcuda","style":{"code":true}},{"type":"text","text":" on your build system)"}]}]}]},{"client_msg_id":"3cbc2cfa-f5be-4769-bd00-b0c42da138cd","type":"message","text":"Is `argmax` not allowed on `CuArray`s?\n","user":"UH9KWTTD3","ts":"1610640747.000800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3=cv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is "},{"type":"text","text":"argmax","style":{"code":true}},{"type":"text","text":" not allowed on `CuArray`s?\n"}]}]}],"thread_ts":"1610640747.000800","reply_count":1,"reply_users_count":1,"latest_reply":"1610640964.001000","reply_users":["UH9KWTTD3"],"subscribed":false},{"client_msg_id":"3cbc2cfa-f5be-4769-bd00-b0c42da138cd","type":"message","text":"```julia&gt; x = gpu([true false; false true])\n2×2 CuArray{Bool,2}:\n 1  0\n 0  1\n\njulia&gt; argmax(x)\nERROR: InvalidIRError: compiling kernel partial_mapreduce_grid(typeof(identity), CUDA.var\"#f#892\"{typeof(CUDA.max),typeof(&gt;)}, Tuple{Bool,CartesianIndex{2}}, CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, Val{false}, CuDeviceArray{Tuple{Bool,CartesianIndex{2}},3,1}, Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(tuple),Tuple{CuDeviceArray{Bool,2,1},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}}}) resulted in invalid LLVM IR\nReason: unsupported dynamic function invocation (call to min)\nStacktrace:\n [1] f at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/indexing.jl:126\n [2] multiple call sites at unknown:0\nStacktrace:\n [1] check_ir(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}, ::LLVM.Module) at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/validation.jl:123\n [2] macro expansion at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:239 [inlined]\n [3] macro expansion at /home/daruwalla/.julia/packages/TimerOutputs/ZmKD7/src/TimerOutput.jl:206 [inlined]\n [4] codegen(::Symbol, ::GPUCompiler.CompilerJob; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:237\n [5] compile(::Symbol, ::GPUCompiler.CompilerJob; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:39\n [6] compile at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:35 [inlined]\n [7] cufunction_compile(::GPUCompiler.FunctionSpec; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/compiler/execution.jl:302\n [8] cufunction_compile(::GPUCompiler.FunctionSpec) at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/compiler/execution.jl:297\n [9] check_cache(::Dict{UInt64,Any}, ::Any, ::Any, ::GPUCompiler.FunctionSpec{typeof(CUDA.partial_mapreduce_grid),Tuple{typeof(identity),CUDA.var\"#f#892\"{typeof(CUDA.max),typeof(&gt;)},Tuple{Bool,CartesianIndex{2}},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}},Val{false},CuDeviceArray{Tuple{Bool,CartesianIndex{2}},3,1},Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(tuple),Tuple{CuDeviceArray{Bool,2,1},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/cache.jl:40\n [10] partial_mapreduce_grid at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/mapreduce.jl:87 [inlined]\n [11] cached_compilation at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/cache.jl:65 [inlined]\n [12] cufunction(::typeof(CUDA.partial_mapreduce_grid), ::Type{Tuple{typeof(identity),CUDA.var\"#f#892\"{typeof(CUDA.max),typeof(&gt;)},Tuple{Bool,CartesianIndex{2}},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}},Val{false},CuDeviceArray{Tuple{Bool,CartesianIndex{2}},3,1},Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(tuple),Tuple{CuDeviceArray{Bool,2,1},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/compiler/execution.jl:289\n [13] cufunction at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/compiler/execution.jl:286 [inlined]\n [14] macro expansion at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/compiler/execution.jl:100 [inlined]\n [15] mapreducedim!(::typeof(identity), ::CUDA.var\"#f#892\"{typeof(CUDA.max),typeof(&gt;)}, ::CuArray{Tuple{Bool,CartesianIndex{2}},2}, ::Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(tuple),Tuple{CuArray{Bool,2},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}}}; init::Tuple{Bool,CartesianIndex{2}}) at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/mapreduce.jl:192\n [16] _mapreduce(::typeof(tuple), ::CUDA.var\"#f#892\"{typeof(CUDA.max),typeof(&gt;)}, ::CuArray{Bool,2}, ::CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}; dims::Colon, init::Tuple{Bool,CartesianIndex{2}}) at /home/daruwalla/.julia/packages/GPUArrays/WV76E/src/host/mapreduce.jl:62\n [17] #mapreduce#15 at /home/daruwalla/.julia/packages/GPUArrays/WV76E/src/host/mapreduce.jl:28 [inlined]\n [18] #findminmax#889 at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/indexing.jl:133 [inlined]\n [19] #findmax#895 at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/indexing.jl:143 [inlined]\n [20] #argmax#656 at ./reducedim.jl:952 [inlined]\n [21] argmax(::CuArray{Bool,2}) at ./reducedim.jl:952\n [22] top-level scope at REPL[47]:1```","user":"UH9KWTTD3","ts":"1610640747.000900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3=cv-rU1kS","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> x = gpu([true false; false true])\n2×2 CuArray{Bool,2}:\n 1  0\n 0  1\n\njulia> argmax(x)\nERROR: InvalidIRError: compiling kernel partial_mapreduce_grid(typeof(identity), CUDA.var\"#f#892\"{typeof(CUDA.max),typeof(>)}, Tuple{Bool,CartesianIndex{2}}, CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, Val{false}, CuDeviceArray{Tuple{Bool,CartesianIndex{2}},3,1}, Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(tuple),Tuple{CuDeviceArray{Bool,2,1},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}}}) resulted in invalid LLVM IR\nReason: unsupported dynamic function invocation (call to min)\nStacktrace:\n [1] f at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/indexing.jl:126\n [2] multiple call sites at unknown:0\nStacktrace:\n [1] check_ir(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}, ::LLVM.Module) at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/validation.jl:123\n [2] macro expansion at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:239 [inlined]\n [3] macro expansion at /home/daruwalla/.julia/packages/TimerOutputs/ZmKD7/src/TimerOutput.jl:206 [inlined]\n [4] codegen(::Symbol, ::GPUCompiler.CompilerJob; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:237\n [5] compile(::Symbol, ::GPUCompiler.CompilerJob; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:39\n [6] compile at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:35 [inlined]\n [7] cufunction_compile(::GPUCompiler.FunctionSpec; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/compiler/execution.jl:302\n [8] cufunction_compile(::GPUCompiler.FunctionSpec) at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/compiler/execution.jl:297\n [9] check_cache(::Dict{UInt64,Any}, ::Any, ::Any, ::GPUCompiler.FunctionSpec{typeof(CUDA.partial_mapreduce_grid),Tuple{typeof(identity),CUDA.var\"#f#892\"{typeof(CUDA.max),typeof(>)},Tuple{Bool,CartesianIndex{2}},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}},Val{false},CuDeviceArray{Tuple{Bool,CartesianIndex{2}},3,1},Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(tuple),Tuple{CuDeviceArray{Bool,2,1},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/cache.jl:40\n [10] partial_mapreduce_grid at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/mapreduce.jl:87 [inlined]\n [11] cached_compilation at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/cache.jl:65 [inlined]\n [12] cufunction(::typeof(CUDA.partial_mapreduce_grid), ::Type{Tuple{typeof(identity),CUDA.var\"#f#892\"{typeof(CUDA.max),typeof(>)},Tuple{Bool,CartesianIndex{2}},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}},Val{false},CuDeviceArray{Tuple{Bool,CartesianIndex{2}},3,1},Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(tuple),Tuple{CuDeviceArray{Bool,2,1},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/compiler/execution.jl:289\n [13] cufunction at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/compiler/execution.jl:286 [inlined]\n [14] macro expansion at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/compiler/execution.jl:100 [inlined]\n [15] mapreducedim!(::typeof(identity), ::CUDA.var\"#f#892\"{typeof(CUDA.max),typeof(>)}, ::CuArray{Tuple{Bool,CartesianIndex{2}},2}, ::Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(tuple),Tuple{CuArray{Bool,2},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}}}; init::Tuple{Bool,CartesianIndex{2}}) at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/mapreduce.jl:192\n [16] _mapreduce(::typeof(tuple), ::CUDA.var\"#f#892\"{typeof(CUDA.max),typeof(>)}, ::CuArray{Bool,2}, ::CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}; dims::Colon, init::Tuple{Bool,CartesianIndex{2}}) at /home/daruwalla/.julia/packages/GPUArrays/WV76E/src/host/mapreduce.jl:62\n [17] #mapreduce#15 at /home/daruwalla/.julia/packages/GPUArrays/WV76E/src/host/mapreduce.jl:28 [inlined]\n [18] #findminmax#889 at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/indexing.jl:133 [inlined]\n [19] #findmax#895 at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/indexing.jl:143 [inlined]\n [20] #argmax#656 at ./reducedim.jl:952 [inlined]\n [21] argmax(::CuArray{Bool,2}) at ./reducedim.jl:952\n [22] top-level scope at REPL[47]:1"}]}]}]},{"client_msg_id":"3c7bc7ad-03ec-416a-8dc7-e79cf10b70ce","type":"message","text":"Is it a current limitation of Zygote+CUDA that this triggers scalar indexing, and/or is there any way to make this work?\n```julia&gt; foo(x,A) = x&gt;0.5 ? A : 0\nfoo (generic function with 2 methods)\n\njulia&gt; arr = cu(rand(10));\n\njulia&gt; gradient(A -&gt; sum(foo.(arr,A)), 1.)\nERROR: scalar getindex is disallowed```\n","user":"UUMJUCYRK","ts":"1610945972.017600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"i61","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is it a current limitation of Zygote+CUDA that this triggers scalar indexing, and/or is there any way to make this work?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> foo(x,A) = x>0.5 ? A : 0\nfoo (generic function with 2 methods)\n\njulia> arr = cu(rand(10));\n\njulia> gradient(A -> sum(foo.(arr,A)), 1.)\nERROR: scalar getindex is disallowed"}]},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"b3b6f61c-aad0-41c3-877b-1b547e9d497e","type":"message","text":"Ah.. if you write it so that its type-stable when a ForwardDiff.Dual goes through it, it works:\n```foo(x::T, A::T) where {T} = x&gt;0.5 ? A : zero(T)\narr = cu(rand(10));\ngradient(A -&gt; sum(foo.(arr,A)), 1f0)```\n","user":"UUMJUCYRK","ts":"1610966325.018700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yJt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah.. if you write it so that its type-stable when a ForwardDiff.Dual goes through it, it works:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"foo(x::T, A::T) where {T} = x>0.5 ? A : zero(T)\narr = cu(rand(10));\ngradient(A -> sum(foo.(arr,A)), 1f0)"}]},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"2678ec7f-97f4-4a04-9d07-22dbbe28de75","type":"message","text":"How do I write a CUDA.jl kernel, that returns a scalar? I tried the following, which does not work:\n```module MWE\nusing CUDA\nfunction give_1_kernel!(out)\n    out[] = 1f0\n    return\nend\nfunction give_1()\n    out = CuRef(0f0)\n    @cuda give_1_kernel!(out)\n    out\nend\n\ngive_1()\n\nend\nGPU compilation of kernel give_1_kernel!(CUDA.CuRefArray{Float32,CuArray{Float32,1}}) failed\nKernelError: kernel returns a value of type `Union{}`\n\nMake sure your kernel function ends in `return`, `return nothing` or `nothing`.\nIf the returned value is of type `Union{}`, your Julia code probably throws an exception.\nInspect the code with `@device_code_warntype` for more details.```","user":"UKZ78GR8Q","ts":"1611041950.021600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j/q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How do I write a CUDA.jl kernel, that returns a scalar? I tried the following, which does not work:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"module MWE\nusing CUDA\nfunction give_1_kernel!(out)\n    out[] = 1f0\n    return\nend\nfunction give_1()\n    out = CuRef(0f0)\n    @cuda give_1_kernel!(out)\n    out\nend\n\ngive_1()\n\nend\nGPU compilation of kernel give_1_kernel!(CUDA.CuRefArray{Float32,CuArray{Float32,1}}) failed\nKernelError: kernel returns a value of type `Union{}`\n\nMake sure your kernel function ends in `return`, `return nothing` or `nothing`.\nIf the returned value is of type `Union{}`, your Julia code probably throws an exception.\nInspect the code with `@device_code_warntype` for more details."}]}]}],"thread_ts":"1611041950.021600","reply_count":13,"reply_users_count":3,"latest_reply":"1611070539.025100","reply_users":["U01C10FSAM8","UKZ78GR8Q","U68A3ASP9"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"can anyone confirm this?","user":"U01C10FSAM8","ts":"1611058650.024100","thread_ts":"1611041950.021600","root":{"client_msg_id":"2678ec7f-97f4-4a04-9d07-22dbbe28de75","type":"message","text":"How do I write a CUDA.jl kernel, that returns a scalar? I tried the following, which does not work:\n```module MWE\nusing CUDA\nfunction give_1_kernel!(out)\n    out[] = 1f0\n    return\nend\nfunction give_1()\n    out = CuRef(0f0)\n    @cuda give_1_kernel!(out)\n    out\nend\n\ngive_1()\n\nend\nGPU compilation of kernel give_1_kernel!(CUDA.CuRefArray{Float32,CuArray{Float32,1}}) failed\nKernelError: kernel returns a value of type `Union{}`\n\nMake sure your kernel function ends in `return`, `return nothing` or `nothing`.\nIf the returned value is of type `Union{}`, your Julia code probably throws an exception.\nInspect the code with `@device_code_warntype` for more details.```","user":"UKZ78GR8Q","ts":"1611041950.021600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j/q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How do I write a CUDA.jl kernel, that returns a scalar? I tried the following, which does not work:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"module MWE\nusing CUDA\nfunction give_1_kernel!(out)\n    out[] = 1f0\n    return\nend\nfunction give_1()\n    out = CuRef(0f0)\n    @cuda give_1_kernel!(out)\n    out\nend\n\ngive_1()\n\nend\nGPU compilation of kernel give_1_kernel!(CUDA.CuRefArray{Float32,CuArray{Float32,1}}) failed\nKernelError: kernel returns a value of type `Union{}`\n\nMake sure your kernel function ends in `return`, `return nothing` or `nothing`.\nIf the returned value is of type `Union{}`, your Julia code probably throws an exception.\nInspect the code with `@device_code_warntype` for more details."}]}]}],"thread_ts":"1611041950.021600","reply_count":13,"reply_users_count":3,"latest_reply":"1611070539.025100","reply_users":["U01C10FSAM8","UKZ78GR8Q","U68A3ASP9"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"sHkyY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"can anyone confirm this?"}]}]}],"client_msg_id":"eff2177a-b1ad-45e5-9409-74195bdfba34","reactions":[{"name":"heart","users":["UKZ78GR8Q"],"count":1}]},{"client_msg_id":"49fdbfa2-5a3d-4cf2-8070-abeda8c5280b","type":"message","text":"Is there any general rule of thumb for choosing the number of workgroups/threads when using KernelAbstractions? If I have an O(n) reduction in the kernel, should I just set it to the number of available CUDA cores? If I set it to the length of the ndrange, will it overschedule? Or is the only way to find the sweet spot really just to benchmark it?","user":"UM30MT6RF","ts":"1611136960.035300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HfrQc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there any general rule of thumb for choosing the number of workgroups/threads when using KernelAbstractions? If I have an O(n) reduction in the kernel, should I just set it to the number of available CUDA cores? If I set it to the length of the ndrange, will it overschedule? Or is the only way to find the sweet spot really just to benchmark it?"}]}]}]},{"client_msg_id":"985d2002-b5e6-422d-960d-b6fd76735768","type":"message","text":"the underlying APIs have tools to calculate the ideal workgroups/threads, e.g. CUDA has the occupancy API, oneAPI has a suggest_groupsize, etc","user":"U68A3ASP9","ts":"1611137302.035800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"al3A","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"the underlying APIs have tools to calculate the ideal workgroups/threads, e.g. CUDA has the occupancy API, oneAPI has a suggest_groupsize, etc"}]}]}]},{"client_msg_id":"bb7bb7d1-a85f-40f1-9820-c5c3cd0558e2","type":"message","text":"you generally can't set it to the max because, at least in the case of CUDA, it is restricted by other factors (e.g. the device max threads may be 1024, but if your kernel uses a lot of registers or shared memory you may not be able to launch as many)","user":"U68A3ASP9","ts":"1611137339.036600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VsqbN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you generally can't set it to the max because, at least in the case of CUDA, it is restricted by other factors (e.g. the device max threads may be 1024, but if your kernel uses a lot of registers or shared memory you may not be able to launch as many)"}]}]}]},{"client_msg_id":"8b99904d-7682-45f4-9a18-d9d7fedacc39","type":"message","text":"Thanks for the answer! Do you have a link where I can find out more about the occupancy API?","user":"UM30MT6RF","ts":"1611137462.037400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UMwi1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for the answer! Do you have a link where I can find out more about the occupancy API?"}]}]}],"thread_ts":"1611137462.037400","reply_count":1,"reply_users_count":1,"latest_reply":"1611137655.038100","reply_users":["U68A3ASP9"],"subscribed":false},{"client_msg_id":"4502901a-3211-4b3e-abe8-48d9abd70c8f","type":"message","text":"<https://github.com/JuliaGPU/CUDA.jl/blob/35150564fac7848cdf4fc3b7e74d3cca201eb442/src/indexing.jl#L32-L36>","user":"U68A3ASP9","ts":"1611137615.037600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Cgv","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/blob/35150564fac7848cdf4fc3b7e74d3cca201eb442/src/indexing.jl#L32-L36"}]}]}],"thread_ts":"1611137615.037600","reply_count":1,"reply_users_count":1,"latest_reply":"1611137831.039100","reply_users":["U68A3ASP9"],"subscribed":false},{"client_msg_id":"c6ba9a7c-c716-4f87-b6ee-9936c85d6489","type":"message","text":"that's a pretty new interface though, it used to be a `config=...` argument to `@cuda`","user":"U68A3ASP9","ts":"1611137640.038000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wl2r","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"that's a pretty new interface though, it used to be a "},{"type":"text","text":"config=...","style":{"code":true}},{"type":"text","text":" argument to "},{"type":"text","text":"@cuda","style":{"code":true}}]}]}]},{"client_msg_id":"11e85465-50c2-46d0-be29-139e096ad689","type":"message","text":"Ah, cool! So I should be able to call `launch_configuration` on my `KernelAbstractions.@kernel` as well?","user":"UM30MT6RF","ts":"1611137815.039000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IJj4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah, cool! So I should be able to call "},{"type":"text","text":"launch_configuration","style":{"code":true}},{"type":"text","text":" on my "},{"type":"text","text":"KernelAbstractions.@kernel","style":{"code":true}},{"type":"text","text":" as well?"}]}]}]},{"client_msg_id":"f56e783f-d749-4040-900f-c984494dc938","type":"message","text":"I don't think KA just returns the underlying CUDA kernel, so no. The library should probably be adjusted to retain that info, but right now I'm not sure what the best approach would be. <@U67BJLYCS> should be able to elaborate in a couple of hours","user":"U68A3ASP9","ts":"1611137911.040500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oD9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I don't think KA just returns the underlying CUDA kernel, so no. The library should probably be adjusted to retain that info, but right now I'm not sure what the best approach would be. "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":" should be able to elaborate in a couple of hours"}]}]}],"reactions":[{"name":"+1","users":["UM30MT6RF"],"count":1}]},{"client_msg_id":"db7cb2ca-fdf8-46f6-8e01-b8655b956a38","type":"message","text":"Yeah KA has been designed more around static workgroup sizes","user":"U67BJLYCS","ts":"1611146437.041500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gHr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah KA has been designed more around static workgroup sizes"}]}]}]},{"client_msg_id":"d5a4ccca-c094-4e41-a9d7-3ef15d1414b6","type":"message","text":"<https://github.com/JuliaGPU/KernelAbstractions.jl/blob/ad2cd8c388143c0333802525063ce6d25ffbfc59/src/backends/cuda.jl#L150|https://github.com/JuliaGPU/KernelAbstractions.jl/blob/ad2cd8c388143c0333802525063ce6d25ffbfc59/src/backends/cuda.jl#L150>","user":"U67BJLYCS","ts":"1611146438.041700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"96O+2","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/JuliaGPU/KernelAbstractions.jl/blob/ad2cd8c388143c0333802525063ce6d25ffbfc59/src/backends/cuda.jl#L150","text":"https://github.com/JuliaGPU/KernelAbstractions.jl/blob/ad2cd8c388143c0333802525063ce6d25ffbfc59/src/backends/cuda.jl#L150"}]}]}]},{"client_msg_id":"4011f313-fb2f-470f-931c-9b095a815dde","type":"message","text":"There is a chicken and an egg problem here","user":"U67BJLYCS","ts":"1611146496.042400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dJ/s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"There is a chicken and an egg problem here"}]}]}]},{"client_msg_id":"373dea20-6b89-49f9-9317-dbffc135e28a","type":"message","text":"And the question; what is the right way of selecting the group size for the CPU","user":"U67BJLYCS","ts":"1611146571.042700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kf7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And the question; what is the right way of selecting the group size for the CPU"}]}]}],"thread_ts":"1611146571.042700","reply_count":1,"reply_users_count":1,"latest_reply":"1611311887.010400","reply_users":["UM30MT6RF"],"subscribed":false},{"client_msg_id":"c7ce31ce-58e9-4930-8c97-53f8375a0afb","type":"message","text":"Which I don't have a good answer too","user":"U67BJLYCS","ts":"1611146585.043100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Najb8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Which I don't have a good answer too"}]}]}]},{"client_msg_id":"8db7de75-365e-4e8e-88a5-885d15b5ad4c","type":"message","text":"But yeah we could lift the cuda kernel creation to instantiation time","user":"U67BJLYCS","ts":"1611146644.043900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"EsIr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But yeah we could lift the cuda kernel creation to instantiation time"}]}]}]},{"client_msg_id":"c4a61047-8f24-40c9-a8f4-7e11213c6aa8","type":"message","text":"And then add a similar api","user":"U67BJLYCS","ts":"1611146656.044300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4MsQR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And then add a similar api"}]}]}]},{"client_msg_id":"2214a435-8c74-42f9-af95-1e3177648bf1","type":"message","text":"Ah, I see, so you typically just tune that by hand? It would be really nice if Tullio.jl could just choose a sensible default for you. Right now, it defaults to 256, which I am not sure is always a good idea.","user":"UM30MT6RF","ts":"1611147591.046400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3WPGY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah, I see, so you typically just tune that by hand? It would be really nice if Tullio.jl could just choose a sensible default for you. Right now, it defaults to 256, which I am not sure is always a good idea."}]}]}]},{"client_msg_id":"71d911ef-86c7-4da0-91b6-11113ef57c3e","type":"message","text":"Yeah in CLIMA we have a algorithm design that limits the size","user":"U67BJLYCS","ts":"1611152528.047600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OP4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah in CLIMA we have a algorithm design that limits the size"}]}]}]},{"client_msg_id":"21d131b6-4533-4345-9c8b-facdd62ff8a3","type":"message","text":"With the new CUDA non spawning kernel this shouldn't be too hard to add","user":"U67BJLYCS","ts":"1611152561.048400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2aA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"With the new CUDA non spawning kernel this shouldn't be too hard to add"}]}]}]},{"client_msg_id":"54d9eb51-8e0d-4b70-a6e5-05a5aa42f52b","type":"message","text":"I welcome any contributions :)","user":"U67BJLYCS","ts":"1611152577.048800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Wnl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I welcome any contributions :)"}]}]}],"thread_ts":"1611152577.048800","reply_count":1,"reply_users_count":1,"latest_reply":"1611314114.010600","reply_users":["UM30MT6RF"],"subscribed":false},{"client_msg_id":"890e498a-71dd-4c48-acbf-0e87cde60450","type":"message","text":"Is there an ideal way of exiting Julia, especially when using CUDA.jl?\nI have been encountering this problem since the past 3 days. Whenever I load CUDA.jl into the Julia REPL on a fresh boot, it runs perfectly and I can initialize arrays using `y=CUDA.fill(1.0f0,1000)` with no problem whatsoever.\nBut then when I quit Julia and start Julia again, load the package, and try to initialize another array, it gives me an error which is seemingly popular-\n```ERROR: CUDA.jl did not successfully initialize, and is not usable.\nIf you did not see any other error message, try again in a new session\nwith the JULIA_DEBUG environment variable set to 'CUDA'.```","user":"UTDSTSANP","ts":"1611163655.051900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eZW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there an ideal way of exiting Julia, especially when using CUDA.jl?\nI have been encountering this problem since the past 3 days. Whenever I load CUDA.jl into the Julia REPL on a fresh boot, it runs perfectly and I can initialize arrays using "},{"type":"text","text":"y=CUDA.fill(1.0f0,1000)","style":{"code":true}},{"type":"text","text":" with no problem whatsoever.\nBut then when I quit Julia and start Julia again, load the package, and try to initialize another array, it gives me an error which is seemingly popular-\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: CUDA.jl did not successfully initialize, and is not usable.\nIf you did not see any other error message, try again in a new session\nwith the JULIA_DEBUG environment variable set to 'CUDA'."}]}]}],"thread_ts":"1611163655.051900","reply_count":5,"reply_users_count":2,"latest_reply":"1611235330.001700","reply_users":["UTDSTSANP","U68A3ASP9"],"subscribed":false},{"client_msg_id":"cca8e87b-2874-48d1-a97c-e6898eb25a81","type":"message","text":"I'm wanting to run a RL algorithm (PPO), where the data gathering step is run on multiple processes to speed it up. However, if I try to run the inference step for a batch of states (ie `actions = model[states]` ) on the gpu, I get this error:\n```CUBLASError: the GPU program failed to execute (code 13, CUBLAS_STATUS_EXECUTION_FAILED)```\nDoes anyone know the reason for this error, and how I could get around it? I have tried making a copy of the model for each process, but this doesn't seem to make a difference.","user":"UQR5DTD99","ts":"1611175170.054600","team":"T68168MUP","edited":{"user":"UQR5DTD99","ts":"1611175246.000000"},"blocks":[{"type":"rich_text","block_id":"EbW9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm wanting to run a RL algorithm (PPO), where the data gathering step is run on multiple processes to speed it up. However, if I try to run the inference step for a batch of states (ie "},{"type":"text","text":"actions = model[states]","style":{"code":true}},{"type":"text","text":" ) on the gpu, I get this error:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"CUBLASError: the GPU program failed to execute (code 13, CUBLAS_STATUS_EXECUTION_FAILED)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Does anyone know the reason for this error, and how I could get around it? I have tried making a copy of the model for each process, but this doesn't seem to make a difference."}]}]}],"thread_ts":"1611175170.054600","reply_count":8,"reply_users_count":3,"latest_reply":"1611252041.001900","reply_users":["U68A3ASP9","UQR5DTD99","UC4QQPG4A"],"subscribed":false},{"client_msg_id":"f66814ca-4c93-44ec-ba7b-2ac3e3931e65","type":"message","text":"How stable is CUDA#master supposed to be against 1.6-beta1 ? Should I be reporting issues I come across, or is it too early / already known?","user":"UUMJUCYRK","ts":"1611192563.000800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Eapc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How stable is CUDA#master supposed to be against 1.6-beta1 ? Should I be reporting issues I come across, or is it too early / already known?"}]}]}],"thread_ts":"1611192563.000800","reply_count":3,"reply_users_count":2,"latest_reply":"1611222495.001500","reply_users":["U68A3ASP9","UUMJUCYRK"],"subscribed":false},{"client_msg_id":"29bec3c6-9424-411f-b3e7-98297e9849d2","type":"message","text":"Is it normal for\n\n```z = @view u[1:1, :]```\nwhere u is a CuArray to not return a CuArray ?\n\nAnd therefore breaks everything later with\n\n```ERROR: ArgumentError: cannot take the CPU address of a CUDA.CuArray{Float32,2}```\n?","user":"UKJSNT1QR","ts":"1611264202.003100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"XLhE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is it normal for\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"z = @view u[1:1, :]"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nwhere u is a CuArray to not return a CuArray ?\n\nAnd therefore breaks everything later with\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: ArgumentError: cannot take the CPU address of a CUDA.CuArray{Float32,2}"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"?"}]}]}]},{"client_msg_id":"756c1273-5cb3-415b-95e3-12adc09f1a49","type":"message","text":"It returns a\n```view(::CUDA.CuArray{Float32,2}, 1:1, :)```\ninstead.\n\nAnd yes I know u[1:1, :] is ugly, but a man gotta do what a man gotta do :slightly_smiling_face:","user":"UKJSNT1QR","ts":"1611264297.004100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"17QZ4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It returns a\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"view(::CUDA.CuArray{Float32,2}, 1:1, :)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"instead.\n\nAnd yes I know u[1:1, :] is ugly, but a man gotta do what a man gotta do "},{"type":"emoji","name":"slightly_smiling_face"}]}]}]},{"client_msg_id":"80178e40-b38b-48dc-82c1-f14b3cf688b0","type":"message","text":"you generally shouldn't ever expect `view` to return a CuArray, but for inference performance reasons we represent contiguous views as CuArray instances (for now). if your view isn't contiguous, or CUDA.jl doesn't detect it as such, you'll just get a SubArray.","user":"U68A3ASP9","ts":"1611264961.005700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vSMDg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you generally shouldn't ever expect "},{"type":"text","text":"view","style":{"code":true}},{"type":"text","text":" to return a CuArray, but for inference performance reasons we represent contiguous views as CuArray instances (for now). if your view isn't contiguous, or CUDA.jl doesn't detect it as such, you'll just get a SubArray."}]}]}]},{"client_msg_id":"b5a5f813-7aa7-4806-9de8-b3e6b51daad1","type":"message","text":"and the \"therefor breaks everything\" is false. many kernels just work for SubArray{&lt;:CuArray}, and even some CUBLAS (e.a.) calls do too; but support for that (ie. for strided inputs) is fairly now and not available for all APIs.","user":"U68A3ASP9","ts":"1611265012.006700","team":"T68168MUP","edited":{"user":"U68A3ASP9","ts":"1611265020.000000"},"blocks":[{"type":"rich_text","block_id":"2BB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"and the \"therefor breaks everything\" is false. many kernels just work for SubArray{<:CuArray}, and even some CUBLAS (e.a.) calls do too; but support for that (ie. for strided inputs) is fairly now and not available for all APIs."}]}]}]},{"client_msg_id":"4c23d68d-5e60-48a9-823e-01aa23e78c68","type":"message","text":"I ran into a weird error while using <#C7T968HRU|diffeq-bridged> and CUDA\nTL;DR: sometimes solvers using ForwardDiff Duals error out when combined with CUDA. This doesn't happen for `Dense` with tanh nor `GRUCell`, it does however happen for `LSTMCell` and a custom layer that mimics the LSTM.\nImportantly it doesn't happen when evaluated on CPU\nMWE\n```using CUDA, Flux, OrdinaryDiffEq, DiffEqSensitivity, Random\nCUDA.allowscalar(false)\n\nRandom.seed!(0)\ntspan = Float32.((0, 100))\nbasic_tgrad(u,p,t) = zero(u)\n\nnhidden = 4\nnin = 2\nnn = Flux.LSTMCell(nin,nhidden) |&gt; gpu\np, re = Flux.destructure(nn)\n\nU = randn(2*nhidden) |&gt; cu\nwrap(u) = (u[1:nhidden],u[nhidden+1:2nhidden])\nunwrap(u) = cat(u[1]...,dims=1)\ndudt_(u,p,t) = re(p)(wrap(u), CUDA.zeros(nin) ) |&gt; unwrap\nff = ODEFunction{false}(dudt_,tgrad=basic_tgrad)\nprob = ODEProblem{false}(ff,U,tspan,p)\nsolve(prob,Rosenbrock23();sense=InterpolatingAdjoint(autojacvec=ZygoteVJP()))```\nError message:\n```ERROR: MethodError: no method matching unsafe_convert(::Type{Float64}, ::ForwardDiff.Dual{Nothing,Float64,8})```\nEnvironment info:\nFlux 0.11.2\nCUDA 2.4.0\nNNlib 0.7.12","user":"UPM0H43C7","ts":"1611281003.007900","team":"T68168MUP","edited":{"user":"UPM0H43C7","ts":"1611281216.000000"},"blocks":[{"type":"rich_text","block_id":"F0m5i","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I ran into a weird error while using "},{"type":"channel","channel_id":"C7T968HRU"},{"type":"text","text":" and CUDA\nTL;DR: sometimes solvers using ForwardDiff Duals error out when combined with CUDA. This doesn't happen for "},{"type":"text","text":"Dense","style":{"code":true}},{"type":"text","text":" with tanh nor "},{"type":"text","text":"GRUCell","style":{"code":true}},{"type":"text","text":", it does however happen for "},{"type":"text","text":"LSTMCell","style":{"code":true}},{"type":"text","text":" and a custom layer that mimics the LSTM.\nImportantly it doesn't happen when evaluated on CPU\nMWE\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using CUDA, Flux, OrdinaryDiffEq, DiffEqSensitivity, Random\nCUDA.allowscalar(false)\n\nRandom.seed!(0)\ntspan = Float32.((0, 100))\nbasic_tgrad(u,p,t) = zero(u)\n\nnhidden = 4\nnin = 2\nnn = Flux.LSTMCell(nin,nhidden) |> gpu\np, re = Flux.destructure(nn)\n\nU = randn(2*nhidden) |> cu\nwrap(u) = (u[1:nhidden],u[nhidden+1:2nhidden])\nunwrap(u) = cat(u[1]...,dims=1)\ndudt_(u,p,t) = re(p)(wrap(u), CUDA.zeros(nin) ) |> unwrap\nff = ODEFunction{false}(dudt_,tgrad=basic_tgrad)\nprob = ODEProblem{false}(ff,U,tspan,p)\nsolve(prob,Rosenbrock23();sense=InterpolatingAdjoint(autojacvec=ZygoteVJP()))"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Error message:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: MethodError: no method matching unsafe_convert(::Type{Float64}, ::ForwardDiff.Dual{Nothing,Float64,8})"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Environment info:\nFlux 0.11.2\nCUDA 2.4.0\nNNlib 0.7.12"}]}]}],"thread_ts":"1611281003.007900","reply_count":8,"reply_users_count":4,"latest_reply":"1611317979.010800","reply_users":["UPM0H43C7","U67BJLYCS","U69BL50BF","UC4QQPG4A"],"subscribed":false},{"client_msg_id":"d8ec5452-f348-4a54-b640-25975d3d7306","type":"message","text":"Tiny announcement of bindings to the NVIDIA AMGX library (multigrid solver) at <https://github.com/JuliaGPU/AMGX.jl>.","user":"U67D54KS8","ts":"1611308776.010200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2r6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Tiny announcement of bindings to the NVIDIA AMGX library (multigrid solver) at "},{"type":"link","url":"https://github.com/JuliaGPU/AMGX.jl"},{"type":"text","text":"."}]}]}],"reactions":[{"name":"100","users":["U68A3ASP9","U680THK2S","U017XL92LJG"],"count":3}]},{"client_msg_id":"6418a582-69f0-4909-9f0f-60c317293984","type":"message","text":"for those people using CUDA.jl with Julia tasks/threads: now would be a good time to test or comment on <https://github.com/JuliaGPU/CUDA.jl/pull/662>\nTL;DR: per-task streams and synchronization, which makes it possible to overlap GPU computations of individual tasks.","user":"U68A3ASP9","ts":"1611330728.012900","team":"T68168MUP","edited":{"user":"U68A3ASP9","ts":"1611330737.000000"},"blocks":[{"type":"rich_text","block_id":"wVV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"for those people using CUDA.jl with Julia tasks/threads: now would be a good time to test or comment on "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/pull/662"},{"type":"text","text":"\nTL;DR: per-task streams and synchronization, which makes it possible to overlap GPU computations of individual tasks."}]}]}],"thread_ts":"1611330728.012900","reply_count":2,"reply_users_count":2,"latest_reply":"1611386533.018500","reply_users":["U01DD7Z0D89","U68A3ASP9"],"subscribed":false,"reactions":[{"name":"party_wizard","users":["U67BJLYCS","ULD19UCPK","U017XL92LJG"],"count":3},{"name":"tada","users":["U01G39CC63F"],"count":1}]},{"client_msg_id":"f6633ac1-b5d2-4830-b55f-3f19ee0fa58a","type":"message","text":"I'm trying to write a kernel function in which I want to update columns of a pre-allocated array `result`  using  `ForwardDiff.jacobian!(result, f!, y, x)` , with some in-place function `f!` . However, I keep on getting the `unsupported dynamic function invocation` error message. I have been trying to search for a bug in my code but so far I haven't been able to fix it. So I was wondering if perhaps I'm trying to do something that's not supported :neutral_face:","user":"UCT34GL7M","ts":"1611331557.015800","team":"T68168MUP","edited":{"user":"UCT34GL7M","ts":"1611331581.000000"},"blocks":[{"type":"rich_text","block_id":"gtRL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm trying to write a kernel function in which I want to update columns of a pre-allocated array "},{"type":"text","text":"result","style":{"code":true}},{"type":"text","text":"  using  "},{"type":"text","text":"ForwardDiff.jacobian!(result, f!, y, x)","style":{"code":true}},{"type":"text","text":" , with some in-place function "},{"type":"text","text":"f!","style":{"code":true}},{"type":"text","text":" . However, I keep on getting the "},{"type":"text","text":"unsupported dynamic function invocation","style":{"code":true}},{"type":"text","text":" error message. I have been trying to search for a bug in my code but so far I haven't been able to fix it. So I was wondering if perhaps I'm trying to do something that's not supported "},{"type":"emoji","name":"neutral_face"}]}]}]},{"client_msg_id":"71646c46-df20-4855-adf0-e343356a2b8f","type":"message","text":"Use `@device_code_typed` to see what the compiler thinks about your code","user":"U67BJLYCS","ts":"1611332446.016500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"cfZn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Use "},{"type":"text","text":"@device_code_typed","style":{"code":true}},{"type":"text","text":" to see what the compiler thinks about your code"}]}]}]},{"client_msg_id":"ae3e70eb-b971-467d-a458-389204f80b6a","type":"message","text":"I get the same error when trying to do that. Here's a MWE (which I'm running on Julia-1.6 and CUDA#master)\n```using CUDA\nusing StaticArrays\nusing ForwardDiff\n\n# in-place function f! and its Jacobian\nf!(y, v) = y[:] .= v' * v\n∇f!(dy, y, v) = ForwardDiff.jacobian!(dy, f!, y, v)\n\n# cpu test\nv = @SVector rand(Float32, 3)\ny = zeros(Float32, 10);\ndy = zeros(Float32, 10, length(v));\nf!(y,v);\n∇f!(dy, y, v);\n\n# gpu functions\nfunction gpu_f!(y, v)\n    threadIdx().x == 1 &amp;&amp; f!(y,v)\n    return nothing\nend\n\nfunction gpu_∇f!(dy, y, v)\n    threadIdx().x == 1 &amp;&amp; ∇f!(dy,y,v)\n    return nothing\nend\n\n# gpu test\ny_d = cu(zeros(10));\ndy_d = cu(zeros(10,length(v)));\n\n@cuda threads=256 gpu_f!(y_d, v)\n\n@device_code_typed @cuda  threads=256 gpu_∇f!(dy_d, y_d, v)```","user":"UCT34GL7M","ts":"1611334323.017200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j/1P","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I get the same error when trying to do that. Here's a MWE (which I'm running on Julia-1.6 and CUDA#master)\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using CUDA\nusing StaticArrays\nusing ForwardDiff\n\n# in-place function f! and its Jacobian\nf!(y, v) = y[:] .= v' * v\n∇f!(dy, y, v) = ForwardDiff.jacobian!(dy, f!, y, v)\n\n# cpu test\nv = @SVector rand(Float32, 3)\ny = zeros(Float32, 10);\ndy = zeros(Float32, 10, length(v));\nf!(y,v);\n∇f!(dy, y, v);\n\n# gpu functions\nfunction gpu_f!(y, v)\n    threadIdx().x == 1 && f!(y,v)\n    return nothing\nend\n\nfunction gpu_∇f!(dy, y, v)\n    threadIdx().x == 1 && ∇f!(dy,y,v)\n    return nothing\nend\n\n# gpu test\ny_d = cu(zeros(10));\ndy_d = cu(zeros(10,length(v)));\n\n@cuda threads=256 gpu_f!(y_d, v)\n\n@device_code_typed @cuda  threads=256 gpu_∇f!(dy_d, y_d, v)"}]}]}]},{"type":"message","subtype":"thread_broadcast","text":"This probably won't have an effect on <https://github.com/JuliaGPU/CUDA.jl/issues/595>, right? (Not able to test the PR branch myself right now, sorry.)","user":"U01DD7Z0D89","ts":"1611353431.018200","thread_ts":"1611330728.012900","root":{"client_msg_id":"6418a582-69f0-4909-9f0f-60c317293984","type":"message","text":"for those people using CUDA.jl with Julia tasks/threads: now would be a good time to test or comment on <https://github.com/JuliaGPU/CUDA.jl/pull/662>\nTL;DR: per-task streams and synchronization, which makes it possible to overlap GPU computations of individual tasks.","user":"U68A3ASP9","ts":"1611330728.012900","team":"T68168MUP","edited":{"user":"U68A3ASP9","ts":"1611330737.000000"},"blocks":[{"type":"rich_text","block_id":"wVV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"for those people using CUDA.jl with Julia tasks/threads: now would be a good time to test or comment on "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/pull/662"},{"type":"text","text":"\nTL;DR: per-task streams and synchronization, which makes it possible to overlap GPU computations of individual tasks."}]}]}],"thread_ts":"1611330728.012900","reply_count":2,"reply_users_count":2,"latest_reply":"1611386533.018500","reply_users":["U01DD7Z0D89","U68A3ASP9"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"MTa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This probably won't have an effect on "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/issues/595"},{"type":"text","text":", right? (Not able to test the PR branch myself right now, sorry.)"}]}]}],"client_msg_id":"8882a938-55d7-4720-97e7-06062537c61e"},{"client_msg_id":"2775ebfc-75f9-4a79-9954-b2a07f8ec14e","type":"message","text":"Am I understanding correctly that in KernelAbstractions, `workgroupsize` is basically the number of threads per block?","user":"UM30MT6RF","ts":"1611417943.019500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FpPe","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Am I understanding correctly that in KernelAbstractions, "},{"type":"text","text":"workgroupsize","style":{"code":true}},{"type":"text","text":" is basically the number of threads per block?"}]}]}]},{"client_msg_id":"e56d5f86-c941-4346-9937-98debf2f4e0a","type":"message","text":"And if `workgroupsize` is specified as an integer, would it perhaps make sense to thread over the second dimension as well, if the `workgroupsize` is larger than the first dimension of the iteration space?","user":"UM30MT6RF","ts":"1611418395.021200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iod","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And if "},{"type":"text","text":"workgroupsize","style":{"code":true}},{"type":"text","text":" is specified as an integer, would it perhaps make sense to thread over the second dimension as well, if the "},{"type":"text","text":"workgroupsize","style":{"code":true}},{"type":"text","text":" is larger than the first dimension of the iteration space?"}]}]}]},{"client_msg_id":"5af37b44-23c6-4b46-8b2d-fc90681bc346","type":"message","text":"Right prod(workgroupsize()) is the number of threads in a block","user":"U67BJLYCS","ts":"1611419117.022300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HpI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Right prod(workgroupsize()) is the number of threads in a block"}]}]}]},{"client_msg_id":"3d2ece81-03e9-4380-8726-fda3c49ffc71","type":"message","text":"Uhm, potentially yes","user":"U67BJLYCS","ts":"1611419179.022800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/+s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Uhm, potentially yes"}]}]}]},{"client_msg_id":"c1b20da5-9093-433e-9c0e-d7b1f885e0d9","type":"message","text":"I haven't thought about that","user":"U67BJLYCS","ts":"1611419224.023500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4Q/j","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I haven't thought about that"}]}]}]},{"client_msg_id":"58a871b0-1d0a-4b2b-a517-04a423f14369","type":"message","text":"It's a bit challenging since the shape of the workgroupsize is actually used for the kernel","user":"U67BJLYCS","ts":"1611419246.024200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kky1z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It's a bit challenging since the shape of the workgroupsize is actually used for the kernel"}]}]}]},{"client_msg_id":"7470a39a-b3f2-4251-bd31-971a2d1757e0","type":"message","text":"Even if it's dynamic?","user":"UM30MT6RF","ts":"1611419349.024500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TnWe","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Even if it's dynamic?"}]}]}]},{"client_msg_id":"a497d463-fa6c-470a-811a-6d2ca77c9eaf","type":"message","text":"Or do you mean in the Cassette pass?","user":"UM30MT6RF","ts":"1611419456.024900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JKd5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Or do you mean in the Cassette pass?"}]}]}]},{"client_msg_id":"429dba30-73d3-48aa-abdc-9d86905ccbcc","type":"message","text":"It depends on what the kernel does","user":"U67BJLYCS","ts":"1611420844.025600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MnEpy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It depends on what the kernel does"}]}]}]},{"client_msg_id":"cb89371f-1c10-45e5-a1ae-80c5331474b8","type":"message","text":"If the kernel uses @private","user":"U67BJLYCS","ts":"1611420854.026000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Mgy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If the kernel uses @private"}]}]}]},{"client_msg_id":"f967b372-7557-4748-9622-b5f38f55e938","type":"message","text":"That requires querying the workgroupsize","user":"U67BJLYCS","ts":"1611420868.026400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hTw5O","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That requires querying the workgroupsize"}]}]}],"thread_ts":"1611420868.026400","reply_count":4,"reply_users_count":2,"latest_reply":"1611456661.031700","reply_users":["UM30MT6RF","U67BJLYCS"],"subscribed":false,"reactions":[{"name":"+1","users":["UM30MT6RF"],"count":1}]},{"client_msg_id":"f5729deb-9eea-4609-924e-aed8331fde28","type":"message","text":"you also may not want to use too many threads if you don't need to, as it can hurt achieved occupancy","user":"U68A3ASP9","ts":"1611421921.027100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"BNLj0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you also may not want to use too many threads if you don't need to, as it can hurt achieved occupancy"}]}]}],"reactions":[{"name":"+1","users":["UM30MT6RF"],"count":1}]},{"client_msg_id":"f4ca3357-f125-4c0e-8dcd-3269c9bb188c","type":"message","text":"but that depends on the specifics of your kernel","user":"U68A3ASP9","ts":"1611421928.027300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Zo6q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"but that depends on the specifics of your kernel"}]}]}]},{"client_msg_id":"65c33f2d-44d4-47f5-952a-c046e085bbcb","type":"message","text":"What do I need to download and install in order to get `nv-nsight-cu-cli` on my machine?","user":"U7THT3TM3","ts":"1611445625.028000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JY0Z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What do I need to download and install in order to get "},{"type":"text","text":"nv-nsight-cu-cli","style":{"code":true}},{"type":"text","text":" on my machine?"}]}]}]},{"client_msg_id":"8b0bd70b-95ba-44f5-9fe7-2bd1c1b443b6","type":"message","text":"Also, on a completely different note.\n\nOn the CPU, OpenBLAS and MKL do not support matmul with integer matrices.\n\nOn the GPU, does cuBLAS support matmul with integer matrices?","user":"U7THT3TM3","ts":"1611456383.029100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"d8=to","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Also, on a completely different note.\n\nOn the CPU, OpenBLAS and MKL do not support matmul with integer matrices.\n\nOn the GPU, does cuBLAS support matmul with integer matrices?"}]}]}]},{"client_msg_id":"c77eaec0-c0f5-4b57-a56a-590d9931fc98","type":"message","text":"I've been trying to use `CUDA.CUBLAS.gemmEx!`, but it keeps giving me errors:\n```julia&gt; CUDA.CUBLAS.gemmEx!('N', 'N', alpha, a, b, beta, c);\nERROR: ArgumentError: gemmEx does not support Int32=Int32*Int32\n\njulia&gt; CUDA.CUBLAS.gemmEx!('N', 'N', alpha, a, b, beta, c);\nERROR: ArgumentError: gemmEx does not support Int16=Int16*Int16```","user":"U7THT3TM3","ts":"1611456406.029700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7+vmU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I've been trying to use "},{"type":"text","text":"CUDA.CUBLAS.gemmEx!","style":{"code":true}},{"type":"text","text":", but it keeps giving me errors:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> CUDA.CUBLAS.gemmEx!('N', 'N', alpha, a, b, beta, c);\nERROR: ArgumentError: gemmEx does not support Int32=Int32*Int32\n\njulia> CUDA.CUBLAS.gemmEx!('N', 'N', alpha, a, b, beta, c);\nERROR: ArgumentError: gemmEx does not support Int16=Int16*Int16"}]}]}]},{"client_msg_id":"7ec13857-2ffa-4041-8d41-872c6a554d25","type":"message","text":"I don't think it does","user":"U67BJLYCS","ts":"1611456417.029900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"phLPL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I don't think it does"}]}]}]},{"client_msg_id":"dcc556da-5cfb-43d3-871d-d045185b7254","type":"message","text":"You could give <https://github.com/JuliaGPU/GemmKernels.jl|https://github.com/JuliaGPU/GemmKernels.jl>","user":"U67BJLYCS","ts":"1611456486.030200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vhsR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You could give "},{"type":"link","url":"https://github.com/JuliaGPU/GemmKernels.jl","text":"https://github.com/JuliaGPU/GemmKernels.jl"}]}]}]},{"client_msg_id":"c8474ce1-21bb-4f95-939f-841ef654de2f","type":"message","text":"A go","user":"U67BJLYCS","ts":"1611456488.030400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eWv=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"A go"}]}]}]},{"client_msg_id":"700cfcc6-d502-485e-ad30-2259388832b8","type":"message","text":"Or use the generic fallback","user":"U67BJLYCS","ts":"1611456496.030800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"AEe9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Or use the generic fallback"}]}]}]},{"client_msg_id":"8810d084-18bb-4a0c-b0a9-1cca1cfc9af2","type":"message","text":"Yep I'm about to try GemmKernels","user":"U7THT3TM3","ts":"1611456512.031100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ti/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yep I'm about to try GemmKernels"}]}]}]},{"client_msg_id":"0df45c95-541a-465a-b2b5-4fd2191b1094","type":"message","text":"No such luck :disappointed:","user":"U7THT3TM3","ts":"1611456755.032100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ya+RS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"No such luck "},{"type":"emoji","name":"disappointed"}]}]}]},{"client_msg_id":"b41a21ef-78f7-4895-8654-afb2e09bc49e","type":"message","text":"```julia&gt; GemmKernels.BLAS.gemmEx!('N', 'N', alpha, a, b, beta, c_gemmkernels)\nERROR: MethodError: no method matching shared_layout_ab(::Type{CuArray{Int16,2}}, ::Val{false})\nClosest candidates are:\n  shared_layout_ab(::Type{CuArray{Float16,N}}, ::Any) where N at /users/daluthge/.julia/packages/GemmKernels/s61GT/src/blas.jl:13\n  shared_layout_ab(::Type{LinearAlgebra.Diagonal{Float16,CuArray{Float16,N}}}, ::Any) where {N, P} at /users/daluthge/.julia/packages/GemmKernels/s61GT/src/blas.jl:14\nStacktrace:\n [1] gemmEx!(::Char, ::Char, ::Int16, ::CuArray{Int16,2}, ::CuArray{Int16,2}, ::Int16, ::CuArray{Int16,2}) at /users/daluthge/.julia/packages/GemmKernels/s61GT/src/blas.jl:46\n [2] top-level scope at REPL[68]:1```","user":"U7THT3TM3","ts":"1611456769.032300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"d3h","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> GemmKernels.BLAS.gemmEx!('N', 'N', alpha, a, b, beta, c_gemmkernels)\nERROR: MethodError: no method matching shared_layout_ab(::Type{CuArray{Int16,2}}, ::Val{false})\nClosest candidates are:\n  shared_layout_ab(::Type{CuArray{Float16,N}}, ::Any) where N at /users/daluthge/.julia/packages/GemmKernels/s61GT/src/blas.jl:13\n  shared_layout_ab(::Type{LinearAlgebra.Diagonal{Float16,CuArray{Float16,N}}}, ::Any) where {N, P} at /users/daluthge/.julia/packages/GemmKernels/s61GT/src/blas.jl:14\nStacktrace:\n [1] gemmEx!(::Char, ::Char, ::Int16, ::CuArray{Int16,2}, ::CuArray{Int16,2}, ::Int16, ::CuArray{Int16,2}) at /users/daluthge/.julia/packages/GemmKernels/s61GT/src/blas.jl:46\n [2] top-level scope at REPL[68]:1"}]}]}]},{"client_msg_id":"329bc424-e961-4388-baa0-5b8e36ba0d28","type":"message","text":"Which implies that GemmKernels also doesn't support integer matrices","user":"U7THT3TM3","ts":"1611456783.032700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"B2sIO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Which implies that GemmKernels also doesn't support integer matrices"}]}]}]},{"client_msg_id":"a8e11a1e-986d-4801-9f3d-c5c4ac3179d5","type":"message","text":"<https://github.com/JuliaGPU/GemmKernels.jl/issues/64>","user":"U7THT3TM3","ts":"1611457500.032900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sJS","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/JuliaGPU/GemmKernels.jl/issues/64"}]}]}]},{"client_msg_id":"f4cc9d30-8b54-4c2a-8ebe-3db57edd772a","type":"message","text":"gemmEx does support some integer scenario's. i8*i8=i32, and i8*i8=i32. so not that useful; GemmKernels would be great here (cc <@UPMLA9F9S>)","user":"U68A3ASP9","ts":"1611473779.033900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"u0o7I","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"gemmEx does support some integer scenario's. i8*i8=i32, and i8*i8=i32. so not that useful; GemmKernels would be great here (cc "},{"type":"user","user_id":"UPMLA9F9S"},{"type":"text","text":")"}]}]}]},{"client_msg_id":"a606ffea-f79b-4fba-a862-3228eea1ccd0","type":"message","text":"also note that those gemmEx versions I mentioned above have requirements in input sizes, and one at least is broken in CUDA 11.2: <https://github.com/JuliaGPU/CUDA.jl/blob/7ff1e2c4d8d645d4433b802262e0218b3f1a69f5/lib/cublas/wrappers.jl#L765-L770>","user":"U68A3ASP9","ts":"1611473861.034500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1B5nz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"also note that those gemmEx versions I mentioned above have requirements in input sizes, and one at least is broken in CUDA 11.2: "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/blob/7ff1e2c4d8d645d4433b802262e0218b3f1a69f5/lib/cublas/wrappers.jl#L765-L770"}]}]}]},{"client_msg_id":"df4e5b97-d95e-4135-b7f2-9cac6acc4672","type":"message","text":"you normally don't notice that because it gracefully falls back to the (super slow) GPUArrays.jl gemm","user":"U68A3ASP9","ts":"1611473881.034900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"E3Y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you normally don't notice that because it gracefully falls back to the (super slow) GPUArrays.jl gemm"}]}]}]},{"client_msg_id":"8c4f9cfc-eb46-47c3-83fe-72ae5433a5cc","type":"message","text":"When I initialize `y=CuArray([1.0f0,2.0f0,3.0f0,4.0f0,5.0f0])`  and then diagonalize it into a matrix `Diagonal(y)` , I get the following output -\n```julia&gt; Diagonal(y)\n5×5 Diagonal{Float32,CuArray{Float32,1}}:\n 1.0   ⋅    ⋅    ⋅    ⋅ \n  ⋅   2.0   ⋅    ⋅    ⋅ \n  ⋅    ⋅   3.0   ⋅    ⋅ \n  ⋅    ⋅    ⋅   4.0   ⋅ \n  ⋅    ⋅    ⋅    ⋅   5.0```\nDo the `.`s mean the matrix is sparse?","user":"UTDSTSANP","ts":"1611498511.037400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bwu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"When I initialize "},{"type":"text","text":"y=CuArray([1.0f0,2.0f0,3.0f0,4.0f0,5.0f0])","style":{"code":true}},{"type":"text","text":"  and then diagonalize it into a matrix "},{"type":"text","text":"Diagonal(y)","style":{"code":true}},{"type":"text","text":" , I get the following output -\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> Diagonal(y)\n5×5 Diagonal{Float32,CuArray{Float32,1}}:\n 1.0   ⋅    ⋅    ⋅    ⋅ \n  ⋅   2.0   ⋅    ⋅    ⋅ \n  ⋅    ⋅   3.0   ⋅    ⋅ \n  ⋅    ⋅    ⋅   4.0   ⋅ \n  ⋅    ⋅    ⋅    ⋅   5.0"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Do the "},{"type":"text","text":".","style":{"code":true}},{"type":"text","text":"s mean the matrix is sparse?"}]}]}]},{"client_msg_id":"4d2237fc-6a84-4782-9682-c0d1a6968afb","type":"message","text":"It means only the diagonal is stored, yes.","user":"U67D54KS8","ts":"1611500098.037800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6e04","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It means only the diagonal is stored, yes."}]}]}],"thread_ts":"1611500098.037800","reply_count":3,"reply_users_count":2,"latest_reply":"1611502901.039600","reply_users":["UTDSTSANP","U67D54KS8"],"subscribed":false,"reactions":[{"name":"+1","users":["UTDSTSANP"],"count":1}]},{"client_msg_id":"5e9d7359-9cf3-44e6-b490-060597f6d2ab","type":"message","text":"How to do 3d linear interpolation on gpu? I am looking for something like this:\n```interpolate!(out, grid_values::CuArray{T,3}, grid_axes::NTuple{3, AbstractRange}, x,y,z)```\npytorch has some equivalent functionality:\n<https://pytorch.org/docs/stable/nn.functional.html#grid-sample>","user":"UKZ78GR8Q","ts":"1611561630.040800","team":"T68168MUP","edited":{"user":"UKZ78GR8Q","ts":"1611561651.000000"},"blocks":[{"type":"rich_text","block_id":"S0p3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How to do 3d linear interpolation on gpu? I am looking for something like this:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"interpolate!(out, grid_values::CuArray{T,3}, grid_axes::NTuple{3, AbstractRange}, x,y,z)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"pytorch has some equivalent functionality:\n"},{"type":"link","url":"https://pytorch.org/docs/stable/nn.functional.html#grid-sample"}]}]}]},{"client_msg_id":"ee41f35c-63eb-4505-bfde-d9668e21b430","type":"message","text":"Following up an example from a few days ago on using ForwardDiff inside of CUDA kernels, I have a slightly different MWE now:\nI have a function in-place function `f!(y,x)` that fills up a vector `y` based on the input `x`. When I make `y` a CuArray like below, the function runs fine on GPU. Also, when I use `ForwardDiff.jacobian!`  like in the code below, I can get the code to run on the GPU. However, when I try to call `ForwardDiff.jacobian!`  from within a kernel function (`g!`), I get a `Reason: unsupported dynamic function invocation (call to jacobian!(result::Union{AbstractArray, DiffResults.DiffResult}, f!, y::AbstractArray, x::AbstractArray) in ForwardDiff` error. I'm trying to understand the stacktrace but it's way over my head (calls to `compile`, `codegen`, `check_ir` functions `GPUCompiler`) so I was wondering if there's someone here who might know how to deal with the problem.\n```# julia-1.6\nusing CUDA # master\nusing ForwardDiff # v0.10.15\nusing StaticArrays # v1.0.1\n\nx = @SVector rand(Float32, 3);\ny = cu(zeros(256));\n\nf!(y, x) = y[:] .= x'*x;\nf!(y,x); # runs\n\ndy = cu(zeros(256,3))\nForwardDiff.jacobian!(dy,f!,y,x); # also runs\n\nfunction g!(dy,y,x)\n    threadIdx().x == 1 &amp;&amp; ForwardDiff.jacobian!(dy,f!,y,x)\n    nothing\nend\n\n@cuda threads=1 g!(dy,y,x) # doesn't run\n@device_code_warntype @cuda threads=1 g!(dy,y,x)```","user":"UCT34GL7M","ts":"1611570192.046900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"BYHx4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Following up an example from a few days ago on using ForwardDiff inside of CUDA kernels, I have a slightly different MWE now:\nI have a function in-place function "},{"type":"text","text":"f!(y,x)","style":{"code":true}},{"type":"text","text":" that fills up a vector "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" based on the input "},{"type":"text","text":"x","style":{"code":true}},{"type":"text","text":". When I make "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" a CuArray like below, the function runs fine on GPU. Also, when I use "},{"type":"text","text":"ForwardDiff.jacobian!","style":{"code":true}},{"type":"text","text":"  like in the code below, I can get the code to run on the GPU. However, when I try to call "},{"type":"text","text":"ForwardDiff.jacobian!","style":{"code":true}},{"type":"text","text":"  from within a kernel function ("},{"type":"text","text":"g!","style":{"code":true}},{"type":"text","text":"), I get a "},{"type":"text","text":"Reason: unsupported dynamic function invocation (call to jacobian!(result::Union{AbstractArray, DiffResults.DiffResult}, f!, y::AbstractArray, x::AbstractArray) in ForwardDiff","style":{"code":true}},{"type":"text","text":" error. I'm trying to understand the stacktrace but it's way over my head (calls to "},{"type":"text","text":"compile","style":{"code":true}},{"type":"text","text":", "},{"type":"text","text":"codegen","style":{"code":true}},{"type":"text","text":", "},{"type":"text","text":"check_ir","style":{"code":true}},{"type":"text","text":" functions "},{"type":"text","text":"GPUCompiler","style":{"code":true}},{"type":"text","text":") so I was wondering if there's someone here who might know how to deal with the problem.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"# julia-1.6\nusing CUDA # master\nusing ForwardDiff # v0.10.15\nusing StaticArrays # v1.0.1\n\nx = @SVector rand(Float32, 3);\ny = cu(zeros(256));\n\nf!(y, x) = y[:] .= x'*x;\nf!(y,x); # runs\n\ndy = cu(zeros(256,3))\nForwardDiff.jacobian!(dy,f!,y,x); # also runs\n\nfunction g!(dy,y,x)\n    threadIdx().x == 1 && ForwardDiff.jacobian!(dy,f!,y,x)\n    nothing\nend\n\n@cuda threads=1 g!(dy,y,x) # doesn't run\n@device_code_warntype @cuda threads=1 g!(dy,y,x)"}]}]}],"thread_ts":"1611570192.046900","reply_count":10,"reply_users_count":2,"latest_reply":"1611577890.052200","reply_users":["U68A3ASP9","UCT34GL7M"],"subscribed":false},{"client_msg_id":"6cc47365-08dc-47ae-b335-b4df6e5782a3","type":"message","text":"I'd like to compute `pairwise(Euclidean(), X, Y, dims = 2)` efficiently on a GPU, where `X` and `Y` are matrices of size e.g. `3072 x 1024` . Any tips how to get this? Do I need to write my own kernel?","user":"UB2NJGVK6","ts":"1611572679.050500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pD0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'd like to compute "},{"type":"text","text":"pairwise(Euclidean(), X, Y, dims = 2)","style":{"code":true}},{"type":"text","text":" efficiently on a GPU, where "},{"type":"text","text":"X","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"Y","style":{"code":true}},{"type":"text","text":" are matrices of size e.g. "},{"type":"text","text":"3072 x 1024","style":{"code":true}},{"type":"text","text":" . Any tips how to get this? Do I need to write my own kernel?"}]}]}],"thread_ts":"1611572679.050500","reply_count":7,"reply_users_count":2,"latest_reply":"1611651075.053300","reply_users":["U68A3ASP9","UB2NJGVK6"],"subscribed":false},{"client_msg_id":"309b053d-e9e0-4189-b492-60c813d92ece","type":"message","text":"Does anyone know how `CUDA.jl` handles shared memory between GPU and CPU? I have the suspicion that it ignores that it is shared, thereby allocating more memory than actually is free and crashing. Unfortunately, I’m not expert enough to be able to prove that somehow…","user":"UNZKG0909","ts":"1611688634.053900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Et0=F","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does anyone know how "},{"type":"text","text":"CUDA.jl","style":{"code":true}},{"type":"text","text":" handles shared memory between GPU and CPU? I have the suspicion that it ignores that it is shared, thereby allocating more memory than actually is free and crashing. Unfortunately, I’m not expert enough to be able to prove that somehow…"}]}]}],"thread_ts":"1611688634.053900","reply_count":9,"reply_users_count":3,"latest_reply":"1611733958.055900","reply_users":["U6A0PD8CR","UNZKG0909","U68A3ASP9"],"subscribed":false},{"client_msg_id":"d47e0d08-8d2a-49f9-8369-0c06f0c7ec97","type":"message","text":"If I have some data in unified memory, and each of several GPU in parallel takes it and does some series of non-mutating operations using it as input, is it true that there's no extra overhead as compared to if the starting input was in regular GPU memory?","user":"UUMJUCYRK","ts":"1611737157.057900","team":"T68168MUP","edited":{"user":"UUMJUCYRK","ts":"1611737170.000000"},"blocks":[{"type":"rich_text","block_id":"FTi9Y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If I have some data in unified memory, and each of several GPU in parallel takes it and does some series of non-mutating operations using it as input, is it true that there's no extra overhead as compared to if the starting input was in regular GPU memory?"}]}]}]},{"client_msg_id":"93d38eee-9099-4ff5-9002-f474ccbfba7f","type":"message","text":"depends: <https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-multi-gpu>","user":"U68A3ASP9","ts":"1611737441.058300","team":"T68168MUP","attachments":[{"title":"Programming Guide :: CUDA Toolkit Documentation","title_link":"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-multi-gpu","text":"The programming guide to the CUDA model and interface.","fallback":"Programming Guide :: CUDA Toolkit Documentation","from_url":"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-multi-gpu","service_name":"docs.nvidia.com","id":1,"original_url":"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-multi-gpu"}],"blocks":[{"type":"rich_text","block_id":"lgHzK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"depends: "},{"type":"link","url":"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-multi-gpu"}]}]}]},{"client_msg_id":"00a823b1-a08e-412f-9c7b-ce469a9351b2","type":"message","text":"It depends on latest AMD hardware too, as data which was created by the CPU can't be in the infinity cache","user":"U9MD78Z9N","ts":"1611737530.059700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/6V","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It depends on latest AMD hardware too, as data which was created by the CPU can't be in the infinity cache"}]}]}],"thread_ts":"1611737530.059700","reply_count":1,"reply_users_count":1,"latest_reply":"1611757361.071400","reply_users":["U6A0PD8CR"],"subscribed":false},{"client_msg_id":"bc37f80c-4b37-4fda-a5ae-ed31b9ce02ad","type":"message","text":"Thanks, that's helpful. The description for &lt;6.x sounds suitable for me, although the one for &gt;6.x (what I have) somehow is less clear, but I guess it can't be worse? Basically I am thinking of\n```unified_memory_array = &lt;...&gt; \n\nThreads.@threads for dev in devices()\n    device!(dev)\n    compute(unified_memory_array)\nend```\nwhere `compute` is a \"long\" running series of GPU-local operations (which don't mutate `unified_memory_array` or need to intercommunicate in any way).","user":"UUMJUCYRK","ts":"1611738036.062700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Xr=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks, that's helpful. The description for <6.x sounds suitable for me, although the one for >6.x (what I have) somehow is less clear, but I guess it can't be worse? Basically I am thinking of\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"unified_memory_array = <...> \n\nThreads.@threads for dev in devices()\n    device!(dev)\n    compute(unified_memory_array)\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"where "},{"type":"text","text":"compute","style":{"code":true}},{"type":"text","text":" is a \"long\" running series of GPU-local operations (which don't mutate "},{"type":"text","text":"unified_memory_array","style":{"code":true}},{"type":"text","text":" or need to intercommunicate in any way)."}]}]}]}]}