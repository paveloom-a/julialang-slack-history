{"cursor": 4, "messages": [{"client_msg_id":"2a24ca73-c087-4b2e-8449-f5f721ede884","type":"message","text":"KA hasn't changed its portability goals, but users may not want to use it for CPU implementations because it's not currently close to optimal performance (because of loss of vectorization)","user":"U6A0PD8CR","ts":"1615403041.019900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qWHP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"KA hasn't changed its portability goals, but users may not want to use it for CPU implementations because it's not currently close to optimal performance (because of loss of vectorization)"}]}]}],"reactions":[{"name":"+1","users":["UDGT4PM41"],"count":1}]},{"client_msg_id":"284db8d2-2d61-4397-b45f-b46aea632883","type":"message","text":"Yup, that bullet was meant to say what Julian just said :slightly_smiling_face:","user":"UH9KWTTD3","ts":"1615410220.020400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nyc3Y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yup, that bullet was meant to say what Julian just said "},{"type":"emoji","name":"slightly_smiling_face"}]}]}]},{"client_msg_id":"41adf65b-96af-4516-aa42-89d0f64e0019","type":"message","text":"Can anyone point me to how displaying a `CuArray` works when scalar indexing is not allowed?","user":"UH9KWTTD3","ts":"1615486119.021900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kLcVt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can anyone point me to how displaying a "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":" works when scalar indexing is not allowed?"}]}]}]},{"client_msg_id":"ab409ae6-85bb-42b4-bcea-d569058eaac3","type":"message","text":"Trying to fix <https://github.com/FluxML/Flux.jl/issues/1532> and it is weirdly not as simple as I thought","user":"UH9KWTTD3","ts":"1615486156.022400","team":"T68168MUP","edited":{"user":"UH9KWTTD3","ts":"1615486162.000000"},"blocks":[{"type":"rich_text","block_id":"ilXIt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Trying to fix "},{"type":"link","url":"https://github.com/FluxML/Flux.jl/issues/1532"},{"type":"text","text":" and it is weirdly not as simple as I thought"}]}]}]},{"client_msg_id":"f603b10b-4a8d-4b04-baff-9320e5274427","type":"message","text":"It problably calls `@cuprint` or something related to it","user":"U01C3624SGJ","ts":"1615486283.023800","team":"T68168MUP","edited":{"user":"U01C3624SGJ","ts":"1615486303.000000"},"blocks":[{"type":"rich_text","block_id":"ioWk6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It problably calls "},{"type":"text","text":"@cuprint","style":{"code":true}},{"type":"text","text":" or something related to it"}]}]}]},{"client_msg_id":"fd245a18-40a3-4065-9eae-04e83c7d5877","type":"message","text":"Then again, checking the stacktrace it doesnt go through the the CUDA print function","user":"U01C3624SGJ","ts":"1615486488.025500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hvA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Then again, checking the stacktrace it doesnt go through the the CUDA print function"}]}]}]},{"client_msg_id":"a0ebe643-8691-43f2-b944-907b96805281","type":"message","text":"Since it is just printing you could temporarily allow it, or just download the data to the host?","user":"U67D54KS8","ts":"1615486507.026100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"w9A=t","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Since it is just printing you could temporarily allow it, or just download the data to the host?"}]}]}]},{"client_msg_id":"53098f4f-fcb5-4a05-a89d-1a499e8813b4","type":"message","text":"Yeah downloading to the host is what I did. I just wanted to see if there was something else I could be doing, but it sounds like it is more complex than it is worth. Thanks!","user":"UH9KWTTD3","ts":"1615486568.027000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"G53mX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah downloading to the host is what I did. I just wanted to see if there was something else I could be doing, but it sounds like it is more complex than it is worth. Thanks!"}]}]}]},{"client_msg_id":"6042dced-4309-4075-bc52-32db2df2bd6e","type":"message","text":"<https://github.com/JuliaGPU/GPUArrays.jl/blob/7f68400531cf06a0b3e3424f94f5e25e8a8a05c1/src/host/abstractarray.jl#L47-L59>","user":"U68A3ASP9","ts":"1615491533.027300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j8R=B","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/JuliaGPU/GPUArrays.jl/blob/7f68400531cf06a0b3e3424f94f5e25e8a8a05c1/src/host/abstractarray.jl#L47-L59"}]}]}]},{"client_msg_id":"43523251-6c8f-48f3-9161-1f28bedee547","type":"message","text":"for quick hacks you can just put `@allowscalar` in front of an expression","user":"U68A3ASP9","ts":"1615491567.027900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"69UA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"for quick hacks you can just put "},{"type":"text","text":"@allowscalar","style":{"code":true}},{"type":"text","text":" in front of an expression"}]}]}],"reactions":[{"name":"+1::skin-tone-5","users":["UH9KWTTD3"],"count":1}]},{"type":"message","text":"how do I figure out what went wrong here?","files":[{"id":"F01RQQAQEF2","created":1615517849,"timestamp":1615517849,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"UPK2KJ95Y","editable":false,"size":15264,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01RQQAQEF2/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01RQQAQEF2/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01RQQAQEF2-db51092581/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01RQQAQEF2-db51092581/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01RQQAQEF2-db51092581/image_360.png","thumb_360_w":360,"thumb_360_h":62,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01RQQAQEF2-db51092581/image_480.png","thumb_480_w":480,"thumb_480_h":82,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01RQQAQEF2-db51092581/image_160.png","original_w":655,"original_h":112,"thumb_tiny":"AwAIADCnwSOQfwxSnp2pi9ad/DTJHGk49utKegpvf8aAHd6a3T/69L3pD92gD//Z","permalink":"https://julialang.slack.com/files/UPK2KJ95Y/F01RQQAQEF2/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01RQQAQEF2-751b5c969a","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"Cqw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"how do I figure out what went wrong here?"}]}]}],"user":"UPK2KJ95Y","display_as_bot":false,"ts":"1615517862.028500"},{"client_msg_id":"f5f3f147-5f12-483b-a02a-7b33a8f40588","type":"message","text":"Run with `julia -g2`","user":"U67BJLYCS","ts":"1615518008.029400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2Ioq2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Run with "},{"type":"text","text":"julia -g2","style":{"code":true}}]}]}]},{"client_msg_id":"e0c86411-2cb7-4cec-a97a-67ffccd85142","type":"message","text":"Sorry for the high-level question, but I’m curious if there are any block-sparse GPU kernels / support in any of the various julia packages.\n\nThe closest thing I found were bindings for CUSPARSE, which is not block-sparse, unfortunately.","user":"U01QUBGB6JJ","ts":"1615524054.031300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Js/C","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Sorry for the high-level question, but I’m curious if there are any block-sparse GPU kernels / support in any of the various julia packages.\n\nThe closest thing I found were bindings for CUSPARSE, which is not block-sparse, unfortunately."}]}]}]},{"type":"message","text":"This is equivalent to `for x_index in 2:101, y_index in 2:101`  right?","files":[{"id":"F01RDRT55C1","created":1615526878,"timestamp":1615526878,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"UPK2KJ95Y","editable":false,"size":12433,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01RDRT55C1/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01RDRT55C1/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01RDRT55C1-225affd499/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01RDRT55C1-225affd499/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01RDRT55C1-225affd499/image_360.png","thumb_360_w":360,"thumb_360_h":57,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01RDRT55C1-225affd499/image_480.png","thumb_480_w":480,"thumb_480_h":76,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01RDRT55C1-225affd499/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01RDRT55C1-225affd499/image_720.png","thumb_720_w":720,"thumb_720_h":113,"original_w":756,"original_h":119,"thumb_tiny":"AwAHADDQLDmkBHWkPU0g+7TsA4sDShh/kUwUg/iosBJkE9vypwIFRD71PosB/9k=","permalink":"https://julialang.slack.com/files/UPK2KJ95Y/F01RDRT55C1/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01RDRT55C1-de791bc6de","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"j0Sn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This is equivalent to "},{"type":"text","text":"for x_index in 2:101, y_index in 2:101  ","style":{"code":true}},{"type":"text","text":"right?"}]}]}],"user":"UPK2KJ95Y","display_as_bot":false,"ts":"1615526950.032100"},{"client_msg_id":"9837b173-e4ec-4382-92a7-24dc847faa10","type":"message","text":"I have a complex gitlab-ci workflow which needs to be ported to buildkite. Is there any guide for a one-to-one conversion.\n\nParticularly I used .gitlab-ci.yml to point another repo which had all the github-bot functions and gitlab-ci parameters, and pass the variables.\n``` include: path/to/another/repo\nvariables: A\n           B```\n (Is making a plugin the only way?)","user":"UN5FQHFNY","ts":"1615552949.034700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1fS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a complex gitlab-ci workflow which needs to be ported to buildkite. Is there any guide for a one-to-one conversion.\n\nParticularly I used .gitlab-ci.yml to point another repo which had all the github-bot functions and gitlab-ci parameters, and pass the variables.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":" include: path/to/another/repo\nvariables: A\n           B"}]},{"type":"rich_text_section","elements":[{"type":"text","text":" (Is making a plugin the only way?)"}]}]}]},{"client_msg_id":"0d640423-7ab9-4f43-8f8d-25e188cf0f79","type":"message","text":"Do any of you know what/where I need to start to make a kernel implementation using Julia as fast as nvcc implementation? I have two different kernel implementations, one is written in C using nvcc and the other in Julia. They are almost the same, except inevitable changes such as a pointer to array in C being implemented using CuDeviceArray in Julia. They were launched using the same kernel configuration. Currently, nvcc implementation is 6 times faster than Julia, and I would like to reduce this performance gap.\n\nIt seems that generated code is very different. In the case of nvcc, the maximum number of registers was 190 but it was 250 for Julia. When I used nvcc, I set `maxrregcount`  to 96 for better occupancy and saw about 30% improvement. I tried similar things for Julia implementation by trying different values of `maxregs` but with no luck. Any help will be greatly appreciated.","user":"U01FXSDEXN3","ts":"1615569301.043900","team":"T68168MUP","edited":{"user":"U01FXSDEXN3","ts":"1615569513.000000"},"blocks":[{"type":"rich_text","block_id":"vZb51","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do any of you know what/where I need to start to make a kernel implementation using Julia as fast as nvcc implementation? I have two different kernel implementations, one is written in C using nvcc and the other in Julia. They are almost the same, except inevitable changes such as a pointer to array in C being implemented using CuDeviceArray in Julia. They were launched using the same kernel configuration. Currently, nvcc implementation is 6 times faster than Julia, and I would like to reduce this performance gap.\n\nIt seems that generated code is very different. In the case of nvcc, the maximum number of registers was 190 but it was 250 for Julia. When I used nvcc, I set "},{"type":"text","text":"maxrregcount","style":{"code":true}},{"type":"text","text":"  to 96 for better occupancy and saw about 30% improvement. I tried similar things for Julia implementation by trying different values of "},{"type":"text","text":"maxregs","style":{"code":true}},{"type":"text","text":" but with no luck. Any help will be greatly appreciated."}]}]}],"thread_ts":"1615569301.043900","reply_count":3,"reply_users_count":2,"latest_reply":"1615570183.044800","reply_users":["U68A3ASP9","U01FXSDEXN3"],"subscribed":false},{"client_msg_id":"b464d63d-1348-4b29-8620-0d55ee2fa92a","type":"message","text":"Hi there. I've run unto an issue with `LowerTriangular` and `CuArray`s:\n```julia&gt; L = LowerTriangular(CUDA.rand(3,3));\njulia&gt; L * CUDA.rand(3)\nERROR: ArgumentError: cannot take the CPU address of a CuArray{Float32,2}\nStacktrace:\n [1] unsafe_convert(::Type{Ptr{Float32}}, ::CuArray{Float32,2}) at /home/vargonis/.julia/packages/CUDA/YeS8q/src/array.jl:211\n [2] trmv!(::Char, ::Char, ::Char, ::CuArray{Float32,2}, ::CuArray{Float32,1}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/blas.jl:1171\n [3] lmul!(::LowerTriangular{Float32,CuArray{Float32,2}}, ::CuArray{Float32,1}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/triangular.jl:737\n [4] *(::LowerTriangular{Float32,CuArray{Float32,2}}, ::CuArray{Float32,1}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/triangular.jl:2003\n [5] top-level scope at REPL[8]:1```\nIt's funny though that `L * L'`, for instance, works fine. Any workaround suggestion, beyond dropping `LowerTriangular`? I'm using it because it is what `cholesky` gives me. I tried using `L.data` but that actually contains stuff above the diagonal.","user":"ULDCM1P9P","ts":"1615669767.050700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/R+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi there. I've run unto an issue with "},{"type":"text","text":"LowerTriangular","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":"s:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> L = LowerTriangular(CUDA.rand(3,3));\njulia> L * CUDA.rand(3)\nERROR: ArgumentError: cannot take the CPU address of a CuArray{Float32,2}\nStacktrace:\n [1] unsafe_convert(::Type{Ptr{Float32}}, ::CuArray{Float32,2}) at /home/vargonis/.julia/packages/CUDA/YeS8q/src/array.jl:211\n [2] trmv!(::Char, ::Char, ::Char, ::CuArray{Float32,2}, ::CuArray{Float32,1}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/blas.jl:1171\n [3] lmul!(::LowerTriangular{Float32,CuArray{Float32,2}}, ::CuArray{Float32,1}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/triangular.jl:737\n [4] *(::LowerTriangular{Float32,CuArray{Float32,2}}, ::CuArray{Float32,1}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/triangular.jl:2003\n [5] top-level scope at REPL[8]:1"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"It's funny though that "},{"type":"text","text":"L * L'","style":{"code":true}},{"type":"text","text":", for instance, works fine. Any workaround suggestion, beyond dropping "},{"type":"text","text":"LowerTriangular","style":{"code":true}},{"type":"text","text":"? I'm using it because it is what "},{"type":"text","text":"cholesky","style":{"code":true}},{"type":"text","text":" gives me. I tried using "},{"type":"text","text":"L.data","style":{"code":true}},{"type":"text","text":" but that actually contains stuff above the diagonal."}]}]}]},{"client_msg_id":"c5cf7f2a-6344-44e2-a3bf-8fb6308247f9","type":"message","text":"Nvidia : <https://twitter.com/blelbach/status/1371137901258448906?s=19|https://twitter.com/blelbach/status/1371137901258448906?s=19>","user":"UDGT4PM41","ts":"1615740090.051400","team":"T68168MUP","attachments":[{"fallback":"<https://twitter.com/blelbach|@blelbach>: <https://twitter.com/AriKatz20|@AriKatz20> <https://twitter.com/attila_f_feher|@attila_f_feher> <https://twitter.com/_joaogui1|@_joaogui1> <https://twitter.com/ClaymorePT|@ClaymorePT> <https://twitter.com/datametrician|@datametrician> You claimed earlier that Julia is a 10x-100x improvement.\n\nConvince me that it's even a 2x improvement and I'd make the investment!","ts":1615739724,"author_name":"Bryce Adelstein Lelbach","author_link":"https://twitter.com/blelbach/status/1371137901258448906","author_icon":"https://pbs.twimg.com/profile_images/1150126498743037952/oHongNvu_normal.jpg","author_subname":"@blelbach","text":"<https://twitter.com/AriKatz20|@AriKatz20> <https://twitter.com/attila_f_feher|@attila_f_feher> <https://twitter.com/_joaogui1|@_joaogui1> <https://twitter.com/ClaymorePT|@ClaymorePT> <https://twitter.com/datametrician|@datametrician> You claimed earlier that Julia is a 10x-100x improvement.\n\nConvince me that it's even a 2x improvement and I'd make the investment!","service_name":"twitter","service_url":"https://twitter.com/","from_url":"https://twitter.com/blelbach/status/1371137901258448906?s=19","id":1,"original_url":"https://twitter.com/blelbach/status/1371137901258448906?s=19","footer":"Twitter","footer_icon":"https://a.slack-edge.com/80588/img/services/twitter_pixel_snapped_32.png"}],"blocks":[{"type":"rich_text","block_id":"2CDb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Nvidia : "},{"type":"link","url":"https://twitter.com/blelbach/status/1371137901258448906?s=19","text":"https://twitter.com/blelbach/status/1371137901258448906?s=19"}]}]}],"reactions":[{"name":"gpu","users":["UGU761DU2"],"count":1}]},{"client_msg_id":"2a2288be-3177-452d-9df1-35c9f05f10f2","type":"message","text":"Ok, but you're literally arguing with the chair of the US C++ committee ;)","user":"U674T3KB3","ts":"1615740812.052800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uSl/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ok, but you're literally arguing with the chair of the US C++ committee ;)"}]}]}]},{"type":"message","subtype":"thread_broadcast","text":"Yea, that explains all the C++ stanning. I just want to say that I really never intended to convey any sort of antagonism at all towards NVIDIA or python, and I understand that it's important not to overdo things like this. I left one offhand comment (following another one about Julia) and got nerdsniped into debating technical stuff and ecosystem development. Never meant to make it into any sort of language war, which is what seems to be perceived","user":"UDGT4PM41","ts":"1615743660.052900","thread_ts":"1615740812.052800","root":{"client_msg_id":"2a2288be-3177-452d-9df1-35c9f05f10f2","type":"message","text":"Ok, but you're literally arguing with the chair of the US C++ committee ;)","user":"U674T3KB3","ts":"1615740812.052800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uSl/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ok, but you're literally arguing with the chair of the US C++ committee ;)"}]}]}],"thread_ts":"1615740812.052800","reply_count":2,"reply_users_count":2,"latest_reply":"1615743739.053200","reply_users":["UDGT4PM41","U674T3KB3"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"l3u2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yea, that explains all the C++ stanning. I just want to say that I really never intended to convey any sort of antagonism at all towards NVIDIA or python, and I understand that it's important not to overdo things like this. I left one offhand comment (following another one about Julia) and got nerdsniped into debating technical stuff and ecosystem development. Never meant to make it into any sort of language war, which is what seems to be perceived"}]}]}],"client_msg_id":"6530d24a-ce0e-41db-bae9-d5e4947a3e10"},{"client_msg_id":"ce5550ea-d60b-446f-897e-8cf51cf04cb7","type":"message","text":"My GPU’s compute capability is less than the requirement set by latest stable CUDA.jl (3.5 &lt; 5.0) but `test CUDA` has  8531/8536 test passing. If I run into compatibility errors are they going to be insidious like the array I’m mutating is going to turn into garbage or will it be more obvious like the program throws an error?","user":"U01122ME7C5","ts":"1615759973.056400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"D/xd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"My GPU’s compute capability is less than the requirement set by latest stable CUDA.jl (3.5 < 5.0) but "},{"type":"text","text":"test CUDA","style":{"code":true}},{"type":"text","text":" has  8531/8536 test passing. If I run into compatibility errors are they going to be insidious like the array I’m mutating is going to turn into garbage or will it be more obvious like the program throws an error?"}]}]}]},{"client_msg_id":"b2ade93c-5c96-4e22-88aa-878a42c9007c","type":"message","text":"no, it's going to be very obvious errors. for now, there's actually not much relying on &gt;sm_35, so don't worry about it.","user":"U68A3ASP9","ts":"1615791586.058300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OL/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"no, it's going to be very obvious errors. for now, there's actually not much relying on >sm_35, so don't worry about it."}]}]}]},{"client_msg_id":"9f399c11-63e8-48cb-a73d-3e7fc1da7e5c","type":"message","text":"Reminder: GPU BoF/call in ~90 minutes -- 16:00 UTC -- for all your GPU questions, ...\n<https://mit.zoom.us/j/92410737378?pwd=MWkrSkIzMEtRWkZHdm1XQmpNbm90UT09>","user":"U68A3ASP9","ts":"1615819190.060600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DBU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Reminder: GPU BoF/call in ~90 minutes -- 16:00 UTC -- for all your GPU questions, ...\n"},{"type":"link","url":"https://mit.zoom.us/j/92410737378?pwd=MWkrSkIzMEtRWkZHdm1XQmpNbm90UT09"}]}]}]},{"client_msg_id":"b0923123-4a58-4d03-bd78-70e3e6660231","type":"message","text":"Is there a way to see ptxas info of each device function? I have a kernel, and it calls some number of device functions. I would like to see ptxas information, such as number of registers, stack frame, and spill stores, of each device function, similar to the output obtained from nvcc compiled with '-Xptxas=-v' flag.","user":"U01FXSDEXN3","ts":"1615840443.064900","team":"T68168MUP","edited":{"user":"U01FXSDEXN3","ts":"1615840481.000000"},"blocks":[{"type":"rich_text","block_id":"twz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to see ptxas info of each device function? I have a kernel, and it calls some number of device functions. I would like to see ptxas information, such as number of registers, stack frame, and spill stores, of each device function, similar to the output obtained from nvcc compiled with '-Xptxas=-v' flag."}]}]}]},{"client_msg_id":"f314336b-344c-4b74-a7e9-e27c15958ae1","type":"message","text":"`JULIA_DEBUG=CUDA`","user":"U67BJLYCS","ts":"1615840768.065300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QjN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"JULIA_DEBUG=CUDA","style":{"code":true}}]}]}]},{"client_msg_id":"66d0860f-8528-4d9e-a38c-74aa81ed877b","type":"message","text":"should do that","user":"U67BJLYCS","ts":"1615840772.065500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"O0sj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"should do that"}]}]}]},{"client_msg_id":"26541daf-5df1-437b-a695-0bfd1e884cc1","type":"message","text":"Thanks!","user":"U01FXSDEXN3","ts":"1615841111.065700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DTqs4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks!"}]}]}]},{"client_msg_id":"7c06c005-d043-421f-820c-44fb4c8555b2","type":"message","text":"IIRC","user":"U67BJLYCS","ts":"1615841232.065900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RkpN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"IIRC"}]}]}]},{"client_msg_id":"8a466748-595b-4ade-86c8-b9bd1e4eef0b","type":"message","text":"If it doesn't come back and complain","user":"U67BJLYCS","ts":"1615841242.066300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PYH9o","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If it doesn't come back and complain"}]}]}]},{"client_msg_id":"ba7ebcdd-b7b3-451f-8676-7a6b60333d15","type":"message","text":"Hi <@U68A3ASP9>, could you help me set up a CI pipeline using BuildKite for my new package <https://github.com/simsurace/BinomialGPU.jl> ?","user":"U01RZAE2CV6","ts":"1615927933.000600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5mJH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi "},{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":", could you help me set up a CI pipeline using BuildKite for my new package "},{"type":"link","url":"https://github.com/simsurace/BinomialGPU.jl"},{"type":"text","text":" ?"}]}]}]},{"client_msg_id":"5fd5b675-50f6-42e8-974b-75e4089410e2","type":"message","text":"it'll need to be part of one of the Julia orgs in Buildkite, so it's easiest if you could transfer it to the JuliaGPU org if that's fine with you","user":"U68A3ASP9","ts":"1615928032.001200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+vpjB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it'll need to be part of one of the Julia orgs in Buildkite, so it's easiest if you could transfer it to the JuliaGPU org if that's fine with you"}]}]}]},{"client_msg_id":"550252de-8b3a-42ff-bcbe-1e9cccf6af79","type":"message","text":"anyway, if you (have somebody) transfer it, or make me a collaborator, I'll set up the Buildkite pipeline tomorrow.","user":"U68A3ASP9","ts":"1615928096.001700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wXz+x","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"anyway, if you (have somebody) transfer it, or make me a collaborator, I'll set up the Buildkite pipeline tomorrow."}]}]}]},{"client_msg_id":"b4875f1e-8ca4-4982-97b5-ae0e199070be","type":"message","text":"for now it's probably easier to just add you as a collaborator, I can always transfer later, right?","user":"U01RZAE2CV6","ts":"1615928475.003100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3XHi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"for now it's probably easier to just add you as a collaborator, I can always transfer later, right?"}]}]}]},{"client_msg_id":"76517a0c-292f-45d8-8a83-74b674f3661f","type":"message","text":"Is there a good way to do non-trivial `mapreduce` reductions on the GPU besides writing a custom block-reduction kernel? I'm trying to do something like\n```U = rand(5, 5, 5)\nΔ = rand(1, 1, 5)\nans = mapreduce((i, j) -&gt; mapreduce((x, y) -&gt; x / y, max, U[i, j, :], Δ), max, 1:5, 1:5)```\nwhere I'm trying to find the maximum of `U[i, j, k] / Δ[k]` but I'm guessing this will launch lots of kernels for large arrays (one kernel launch per inner `mapreduce`)? Tullio.jl would be perfect for this but it seems to error on the GPU :disappointed:","user":"UEP056STX","ts":"1616043751.016700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mYfRA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a good way to do non-trivial "},{"type":"text","text":"mapreduce","style":{"code":true}},{"type":"text","text":" reductions on the GPU besides writing a custom block-reduction kernel? I'm trying to do something like\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"U = rand(5, 5, 5)\nΔ = rand(1, 1, 5)\nans = mapreduce((i, j) -> mapreduce((x, y) -> x / y, max, U[i, j, :], Δ), max, 1:5, 1:5)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"where I'm trying to find the maximum of "},{"type":"text","text":"U[i, j, k] / Δ[k]","style":{"code":true}},{"type":"text","text":" but I'm guessing this will launch lots of kernels for large arrays (one kernel launch per inner "},{"type":"text","text":"mapreduce","style":{"code":true}},{"type":"text","text":")? Tullio.jl would be perfect for this but it seems to error on the GPU "},{"type":"emoji","name":"disappointed"}]}]}]},{"client_msg_id":"358484b1-fd09-431d-b820-c89ed1ef3b0e","type":"message","text":"I left a comment in <https://github.com/mcabbott/Tullio.jl/issues/91>, one stopgap solution would be\n```julia&gt; let U = cu(rand(3, 3, 3)), Δ = cu(rand(3))\n           @tullio (max) out[l] := U[i, j, k] / Δ[k] l ∈ (1:1)\n           sum(out)\n       end\n1.8847318f0```\nIt's probably not tooo difficult to write your own custom KernelAbstractions.jl kernel for this, but it'd be nicer if Tullio just handled it","user":"U8D9768Q6","ts":"1616044705.018200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"d0F","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I left a comment in "},{"type":"link","url":"https://github.com/mcabbott/Tullio.jl/issues/91"},{"type":"text","text":", one stopgap solution would be\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> let U = cu(rand(3, 3, 3)), Δ = cu(rand(3))\n           @tullio (max) out[l] := U[i, j, k] / Δ[k] l ∈ (1:1)\n           sum(out)\n       end\n1.8847318f0"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"It's probably not tooo difficult to write your own custom KernelAbstractions.jl kernel for this, but it'd be nicer if Tullio just handled it"}]}]}]},{"client_msg_id":"74b49f1e-6d76-4943-8e70-fefa22a715fb","type":"message","text":"Thank you <@U8D9768Q6>! Will play around with your solution on the GPU later today.","user":"UEP056STX","ts":"1616075279.019400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"z+X","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thank you "},{"type":"user","user_id":"U8D9768Q6"},{"type":"text","text":"! Will play around with your solution on the GPU later today."}]}]}]},{"client_msg_id":"9f7818e8-1d97-47d4-bdba-9e13d3f9b77b","type":"message","text":"I'm looking at some (package) code that has:\n\n```if !isgpu(p)\n    W = @view p[reshape(1:(f.out*f.in),f.out,f.in)]\n  else\n    W = reshape(@view(p[1:(f.out*f.in)]),f.out,f.in)\n  end```\nSurely it's there for a reason but I don't understand why the order matters on CPU vs GPU","user":"UKJSNT1QR","ts":"1616103743.020700","team":"T68168MUP","edited":{"user":"UKJSNT1QR","ts":"1616103757.000000"},"blocks":[{"type":"rich_text","block_id":"hbWKL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm looking at some (package) code that has:\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"if !isgpu(p)\n    W = @view p[reshape(1:(f.out*f.in),f.out,f.in)]\n  else\n    W = reshape(@view(p[1:(f.out*f.in)]),f.out,f.in)\n  end"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nSurely it's there for a reason but I don't understand why the order matters on CPU vs GPU"}]}]}]},{"client_msg_id":"f2c9331e-a18c-45a5-bdad-d1c697765b14","type":"message","text":"on CPU it allows for keeping BLAS intact. GPUs are missing that indexing feature.","user":"U69BL50BF","ts":"1616104285.021300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Q7l/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"on CPU it allows for keeping BLAS intact. GPUs are missing that indexing feature."}]}]}]},{"type":"message","text":"","user":"U017B8WSK5X","ts":"1616164282.021800","team":"T68168MUP","attachments":[{"fallback":"[March 19th, 2021 8:48 AM] mahta.ramezanian: Any clues would be appreciated:\n\n`using` certain packages like `Flux` or `CUDA`  on compute node kills Julia. The same doesn't happen on login node or on compute node but for other packages like `PyCall` .","ts":"1616158138.178000","author_id":"U017B8WSK5X","author_subname":"Mahta","channel_id":"C6A044SQH","channel_name":"helpdesk","is_msg_unfurl":true,"is_thread_root_unfurl":true,"text":"Any clues would be appreciated:\n\n`using` certain packages like `Flux` or `CUDA`  on compute node kills Julia. The same doesn't happen on login node or on compute node but for other packages like `PyCall` .","author_name":"Mahta","author_link":"https://julialang.slack.com/team/U017B8WSK5X","author_icon":"https://avatars.slack-edge.com/2020-07-12/1260588022176_09d6f0b1caef87269bf1_48.jpg","mrkdwn_in":["text"],"color":"D0D0D0","from_url":"https://julialang.slack.com/archives/C6A044SQH/p1616158138178000?thread_ts=1616158138178000&cid=C6A044SQH","is_share":true,"footer":"Thread in #helpdesk"}]},{"client_msg_id":"d8609c67-0bb1-4aa2-976c-388ee7729d07","type":"message","text":"you didn't post any details, so we can't help. which version of Julia? which versions of packages? what's the error? `kill Julia` doesn't provide much info","user":"U68A3ASP9","ts":"1616164359.022700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kqY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you didn't post any details, so we can't help. which version of Julia? which versions of packages? what's the error? "},{"type":"text","text":"kill Julia","style":{"code":true}},{"type":"text","text":" doesn't provide much info"}]}]}],"thread_ts":"1616164359.022700","reply_count":7,"reply_users_count":2,"latest_reply":"1616165397.024100","reply_users":["U017B8WSK5X","U68A3ASP9"],"subscribed":false},{"client_msg_id":"2d40c8a7-c2d0-410b-b86a-7cc0a67d26af","type":"message","text":"I just cloned CUDA.jl and I can't get it to build successfully. Normally I'd do\n```] activate Project.toml\ninclude(\"test/setup.jl\")```\nand it everything would work as though I had just done `using CUDA`.\n\nNow, however, I get:\n```julia&gt; include(\"test/setup.jl\")\n[ Info: Precompiling CUDA [052768ef-5323-5732-b1bb-66c8b64840ba]\nERROR: LoadError: LoadError: UndefVarError: ci_cache not defined\nStacktrace:\n  [1] getproperty(x::Module, f::Symbol)\n    @ Base ./Base.jl:26\n  [2] top-level scope\n    @ ~/CUDA.jl/src/compiler/gpucompiler.jl:43\n  [3] include(mod::Module, _path::String)\n    @ Base ./Base.jl:386\n  [4] include(x::String)\n    @ CUDA ~/CUDA.jl/src/CUDA.jl:1\n  [5] top-level scope\n    @ ~/CUDA.jl/src/CUDA.jl:69\n  [6] include\n    @ ./Base.jl:386 [inlined]\n  [7] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String)\n    @ Base ./loading.jl:1209\n  [8] top-level scope\n    @ none:1\n  [9] eval\n    @ ./boot.jl:360 [inlined]\n [10] eval(x::Expr)\n    @ Base.MainInclude ./client.jl:446\n [11] top-level scope\n    @ none:1\nin expression starting at /home/ec2-user/CUDA.jl/src/compiler/gpucompiler.jl:43\nin expression starting at /home/ec2-user/CUDA.jl/src/CUDA.jl:1\nERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to /home/ec2-user/.julia/compiled/v1.6/CUDA/jl_LSCwP5.\nStacktrace:\n [1] error(s::String)\n   @ Base ./error.jl:33\n [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::Base.TTY, internal_stdout::Base.TTY)\n   @ Base ./loading.jl:1356\n [3] compilecache(pkg::Base.PkgId, path::String)\n   @ Base ./loading.jl:1302\n [4] _require(pkg::Base.PkgId)\n   @ Base ./loading.jl:1017\n [5] require(uuidkey::Base.PkgId)\n   @ Base ./loading.jl:910\n [6] require(into::Module, mod::Symbol)\n   @ Base ./loading.jl:897\n [7] include(fname::String)\n   @ Base.MainInclude ./client.jl:444\n [8] top-level scope\n   @ REPL[4]:1\nin expression starting at /home/ec2-user/CUDA.jl/test/setup.jl:1```\nAlternatively, if I try `] build`:\n```(CUDA) pkg&gt; build\nPrecompiling project...\n  Progress [========================================&gt;]  1/1\n  ✗ CUDA\n0 dependencies successfully precompiled in 17 seconds (26 already precompiled)\n1 dependency errored```\nWhat's the recommended way to clone and build the repo now?","user":"U01G39CC63F","ts":"1616173015.025900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"t8fG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I just cloned CUDA.jl and I can't get it to build successfully. Normally I'd do\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"] activate Project.toml\ninclude(\"test/setup.jl\")"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"and it everything would work as though I had just done "},{"type":"text","text":"using CUDA","style":{"code":true}},{"type":"text","text":".\n\nNow, however, I get:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> include(\"test/setup.jl\")\n[ Info: Precompiling CUDA [052768ef-5323-5732-b1bb-66c8b64840ba]\nERROR: LoadError: LoadError: UndefVarError: ci_cache not defined\nStacktrace:\n  [1] getproperty(x::Module, f::Symbol)\n    @ Base ./Base.jl:26\n  [2] top-level scope\n    @ ~/CUDA.jl/src/compiler/gpucompiler.jl:43\n  [3] include(mod::Module, _path::String)\n    @ Base ./Base.jl:386\n  [4] include(x::String)\n    @ CUDA ~/CUDA.jl/src/CUDA.jl:1\n  [5] top-level scope\n    @ ~/CUDA.jl/src/CUDA.jl:69\n  [6] include\n    @ ./Base.jl:386 [inlined]\n  [7] include_package_for_output(pkg::Base.PkgId, input::String, depot_path::Vector{String}, dl_load_path::Vector{String}, load_path::Vector{String}, concrete_deps::Vector{Pair{Base.PkgId, UInt64}}, source::String)\n    @ Base ./loading.jl:1209\n  [8] top-level scope\n    @ none:1\n  [9] eval\n    @ ./boot.jl:360 [inlined]\n [10] eval(x::Expr)\n    @ Base.MainInclude ./client.jl:446\n [11] top-level scope\n    @ none:1\nin expression starting at /home/ec2-user/CUDA.jl/src/compiler/gpucompiler.jl:43\nin expression starting at /home/ec2-user/CUDA.jl/src/CUDA.jl:1\nERROR: LoadError: Failed to precompile CUDA [052768ef-5323-5732-b1bb-66c8b64840ba] to /home/ec2-user/.julia/compiled/v1.6/CUDA/jl_LSCwP5.\nStacktrace:\n [1] error(s::String)\n   @ Base ./error.jl:33\n [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::Base.TTY, internal_stdout::Base.TTY)\n   @ Base ./loading.jl:1356\n [3] compilecache(pkg::Base.PkgId, path::String)\n   @ Base ./loading.jl:1302\n [4] _require(pkg::Base.PkgId)\n   @ Base ./loading.jl:1017\n [5] require(uuidkey::Base.PkgId)\n   @ Base ./loading.jl:910\n [6] require(into::Module, mod::Symbol)\n   @ Base ./loading.jl:897\n [7] include(fname::String)\n   @ Base.MainInclude ./client.jl:444\n [8] top-level scope\n   @ REPL[4]:1\nin expression starting at /home/ec2-user/CUDA.jl/test/setup.jl:1"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nAlternatively, if I try "},{"type":"text","text":"] build","style":{"code":true}},{"type":"text","text":":\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"(CUDA) pkg> build\nPrecompiling project...\n  Progress [========================================>]  1/1\n  ✗ CUDA\n0 dependencies successfully precompiled in 17 seconds (26 already precompiled)\n1 dependency errored"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nWhat's the recommended way to clone and build the repo now?"}]}]}],"thread_ts":"1616173015.025900","reply_count":6,"reply_users_count":2,"latest_reply":"1616175137.027200","reply_users":["U6A0PD8CR","U01G39CC63F"],"subscribed":false},{"client_msg_id":"0b16b96e-4747-4e61-8518-36c52f8ee920","type":"message","text":"```julia&gt; using TropicalNumbers, CUDA, BenchmarkTools, LinearAlgebra\n\njulia&gt; a = Tropical.(CUDA.randn(Float64, 1000, 1000));\n\njulia&gt; @benchmark CUDA.@sync LinearAlgebra.mul!($(zero(a)), $a, $a)\naBenchmarkTools.Trial: \n  memory estimate:  2.14 KiB\n  allocs estimate:  65\n  --------------\n  minimum time:     4.022 ms (0.00% GC)\n  median time:      4.127 ms (0.00% GC)\n  mean time:        4.315 ms (0.00% GC)\n  maximum time:     7.315 ms (0.00% GC)\n  --------------\n  samples:          1159\n  evals/sample:     1\n\njulia&gt; a = Tropical.(CUDA.randn(Float64, 2000, 2000));\n\njulia&gt; @benchmark CUDA.@sync LinearAlgebra.mul!($(zero(a)), $a, $a)\nBenchmarkTools.Trial: \n  memory estimate:  2.14 KiB\n  allocs estimate:  65\n  --------------\n  minimum time:     84.793 ms (0.00% GC)\n  median time:      85.957 ms (0.00% GC)\n  mean time:        86.072 ms (0.00% GC)\n  maximum time:     88.835 ms (0.00% GC)\n  --------------\n  samples:          59\n  evals/sample:     1```\nHi, the scaling of generic gemm looks bad? Should be 2^3 more time, but get something like 2^4. The non-generic GEMM looks good.","user":"UCD4Z3NJZ","ts":"1616350001.029600","team":"T68168MUP","edited":{"user":"UCD4Z3NJZ","ts":"1616350202.000000"},"blocks":[{"type":"rich_text","block_id":"RxKJE","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> using TropicalNumbers, CUDA, BenchmarkTools, LinearAlgebra\n\njulia> a = Tropical.(CUDA.randn(Float64, 1000, 1000));\n\njulia> @benchmark CUDA.@sync LinearAlgebra.mul!($(zero(a)), $a, $a)\naBenchmarkTools.Trial: \n  memory estimate:  2.14 KiB\n  allocs estimate:  65\n  --------------\n  minimum time:     4.022 ms (0.00% GC)\n  median time:      4.127 ms (0.00% GC)\n  mean time:        4.315 ms (0.00% GC)\n  maximum time:     7.315 ms (0.00% GC)\n  --------------\n  samples:          1159\n  evals/sample:     1\n\njulia> a = Tropical.(CUDA.randn(Float64, 2000, 2000));\n\njulia> @benchmark CUDA.@sync LinearAlgebra.mul!($(zero(a)), $a, $a)\nBenchmarkTools.Trial: \n  memory estimate:  2.14 KiB\n  allocs estimate:  65\n  --------------\n  minimum time:     84.793 ms (0.00% GC)\n  median time:      85.957 ms (0.00% GC)\n  mean time:        86.072 ms (0.00% GC)\n  maximum time:     88.835 ms (0.00% GC)\n  --------------\n  samples:          59\n  evals/sample:     1"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, the scaling of generic gemm looks bad? Should be 2^3 more time, but get something like 2^4. The non-generic GEMM looks good."}]}]}]},{"client_msg_id":"8a659475-992d-4b1d-8928-1a5eb371f819","type":"message","text":"I am using Titan V.","user":"UCD4Z3NJZ","ts":"1616350056.030100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iPq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am using Titan V."}]}]}]},{"client_msg_id":"c7181ada-2d2c-44fe-b4b4-28942e499a9e","type":"message","text":"the current fallback GEMM from GPUArrays.jl is _really_ bad. I had hoped to replace it with a GemmKernels.jl-based one, but the current implementation is not generic, and I haven't had the time to work on that. contributions are welcome there!","user":"U68A3ASP9","ts":"1616358330.032400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LAuMM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"the current fallback GEMM from GPUArrays.jl is "},{"type":"text","text":"really","style":{"italic":true}},{"type":"text","text":" bad. I had hoped to replace it with a GemmKernels.jl-based one, but the current implementation is not generic, and I haven't had the time to work on that. contributions are welcome there!"}]}]}],"reactions":[{"name":"+1","users":["U7THT3TM3","UCD4Z3NJZ"],"count":2}]},{"client_msg_id":"0eeda3f6-6a78-4a5e-9c7f-870135670781","type":"message","text":"There is a nice example of generic matmul in KernelAbstractions <https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/examples/performant_matmul.jl|https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/examples/performant_matmul.jl>","user":"U6BJ9E351","ts":"1616359599.035200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LZXZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"There is a nice example of generic matmul in KernelAbstractions "},{"type":"link","url":"https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/examples/performant_matmul.jl","text":"https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/examples/performant_matmul.jl"}]}]}]},{"client_msg_id":"b1a574a2-8b28-4354-b47d-300eeaf41d4a","type":"message","text":"Maybe it can be used also for tropical algebra, but I haven't tested it with nonstandard types","user":"U6BJ9E351","ts":"1616359744.038100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"y=5Jz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Maybe it can be used also for tropical algebra, but I haven't tested it with nonstandard types"}]}]}]},{"client_msg_id":"e3920e3f-73f9-45c6-94bb-6de18cc82a1b","type":"message","text":"It won't be faster than the CUDA one","user":"U67BJLYCS","ts":"1616362255.039200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"R6G7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It won't be faster than the CUDA one"}]}]}]},{"client_msg_id":"7de3d77b-6af9-4bc1-9781-22b64a8508c9","type":"message","text":"Hi there! I'm trying to do im2col on CuArrays... I see there's a `cudnnIm2Col` in CUDA.jl, but have no clue about how to use it. Could you help me with it, please?","user":"ULDCM1P9P","ts":"1616596381.042300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Pnzl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi there! I'm trying to do im2col on CuArrays... I see there's a "},{"type":"text","text":"cudnnIm2Col","style":{"code":true}},{"type":"text","text":" in CUDA.jl, but have no clue about how to use it. Could you help me with it, please?"}]}]}]},{"client_msg_id":"a45c9e33-bf03-4511-acfa-c9dc5c8e0ecf","type":"message","text":"that's an internal function from libcudnn, not wrapped by CUDA.jl. have a look at the CUDNN docs and existing libcudnn wrappers in CUDA.jl","user":"U68A3ASP9","ts":"1616597677.043100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Y+SzE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"that's an internal function from libcudnn, not wrapped by CUDA.jl. have a look at the CUDNN docs and existing libcudnn wrappers in CUDA.jl"}]}]}]},{"client_msg_id":"abc9eb3f-ec12-4053-a9c6-7ffa5d06261a","type":"message","text":"<https://www.youtube.com/channel/UCl7ip3YA68t3XaNNLEci8og/videos>","user":"U018Z5UTBPH","ts":"1616613821.044700","team":"T68168MUP","attachments":[{"service_name":"YouTube","title":"UCF Consortium","title_link":"https://www.youtube.com/channel/UCl7ip3YA68t3XaNNLEci8og/videos","text":"Unified Communication Framework - Collaboration between industry, laboratories, and academia to create production grade communication frameworks and open standards for data-centric and high-performance applications.","fallback":"YouTube: UCF Consortium","thumb_url":"https://yt3.ggpht.com/ytc/AAUvwnhxW1QpzFVmbIioHkuM9hhx7Zutd2TJMTSKl8dP=s900-c-k-c0x00ffffff-no-rj","from_url":"https://www.youtube.com/channel/UCl7ip3YA68t3XaNNLEci8og/videos","thumb_width":900,"thumb_height":900,"service_icon":"https://www.youtube.com/s/desktop/54958e78/img/favicon.ico","id":1,"original_url":"https://www.youtube.com/channel/UCl7ip3YA68t3XaNNLEci8og/videos"}],"blocks":[{"type":"rich_text","block_id":"yfQUV","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://www.youtube.com/channel/UCl7ip3YA68t3XaNNLEci8og/videos"}]}]}]},{"client_msg_id":"8e2dcfc4-fa9a-43e3-bd0b-fc9c319ac0e6","type":"message","text":"Oh neat","user":"U67BJLYCS","ts":"1616614688.045000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+g8J","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oh neat"}]}]}]},{"client_msg_id":"b328e2b5-1a60-4333-8190-971af03b1f2e","type":"message","text":"some interesting videos that I need to look at","user":"U67BJLYCS","ts":"1616615779.045400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"crbnm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"some interesting videos that I need to look at"}]}]}]},{"client_msg_id":"c448560a-b8c1-426f-8325-b33e8cc73e41","type":"message","text":"I'm trying to improve performance of my Julia CUDA code by checking the output of ptxas to see how code is compiled. When interpreting the ptxas info generated by running with  `JULIA_DEBUG=CUDA` , do user-defined device functions have names in a form \"julia_NAME_somenumber\" in the output? For example, if my device function name is gpnorm, then julia_gpnorm_somenumber would be the corresponding name?\n\nIf they have such names, I have some weird output. The information about stack frame, spill stores, and spill loads is very different from what I get from nvcc. When I used nvcc, all of stack frame, spill stores, and spill loads are zero, but they have some high numbers when I used Julia.\n\nDo you happen to know what I could do in this case to figure out or fix the issue: high numbers for stack frame, spill stores, and spill loads that seem to be the cause slowing down performance?","user":"U01FXSDEXN3","ts":"1616786667.058500","team":"T68168MUP","edited":{"user":"U01FXSDEXN3","ts":"1616787434.000000"},"blocks":[{"type":"rich_text","block_id":"dntxG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm trying to improve performance of my Julia CUDA code by checking the output of ptxas to see how code is compiled. When interpreting the ptxas info generated by running with  "},{"type":"text","text":"JULIA_DEBUG=CUDA","style":{"code":true}},{"type":"text","text":" , do user-defined device functions have names in a form \"julia_NAME_somenumber\" in the output? For example, if my device function name is gpnorm, then julia_gpnorm_somenumber would be the corresponding name?\n\nIf they have such names, I have some weird output. The information about stack frame, spill stores, and spill loads is very different from what I get from nvcc. When I used nvcc, all of stack frame, spill stores, and spill loads are zero, but they have some high numbers when I used Julia.\n\nDo you happen to know what I could do in this case to figure out or fix the issue: high numbers for stack frame, spill stores, and spill loads that seem to be the cause slowing down performance?"}]}]}],"thread_ts":"1616786667.058500","reply_count":3,"reply_users_count":2,"latest_reply":"1616789274.059100","reply_users":["U68A3ASP9","U01FXSDEXN3"],"is_locked":false,"subscribed":false},{"client_msg_id":"3e29874b-d616-408a-a4fb-b2cfa2ca391f","type":"message","text":"Hi I just upgraded to 1.6 and am running into a new error computing the gradient of a cholesky decomposition with cuarrays. I tried reverting a bunch of versions of Zygote and a bunch of versions of CUDA with no dice, in addition to trying master, so its almost definitely something in the jump to 1.6. To be clear this works on 1.5.4:\n```using CUDA, Zygote\nX = rand(5,2) |&gt; CuArray{Float64}\ny=rand(5)|&gt; CuArray{Float64}\ntf(X,y) = sum(CUDA.cholesky(X'*X)\\ (X' * y))\ntf(X,y) #works fine\ngradient((X)-&gt;tf(X,y),X) \n\nERROR: ArgumentError: cannot take the CPU address of a CuArray{Float64, 2}\nStacktrace:\n  [1] unsafe_convert(#unused#::Type{Ptr{Float64}}, x::CuArray{Float64, 2})\n    @ CUDA ~\\.julia\\packages\\CUDA\\qEV3Y\\src\\array.jl:211\n  [2] trsm!(side::Char, uplo::Char, transa::Char, diag::Char, alpha::Float64, A::CuArray{Float64, 2}, B::CuArray{Float64, 2})\n    @ LinearAlgebra.BLAS C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.6\\LinearAlgebra\\src\\blas.jl:1995\n  [3] (::Zygote.var\"#779#780\"{CuArray{Float64, 2}, Cholesky{Float64, CuArray{Float64, 2}}})(Δ::NamedTuple{(:uplo, :status, :factors), Tuple{Nothing, Nothing, CuArray{Float64, 2}}})\n    @ Zygote ~\\.julia\\packages\\Zygote\\9X8lu\\src\\lib\\array.jl:544\n  [4] (::Zygote.var\"#3113#back#781\"{Zygote.var\"#779#780\"{CuArray{Float64, 2}, Cholesky{Float64, CuArray{Float64, 2}}}})(Δ::NamedTuple{(:uplo, :status, :factors), Tuple{Nothing, Nothing, CuArray{Float64, 2}}})\n    @ Zygote ~\\.julia\\packages\\ZygoteRules\\OjfTt\\src\\adjoint.jl:59\n  [5] Pullback\n    @ .\\REPL[9]:1 [inlined]\n  [6] (::typeof(∂(tf)))(Δ::Float64)\n    @ Zygote ~\\.julia\\packages\\Zygote\\9X8lu\\src\\compiler\\interface2.jl:0\n  [7] Pullback\n    @ .\\REPL[14]:1 [inlined]\n  [8] (::typeof(∂(#3)))(Δ::Float64)\n    @ Zygote ~\\.julia\\packages\\Zygote\\9X8lu\\src\\compiler\\interface2.jl:0\n  [9] (::Zygote.var\"#41#42\"{typeof(∂(#3))})(Δ::Float64)\n    @ Zygote ~\\.julia\\packages\\Zygote\\9X8lu\\src\\compiler\\interface.jl:41\n [10] gradient(f::Function, args::CuArray{Float64, 2})\n    @ Zygote ~\\.julia\\packages\\Zygote\\9X8lu\\src\\compiler\\interface.jl:59\n [11] top-level scope\n    @ REPL[14]:1```\nI posted an issue for Zygote (<https://github.com/FluxML/Zygote.jl/issues/933>) since it only happens with the gradient, but I can cross-post it on CUDA if that would be helpful- in any case, would be grateful for any help on this- regression code is my main use case","user":"U011ZPX1T5G","ts":"1616807839.065100","team":"T68168MUP","edited":{"user":"U011ZPX1T5G","ts":"1616807900.000000"},"blocks":[{"type":"rich_text","block_id":"zzB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi I just upgraded to 1.6 and am running into a new error computing the gradient of a cholesky decomposition with cuarrays. I tried reverting a bunch of versions of Zygote and a bunch of versions of CUDA with no dice, in addition to trying master, so its almost definitely something in the jump to 1.6. To be clear this works on 1.5.4:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using CUDA, Zygote\nX = rand(5,2) |> CuArray{Float64}\ny=rand(5)|> CuArray{Float64}\ntf(X,y) = sum(CUDA.cholesky(X'*X)\\ (X' * y))\ntf(X,y) #works fine\ngradient((X)->tf(X,y),X) \n\nERROR: ArgumentError: cannot take the CPU address of a CuArray{Float64, 2}\nStacktrace:\n  [1] unsafe_convert(#unused#::Type{Ptr{Float64}}, x::CuArray{Float64, 2})\n    @ CUDA ~\\.julia\\packages\\CUDA\\qEV3Y\\src\\array.jl:211\n  [2] trsm!(side::Char, uplo::Char, transa::Char, diag::Char, alpha::Float64, A::CuArray{Float64, 2}, B::CuArray{Float64, 2})\n    @ LinearAlgebra.BLAS C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.6\\LinearAlgebra\\src\\blas.jl:1995\n  [3] (::Zygote.var\"#779#780\"{CuArray{Float64, 2}, Cholesky{Float64, CuArray{Float64, 2}}})(Δ::NamedTuple{(:uplo, :status, :factors), Tuple{Nothing, Nothing, CuArray{Float64, 2}}})\n    @ Zygote ~\\.julia\\packages\\Zygote\\9X8lu\\src\\lib\\array.jl:544\n  [4] (::Zygote.var\"#3113#back#781\"{Zygote.var\"#779#780\"{CuArray{Float64, 2}, Cholesky{Float64, CuArray{Float64, 2}}}})(Δ::NamedTuple{(:uplo, :status, :factors), Tuple{Nothing, Nothing, CuArray{Float64, 2}}})\n    @ Zygote ~\\.julia\\packages\\ZygoteRules\\OjfTt\\src\\adjoint.jl:59\n  [5] Pullback\n    @ .\\REPL[9]:1 [inlined]\n  [6] (::typeof(∂(tf)))(Δ::Float64)\n    @ Zygote ~\\.julia\\packages\\Zygote\\9X8lu\\src\\compiler\\interface2.jl:0\n  [7] Pullback\n    @ .\\REPL[14]:1 [inlined]\n  [8] (::typeof(∂(#3)))(Δ::Float64)\n    @ Zygote ~\\.julia\\packages\\Zygote\\9X8lu\\src\\compiler\\interface2.jl:0\n  [9] (::Zygote.var\"#41#42\"{typeof(∂(#3))})(Δ::Float64)\n    @ Zygote ~\\.julia\\packages\\Zygote\\9X8lu\\src\\compiler\\interface.jl:41\n [10] gradient(f::Function, args::CuArray{Float64, 2})\n    @ Zygote ~\\.julia\\packages\\Zygote\\9X8lu\\src\\compiler\\interface.jl:59\n [11] top-level scope\n    @ REPL[14]:1"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I posted an issue for Zygote ("},{"type":"link","url":"https://github.com/FluxML/Zygote.jl/issues/933"},{"type":"text","text":") since it only happens with the gradient, but I can cross-post it on CUDA if that would be helpful- in any case, would be grateful for any help on this- regression code is my main use case"}]}]}]},{"client_msg_id":"3fa891c8-723e-4b6d-9ea7-bb732727f0ab","type":"message","text":"GPU BoF/User call today at 12 Eastern (so in ~2h)","user":"U67BJLYCS","ts":"1617027513.068500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mXS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"GPU BoF/User call today at 12 Eastern (so in ~2h)"}]}]}]},{"client_msg_id":"4f7bc112-aade-40d7-aeb4-141f2029b5bc","type":"message","text":"<https://mit.zoom.us/j/92410737378?pwd=MWkrSkIzMEtRWkZHdm1XQmpNbm90UT09>","user":"U67BJLYCS","ts":"1617027514.068700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FEDWM","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://mit.zoom.us/j/92410737378?pwd=MWkrSkIzMEtRWkZHdm1XQmpNbm90UT09"}]}]}]},{"client_msg_id":"48c38255-6bd0-4f17-8932-a336879910f6","type":"message","text":"Is there a recommended AD package to use with CUDA.jl?","user":"U01122ME7C5","ts":"1617119227.071500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sTFGZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a recommended AD package to use with CUDA.jl?"}]}]}]},{"client_msg_id":"553e1556-5bd2-457c-b6bf-aaf2b75100c5","type":"message","text":"ForwardDiff","user":"U85JBUGGP","ts":"1617121133.071800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Sn8H","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ForwardDiff"}]}]}]},{"client_msg_id":"e54899a0-aa09-48a6-8c22-9735949bf153","type":"message","text":"ChainRules an/or Zygote may give you most flexibility","user":"U010WA4SZK5","ts":"1617121178.072600","team":"T68168MUP","edited":{"user":"U010WA4SZK5","ts":"1617121192.000000"},"blocks":[{"type":"rich_text","block_id":"Hca","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ChainRules an/or Zygote may give you most flexibility"}]}]}]},{"client_msg_id":"11cb9a81-bb43-44ca-b3c8-3f87a3d74616","type":"message","text":"Thank you for the responses. In particular I’m trying to differentiate through `Base.:exp` (matrix exponential). I overloaded `exp` with my own method and it’s working up until differentiating through `Base.:\\`. Here’s a MWE\n```using CUDA\nusing ForwardDiff\nA = CuArray([1. 0; 0 1])\nb = CuArray([1.; 1]) \ng(b_) = A\\b_\nForwardDiff.jacobian(g,b)\nERROR: MethodError: no method matching getrs!...```\nBased on CUDA’s definition of `\\` [0] it looks like I need to provide getrf! [1] and getrs! [2] that work with ForwardDiff’s dual type, so I tried to implement my own versions of [1] and [2] that don’t do type checking on the eltype for function signature, and just use the Float64 methods. This is giving a weird error that I don’t know how to resolve though.\n```ERROR: CUDA error: an illegal memory access was encountered (code 700, ERROR_ILLEGAL_ADDRESS)  ```\nI don’t understand what any of these functions are doing so if anyone sees something obviously wrong it would be helpful if you could point it out.\n[0] <https://github.com/JuliaGPU/CUDA.jl/blob/master/src/linalg.jl#L9>\n[1] <https://github.com/JuliaGPU/CUDA.jl/blob/621972b4cb3e386dbbd43f2cd873325e59c8d60f/lib/cusolver/dense.jl#L106>\n[2] <https://github.com/JuliaGPU/CUDA.jl/blob/621972b4cb3e386dbbd43f2cd873325e59c8d60f/lib/cusolver/dense.jl#L197>","user":"U01122ME7C5","ts":"1617121744.078400","team":"T68168MUP","edited":{"user":"U01122ME7C5","ts":"1617122041.000000"},"blocks":[{"type":"rich_text","block_id":"BpaG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thank you for the responses. In particular I’m trying to differentiate through "},{"type":"text","text":"Base.:exp","style":{"code":true}},{"type":"text","text":" (matrix exponential). I overloaded "},{"type":"text","text":"exp","style":{"code":true}},{"type":"text","text":" with my own method and it’s working up until differentiating through "},{"type":"text","text":"Base.:\\","style":{"code":true}},{"type":"text","text":". Here’s a MWE\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using CUDA\nusing ForwardDiff\nA = CuArray([1. 0; 0 1])\nb = CuArray([1.; 1]) \ng(b_) = A\\b_\nForwardDiff.jacobian(g,b)\nERROR: MethodError: no method matching getrs!..."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Based on CUDA’s definition of "},{"type":"text","text":"\\","style":{"code":true}},{"type":"text","text":" [0] it looks like I need to provide getrf! [1] and getrs! [2] that work with ForwardDiff’s dual type, so I tried to implement my own versions of [1] and [2] that don’t do type checking on the eltype for function signature, and just use the Float64 methods. This is giving a weird error that I don’t know how to resolve though.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: CUDA error: an illegal memory access was encountered (code 700, ERROR_ILLEGAL_ADDRESS)  "}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I don’t understand what any of these functions are doing so if anyone sees something obviously wrong it would be helpful if you could point it out.\n[0] "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/blob/master/src/linalg.jl#L9"},{"type":"text","text":"\n[1] "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/blob/621972b4cb3e386dbbd43f2cd873325e59c8d60f/lib/cusolver/dense.jl#L106"},{"type":"text","text":"\n[2] "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/blob/621972b4cb3e386dbbd43f2cd873325e59c8d60f/lib/cusolver/dense.jl#L197"}]}]}]}]}