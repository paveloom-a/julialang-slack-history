{"cursor": 3, "messages": [{"client_msg_id":"688b7b6e-4705-4458-80f2-a3878c5335d0","type":"message","text":"I think we also figured out what the problem could be. The problem is related to the architecture (ppc64le).\nAccording to the documentation of <https://julialang.org/downloads/#currently_supported_platforms>\nCUDA.jl does not support Power 9 AC922. \n\nWe were wondering if we could get some help installing the CUDA.jl. \n\nThank you and sorry for the long message.","user":"U015TV044AV","ts":"1613410443.183500","team":"T68168MUP","edited":{"user":"U015TV044AV","ts":"1613410457.000000"},"blocks":[{"type":"rich_text","block_id":"MRs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think we also figured out what the problem could be. The problem is related to the architecture (ppc64le).\nAccording to the documentation of "},{"type":"link","url":"https://julialang.org/downloads/#currently_supported_platforms"},{"type":"text","text":"\nCUDA.jl does not support Power 9 AC922. \n\nWe were wondering if we could get some help installing the CUDA.jl. \n\nThank you and sorry for the long message."}]}]}]},{"client_msg_id":"6ebc8cd3-1509-4118-86df-34a831f0cf92","type":"message","text":"<@U67BJLYCS> is the person to ask","user":"U674T3KB3","ts":"1613410532.184400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"T9jjE","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":" is the person to ask"}]}]}]},{"client_msg_id":"3dd0b04d-a98a-4068-9e30-bffdd8797c07","type":"message","text":"It definitely works, but there may be quirks, since it's not a mainstream platform","user":"U674T3KB3","ts":"1613410550.185200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SR0dv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It definitely works, but there may be quirks, since it's not a mainstream platform"}]}]}],"reactions":[{"name":"+1","users":["UEP056STX"],"count":1}]},{"client_msg_id":"4909949d-17a9-487f-8993-c53a9f131596","type":"message","text":"<@U015TV044AV> we are running Julia on our local powerpc cluster at MIT","user":"U67BJLYCS","ts":"1613411360.186100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"k9fnJ","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U015TV044AV"},{"type":"text","text":" we are running Julia on our local powerpc cluster at MIT"}]}]}]},{"client_msg_id":"4a799ce3-906d-4833-b8b3-fb142bc02a56","type":"message","text":"You might want to set `export JULIA_CUDA_USE_BINARYBUILDER=false`","user":"U67BJLYCS","ts":"1613411435.186400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FQa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You might want to set "},{"type":"text","text":"export JULIA_CUDA_USE_BINARYBUILDER=false","style":{"code":true}}]}]}]},{"client_msg_id":"74d9644b-8a93-4a18-8c33-748ab200d19b","type":"message","text":"but CUDA_jll 11+ is available for PPC","user":"U67BJLYCS","ts":"1613412120.186900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"X9j","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"but CUDA_jll 11+ is available for PPC"}]}]}]},{"client_msg_id":"d23cc184-bc2e-4b54-b98e-cc1770a247d4","type":"message","text":"So what issue are you running into?","user":"U67BJLYCS","ts":"1613412131.187200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"CBqp1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So what issue are you running into?"}]}]}]},{"client_msg_id":"3382ded7-8fc6-42d3-acab-ba32959d4191","type":"message","text":"<@U68A3ASP9> do I understand correctly that the issue basically is that CUDA 10 only was distributed as an rpm, for PPC?","user":"U67BJLYCS","ts":"1613413774.188000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TwhG","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":" do I understand correctly that the issue basically is that CUDA 10 only was distributed as an rpm, for PPC?"}]}]}]},{"client_msg_id":"73937155-4ac1-49a3-a491-625bb93df452","type":"message","text":"Hey, do I understand it right that if I want to run my code on CUDA I should use CUDAArrays and write all my algorithms using array calculations mostly?","user":"U7PD3M3L5","ts":"1613470136.190600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"e1uo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey, do I understand it right that if I want to run my code on CUDA I should use CUDAArrays and write all my algorithms using array calculations mostly?"}]}]}]},{"client_msg_id":"231ebdc3-2a1f-478d-8ee7-da081440220e","type":"message","text":"I am a bit confused since how I imagine a GPU I should not be using normal Julia code with `if`  statements or `for`  loops with complicated conditions right?","user":"U7PD3M3L5","ts":"1613470186.191700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FgT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am a bit confused since how I imagine a GPU I should not be using normal Julia code with "},{"type":"text","text":"if","style":{"code":true}},{"type":"text","text":"  statements or "},{"type":"text","text":"for","style":{"code":true}},{"type":"text","text":"  loops with complicated conditions right?"}]}]}]},{"client_msg_id":"2abdda4e-b333-483d-baa9-4ddde3c1cae3","type":"message","text":"<https://juliagpu.gitlab.io/CUDA.jl/usage/overview/>","user":"U68A3ASP9","ts":"1613470991.191900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=DMkc","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://juliagpu.gitlab.io/CUDA.jl/usage/overview/"}]}]}]},{"client_msg_id":"d54733cf-3fcb-4cea-a541-cd6bed33e4a8","type":"message","text":"you can use kernels with conditionals and everything, but the same restrictions as CUDA C apply then (i.e. you probably want to avoid those as they can be slow)","user":"U68A3ASP9","ts":"1613471080.192500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Dzg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you can use kernels with conditionals and everything, but the same restrictions as CUDA C apply then (i.e. you probably want to avoid those as they can be slow)"}]}]}]},{"client_msg_id":"dd702b83-0b8d-4442-aafb-4d869fad7468","type":"message","text":"Thank you, the link helped a bit and the other docs there look promising","user":"U7PD3M3L5","ts":"1613471343.193300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pVpZj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thank you, the link helped a bit and the other docs there look promising"}]}]}]},{"client_msg_id":"33f700e0-e27f-4a13-b1e0-f941f90d3c76","type":"message","text":"Is using Flux on the GPU as easy and performant as using PyTorch? Or basically what do I get for GPU programming when switching from python to julia. For CPU it is pretty clear: I can just write most stuff in for loops etc and not worry. In Pytorch I am doing nearly everything with predefined objects that operate kernels on the GPU anyways, how does julia make things easier here?","user":"U7PD3M3L5","ts":"1613471443.195100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1hLli","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is using Flux on the GPU as easy and performant as using PyTorch? Or basically what do I get for GPU programming when switching from python to julia. For CPU it is pretty clear: I can just write most stuff in for loops etc and not worry. In Pytorch I am doing nearly everything with predefined objects that operate kernels on the GPU anyways, how does julia make things easier here?"}]}]}]},{"client_msg_id":"524f60ff-e88f-4129-9f80-6cbaa3b7f534","type":"message","text":"Not trying to rant or something, I just want to understand","user":"U7PD3M3L5","ts":"1613471463.195600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oU/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not trying to rant or something, I just want to understand"}]}]}]},{"client_msg_id":"96ba99e6-e934-4a04-ae39-31e165a38aa7","type":"message","text":"you generally need to stick to vectorized array expressions to get portability across CPU/GPU (but in Julia that's a much more powerful set of operations). and when you want to optimize, you can write your GPU kernels in Julia too, which is the main advantage over other languages (where you need to use CUDA C).\nwrt. performance compared to pytorch, we probably aren't as fast because we don't have as many optimized kernels, and there can be some issues with the GC when running close to OOM, but better check in some of the machine learning channels for specific numbers.","user":"U68A3ASP9","ts":"1613471972.196100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YIv/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you generally need to stick to vectorized array expressions to get portability across CPU/GPU (but in Julia that's a much more powerful set of operations). and when you want to optimize, you can write your GPU kernels in Julia too, which is the main advantage over other languages (where you need to use CUDA C).\nwrt. performance compared to pytorch, we probably aren't as fast because we don't have as many optimized kernels, and there can be some issues with the GC when running close to OOM, but better check in some of the machine learning channels for specific numbers."}]}]}]},{"client_msg_id":"ca070fcf-e8c7-4e81-9119-538c5b674b42","type":"message","text":"So the advantages go beyond the array kernels. There is a very rich ecosystem of GPU capable packages, thanks in part to the ease with which one can write compatible code, this increases the modelling surface quite significantly.\n\nThere are further optimisations that we can get from existing cuda c kernels as in cudnn, but often one can write code to optimise it without going into cuda, directly in Julia.\n\nThere's also the question of complexity. Tim mentioned how one can write array operations for GPU compatible code. This means that code you write can be transformed easily to GPU kernels, and you can use ad on it the same way. The whole composable nature of it also allows for the same optimisation tricks that the compilers can do on the code generated, since it's all Julia code","user":"UC4QQPG4A","ts":"1613478627.204600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Vz9U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So the advantages go beyond the array kernels. There is a very rich ecosystem of GPU capable packages, thanks in part to the ease with which one can write compatible code, this increases the modelling surface quite significantly.\n\nThere are further optimisations that we can get from existing cuda c kernels as in cudnn, but often one can write code to optimise it without going into cuda, directly in Julia.\n\nThere's also the question of complexity. Tim mentioned how one can write array operations for GPU compatible code. This means that code you write can be transformed easily to GPU kernels, and you can use ad on it the same way. The whole composable nature of it also allows for the same optimisation tricks that the compilers can do on the code generated, since it's all Julia code"}]}]}]},{"client_msg_id":"27c1125c-1933-488b-ad38-fb55323a2065","type":"message","text":"Thanks for the answers, that shed some light. All in all this means that if I can write my code completly in PyTorch i.e. in Tensor notation then probably pytorch is faster since their kernels are more optimized. If I'd need something custom, I will have it easier in Julia since I can just write Julia code/use existing libraries?","user":"U7PD3M3L5","ts":"1613487521.206600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"CA9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for the answers, that shed some light. All in all this means that if I can write my code completly in PyTorch i.e. in Tensor notation then probably pytorch is faster since their kernels are more optimized. If I'd need something custom, I will have it easier in Julia since I can just write Julia code/use existing libraries?"}]}]}]},{"client_msg_id":"8762e1ff-635f-48da-b855-92fa57a9c8b5","type":"message","text":"I haven't followed its development very closely but Torch.jl also exists which means you can stay inside julia and use the C++ pytorch library","user":"U01C3624SGJ","ts":"1613487982.208200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sWA2V","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I haven't followed its development very closely but Torch.jl also exists which means you can stay inside julia and use the C++ pytorch library"}]}]}]},{"client_msg_id":"860087dc-e10c-4e2d-9f8a-36517effe9a7","type":"message","text":"<@U7PD3M3L5> that's not generally true","user":"U69BL50BF","ts":"1613488367.208400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"h537A","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U7PD3M3L5"},{"type":"text","text":" that's not generally true"}]}]}]},{"client_msg_id":"3bddbe18-ae34-45f4-a333-8d0779757d08","type":"message","text":"if you're doing a lot of broadcasted expressions then the codegen of CUDA.jl will definitely lead to more optimized code because of the kernel fusion","user":"U69BL50BF","ts":"1613488397.209100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ELnB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"if you're doing a lot of broadcasted expressions then the codegen of CUDA.jl will definitely lead to more optimized code because of the kernel fusion"}]}]}]},{"client_msg_id":"6720fef2-97ea-4142-b42c-5b92fb83a37d","type":"message","text":"But there are some specifically good kernels that PyTorch more readily uses. I think the RNN ones IIRC. There was some stuff in the CNN kernels but I think those were fixed up just 2 weeks ago (<@U68A3ASP9> would know that part). It's the RNN kernels that don't do all of the fusions PyTorch goes though.","user":"U69BL50BF","ts":"1613488458.210200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0aKB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But there are some specifically good kernels that PyTorch more readily uses. I think the RNN ones IIRC. There was some stuff in the CNN kernels but I think those were fixed up just 2 weeks ago ("},{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":" would know that part). It's the RNN kernels that don't do all of the fusions PyTorch goes though."}]}]}]},{"client_msg_id":"757d34a5-dede-437f-b4a2-3f30a6786973","type":"message","text":"I thought the Pytorch RNNs just shell out to cuDNN?","user":"UMY1LV01G","ts":"1613493060.210700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j2TZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I thought the Pytorch RNNs just shell out to cuDNN?"}]}]}]},{"client_msg_id":"a76cec61-7bf1-4ea0-98ea-5b300fa14abc","type":"message","text":"they fuse in some nice way IIRC right <@UC4QQPG4A>?","user":"U69BL50BF","ts":"1613493152.211000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LVx8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"they fuse in some nice way IIRC right "},{"type":"user","user_id":"UC4QQPG4A"},{"type":"text","text":"?"}]}]}],"thread_ts":"1613493152.211000","reply_count":4,"reply_users_count":2,"latest_reply":"1613496605.219300","reply_users":["UMY1LV01G","UC4QQPG4A"],"subscribed":false},{"client_msg_id":"782e5c6d-682b-49ab-b7c1-83630e8e922d","type":"message","text":"This is a bit of an odd request, but I'm looking for the machinery to flip `c.colVal` and `c.rowPtr`  to `c.rowVal` and `c.colPtr` where `c` is a sparse matrix. I'm using this structure to represent a graph on the gpu, and this \"transposes\" the graph. I was wondering, where can I find a function that does that without having to build sparse matrices? It is a bit wasteful to allocate a vector of floats that I don't use.\n\nIt would be great for me to have something more \"hackable\", where I can play with the implementation (also because I'm working with multigraphs, so not all the assumptions for the `CuSparseMatrixCSR` format hold. I started playing with my own implementation but it seemed a bit tricky. From what I understand, the current sorting implementation (<https://github.com/JuliaGPU/CUDA.jl/pull/431/files>) does not have an optimized version for integer arrays with few unique values, which is the one I would need.","user":"U6BJ9E351","ts":"1613493902.217300","team":"T68168MUP","edited":{"user":"U6BJ9E351","ts":"1613494031.000000"},"blocks":[{"type":"rich_text","block_id":"F/H1T","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This is a bit of an odd request, but I'm looking for the machinery to flip "},{"type":"text","text":"c.colVal","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"c.rowPtr","style":{"code":true}},{"type":"text","text":"  to "},{"type":"text","text":"c.rowVal","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"c.colPtr","style":{"code":true}},{"type":"text","text":" where "},{"type":"text","text":"c","style":{"code":true}},{"type":"text","text":" is a sparse matrix. I'm using this structure to represent a graph on the gpu, and this \"transposes\" the graph. I was wondering, where can I find a function that does that without having to build sparse matrices? It is a bit wasteful to allocate a vector of floats that I don't use.\n\nIt would be great for me to have something more \"hackable\", where I can play with the implementation (also because I'm working with multigraphs, so not all the assumptions for the "},{"type":"text","text":"CuSparseMatrixCSR","style":{"code":true}},{"type":"text","text":" format hold. I started playing with my own implementation but it seemed a bit tricky. From what I understand, the current sorting implementation ("},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/pull/431/files"},{"type":"text","text":") does not have an optimized version for integer arrays with few unique values, which is the one I would need."}]}]}]},{"client_msg_id":"f15c9771-9257-414c-8aee-401806a2fcdb","type":"message","text":"you can always use a lazy transpose/adjoint to turn the columns to rows","user":"U85JBUGGP","ts":"1613495829.218900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1a+0s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you can always use a lazy transpose/adjoint to turn the columns to rows"}]}]}]},{"client_msg_id":"42b6dc92-7083-4e1d-96f6-5f846eb65532","type":"message","text":"Hey everyone, I'm trying to install CUDA.jl without artifacts and I'm getting the following exception:\n`CUDA.jl does not yet support CUDA with nvdisasm 11.0.167`\nDiscussions on GitHub suggest that this was fixed for 11.0, but I'm still getting it, maybe something went wrong on further updates? Any help would be much appreciated!","user":"U01NHTMSEHG","ts":"1613643051.224700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/WNJr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey everyone, I'm trying to install CUDA.jl without artifacts and I'm getting the following exception:\n"},{"type":"text","text":"CUDA.jl does not yet support CUDA with nvdisasm 11.0.167","style":{"code":true}},{"type":"text","text":"\nDiscussions on GitHub suggest that this was fixed for 11.0, but I'm still getting it, maybe something went wrong on further updates? Any help would be much appreciated!"}]}]}]},{"client_msg_id":"96fbb82f-f7d5-4e5e-acbf-a41869fdb40d","type":"message","text":"I also couldn't get it to work with artifacts so maybe I'm doing something wrong...","user":"U01NHTMSEHG","ts":"1613643102.225200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vz4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I also couldn't get it to work with artifacts so maybe I'm doing something wrong..."}]}]}]},{"client_msg_id":"2c67042a-4184-429c-bd65-3fccd4976380","type":"message","text":"NVM, did a clean install, installation with artifacts works now (though non-artifact installation is still throwing the same error)","user":"U01NHTMSEHG","ts":"1613651365.226200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LeDRq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"NVM, did a clean install, installation with artifacts works now (though non-artifact installation is still throwing the same error)"}]}]}]},{"client_msg_id":"4766f4b6-3dee-47b6-9472-3c42cf22dcbc","type":"message","text":"What's the best way to get a single element from a `CuArray` if I disallow scalar indexing? I can hack it through a `copyto!` with a temporary `Array`, but wondering if there was a more elegant approach.","user":"UCRHP2GHE","ts":"1613685299.229300","team":"T68168MUP","edited":{"user":"UCRHP2GHE","ts":"1613685334.000000"},"blocks":[{"type":"rich_text","block_id":"xcBaa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What's the best way to get a single element from a "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":" if I disallow scalar indexing? I can hack it through a "},{"type":"text","text":"copyto!","style":{"code":true}},{"type":"text","text":" with a temporary "},{"type":"text","text":"Array","style":{"code":true}},{"type":"text","text":", but wondering if there was a more elegant approach."}]}]}],"thread_ts":"1613685299.229300","reply_count":1,"reply_users_count":1,"latest_reply":"1613686106.229500","reply_users":["UM30MT6RF"],"subscribed":false},{"client_msg_id":"7af3b557-eb74-4104-9dd6-4452bc4ebe33","type":"message","text":"Is there a workaround for <https://github.com/JuliaGPU/KernelAbstractions.jl/issues/7>? I need to compute the maximum of a given value (which is thread dependent) within a block and could only think of `CUDA.atomic_max!`  as a way to do that","user":"U6BJ9E351","ts":"1613732616.231000","team":"T68168MUP","edited":{"user":"U6BJ9E351","ts":"1613732627.000000"},"blocks":[{"type":"rich_text","block_id":"Y2S","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a workaround for "},{"type":"link","url":"https://github.com/JuliaGPU/KernelAbstractions.jl/issues/7"},{"type":"text","text":"? I need to compute the maximum of a given value (which is thread dependent) within a block and could only think of "},{"type":"text","text":"CUDA.atomic_max!","style":{"code":true}},{"type":"text","text":"  as a way to do that"}]}]}]},{"client_msg_id":"c7059b03-8a80-410e-a797-7ed7874bf3c8","type":"message","text":"No I don't think there is a work around, besides implementing either portable atomics or block level reductions in KA","user":"U67BJLYCS","ts":"1613737438.232700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UMH8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"No I don't think there is a work around, besides implementing either portable atomics or block level reductions in KA"}]}]}]},{"client_msg_id":"7b7338a4-5fe5-4685-95bb-97fd4dedbf95","type":"message","text":"Not impossible, just hard","user":"U67BJLYCS","ts":"1613737491.233200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=5T","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not impossible, just hard"}]}]}]},{"client_msg_id":"2ba08f83-3b73-4c3e-9c8a-ebba14886b63","type":"message","text":"Is there a way of profiling a device function? In my kernel function, I have several device functions, and I would like to profile them. nvprof() or other similar tool seems to give statistics of a kernel function only. In NVIDIA document, they seem to recommend to use clock() inside a device function to profile. CUDA.jl has a similar function?","user":"U01FXSDEXN3","ts":"1613766676.235800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Tnbp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way of profiling a device function? In my kernel function, I have several device functions, and I would like to profile them. nvprof() or other similar tool seems to give statistics of a kernel function only. In NVIDIA document, they seem to recommend to use clock() inside a device function to profile. CUDA.jl has a similar function?"}]}]}]},{"client_msg_id":"c1e3bcae-e3fb-488f-bcc8-cd0d0b1c12d3","type":"message","text":"NVidia profiling has PC profiling","user":"U67BJLYCS","ts":"1613770233.236300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IFkl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"NVidia profiling has PC profiling"}]}]}],"thread_ts":"1613770233.236300","reply_count":3,"reply_users_count":2,"latest_reply":"1613770807.236900","reply_users":["U01FXSDEXN3","U67BJLYCS"],"subscribed":false},{"client_msg_id":"292632b6-c949-4d35-a28d-736460e7f174","type":"message","text":"sorry for spamming this channel with KernelAbstractions question, but I'm a bit new to GPU computing and I'm finding out there is a bit of a learning curve. I was trying to figure out, is the relationship between Cartesian and Linear indexing the same as on the CPU? So basically things like `ndrange = (32, 32), groupsize = (16, 16)` (to write on an array of size `ndrange` ) would be slightly slower than `ndrange = (1024,), groupsize = (256,)`  (unless one has to manually use `fldmod1` to get back the cartesian indices inside the kernel)? Or are there some performance advantages in having a 2D groupsize?","user":"U6BJ9E351","ts":"1613841602.004400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vvq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"sorry for spamming this channel with KernelAbstractions question, but I'm a bit new to GPU computing and I'm finding out there is a bit of a learning curve. I was trying to figure out, is the relationship between Cartesian and Linear indexing the same as on the CPU? So basically things like "},{"type":"text","text":"ndrange = (32, 32), groupsize = (16, 16)","style":{"code":true}},{"type":"text","text":" (to write on an array of size "},{"type":"text","text":"ndrange","style":{"code":true}},{"type":"text","text":" ) would be slightly slower than "},{"type":"text","text":"ndrange = (1024,), groupsize = (256,)","style":{"code":true}},{"type":"text","text":"  (unless one has to manually use "},{"type":"text","text":"fldmod1","style":{"code":true}},{"type":"text","text":" to get back the cartesian indices inside the kernel)? Or are there some performance advantages in having a 2D groupsize?"}]}]}]},{"client_msg_id":"3c3367b0-74c2-449f-927e-a7af0c237aff","type":"message","text":"<https://discourse.julialang.org/t/how-to-generate-a-random-number-in-cuda-kernel-function/50364|this> discourse post by Tim Besard in nov 2020 says that random number generation is not supported in GPU kernels right now. Is that still the case?","user":"U011V2YN59N","ts":"1613844813.005600","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"How to generate a random number in CUDA kernel function","title_link":"https://discourse.julialang.org/t/how-to-generate-a-random-number-in-cuda-kernel-function/50364","text":"All the rand like function in CUDA.jl will get a CuArray. But memory allocation is not allowed in a kernel function. Thus, a CuArray can not be generated. So, are there any method to get a pure random number instead of an Array.","fallback":"JuliaLang: How to generate a random number in CUDA kernel function","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","fields":[{"title":"Reading time","value":"1 mins :clock2:","short":true},{"title":"Likes","value":"7 :heart:","short":true}],"ts":1605690854,"from_url":"https://discourse.julialang.org/t/how-to-generate-a-random-number-in-cuda-kernel-function/50364","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/how-to-generate-a-random-number-in-cuda-kernel-function/50364"}],"blocks":[{"type":"rich_text","block_id":"zHupm","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://discourse.julialang.org/t/how-to-generate-a-random-number-in-cuda-kernel-function/50364","text":"this"},{"type":"text","text":" discourse post by Tim Besard in nov 2020 says that random number generation is not supported in GPU kernels right now. Is that still the case?"}]}]}]},{"client_msg_id":"78ef8434-de1b-4d23-91ec-e9a932bef7fd","type":"message","text":"With a bit of care you can have random numbers on the GPU","user":"U67BJLYCS","ts":"1613846094.007500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jjn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"With a bit of care you can have random numbers on the GPU"}]}]}]},{"client_msg_id":"a2af553e-756e-45ac-9ca4-15b6f7622288","type":"message","text":"<https://juliafolds.github.io/FoldsCUDA.jl/dev/examples/monte_carlo_pi/>","user":"U67BJLYCS","ts":"1613846094.007700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Wfn","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://juliafolds.github.io/FoldsCUDA.jl/dev/examples/monte_carlo_pi/"}]}]}]},{"client_msg_id":"b2358d3b-9a40-4cef-8463-22e8d9b0d2b2","type":"message","text":"mm my gpu does not like that example :disappointed:","user":"U011V2YN59N","ts":"1613846510.008200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KC7S","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"mm my gpu does not like that example "},{"type":"emoji","name":"disappointed"}]}]}]},{"client_msg_id":"a9d3957b-304b-4def-84f4-506f36e45ff0","type":"message","text":"In general actually, `test CUDA`  produces many errors.. is it worth opening an issue?","user":"U011V2YN59N","ts":"1613846560.009000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bqQM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In general actually, "},{"type":"text","text":"test CUDA","style":{"code":true}},{"type":"text","text":"  produces many errors.. is it worth opening an issue?"}]}]}]},{"type":"message","text":"I wrote this tiny wrapper which \"adapts\" an array to unified memory instead of per-device memory. Its not quite right w.r.t. GC yet (but I think I can fix, granted I'm a noob here), but curious if something like this would be useful to have in CUDA, at least as a stopgap until more first-class unified memory support is achieved?","files":[{"id":"F01NSJ0PK1U","created":1614038063,"timestamp":1614038063,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"UUMJUCYRK","editable":false,"size":105967,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01NSJ0PK1U/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01NSJ0PK1U/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01NSJ0PK1U-651f9c7346/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01NSJ0PK1U-651f9c7346/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01NSJ0PK1U-651f9c7346/image_360.png","thumb_360_w":360,"thumb_360_h":331,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01NSJ0PK1U-651f9c7346/image_480.png","thumb_480_w":480,"thumb_480_h":441,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01NSJ0PK1U-651f9c7346/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01NSJ0PK1U-651f9c7346/image_720.png","thumb_720_w":720,"thumb_720_h":662,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01NSJ0PK1U-651f9c7346/image_800.png","thumb_800_w":800,"thumb_800_h":735,"original_w":892,"original_h":820,"thumb_tiny":"AwAsADC+3BpOc8Zp31Umlz7GgBoBPXNOx7mj8DS0AN2+5pcClooAbjJ6GlAApMZPQ0oGKAFzRmjNGaADNGaM0ZoAbjJzg0oGBSY+Y0qDigBc0ZpaKAEzRmlooA//2Q==","permalink":"https://julialang.slack.com/files/UUMJUCYRK/F01NSJ0PK1U/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01NSJ0PK1U-33a829397b","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"QYmt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I wrote this tiny wrapper which \"adapts\" an array to unified memory instead of per-device memory. Its not quite right w.r.t. GC yet (but I think I can fix, granted I'm a noob here), but curious if something like this would be useful to have in CUDA, at least as a stopgap until more first-class unified memory support is achieved?"}]}]}],"user":"UUMJUCYRK","display_as_bot":false,"ts":"1614038094.011300"},{"client_msg_id":"7223d48a-7572-4c62-8f98-32f6005d5b98","type":"message","text":"Hi, can someone enable buildkite for <https://github.com/JuliaFolds/FoldsKernelAbstractions.jl>? (ping <@U68A3ASP9>, <@U67BJLYCS>, <@U7THT3TM3>)","user":"UC7AF7NSU","ts":"1614038411.012100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+O+M4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, can someone enable buildkite for "},{"type":"link","url":"https://github.com/JuliaFolds/FoldsKernelAbstractions.jl"},{"type":"text","text":"? (ping "},{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":", "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":", "},{"type":"user","user_id":"U7THT3TM3"},{"type":"text","text":")"}]}]}]},{"client_msg_id":"2f34f1d9-a7bc-4b25-a8f1-cb879f63edf9","type":"message","text":"also, can I access both Nividia and AMD machines from this repo? if that's not possible, accessing only Nvidia machine is still great","user":"UC7AF7NSU","ts":"1614038979.013600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Huj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"also, can I access both Nividia and AMD machines from this repo? if that's not possible, accessing only Nvidia machine is still great"}]}]}],"thread_ts":"1614038979.013600","reply_count":1,"reply_users_count":1,"latest_reply":"1614039098.013700","reply_users":["U6A0PD8CR"],"subscribed":false},{"client_msg_id":"2c697392-2b59-4f44-bd68-4601ea1ca546","type":"message","text":"Julian is working on it now","user":"UC7AF7NSU","ts":"1614039673.014100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tGOn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Julian is working on it now"}]}]}]},{"client_msg_id":"bd5b7d0c-99a7-4c98-ba8f-9b3d2da564ef","type":"message","text":"We still have trouble fully enabling it: <https://julialang.slack.com/archives/C689Y34LE/p1614041651014200?thread_ts=1614038411.012100&amp;cid=C689Y34LE>","user":"UC7AF7NSU","ts":"1614041979.014800","team":"T68168MUP","attachments":[{"from_url":"https://julialang.slack.com/archives/C689Y34LE/p1614041651014200?thread_ts=1614038411.012100&amp;cid=C689Y34LE","fallback":"[February 22nd, 2021 4:54 PM] jpsamaroo: I'm not an admin, so I can't make the pipeline public. Can one of the above-mentioned admins make it so?","ts":"1614041651.014200","author_id":"U6A0PD8CR","author_subname":"Julian Samaroo","channel_id":"C689Y34LE","channel_name":"gpu","is_msg_unfurl":true,"is_reply_unfurl":true,"text":"I'm not an admin, so I can't make the pipeline public. Can one of the above-mentioned admins make it so?","author_name":"Julian Samaroo","author_link":"https://julialang.slack.com/team/U6A0PD8CR","author_icon":"https://avatars.slack-edge.com/2017-07-18/215660769271_7ef635ba687405f810d2_48.jpg","mrkdwn_in":["text"],"id":1,"original_url":"https://julialang.slack.com/archives/C689Y34LE/p1614041651014200?thread_ts=1614038411.012100&amp;cid=C689Y34LE","footer":"From a thread in #gpu"}],"blocks":[{"type":"rich_text","block_id":"jyZ3s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We still have trouble fully enabling it: "},{"type":"link","url":"https://julialang.slack.com/archives/C689Y34LE/p1614041651014200?thread_ts=1614038411.012100&cid=C689Y34LE"}]}]}]},{"client_msg_id":"c17b295b-3668-48ef-833d-25aa750335ab","type":"message","text":"I'm trying to skip GPU CI on buildkite with PR labels. I'm trying\n\n```if: |\n  !(\n      build.pull_request.labels includes \"no cuda\" ||\n      build.pull_request.labels includes \"no gpu\"\n  )```\nBut this doesn't work. Does anyone make similar thing work? Is `if: build.message !~ /\\[skip tests\\]/` more reliable?\n\n<https://github.com/JuliaFolds/FoldsKernelAbstractions.jl/pull/5>","user":"UC7AF7NSU","ts":"1614047660.017400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Q=WjT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm trying to skip GPU CI on buildkite with PR labels. I'm trying\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"if: |\n  !(\n      build.pull_request.labels includes \"no cuda\" ||\n      build.pull_request.labels includes \"no gpu\"\n  )"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nBut this doesn't work. Does anyone make similar thing work? Is "},{"type":"text","text":"if: build.message !~ /\\[skip tests\\]/","style":{"code":true}},{"type":"text","text":" more reliable?\n\n"},{"type":"link","url":"https://github.com/JuliaFolds/FoldsKernelAbstractions.jl/pull/5"}]}]}]},{"client_msg_id":"f51ee45f-e7b8-44ee-a9db-86d5371442a9","type":"message","text":"I think I understood the strategy for performant matmul here (<https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/examples/performant_matmul.jl|https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/examples/performant_matmul.jl>), that example is very instructive. I'm trying to wrap my head around whether strategy is also effective for things like `conv`.\n\nIn particular, would one just use a similar tiling strategy summing over `tile1[i, k] * tile2[k, j]`, where there are as many values of `k` as there are \"compatible pixels\" (size filter times size image), or is it more effective to make fewer tiles and sum over `tile1[i, k] * tile2[k + s, j]` for `s` varying among the indices of the filter?","user":"U6BJ9E351","ts":"1614074563.028000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KSRmu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think I understood the strategy for performant matmul here ("},{"type":"link","url":"https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/examples/performant_matmul.jl","text":"https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/examples/performant_matmul.jl"},{"type":"text","text":"), that example is very instructive. I'm trying to wrap my head around whether strategy is also effective for things like "},{"type":"text","text":"conv","style":{"code":true}},{"type":"text","text":".\n\nIn particular, would one just use a similar tiling strategy summing over "},{"type":"text","text":"tile1[i, k] * tile2[k, j]","style":{"code":true}},{"type":"text","text":", where there are as many values of "},{"type":"text","text":"k","style":{"code":true}},{"type":"text","text":" as there are \"compatible pixels\" (size filter times size image), or is it more effective to make fewer tiles and sum over "},{"type":"text","text":"tile1[i, k] * tile2[k + s, j]","style":{"code":true}},{"type":"text","text":" for "},{"type":"text","text":"s ","style":{"code":true}},{"type":"text","text":"varying among the indices of the filter?"}]}]}]},{"client_msg_id":"8a932b1e-211c-4c20-8e07-9082e0f9ee15","type":"message","text":"The first strategy (pretend it's `matmul`) sounds a bit naive, but given that the \"copying the tiles\" part has lower computational complexity, I was wondering whether it was enough to get things packed together to speed up the multiplication part (with `@simd` and `@inbounds`). (I hope this is not too fuzzy/unclear!)","user":"U6BJ9E351","ts":"1614074781.032800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9fSC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The first strategy (pretend it's "},{"type":"text","text":"matmul","style":{"code":true}},{"type":"text","text":") sounds a bit naive, but given that the \"copying the tiles\" part has lower computational complexity, I was wondering whether it was enough to get things packed together to speed up the multiplication part (with "},{"type":"text","text":"@simd","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"@inbounds","style":{"code":true}},{"type":"text","text":"). (I hope this is not too fuzzy/unclear!)"}]}]}],"thread_ts":"1614074781.032800","reply_count":1,"reply_users_count":1,"latest_reply":"1614075180.032900","reply_users":["U6BJ9E351"],"subscribed":false},{"client_msg_id":"5dcf9bb3-b392-41fe-8315-956730d923aa","type":"message","text":"<@UC7AF7NSU> does tapir have any implications for gpu / accelerator programming ?","user":"UDGT4PM41","ts":"1614091461.034000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"AWvNU","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UC7AF7NSU"},{"type":"text","text":" does tapir have any implications for gpu / accelerator programming ?"}]}]}],"thread_ts":"1614091461.034000","reply_count":5,"reply_users_count":2,"latest_reply":"1614092106.035100","reply_users":["UC7AF7NSU","UDGT4PM41"],"subscribed":false},{"client_msg_id":"84de9945-58a8-4479-a59e-35a8337b1bf9","type":"message","text":"I've been trying to get one-GPU-per-thread parallelization working (mainly to avoid paying the N-process Julia startup cost of the standard one-GPU-per-process way which is otherwise very robust). Between unified memory, <https://github.com/JuliaGPU/CUDA.jl/issues/731|#731> getting fixed, and recent stuff on master, quite a lot is working well! I'm stuck on the following thing though, wondering if I might get some hints of what might be going wrong that could help get a MWE. The general outline of what I'm running is\n```x = move_to_unified_memory(CuArray(...))\n\nThreads.@sync begin\n    Threads.@spawn begin\n        device!(0)\n        some_gpu_code(x)\n    end\n    Threads.@spawn begin\n        device!(1)\n        some_gpu_code(x)\n    end\nend```\nFor `some_gpu_code` being simple FFTs or array broadcasts, it works. For my more complex actual calculations (which in theory should just be a whole bunch of these chained together in some complex way), I get the segfault I attach in the thread. Any ideas? This is CUDA#master.","user":"UUMJUCYRK","ts":"1614162324.041500","team":"T68168MUP","edited":{"user":"UUMJUCYRK","ts":"1614162388.000000"},"blocks":[{"type":"rich_text","block_id":"/hNZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I've been trying to get one-GPU-per-thread parallelization working (mainly to avoid paying the N-process Julia startup cost of the standard one-GPU-per-process way which is otherwise very robust). Between unified memory, "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/issues/731","text":"#731"},{"type":"text","text":" getting fixed, and recent stuff on master, quite a lot is working well! I'm stuck on the following thing though, wondering if I might get some hints of what might be going wrong that could help get a MWE. The general outline of what I'm running is\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"x = move_to_unified_memory(CuArray(...))\n\nThreads.@sync begin\n    Threads.@spawn begin\n        device!(0)\n        some_gpu_code(x)\n    end\n    Threads.@spawn begin\n        device!(1)\n        some_gpu_code(x)\n    end\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"For "},{"type":"text","text":"some_gpu_code","style":{"code":true}},{"type":"text","text":" being simple FFTs or array broadcasts, it works. For my more complex actual calculations (which in theory should just be a whole bunch of these chained together in some complex way), I get the segfault I attach in the thread. Any ideas? This is CUDA#master."}]}]}],"thread_ts":"1614162324.041500","reply_count":18,"reply_users_count":2,"latest_reply":"1614164847.046500","reply_users":["UUMJUCYRK","U68A3ASP9"],"subscribed":false},{"client_msg_id":"4a863dbc-d55a-48d0-bf3f-e1f93f5786d5","type":"message","text":"Is there a straightforward way to get poisson samples in GPU kernels?\n\nI know that `CUDA.rand_poisson!` exists but that seems to be host only. I saw the example by <@UC7AF7NSU> that uses `Random123.jl` to get random numbers in kernels, and that works for simple things like samples from geometric distributions, but it seems like sampling from poisson efficiently is quite complex, so I am wondering if this has already been done somewhere.","user":"U011V2YN59N","ts":"1614170758.051000","team":"T68168MUP","edited":{"user":"U011V2YN59N","ts":"1614172157.000000"},"blocks":[{"type":"rich_text","block_id":"Cyq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a straightforward way to get poisson samples in GPU kernels?\n\nI know that "},{"type":"text","text":"CUDA.rand_poisson!","style":{"code":true}},{"type":"text","text":" exists but that seems to be host only. I saw the example by "},{"type":"user","user_id":"UC7AF7NSU"},{"type":"text","text":" that uses "},{"type":"text","text":"Random123.jl","style":{"code":true}},{"type":"text","text":" to get random numbers in kernels, and that works for simple things like samples from geometric distributions, but it seems like sampling from poisson efficiently is quite complex, so I am wondering if this has already been done somewhere."}]}]}]},{"client_msg_id":"2d66fbac-50b2-4758-930a-8c677ae1f953","type":"message","text":"I couldn't get the poisson sampler in Distributions.jl to work out of the box, either, because it depends on `randexp`, which uses the tables for normal variates in `Base`, which are not `CuArrays`. I suppose I could just copy and paste all that with tweaks, but again I am wondering if this has been done somewhere.","user":"U011V2YN59N","ts":"1614172274.052700","team":"T68168MUP","edited":{"user":"U011V2YN59N","ts":"1614172302.000000"},"blocks":[{"type":"rich_text","block_id":"tYvF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I couldn't get the poisson sampler in Distributions.jl to work out of the box, either, because it depends on "},{"type":"text","text":"randexp","style":{"code":true}},{"type":"text","text":", which uses the tables for normal variates in "},{"type":"text","text":"Base","style":{"code":true}},{"type":"text","text":", which are not "},{"type":"text","text":"CuArrays","style":{"code":true}},{"type":"text","text":". I suppose I could just copy and paste all that with tweaks, but again I am wondering if this has been done somewhere."}]}]}]},{"type":"message","text":"<https://github.com/vosen/ZLUDA> That looks fun.","user":"U9MD78Z9N","ts":"1614258546.054500","team":"T68168MUP","thread_ts":"1614258546.054500","reply_count":1,"reply_users_count":1,"latest_reply":"1614258962.054600","reply_users":["U01C3624SGJ"],"subscribed":false},{"client_msg_id":"093682f8-1f50-45dc-bf1b-37def0aa93af","type":"message","text":"Tried to run buildkite with julia v1.4; is this even possible?\nI get an error that I can’t interpret. I guess the error is clear that I need to file an issue :wink:\n`LoadError: CUDA.jl does not yet support CUDA with nvdisasm 11.1.74; please file an issue.`\n\n<https://buildkite.com/julialang/geophysicalflows-dot-jl/builds/105#cae0280c-c34c-49e8-b75c-f718b2d4f2b5>","user":"UBY3ENVBK","ts":"1614503194.060000","team":"T68168MUP","attachments":[{"service_name":"Buildkite","title":"GeophysicalFlows.jl #105 · JuliaLang","title_link":"https://buildkite.com/julialang/geophysicalflows-dot-jl/builds/105#cae0280c-c34c-49e8-b75c-f718b2d4f2b5","text":"julia 1.4","fallback":"Buildkite: GeophysicalFlows.jl #105 · JuliaLang","thumb_url":"https://avatars.githubusercontent.com/u/7112768?v=4","from_url":"https://buildkite.com/julialang/geophysicalflows-dot-jl/builds/105#cae0280c-c34c-49e8-b75c-f718b2d4f2b5","thumb_width":301,"thumb_height":301,"service_icon":"https://buildkite.com/favicon.ico","id":1,"original_url":"https://buildkite.com/julialang/geophysicalflows-dot-jl/builds/105#cae0280c-c34c-49e8-b75c-f718b2d4f2b5"}],"blocks":[{"type":"rich_text","block_id":"9Lw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Tried to run buildkite with julia v1.4; is this even possible?\nI get an error that I can’t interpret. I guess the error is clear that I need to file an issue "},{"type":"emoji","name":"wink"},{"type":"text","text":"\n"},{"type":"text","text":"LoadError: CUDA.jl does not yet support CUDA with nvdisasm 11.1.74; please file an issue.","style":{"code":true}},{"type":"text","text":"\n\n"},{"type":"link","url":"https://buildkite.com/julialang/geophysicalflows-dot-jl/builds/105#cae0280c-c34c-49e8-b75c-f718b2d4f2b5"}]}]}]},{"client_msg_id":"86de8e28-e2cc-405a-99e7-049a87e51a30","type":"message","text":"Old version of CUDA.jl. Either upgrade or use the cuda tag to request an older version. You can also force it to use artifacts, that's normally disabled on ci to save on network traffic.","user":"U68A3ASP9","ts":"1614506110.062200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"77NEA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Old version of CUDA.jl. Either upgrade or use the cuda tag to request an older version. You can also force it to use artifacts, that's normally disabled on ci to save on network traffic."}]}]}]},{"client_msg_id":"dc5774a9-56b1-49e2-97d1-81f051c7ccde","type":"message","text":"Biweekly office hours happening in 45minutes","user":"U67BJLYCS","ts":"1614615460.066100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lZs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Biweekly office hours happening in 45minutes"}]}]}],"thread_ts":"1614615460.066100","reply_count":3,"reply_users_count":3,"latest_reply":"1614615552.067800","reply_users":["U9MD78Z9N","U6A0PD8CR","U67BJLYCS"],"subscribed":false},{"client_msg_id":"bf1c5ff4-69f3-4e94-a43e-15ac6cec72bc","type":"message","text":"link in the channel description","user":"U67BJLYCS","ts":"1614615468.066400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JLP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"link in the channel description"}]}]}]},{"client_msg_id":"c143f3ed-5e3e-4b86-a4f7-bb0be7269146","type":"message","text":"I use `GPUArrays.default_rgn(CuArray)` to generate random numbers for a `CuArray` and found that it can only generate a maximum of 256 random numbers at one time. Starting form 257th, it’s the repeat of the first 256 numbers. Below is an example:\n```using CUDA, Random\nusing CUDA: GPUArrays\na = zeros(256*3) |&gt; CuArray\nrand!(GPUArrays.default_rng(CuArray), a)\na[1:256] == a[257:512] # true\na[1:256] == a[513:end] # true```\nI’m wondering is there a way to avoid this?","user":"U019U0QCF7F","ts":"1614615646.069100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7Oks","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I use "},{"type":"text","text":"GPUArrays.default_rgn(CuArray)","style":{"code":true}},{"type":"text","text":" to generate random numbers for a "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":" and found that it can only generate a maximum of 256 random numbers at one time. Starting form 257th, it’s the repeat of the first 256 numbers. Below is an example:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using CUDA, Random\nusing CUDA: GPUArrays\na = zeros(256*3) |> CuArray\nrand!(GPUArrays.default_rng(CuArray), a)\na[1:256] == a[257:512] # true\na[1:256] == a[513:end] # true"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I’m wondering is there a way to avoid this?"}]}]}]},{"client_msg_id":"e777fe10-f572-4a87-9d0a-2cc2cd42a3c7","type":"message","text":"<@U019U0QCF7F> Not sure about `GPUArrays.default_rgn` but why not use `Base.rand` and `Base.randn`? Could just do\n```a = rand(256*3) |&gt; CuArray```","user":"UEP056STX","ts":"1614616926.070000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Gj00","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U019U0QCF7F"},{"type":"text","text":" Not sure about "},{"type":"text","text":"GPUArrays.default_rgn","style":{"code":true}},{"type":"text","text":" but why not use "},{"type":"text","text":"Base.rand","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"Base.randn","style":{"code":true}},{"type":"text","text":"? Could just do\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"a = rand(256*3) |> CuArray"}]}]}],"thread_ts":"1614616926.070000","reply_count":1,"reply_users_count":1,"latest_reply":"1614618632.070500","reply_users":["U019U0QCF7F"],"subscribed":false},{"client_msg_id":"a7e1e627-b461-41a0-af1e-9dadfec34f3a","type":"message","text":"Currently the only really robust way to do multi-GPU is with separate processes controlling each GPU. One downside is that transferring memory between GPUs has to go through the CPU as its serialized/deserialized by Julia (unless I'm missing something). Is there any other way to do transfers between GPUs? Ideally what I'm looking for is something like the pseudo-code:\n```device!(0)\nx = unified_memory(CuArray(...))\nx_handle = some_sort_of_handle(x)\n\npmap(workers()) do i\n    device!(i)\n    local_x = CuArray(...)\n    copy_to!(local_x, x_handle)\n    ...\nend```\nI started reading about CUDA MPS but I don't know enough if its the right thing to devote more time to understanding, or if it can be used from CUDA.jl at all. I'm also aware of CUDA-aware MPI, but since in my case most of my work is from Jupyter notebooks, I don't think it can really work. I've also been playing alot lately with threads instead of processes, but that doesn't seem stable enough to rely on quite yet. Curious if there's some other (easy?) option with processes I'm missing?","user":"UUMJUCYRK","ts":"1614672449.074900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gLvc3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Currently the only really robust way to do multi-GPU is with separate processes controlling each GPU. One downside is that transferring memory between GPUs has to go through the CPU as its serialized/deserialized by Julia (unless I'm missing something). Is there any other way to do transfers between GPUs? Ideally what I'm looking for is something like the pseudo-code:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"device!(0)\nx = unified_memory(CuArray(...))\nx_handle = some_sort_of_handle(x)\n\npmap(workers()) do i\n    device!(i)\n    local_x = CuArray(...)\n    copy_to!(local_x, x_handle)\n    ...\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I started reading about CUDA MPS but I don't know enough if its the right thing to devote more time to understanding, or if it can be used from CUDA.jl at all. I'm also aware of CUDA-aware MPI, but since in my case most of my work is from Jupyter notebooks, I don't think it can really work. I've also been playing alot lately with threads instead of processes, but that doesn't seem stable enough to rely on quite yet. Curious if there's some other (easy?) option with processes I'm missing?"}]}]}],"thread_ts":"1614672449.074900","reply_count":1,"reply_users_count":1,"latest_reply":"1614672901.075000","reply_users":["UMY1LV01G"],"subscribed":false},{"client_msg_id":"3b998b4d-8ffb-44b4-b9e2-d9fafeb5ed17","type":"message","text":"<https://www.phoronix.com/scan.php?page=news_item&amp;px=Intel-2021-LLVM-SPIR-V-Backend> no more llvm-spirv translator and easy Vulkan codegen in GPUCompiler, perhaps?","user":"UMY1LV01G","ts":"1614732027.081300","team":"T68168MUP","attachments":[{"title":"Intel Looking To Upstream A Proper SPIR-V Compute Back-End For LLVM - Phoronix","title_link":"https://www.phoronix.com/scan.php?page=news_item&px=Intel-2021-LLVM-SPIR-V-Backend","text":"Phoronix is the leading technology website for Linux hardware reviews, open-source news, Linux benchmarks, open-source benchmarks, and computer hardware tests.","fallback":"Intel Looking To Upstream A Proper SPIR-V Compute Back-End For LLVM - Phoronix","from_url":"https://www.phoronix.com/scan.php?page=news_item&px=Intel-2021-LLVM-SPIR-V-Backend","service_icon":"https://www.phoronix.com/apple-touch-icon-57x57.png","service_name":"phoronix.com","id":1,"original_url":"https://www.phoronix.com/scan.php?page=news_item&amp;px=Intel-2021-LLVM-SPIR-V-Backend"}],"blocks":[{"type":"rich_text","block_id":"5f9Nn","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://www.phoronix.com/scan.php?page=news_item&px=Intel-2021-LLVM-SPIR-V-Backend"},{"type":"text","text":" no more llvm-spirv translator and easy Vulkan codegen in GPUCompiler, perhaps?"}]}]}]},{"client_msg_id":"1d5bc48f-64e7-450e-b002-0ccf36b7c5a1","type":"message","text":"are enums, defined and used on the host, supported on the device via Kernel Abstractions?","user":"U0178LR5K7F","ts":"1614742571.082600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RY+T","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"are enums, defined and used on the host, supported on the device via Kernel Abstractions?"}]}]}]},{"client_msg_id":"fa8c3550-6ec8-4da3-847f-820a1564dbbc","type":"message","text":"Enums are just constants so yeah","user":"U67BJLYCS","ts":"1614745417.083000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"y2I","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Enums are just constants so yeah"}]}]}]},{"client_msg_id":"fd700671-6c27-457e-8628-44093645f215","type":"message","text":"I am working on isolating the problem to get a MWE but this is the error I am seeing\n```ERROR: GPUCompiler.jl encountered an unexpected internal error.\nPlease file an issue attaching the following information, including the backtrace,\nas well as a reproducible example (if possible).\n\nInternalCompilerError: length(frames) == 1, at /home/aleonard31/.julia/packages/GPUCompiler/uTpNx/src/validation.jl:246```\nif I comment out a line using an enum as an argument I do not see a error","user":"U0178LR5K7F","ts":"1614748130.084700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6JI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am working on isolating the problem to get a MWE but this is the error I am seeing\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: GPUCompiler.jl encountered an unexpected internal error.\nPlease file an issue attaching the following information, including the backtrace,\nas well as a reproducible example (if possible).\n\nInternalCompilerError: length(frames) == 1, at /home/aleonard31/.julia/packages/GPUCompiler/uTpNx/src/validation.jl:246"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"if I comment out a line using an enum as an argument I do not see a error"}]}]}]},{"client_msg_id":"e9489e86-f532-47de-b8e2-b97826d7ce9b","type":"message","text":"I have a Flux model that is a bit too big to run on one GPU. I could split it into an encoder and decoder part, and the data that would need to be sent between them could potentially be quite small, like a 1000^2 matrix at the most. What should I be using to sync data back and forth?","user":"U6C8RR26P","ts":"1614785827.088600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Wh6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a Flux model that is a bit too big to run on one GPU. I could split it into an encoder and decoder part, and the data that would need to be sent between them could potentially be quite small, like a 1000^2 matrix at the most. What should I be using to sync data back and forth?"}]}]}]},{"client_msg_id":"c0a8ec57-5608-46e5-a6d6-474f28700608","type":"message","text":"Hi, I am broadcasting a `CuArray` over a function that has some trig functions in it and I am seeing\n```┌ Warning: calls to Base intrinsics might be GPU incompatible\n│   exception =\n│    You called sin(x::T) where T&lt;:Union{Float32, Float64} in Base.Math at special/trig.jl:29, maybe you intended to call sin(x::Float32) in CUDA at /home/aleonard31/.julia/packages/CUDA/wTQsK/src/device/intrinsics/math.jl:13 instead?\n│    Stacktrace:\n│     [1] sin at special/trig.jl:29\n│     [2] broadcast_kernel at /home/aleonard31/.julia/packages/GPUArrays/WV76E/src/host/broadcast.jl:59```\nis there a way to handle this without changing the sin/cos to CUDA.sin/CUDA.cos?","user":"U0178LR5K7F","ts":"1614797995.090100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2al","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I am broadcasting a "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":" over a function that has some trig functions in it and I am seeing\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"┌ Warning: calls to Base intrinsics might be GPU incompatible\n│   exception =\n│    You called sin(x::T) where T<:Union{Float32, Float64} in Base.Math at special/trig.jl:29, maybe you intended to call sin(x::Float32) in CUDA at /home/aleonard31/.julia/packages/CUDA/wTQsK/src/device/intrinsics/math.jl:13 instead?\n│    Stacktrace:\n│     [1] sin at special/trig.jl:29\n│     [2] broadcast_kernel at /home/aleonard31/.julia/packages/GPUArrays/WV76E/src/host/broadcast.jl:59"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"is there a way to handle this without changing the sin/cos to CUDA.sin/CUDA.cos?"}]}]}],"thread_ts":"1614797995.090100","reply_count":3,"reply_users_count":2,"latest_reply":"1614798487.090800","reply_users":["U0178LR5K7F","U6A0PD8CR"],"subscribed":false},{"client_msg_id":"507b3049-3add-4363-a55a-388a74e55b76","type":"message","text":"Pytorch 1.8 has ROCm support apparently","user":"UDGT4PM41","ts":"1614949390.094500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Hth9u","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Pytorch 1.8 has ROCm support apparently"}]}]}]},{"client_msg_id":"e797ef8d-74a6-406c-b405-8250132c4865","type":"message","text":"Do GPU libraries use something like `GC.@preserve`?","user":"UCLGS1HML","ts":"1614954013.095600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0LNV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do GPU libraries use something like "},{"type":"text","text":"GC.@preserve","style":{"code":true}},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"85ff17f6-1851-4ea7-b94b-01b77fb8b428","type":"message","text":"generally not, since GPU operations are stream-ordered so the free can't happen before e.g. the kernel finished executing. but we'll probably have to be more careful about that in the future.","user":"U68A3ASP9","ts":"1614954202.096500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dHIr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"generally not, since GPU operations are stream-ordered so the free can't happen before e.g. the kernel finished executing. but we'll probably have to be more careful about that in the future."}]}]}]},{"client_msg_id":"9f26bb81-66d7-40d7-a74b-b4537631fb04","type":"message","text":"Thanks! I’m looking through the CUDA.jl docs and didn’t find anything obvious.\n\n&gt;  but we’ll probably have to be more careful about that in the future\nDoes this mean that you will have your own thing like `CUDA.@preserve x op(pointer(x))` for CUDA’s\nown garbage collector or does this just mean that you plan on strictly ensuring garbage collection\ndoesn’t occur within a kernel?","user":"UCLGS1HML","ts":"1614955090.096800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bYy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks! I’m looking through the CUDA.jl docs and didn’t find anything obvious.\n\n"}]},{"type":"rich_text_quote","elements":[{"type":"text","text":" but we’ll probably have to be more careful about that in the future"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nDoes this mean that you will have your own thing like "},{"type":"text","text":"CUDA.@preserve x op(pointer(x))","style":{"code":true}},{"type":"text","text":" for CUDA’s\nown garbage collector or does this just mean that you plan on strictly ensuring garbage collection\ndoesn’t occur within a kernel?"}]}]}]},{"client_msg_id":"c5046b5b-9a2c-49c8-845b-91955fd94a9a","type":"message","text":"CUDA.jl doesn't have its own garbage collector, on the host we use Julia's GC and in kernels we currently don't really support dynamic allocations. and currently GC can trigger during kernel execution, but that will just enqueue a `free` operation which can only execute after the kernel is done, so I haven't thought of an API to deal with the scenario where that would lead to bad results.","user":"U68A3ASP9","ts":"1614957356.099800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pIQ+r","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CUDA.jl doesn't have its own garbage collector, on the host we use Julia's GC and in kernels we currently don't really support dynamic allocations. and currently GC can trigger during kernel execution, but that will just enqueue a "},{"type":"text","text":"free","style":{"code":true}},{"type":"text","text":" operation which can only execute after the kernel is done, so I haven't thought of an API to deal with the scenario where that would lead to bad results."}]}]}]},{"client_msg_id":"86db38d0-6b4e-4407-ab74-96f21e4f5084","type":"message","text":"We’re trying to figure out a generic interface for this sort of thing in ArrayInterface.jl and would like it to eventually be useful for GPUs too (<https://github.com/JuliaArrays/ArrayInterface.jl/issues/104>\n, <https://github.com/JuliaArrays/ArrayInterface.jl/issues/130>).\nThe idea is that we could have something that is one step back from `Libc.malloc` and do something like...\n\n```buffer = allocate(...)\nptr = pointer(buffer)\nGC.@preserve buffer f(ptr)  # `f` is the actually function body that users care about\ndereference(buffer)```\nThere’s a lot of abstraction here (`ptr` could still be carrying info about the size, strides, etc.).\nI think we’ll need to figure out exactly what info needs to be propagated between methods before this thing really materializes though.","user":"UCLGS1HML","ts":"1614957702.100100","team":"T68168MUP","edited":{"user":"UCLGS1HML","ts":"1614957750.000000"},"blocks":[{"type":"rich_text","block_id":"RckLZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We’re trying to figure out a generic interface for this sort of thing in ArrayInterface.jl and would like it to eventually be useful for GPUs too ("},{"type":"link","url":"https://github.com/JuliaArrays/ArrayInterface.jl/issues/104"},{"type":"text","text":"\n, "},{"type":"link","url":"https://github.com/JuliaArrays/ArrayInterface.jl/issues/130"},{"type":"text","text":").\nThe idea is that we could have something that is one step back from "},{"type":"text","text":"Libc.malloc","style":{"code":true}},{"type":"text","text":" and do something like...\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"buffer = allocate(...)\nptr = pointer(buffer)\nGC.@preserve buffer f(ptr)  # `f` is the actually function body that users care about\ndereference(buffer)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"There’s a lot of abstraction here ("},{"type":"text","text":"ptr","style":{"code":true}},{"type":"text","text":" could still be carrying info about the size, strides, etc.).\nI think we’ll need to figure out exactly what info needs to be propagated between methods before this thing really materializes though."}]}]}]},{"client_msg_id":"b5ff0c13-ed9c-4e2c-be98-4feef14a5e72","type":"message","text":"oh, interesting. that might be useful for the GPU stack indeed; CUDA.jl's current Buffer abstraction is pretty ad hoc. it does need to deal with a variety of types of buffers though (some typed, others untyped, etc), so it might be hard to generalize.","user":"U68A3ASP9","ts":"1614957978.101800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"XOWc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"oh, interesting. that might be useful for the GPU stack indeed; CUDA.jl's current Buffer abstraction is pretty ad hoc. it does need to deal with a variety of types of buffers though (some typed, others untyped, etc), so it might be hard to generalize."}]}]}]},{"client_msg_id":"9175dc27-500d-4907-a109-76c32b580365","type":"message","text":"most of that is in here: <https://github.com/JuliaGPU/CUDA.jl/blob/master/lib/cudadrv/memory.jl>","user":"U68A3ASP9","ts":"1614958001.102000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6xd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"most of that is in here: "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/blob/master/lib/cudadrv/memory.jl"}]}]}]},{"client_msg_id":"5eb1c431-6f30-40c9-8f15-018d376da239","type":"message","text":"oneAPI's is here: <https://github.com/JuliaGPU/oneAPI.jl/blob/master/lib/level-zero/memory.jl>. pretty similar, but as always with some subtle differences.","user":"U68A3ASP9","ts":"1614958073.102900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3Rcj0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"oneAPI's is here: "},{"type":"link","url":"https://github.com/JuliaGPU/oneAPI.jl/blob/master/lib/level-zero/memory.jl"},{"type":"text","text":". pretty similar, but as always with some subtle differences."}]}]}]},{"client_msg_id":"665bf21d-a566-409d-9d49-aeb406c7ab03","type":"message","text":"Thanks! I’ll take a look at those.\n\n&gt; it does need to deal with a variety of types of buffers though\nI anticipate there needing to be a lot of wiggle room here.\nJust in terms of CPUs we need completely unique buffers that manage mutability/immutability and static/dynamic/variable sizes.\nThis has the potential to be a very deep interface so we’re just trying to get the most superficial and generic methods in place.\nHopefully this will let us hack on the more intricate details for a long time (optimizing GC, threading, bit masks etc.) while allowing users a consistent experience.","user":"UCLGS1HML","ts":"1614958530.104300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2VLE8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks! I’ll take a look at those.\n\n"}]},{"type":"rich_text_quote","elements":[{"type":"text","text":"it does need to deal with a variety of types of buffers though"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nI anticipate there needing to be a lot of wiggle room here.\nJust in terms of CPUs we need completely unique buffers that manage mutability/immutability and static/dynamic/variable sizes.\nThis has the potential to be a very deep interface so we’re just trying to get the most superficial and generic methods in place.\nHopefully this will let us hack on the more intricate details for a long time (optimizing GC, threading, bit masks etc.) while allowing users a consistent experience."}]}]}]},{"client_msg_id":"71318651-33ac-4e6f-ab97-03070dc7cefe","type":"message","text":"Is there an option to restrict the maximum number of registers used in a kernel like -maxrregcount option for nvcc?","user":"U01FXSDEXN3","ts":"1614959005.105400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Wg7X2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there an option to restrict the maximum number of registers used in a kernel like -maxrregcount option for nvcc?"}]}]}]},{"client_msg_id":"fae17c2c-e70b-4fc9-a5eb-6e9b6be8c39c","type":"message","text":"yes, see <https://juliagpu.github.io/CUDA.jl/stable/api/compiler/#CUDA.cufunction>. you can pass these args to `@cuda`","user":"U68A3ASP9","ts":"1614959061.105700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wpD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yes, see "},{"type":"link","url":"https://juliagpu.github.io/CUDA.jl/stable/api/compiler/#CUDA.cufunction"},{"type":"text","text":". you can pass these args to "},{"type":"text","text":"@cuda","style":{"code":true}}]}]}]},{"client_msg_id":"8db22a36-e7c1-406a-992f-7185ffdda52d","type":"message","text":"Thanks!","user":"U01FXSDEXN3","ts":"1614959090.105900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Gii","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks!"}]}]}]},{"client_msg_id":"544c6a15-cbc7-49f2-ad60-a689f8a41921","type":"message","text":"when creating kernels with KernelAbstractions, does KernelAbstractions figure out on its own whether there are  enough blocks on the GPU to fill the `ndrange` (and otherwise each \"physical block\" sequentially works on several blocks in the partition), or is it up to each individual kernel to deal with this?","user":"U6BJ9E351","ts":"1614961295.108100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"v3i","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"when creating kernels with KernelAbstractions, does KernelAbstractions figure out on its own whether there are  enough blocks on the GPU to fill the "},{"type":"text","text":"ndrange","style":{"code":true}},{"type":"text","text":" (and otherwise each \"physical block\" sequentially works on several blocks in the partition), or is it up to each individual kernel to deal with this?"}]}]}]},{"client_msg_id":"f9e3c26f-bab3-4ba5-a617-f043687467cf","type":"message","text":"You can oversubscribe","user":"U67BJLYCS","ts":"1614963149.108400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UaWp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You can oversubscribe"}]}]}]},{"client_msg_id":"81c3fb80-3199-4ed1-9471-9c07fad4008c","type":"message","text":"There are some limits with how big a CUDA grid can be","user":"U67BJLYCS","ts":"1614963182.109000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TSb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"There are some limits with how big a CUDA grid can be"}]}]}]},{"client_msg_id":"ae697332-1f13-45a6-b8aa-2652930efd40","type":"message","text":"cool! I'm getting used to the `ndrange` system and it's actually pretty nice that it handles a fair amount of stuff automatically. Slightly unrelated, I also find it nice that I can use `groupsize` longer than 3, say `(1, 1, 32, 32)`, and it doesn't complain: it can simplify indexing a lot in some cases.","user":"U6BJ9E351","ts":"1614963806.111600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MvQU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"cool! I'm getting used to the "},{"type":"text","text":"ndrange","style":{"code":true}},{"type":"text","text":" system and it's actually pretty nice that it handles a fair amount of stuff automatically. Slightly unrelated, I also find it nice that I can use "},{"type":"text","text":"groupsize","style":{"code":true}},{"type":"text","text":" longer than 3, say "},{"type":"text","text":"(1, 1, 32, 32)","style":{"code":true}},{"type":"text","text":", and it doesn't complain: it can simplify indexing a lot in some cases."}]}]}]}]}