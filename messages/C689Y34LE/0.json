{"cursor": 3, "messages": [{"client_msg_id":"688b7b6e-4705-4458-80f2-a3878c5335d0","type":"message","text":"I think we also figured out what the problem could be. The problem is related to the architecture (ppc64le).\nAccording to the documentation of <https://julialang.org/downloads/#currently_supported_platforms>\nCUDA.jl does not support Power 9 AC922. \n\nWe were wondering if we could get some help installing the CUDA.jl. \n\nThank you and sorry for the long message.","user":"U015TV044AV","ts":"1613410443.183500","team":"T68168MUP","edited":{"user":"U015TV044AV","ts":"1613410457.000000"},"blocks":[{"type":"rich_text","block_id":"MRs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think we also figured out what the problem could be. The problem is related to the architecture (ppc64le).\nAccording to the documentation of "},{"type":"link","url":"https://julialang.org/downloads/#currently_supported_platforms"},{"type":"text","text":"\nCUDA.jl does not support Power 9 AC922. \n\nWe were wondering if we could get some help installing the CUDA.jl. \n\nThank you and sorry for the long message."}]}]}]},{"client_msg_id":"6ebc8cd3-1509-4118-86df-34a831f0cf92","type":"message","text":"<@U67BJLYCS> is the person to ask","user":"U674T3KB3","ts":"1613410532.184400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"T9jjE","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":" is the person to ask"}]}]}]},{"client_msg_id":"3dd0b04d-a98a-4068-9e30-bffdd8797c07","type":"message","text":"It definitely works, but there may be quirks, since it's not a mainstream platform","user":"U674T3KB3","ts":"1613410550.185200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SR0dv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It definitely works, but there may be quirks, since it's not a mainstream platform"}]}]}],"reactions":[{"name":"+1","users":["UEP056STX"],"count":1}]},{"client_msg_id":"4909949d-17a9-487f-8993-c53a9f131596","type":"message","text":"<@U015TV044AV> we are running Julia on our local powerpc cluster at MIT","user":"U67BJLYCS","ts":"1613411360.186100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"k9fnJ","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U015TV044AV"},{"type":"text","text":" we are running Julia on our local powerpc cluster at MIT"}]}]}]},{"client_msg_id":"4a799ce3-906d-4833-b8b3-fb142bc02a56","type":"message","text":"You might want to set `export JULIA_CUDA_USE_BINARYBUILDER=false`","user":"U67BJLYCS","ts":"1613411435.186400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FQa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You might want to set "},{"type":"text","text":"export JULIA_CUDA_USE_BINARYBUILDER=false","style":{"code":true}}]}]}]},{"client_msg_id":"74d9644b-8a93-4a18-8c33-748ab200d19b","type":"message","text":"but CUDA_jll 11+ is available for PPC","user":"U67BJLYCS","ts":"1613412120.186900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"X9j","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"but CUDA_jll 11+ is available for PPC"}]}]}]},{"client_msg_id":"d23cc184-bc2e-4b54-b98e-cc1770a247d4","type":"message","text":"So what issue are you running into?","user":"U67BJLYCS","ts":"1613412131.187200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"CBqp1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So what issue are you running into?"}]}]}]},{"client_msg_id":"3382ded7-8fc6-42d3-acab-ba32959d4191","type":"message","text":"<@U68A3ASP9> do I understand correctly that the issue basically is that CUDA 10 only was distributed as an rpm, for PPC?","user":"U67BJLYCS","ts":"1613413774.188000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TwhG","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":" do I understand correctly that the issue basically is that CUDA 10 only was distributed as an rpm, for PPC?"}]}]}]},{"client_msg_id":"73937155-4ac1-49a3-a491-625bb93df452","type":"message","text":"Hey, do I understand it right that if I want to run my code on CUDA I should use CUDAArrays and write all my algorithms using array calculations mostly?","user":"U7PD3M3L5","ts":"1613470136.190600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"e1uo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey, do I understand it right that if I want to run my code on CUDA I should use CUDAArrays and write all my algorithms using array calculations mostly?"}]}]}]},{"client_msg_id":"231ebdc3-2a1f-478d-8ee7-da081440220e","type":"message","text":"I am a bit confused since how I imagine a GPU I should not be using normal Julia code with `if`  statements or `for`  loops with complicated conditions right?","user":"U7PD3M3L5","ts":"1613470186.191700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FgT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am a bit confused since how I imagine a GPU I should not be using normal Julia code with "},{"type":"text","text":"if","style":{"code":true}},{"type":"text","text":"  statements or "},{"type":"text","text":"for","style":{"code":true}},{"type":"text","text":"  loops with complicated conditions right?"}]}]}]},{"client_msg_id":"2abdda4e-b333-483d-baa9-4ddde3c1cae3","type":"message","text":"<https://juliagpu.gitlab.io/CUDA.jl/usage/overview/>","user":"U68A3ASP9","ts":"1613470991.191900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=DMkc","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://juliagpu.gitlab.io/CUDA.jl/usage/overview/"}]}]}]},{"client_msg_id":"d54733cf-3fcb-4cea-a541-cd6bed33e4a8","type":"message","text":"you can use kernels with conditionals and everything, but the same restrictions as CUDA C apply then (i.e. you probably want to avoid those as they can be slow)","user":"U68A3ASP9","ts":"1613471080.192500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Dzg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you can use kernels with conditionals and everything, but the same restrictions as CUDA C apply then (i.e. you probably want to avoid those as they can be slow)"}]}]}]},{"client_msg_id":"dd702b83-0b8d-4442-aafb-4d869fad7468","type":"message","text":"Thank you, the link helped a bit and the other docs there look promising","user":"U7PD3M3L5","ts":"1613471343.193300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pVpZj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thank you, the link helped a bit and the other docs there look promising"}]}]}]},{"client_msg_id":"33f700e0-e27f-4a13-b1e0-f941f90d3c76","type":"message","text":"Is using Flux on the GPU as easy and performant as using PyTorch? Or basically what do I get for GPU programming when switching from python to julia. For CPU it is pretty clear: I can just write most stuff in for loops etc and not worry. In Pytorch I am doing nearly everything with predefined objects that operate kernels on the GPU anyways, how does julia make things easier here?","user":"U7PD3M3L5","ts":"1613471443.195100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1hLli","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is using Flux on the GPU as easy and performant as using PyTorch? Or basically what do I get for GPU programming when switching from python to julia. For CPU it is pretty clear: I can just write most stuff in for loops etc and not worry. In Pytorch I am doing nearly everything with predefined objects that operate kernels on the GPU anyways, how does julia make things easier here?"}]}]}]},{"client_msg_id":"524f60ff-e88f-4129-9f80-6cbaa3b7f534","type":"message","text":"Not trying to rant or something, I just want to understand","user":"U7PD3M3L5","ts":"1613471463.195600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oU/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not trying to rant or something, I just want to understand"}]}]}]},{"client_msg_id":"96ba99e6-e934-4a04-ae39-31e165a38aa7","type":"message","text":"you generally need to stick to vectorized array expressions to get portability across CPU/GPU (but in Julia that's a much more powerful set of operations). and when you want to optimize, you can write your GPU kernels in Julia too, which is the main advantage over other languages (where you need to use CUDA C).\nwrt. performance compared to pytorch, we probably aren't as fast because we don't have as many optimized kernels, and there can be some issues with the GC when running close to OOM, but better check in some of the machine learning channels for specific numbers.","user":"U68A3ASP9","ts":"1613471972.196100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YIv/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you generally need to stick to vectorized array expressions to get portability across CPU/GPU (but in Julia that's a much more powerful set of operations). and when you want to optimize, you can write your GPU kernels in Julia too, which is the main advantage over other languages (where you need to use CUDA C).\nwrt. performance compared to pytorch, we probably aren't as fast because we don't have as many optimized kernels, and there can be some issues with the GC when running close to OOM, but better check in some of the machine learning channels for specific numbers."}]}]}]},{"client_msg_id":"ca070fcf-e8c7-4e81-9119-538c5b674b42","type":"message","text":"So the advantages go beyond the array kernels. There is a very rich ecosystem of GPU capable packages, thanks in part to the ease with which one can write compatible code, this increases the modelling surface quite significantly.\n\nThere are further optimisations that we can get from existing cuda c kernels as in cudnn, but often one can write code to optimise it without going into cuda, directly in Julia.\n\nThere's also the question of complexity. Tim mentioned how one can write array operations for GPU compatible code. This means that code you write can be transformed easily to GPU kernels, and you can use ad on it the same way. The whole composable nature of it also allows for the same optimisation tricks that the compilers can do on the code generated, since it's all Julia code","user":"UC4QQPG4A","ts":"1613478627.204600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Vz9U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So the advantages go beyond the array kernels. There is a very rich ecosystem of GPU capable packages, thanks in part to the ease with which one can write compatible code, this increases the modelling surface quite significantly.\n\nThere are further optimisations that we can get from existing cuda c kernels as in cudnn, but often one can write code to optimise it without going into cuda, directly in Julia.\n\nThere's also the question of complexity. Tim mentioned how one can write array operations for GPU compatible code. This means that code you write can be transformed easily to GPU kernels, and you can use ad on it the same way. The whole composable nature of it also allows for the same optimisation tricks that the compilers can do on the code generated, since it's all Julia code"}]}]}]},{"client_msg_id":"27c1125c-1933-488b-ad38-fb55323a2065","type":"message","text":"Thanks for the answers, that shed some light. All in all this means that if I can write my code completly in PyTorch i.e. in Tensor notation then probably pytorch is faster since their kernels are more optimized. If I'd need something custom, I will have it easier in Julia since I can just write Julia code/use existing libraries?","user":"U7PD3M3L5","ts":"1613487521.206600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"CA9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for the answers, that shed some light. All in all this means that if I can write my code completly in PyTorch i.e. in Tensor notation then probably pytorch is faster since their kernels are more optimized. If I'd need something custom, I will have it easier in Julia since I can just write Julia code/use existing libraries?"}]}]}]},{"client_msg_id":"8762e1ff-635f-48da-b855-92fa57a9c8b5","type":"message","text":"I haven't followed its development very closely but Torch.jl also exists which means you can stay inside julia and use the C++ pytorch library","user":"U01C3624SGJ","ts":"1613487982.208200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sWA2V","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I haven't followed its development very closely but Torch.jl also exists which means you can stay inside julia and use the C++ pytorch library"}]}]}]},{"client_msg_id":"860087dc-e10c-4e2d-9f8a-36517effe9a7","type":"message","text":"<@U7PD3M3L5> that's not generally true","user":"U69BL50BF","ts":"1613488367.208400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"h537A","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U7PD3M3L5"},{"type":"text","text":" that's not generally true"}]}]}]},{"client_msg_id":"3bddbe18-ae34-45f4-a333-8d0779757d08","type":"message","text":"if you're doing a lot of broadcasted expressions then the codegen of CUDA.jl will definitely lead to more optimized code because of the kernel fusion","user":"U69BL50BF","ts":"1613488397.209100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ELnB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"if you're doing a lot of broadcasted expressions then the codegen of CUDA.jl will definitely lead to more optimized code because of the kernel fusion"}]}]}]},{"client_msg_id":"6720fef2-97ea-4142-b42c-5b92fb83a37d","type":"message","text":"But there are some specifically good kernels that PyTorch more readily uses. I think the RNN ones IIRC. There was some stuff in the CNN kernels but I think those were fixed up just 2 weeks ago (<@U68A3ASP9> would know that part). It's the RNN kernels that don't do all of the fusions PyTorch goes though.","user":"U69BL50BF","ts":"1613488458.210200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0aKB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But there are some specifically good kernels that PyTorch more readily uses. I think the RNN ones IIRC. There was some stuff in the CNN kernels but I think those were fixed up just 2 weeks ago ("},{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":" would know that part). It's the RNN kernels that don't do all of the fusions PyTorch goes though."}]}]}]},{"client_msg_id":"757d34a5-dede-437f-b4a2-3f30a6786973","type":"message","text":"I thought the Pytorch RNNs just shell out to cuDNN?","user":"UMY1LV01G","ts":"1613493060.210700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j2TZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I thought the Pytorch RNNs just shell out to cuDNN?"}]}]}]},{"client_msg_id":"a76cec61-7bf1-4ea0-98ea-5b300fa14abc","type":"message","text":"they fuse in some nice way IIRC right <@UC4QQPG4A>?","user":"U69BL50BF","ts":"1613493152.211000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LVx8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"they fuse in some nice way IIRC right "},{"type":"user","user_id":"UC4QQPG4A"},{"type":"text","text":"?"}]}]}],"thread_ts":"1613493152.211000","reply_count":4,"reply_users_count":2,"latest_reply":"1613496605.219300","reply_users":["UMY1LV01G","UC4QQPG4A"],"subscribed":false},{"client_msg_id":"782e5c6d-682b-49ab-b7c1-83630e8e922d","type":"message","text":"This is a bit of an odd request, but I'm looking for the machinery to flip `c.colVal` and `c.rowPtr`  to `c.rowVal` and `c.colPtr` where `c` is a sparse matrix. I'm using this structure to represent a graph on the gpu, and this \"transposes\" the graph. I was wondering, where can I find a function that does that without having to build sparse matrices? It is a bit wasteful to allocate a vector of floats that I don't use.\n\nIt would be great for me to have something more \"hackable\", where I can play with the implementation (also because I'm working with multigraphs, so not all the assumptions for the `CuSparseMatrixCSR` format hold. I started playing with my own implementation but it seemed a bit tricky. From what I understand, the current sorting implementation (<https://github.com/JuliaGPU/CUDA.jl/pull/431/files>) does not have an optimized version for integer arrays with few unique values, which is the one I would need.","user":"U6BJ9E351","ts":"1613493902.217300","team":"T68168MUP","edited":{"user":"U6BJ9E351","ts":"1613494031.000000"},"blocks":[{"type":"rich_text","block_id":"F/H1T","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This is a bit of an odd request, but I'm looking for the machinery to flip "},{"type":"text","text":"c.colVal","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"c.rowPtr","style":{"code":true}},{"type":"text","text":"  to "},{"type":"text","text":"c.rowVal","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"c.colPtr","style":{"code":true}},{"type":"text","text":" where "},{"type":"text","text":"c","style":{"code":true}},{"type":"text","text":" is a sparse matrix. I'm using this structure to represent a graph on the gpu, and this \"transposes\" the graph. I was wondering, where can I find a function that does that without having to build sparse matrices? It is a bit wasteful to allocate a vector of floats that I don't use.\n\nIt would be great for me to have something more \"hackable\", where I can play with the implementation (also because I'm working with multigraphs, so not all the assumptions for the "},{"type":"text","text":"CuSparseMatrixCSR","style":{"code":true}},{"type":"text","text":" format hold. I started playing with my own implementation but it seemed a bit tricky. From what I understand, the current sorting implementation ("},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/pull/431/files"},{"type":"text","text":") does not have an optimized version for integer arrays with few unique values, which is the one I would need."}]}]}]},{"client_msg_id":"f15c9771-9257-414c-8aee-401806a2fcdb","type":"message","text":"you can always use a lazy transpose/adjoint to turn the columns to rows","user":"U85JBUGGP","ts":"1613495829.218900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1a+0s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you can always use a lazy transpose/adjoint to turn the columns to rows"}]}]}]},{"client_msg_id":"42b6dc92-7083-4e1d-96f6-5f846eb65532","type":"message","text":"Hey everyone, I'm trying to install CUDA.jl without artifacts and I'm getting the following exception:\n`CUDA.jl does not yet support CUDA with nvdisasm 11.0.167`\nDiscussions on GitHub suggest that this was fixed for 11.0, but I'm still getting it, maybe something went wrong on further updates? Any help would be much appreciated!","user":"U01NHTMSEHG","ts":"1613643051.224700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/WNJr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey everyone, I'm trying to install CUDA.jl without artifacts and I'm getting the following exception:\n"},{"type":"text","text":"CUDA.jl does not yet support CUDA with nvdisasm 11.0.167","style":{"code":true}},{"type":"text","text":"\nDiscussions on GitHub suggest that this was fixed for 11.0, but I'm still getting it, maybe something went wrong on further updates? Any help would be much appreciated!"}]}]}]},{"client_msg_id":"96fbb82f-f7d5-4e5e-acbf-a41869fdb40d","type":"message","text":"I also couldn't get it to work with artifacts so maybe I'm doing something wrong...","user":"U01NHTMSEHG","ts":"1613643102.225200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vz4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I also couldn't get it to work with artifacts so maybe I'm doing something wrong..."}]}]}]},{"client_msg_id":"2c67042a-4184-429c-bd65-3fccd4976380","type":"message","text":"NVM, did a clean install, installation with artifacts works now (though non-artifact installation is still throwing the same error)","user":"U01NHTMSEHG","ts":"1613651365.226200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LeDRq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"NVM, did a clean install, installation with artifacts works now (though non-artifact installation is still throwing the same error)"}]}]}]},{"client_msg_id":"4766f4b6-3dee-47b6-9472-3c42cf22dcbc","type":"message","text":"What's the best way to get a single element from a `CuArray` if I disallow scalar indexing? I can hack it through a `copyto!` with a temporary `Array`, but wondering if there was a more elegant approach.","user":"UCRHP2GHE","ts":"1613685299.229300","team":"T68168MUP","edited":{"user":"UCRHP2GHE","ts":"1613685334.000000"},"blocks":[{"type":"rich_text","block_id":"xcBaa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What's the best way to get a single element from a "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":" if I disallow scalar indexing? I can hack it through a "},{"type":"text","text":"copyto!","style":{"code":true}},{"type":"text","text":" with a temporary "},{"type":"text","text":"Array","style":{"code":true}},{"type":"text","text":", but wondering if there was a more elegant approach."}]}]}],"thread_ts":"1613685299.229300","reply_count":1,"reply_users_count":1,"latest_reply":"1613686106.229500","reply_users":["UM30MT6RF"],"subscribed":false},{"client_msg_id":"7af3b557-eb74-4104-9dd6-4452bc4ebe33","type":"message","text":"Is there a workaround for <https://github.com/JuliaGPU/KernelAbstractions.jl/issues/7>? I need to compute the maximum of a given value (which is thread dependent) within a block and could only think of `CUDA.atomic_max!`  as a way to do that","user":"U6BJ9E351","ts":"1613732616.231000","team":"T68168MUP","edited":{"user":"U6BJ9E351","ts":"1613732627.000000"},"blocks":[{"type":"rich_text","block_id":"Y2S","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a workaround for "},{"type":"link","url":"https://github.com/JuliaGPU/KernelAbstractions.jl/issues/7"},{"type":"text","text":"? I need to compute the maximum of a given value (which is thread dependent) within a block and could only think of "},{"type":"text","text":"CUDA.atomic_max!","style":{"code":true}},{"type":"text","text":"  as a way to do that"}]}]}]},{"client_msg_id":"c7059b03-8a80-410e-a797-7ed7874bf3c8","type":"message","text":"No I don't think there is a work around, besides implementing either portable atomics or block level reductions in KA","user":"U67BJLYCS","ts":"1613737438.232700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UMH8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"No I don't think there is a work around, besides implementing either portable atomics or block level reductions in KA"}]}]}]},{"client_msg_id":"7b7338a4-5fe5-4685-95bb-97fd4dedbf95","type":"message","text":"Not impossible, just hard","user":"U67BJLYCS","ts":"1613737491.233200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=5T","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not impossible, just hard"}]}]}]},{"client_msg_id":"2ba08f83-3b73-4c3e-9c8a-ebba14886b63","type":"message","text":"Is there a way of profiling a device function? In my kernel function, I have several device functions, and I would like to profile them. nvprof() or other similar tool seems to give statistics of a kernel function only. In NVIDIA document, they seem to recommend to use clock() inside a device function to profile. CUDA.jl has a similar function?","user":"U01FXSDEXN3","ts":"1613766676.235800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Tnbp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way of profiling a device function? In my kernel function, I have several device functions, and I would like to profile them. nvprof() or other similar tool seems to give statistics of a kernel function only. In NVIDIA document, they seem to recommend to use clock() inside a device function to profile. CUDA.jl has a similar function?"}]}]}]},{"client_msg_id":"c1e3bcae-e3fb-488f-bcc8-cd0d0b1c12d3","type":"message","text":"NVidia profiling has PC profiling","user":"U67BJLYCS","ts":"1613770233.236300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IFkl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"NVidia profiling has PC profiling"}]}]}],"thread_ts":"1613770233.236300","reply_count":3,"reply_users_count":2,"latest_reply":"1613770807.236900","reply_users":["U01FXSDEXN3","U67BJLYCS"],"subscribed":false},{"client_msg_id":"292632b6-c949-4d35-a28d-736460e7f174","type":"message","text":"sorry for spamming this channel with KernelAbstractions question, but I'm a bit new to GPU computing and I'm finding out there is a bit of a learning curve. I was trying to figure out, is the relationship between Cartesian and Linear indexing the same as on the CPU? So basically things like `ndrange = (32, 32), groupsize = (16, 16)` (to write on an array of size `ndrange` ) would be slightly slower than `ndrange = (1024,), groupsize = (256,)`  (unless one has to manually use `fldmod1` to get back the cartesian indices inside the kernel)? Or are there some performance advantages in having a 2D groupsize?","user":"U6BJ9E351","ts":"1613841602.004400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vvq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"sorry for spamming this channel with KernelAbstractions question, but I'm a bit new to GPU computing and I'm finding out there is a bit of a learning curve. I was trying to figure out, is the relationship between Cartesian and Linear indexing the same as on the CPU? So basically things like "},{"type":"text","text":"ndrange = (32, 32), groupsize = (16, 16)","style":{"code":true}},{"type":"text","text":" (to write on an array of size "},{"type":"text","text":"ndrange","style":{"code":true}},{"type":"text","text":" ) would be slightly slower than "},{"type":"text","text":"ndrange = (1024,), groupsize = (256,)","style":{"code":true}},{"type":"text","text":"  (unless one has to manually use "},{"type":"text","text":"fldmod1","style":{"code":true}},{"type":"text","text":" to get back the cartesian indices inside the kernel)? Or are there some performance advantages in having a 2D groupsize?"}]}]}]},{"client_msg_id":"3c3367b0-74c2-449f-927e-a7af0c237aff","type":"message","text":"<https://discourse.julialang.org/t/how-to-generate-a-random-number-in-cuda-kernel-function/50364|this> discourse post by Tim Besard in nov 2020 says that random number generation is not supported in GPU kernels right now. Is that still the case?","user":"U011V2YN59N","ts":"1613844813.005600","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"How to generate a random number in CUDA kernel function","title_link":"https://discourse.julialang.org/t/how-to-generate-a-random-number-in-cuda-kernel-function/50364","text":"All the rand like function in CUDA.jl will get a CuArray. But memory allocation is not allowed in a kernel function. Thus, a CuArray can not be generated. So, are there any method to get a pure random number instead of an Array.","fallback":"JuliaLang: How to generate a random number in CUDA kernel function","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","fields":[{"title":"Reading time","value":"1 mins :clock2:","short":true},{"title":"Likes","value":"7 :heart:","short":true}],"ts":1605690854,"from_url":"https://discourse.julialang.org/t/how-to-generate-a-random-number-in-cuda-kernel-function/50364","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/how-to-generate-a-random-number-in-cuda-kernel-function/50364"}],"blocks":[{"type":"rich_text","block_id":"zHupm","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://discourse.julialang.org/t/how-to-generate-a-random-number-in-cuda-kernel-function/50364","text":"this"},{"type":"text","text":" discourse post by Tim Besard in nov 2020 says that random number generation is not supported in GPU kernels right now. Is that still the case?"}]}]}]},{"client_msg_id":"78ef8434-de1b-4d23-91ec-e9a932bef7fd","type":"message","text":"With a bit of care you can have random numbers on the GPU","user":"U67BJLYCS","ts":"1613846094.007500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jjn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"With a bit of care you can have random numbers on the GPU"}]}]}]},{"client_msg_id":"a2af553e-756e-45ac-9ca4-15b6f7622288","type":"message","text":"<https://juliafolds.github.io/FoldsCUDA.jl/dev/examples/monte_carlo_pi/>","user":"U67BJLYCS","ts":"1613846094.007700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Wfn","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://juliafolds.github.io/FoldsCUDA.jl/dev/examples/monte_carlo_pi/"}]}]}]},{"client_msg_id":"b2358d3b-9a40-4cef-8463-22e8d9b0d2b2","type":"message","text":"mm my gpu does not like that example :disappointed:","user":"U011V2YN59N","ts":"1613846510.008200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KC7S","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"mm my gpu does not like that example "},{"type":"emoji","name":"disappointed"}]}]}]},{"client_msg_id":"a9d3957b-304b-4def-84f4-506f36e45ff0","type":"message","text":"In general actually, `test CUDA`  produces many errors.. is it worth opening an issue?","user":"U011V2YN59N","ts":"1613846560.009000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bqQM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In general actually, "},{"type":"text","text":"test CUDA","style":{"code":true}},{"type":"text","text":"  produces many errors.. is it worth opening an issue?"}]}]}]},{"type":"message","text":"I wrote this tiny wrapper which \"adapts\" an array to unified memory instead of per-device memory. Its not quite right w.r.t. GC yet (but I think I can fix, granted I'm a noob here), but curious if something like this would be useful to have in CUDA, at least as a stopgap until more first-class unified memory support is achieved?","files":[{"id":"F01NSJ0PK1U","created":1614038063,"timestamp":1614038063,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"UUMJUCYRK","editable":false,"size":105967,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01NSJ0PK1U/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01NSJ0PK1U/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01NSJ0PK1U-651f9c7346/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01NSJ0PK1U-651f9c7346/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01NSJ0PK1U-651f9c7346/image_360.png","thumb_360_w":360,"thumb_360_h":331,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01NSJ0PK1U-651f9c7346/image_480.png","thumb_480_w":480,"thumb_480_h":441,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01NSJ0PK1U-651f9c7346/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01NSJ0PK1U-651f9c7346/image_720.png","thumb_720_w":720,"thumb_720_h":662,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01NSJ0PK1U-651f9c7346/image_800.png","thumb_800_w":800,"thumb_800_h":735,"original_w":892,"original_h":820,"thumb_tiny":"AwAsADC+3BpOc8Zp31Umlz7GgBoBPXNOx7mj8DS0AN2+5pcClooAbjJ6GlAApMZPQ0oGKAFzRmjNGaADNGaM0ZoAbjJzg0oGBSY+Y0qDigBc0ZpaKAEzRmlooA//2Q==","permalink":"https://julialang.slack.com/files/UUMJUCYRK/F01NSJ0PK1U/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01NSJ0PK1U-33a829397b","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"QYmt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I wrote this tiny wrapper which \"adapts\" an array to unified memory instead of per-device memory. Its not quite right w.r.t. GC yet (but I think I can fix, granted I'm a noob here), but curious if something like this would be useful to have in CUDA, at least as a stopgap until more first-class unified memory support is achieved?"}]}]}],"user":"UUMJUCYRK","display_as_bot":false,"ts":"1614038094.011300"},{"client_msg_id":"7223d48a-7572-4c62-8f98-32f6005d5b98","type":"message","text":"Hi, can someone enable buildkite for <https://github.com/JuliaFolds/FoldsKernelAbstractions.jl>? (ping <@U68A3ASP9>, <@U67BJLYCS>, <@U7THT3TM3>)","user":"UC7AF7NSU","ts":"1614038411.012100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+O+M4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, can someone enable buildkite for "},{"type":"link","url":"https://github.com/JuliaFolds/FoldsKernelAbstractions.jl"},{"type":"text","text":"? (ping "},{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":", "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":", "},{"type":"user","user_id":"U7THT3TM3"},{"type":"text","text":")"}]}]}]},{"client_msg_id":"2f34f1d9-a7bc-4b25-a8f1-cb879f63edf9","type":"message","text":"also, can I access both Nividia and AMD machines from this repo? if that's not possible, accessing only Nvidia machine is still great","user":"UC7AF7NSU","ts":"1614038979.013600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Huj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"also, can I access both Nividia and AMD machines from this repo? if that's not possible, accessing only Nvidia machine is still great"}]}]}],"thread_ts":"1614038979.013600","reply_count":1,"reply_users_count":1,"latest_reply":"1614039098.013700","reply_users":["U6A0PD8CR"],"subscribed":false},{"client_msg_id":"2c697392-2b59-4f44-bd68-4601ea1ca546","type":"message","text":"Julian is working on it now","user":"UC7AF7NSU","ts":"1614039673.014100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tGOn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Julian is working on it now"}]}]}]},{"client_msg_id":"bd5b7d0c-99a7-4c98-ba8f-9b3d2da564ef","type":"message","text":"We still have trouble fully enabling it: <https://julialang.slack.com/archives/C689Y34LE/p1614041651014200?thread_ts=1614038411.012100&amp;cid=C689Y34LE>","user":"UC7AF7NSU","ts":"1614041979.014800","team":"T68168MUP","attachments":[{"from_url":"https://julialang.slack.com/archives/C689Y34LE/p1614041651014200?thread_ts=1614038411.012100&amp;cid=C689Y34LE","fallback":"[February 22nd, 2021 4:54 PM] jpsamaroo: I'm not an admin, so I can't make the pipeline public. Can one of the above-mentioned admins make it so?","ts":"1614041651.014200","author_id":"U6A0PD8CR","author_subname":"Julian Samaroo","channel_id":"C689Y34LE","channel_name":"gpu","is_msg_unfurl":true,"is_reply_unfurl":true,"text":"I'm not an admin, so I can't make the pipeline public. Can one of the above-mentioned admins make it so?","author_name":"Julian Samaroo","author_link":"https://julialang.slack.com/team/U6A0PD8CR","author_icon":"https://avatars.slack-edge.com/2017-07-18/215660769271_7ef635ba687405f810d2_48.jpg","mrkdwn_in":["text"],"id":1,"original_url":"https://julialang.slack.com/archives/C689Y34LE/p1614041651014200?thread_ts=1614038411.012100&amp;cid=C689Y34LE","footer":"From a thread in #gpu"}],"blocks":[{"type":"rich_text","block_id":"jyZ3s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We still have trouble fully enabling it: "},{"type":"link","url":"https://julialang.slack.com/archives/C689Y34LE/p1614041651014200?thread_ts=1614038411.012100&cid=C689Y34LE"}]}]}]},{"client_msg_id":"c17b295b-3668-48ef-833d-25aa750335ab","type":"message","text":"I'm trying to skip GPU CI on buildkite with PR labels. I'm trying\n\n```if: |\n  !(\n      build.pull_request.labels includes \"no cuda\" ||\n      build.pull_request.labels includes \"no gpu\"\n  )```\nBut this doesn't work. Does anyone make similar thing work? Is `if: build.message !~ /\\[skip tests\\]/` more reliable?\n\n<https://github.com/JuliaFolds/FoldsKernelAbstractions.jl/pull/5>","user":"UC7AF7NSU","ts":"1614047660.017400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Q=WjT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm trying to skip GPU CI on buildkite with PR labels. I'm trying\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"if: |\n  !(\n      build.pull_request.labels includes \"no cuda\" ||\n      build.pull_request.labels includes \"no gpu\"\n  )"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nBut this doesn't work. Does anyone make similar thing work? Is "},{"type":"text","text":"if: build.message !~ /\\[skip tests\\]/","style":{"code":true}},{"type":"text","text":" more reliable?\n\n"},{"type":"link","url":"https://github.com/JuliaFolds/FoldsKernelAbstractions.jl/pull/5"}]}]}]},{"client_msg_id":"f51ee45f-e7b8-44ee-a9db-86d5371442a9","type":"message","text":"I think I understood the strategy for performant matmul here (<https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/examples/performant_matmul.jl|https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/examples/performant_matmul.jl>), that example is very instructive. I'm trying to wrap my head around whether strategy is also effective for things like `conv`.\n\nIn particular, would one just use a similar tiling strategy summing over `tile1[i, k] * tile2[k, j]`, where there are as many values of `k` as there are \"compatible pixels\" (size filter times size image), or is it more effective to make fewer tiles and sum over `tile1[i, k] * tile2[k + s, j]` for `s` varying among the indices of the filter?","user":"U6BJ9E351","ts":"1614074563.028000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KSRmu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think I understood the strategy for performant matmul here ("},{"type":"link","url":"https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/examples/performant_matmul.jl","text":"https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/examples/performant_matmul.jl"},{"type":"text","text":"), that example is very instructive. I'm trying to wrap my head around whether strategy is also effective for things like "},{"type":"text","text":"conv","style":{"code":true}},{"type":"text","text":".\n\nIn particular, would one just use a similar tiling strategy summing over "},{"type":"text","text":"tile1[i, k] * tile2[k, j]","style":{"code":true}},{"type":"text","text":", where there are as many values of "},{"type":"text","text":"k","style":{"code":true}},{"type":"text","text":" as there are \"compatible pixels\" (size filter times size image), or is it more effective to make fewer tiles and sum over "},{"type":"text","text":"tile1[i, k] * tile2[k + s, j]","style":{"code":true}},{"type":"text","text":" for "},{"type":"text","text":"s ","style":{"code":true}},{"type":"text","text":"varying among the indices of the filter?"}]}]}]},{"client_msg_id":"8a932b1e-211c-4c20-8e07-9082e0f9ee15","type":"message","text":"The first strategy (pretend it's `matmul`) sounds a bit naive, but given that the \"copying the tiles\" part has lower computational complexity, I was wondering whether it was enough to get things packed together to speed up the multiplication part (with `@simd` and `@inbounds`). (I hope this is not too fuzzy/unclear!)","user":"U6BJ9E351","ts":"1614074781.032800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9fSC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The first strategy (pretend it's "},{"type":"text","text":"matmul","style":{"code":true}},{"type":"text","text":") sounds a bit naive, but given that the \"copying the tiles\" part has lower computational complexity, I was wondering whether it was enough to get things packed together to speed up the multiplication part (with "},{"type":"text","text":"@simd","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"@inbounds","style":{"code":true}},{"type":"text","text":"). (I hope this is not too fuzzy/unclear!)"}]}]}],"thread_ts":"1614074781.032800","reply_count":1,"reply_users_count":1,"latest_reply":"1614075180.032900","reply_users":["U6BJ9E351"],"subscribed":false},{"client_msg_id":"5dcf9bb3-b392-41fe-8315-956730d923aa","type":"message","text":"<@UC7AF7NSU> does tapir have any implications for gpu / accelerator programming ?","user":"UDGT4PM41","ts":"1614091461.034000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"AWvNU","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UC7AF7NSU"},{"type":"text","text":" does tapir have any implications for gpu / accelerator programming ?"}]}]}],"thread_ts":"1614091461.034000","reply_count":5,"reply_users_count":2,"latest_reply":"1614092106.035100","reply_users":["UC7AF7NSU","UDGT4PM41"],"subscribed":false},{"client_msg_id":"84de9945-58a8-4479-a59e-35a8337b1bf9","type":"message","text":"I've been trying to get one-GPU-per-thread parallelization working (mainly to avoid paying the N-process Julia startup cost of the standard one-GPU-per-process way which is otherwise very robust). Between unified memory, <https://github.com/JuliaGPU/CUDA.jl/issues/731|#731> getting fixed, and recent stuff on master, quite a lot is working well! I'm stuck on the following thing though, wondering if I might get some hints of what might be going wrong that could help get a MWE. The general outline of what I'm running is\n```x = move_to_unified_memory(CuArray(...))\n\nThreads.@sync begin\n    Threads.@spawn begin\n        device!(0)\n        some_gpu_code(x)\n    end\n    Threads.@spawn begin\n        device!(1)\n        some_gpu_code(x)\n    end\nend```\nFor `some_gpu_code` being simple FFTs or array broadcasts, it works. For my more complex actual calculations (which in theory should just be a whole bunch of these chained together in some complex way), I get the segfault I attach in the thread. Any ideas? This is CUDA#master.","user":"UUMJUCYRK","ts":"1614162324.041500","team":"T68168MUP","edited":{"user":"UUMJUCYRK","ts":"1614162388.000000"},"blocks":[{"type":"rich_text","block_id":"/hNZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I've been trying to get one-GPU-per-thread parallelization working (mainly to avoid paying the N-process Julia startup cost of the standard one-GPU-per-process way which is otherwise very robust). Between unified memory, "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/issues/731","text":"#731"},{"type":"text","text":" getting fixed, and recent stuff on master, quite a lot is working well! I'm stuck on the following thing though, wondering if I might get some hints of what might be going wrong that could help get a MWE. The general outline of what I'm running is\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"x = move_to_unified_memory(CuArray(...))\n\nThreads.@sync begin\n    Threads.@spawn begin\n        device!(0)\n        some_gpu_code(x)\n    end\n    Threads.@spawn begin\n        device!(1)\n        some_gpu_code(x)\n    end\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"For "},{"type":"text","text":"some_gpu_code","style":{"code":true}},{"type":"text","text":" being simple FFTs or array broadcasts, it works. For my more complex actual calculations (which in theory should just be a whole bunch of these chained together in some complex way), I get the segfault I attach in the thread. Any ideas? This is CUDA#master."}]}]}],"thread_ts":"1614162324.041500","reply_count":18,"reply_users_count":2,"latest_reply":"1614164847.046500","reply_users":["UUMJUCYRK","U68A3ASP9"],"subscribed":false},{"client_msg_id":"4a863dbc-d55a-48d0-bf3f-e1f93f5786d5","type":"message","text":"Is there a straightforward way to get poisson samples in GPU kernels?\n\nI know that `CUDA.rand_poisson!` exists but that seems to be host only. I saw the example by <@UC7AF7NSU> that uses `Random123.jl` to get random numbers in kernels, and that works for simple things like samples from geometric distributions, but it seems like sampling from poisson efficiently is quite complex, so I am wondering if this has already been done somewhere.","user":"U011V2YN59N","ts":"1614170758.051000","team":"T68168MUP","edited":{"user":"U011V2YN59N","ts":"1614172157.000000"},"blocks":[{"type":"rich_text","block_id":"Cyq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a straightforward way to get poisson samples in GPU kernels?\n\nI know that "},{"type":"text","text":"CUDA.rand_poisson!","style":{"code":true}},{"type":"text","text":" exists but that seems to be host only. I saw the example by "},{"type":"user","user_id":"UC7AF7NSU"},{"type":"text","text":" that uses "},{"type":"text","text":"Random123.jl","style":{"code":true}},{"type":"text","text":" to get random numbers in kernels, and that works for simple things like samples from geometric distributions, but it seems like sampling from poisson efficiently is quite complex, so I am wondering if this has already been done somewhere."}]}]}]},{"client_msg_id":"2d66fbac-50b2-4758-930a-8c677ae1f953","type":"message","text":"I couldn't get the poisson sampler in Distributions.jl to work out of the box, either, because it depends on `randexp`, which uses the tables for normal variates in `Base`, which are not `CuArrays`. I suppose I could just copy and paste all that with tweaks, but again I am wondering if this has been done somewhere.","user":"U011V2YN59N","ts":"1614172274.052700","team":"T68168MUP","edited":{"user":"U011V2YN59N","ts":"1614172302.000000"},"blocks":[{"type":"rich_text","block_id":"tYvF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I couldn't get the poisson sampler in Distributions.jl to work out of the box, either, because it depends on "},{"type":"text","text":"randexp","style":{"code":true}},{"type":"text","text":", which uses the tables for normal variates in "},{"type":"text","text":"Base","style":{"code":true}},{"type":"text","text":", which are not "},{"type":"text","text":"CuArrays","style":{"code":true}},{"type":"text","text":". I suppose I could just copy and paste all that with tweaks, but again I am wondering if this has been done somewhere."}]}]}]},{"type":"message","text":"<https://github.com/vosen/ZLUDA> That looks fun.","user":"U9MD78Z9N","ts":"1614258546.054500","team":"T68168MUP","thread_ts":"1614258546.054500","reply_count":1,"reply_users_count":1,"latest_reply":"1614258962.054600","reply_users":["U01C3624SGJ"],"subscribed":false},{"client_msg_id":"093682f8-1f50-45dc-bf1b-37def0aa93af","type":"message","text":"Tried to run buildkite with julia v1.4; is this even possible?\nI get an error that I can’t interpret. I guess the error is clear that I need to file an issue :wink:\n`LoadError: CUDA.jl does not yet support CUDA with nvdisasm 11.1.74; please file an issue.`\n\n<https://buildkite.com/julialang/geophysicalflows-dot-jl/builds/105#cae0280c-c34c-49e8-b75c-f718b2d4f2b5>","user":"UBY3ENVBK","ts":"1614503194.060000","team":"T68168MUP","attachments":[{"service_name":"Buildkite","title":"GeophysicalFlows.jl #105 · JuliaLang","title_link":"https://buildkite.com/julialang/geophysicalflows-dot-jl/builds/105#cae0280c-c34c-49e8-b75c-f718b2d4f2b5","text":"julia 1.4","fallback":"Buildkite: GeophysicalFlows.jl #105 · JuliaLang","thumb_url":"https://avatars.githubusercontent.com/u/7112768?v=4","from_url":"https://buildkite.com/julialang/geophysicalflows-dot-jl/builds/105#cae0280c-c34c-49e8-b75c-f718b2d4f2b5","thumb_width":301,"thumb_height":301,"service_icon":"https://buildkite.com/favicon.ico","id":1,"original_url":"https://buildkite.com/julialang/geophysicalflows-dot-jl/builds/105#cae0280c-c34c-49e8-b75c-f718b2d4f2b5"}],"blocks":[{"type":"rich_text","block_id":"9Lw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Tried to run buildkite with julia v1.4; is this even possible?\nI get an error that I can’t interpret. I guess the error is clear that I need to file an issue "},{"type":"emoji","name":"wink"},{"type":"text","text":"\n"},{"type":"text","text":"LoadError: CUDA.jl does not yet support CUDA with nvdisasm 11.1.74; please file an issue.","style":{"code":true}},{"type":"text","text":"\n\n"},{"type":"link","url":"https://buildkite.com/julialang/geophysicalflows-dot-jl/builds/105#cae0280c-c34c-49e8-b75c-f718b2d4f2b5"}]}]}]},{"client_msg_id":"86de8e28-e2cc-405a-99e7-049a87e51a30","type":"message","text":"Old version of CUDA.jl. Either upgrade or use the cuda tag to request an older version. You can also force it to use artifacts, that's normally disabled on ci to save on network traffic.","user":"U68A3ASP9","ts":"1614506110.062200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"77NEA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Old version of CUDA.jl. Either upgrade or use the cuda tag to request an older version. You can also force it to use artifacts, that's normally disabled on ci to save on network traffic."}]}]}]},{"client_msg_id":"dc5774a9-56b1-49e2-97d1-81f051c7ccde","type":"message","text":"Biweekly office hours happening in 45minutes","user":"U67BJLYCS","ts":"1614615460.066100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lZs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Biweekly office hours happening in 45minutes"}]}]}],"thread_ts":"1614615460.066100","reply_count":3,"reply_users_count":3,"latest_reply":"1614615552.067800","reply_users":["U9MD78Z9N","U6A0PD8CR","U67BJLYCS"],"subscribed":false},{"client_msg_id":"bf1c5ff4-69f3-4e94-a43e-15ac6cec72bc","type":"message","text":"link in the channel description","user":"U67BJLYCS","ts":"1614615468.066400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JLP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"link in the channel description"}]}]}]},{"client_msg_id":"c143f3ed-5e3e-4b86-a4f7-bb0be7269146","type":"message","text":"I use `GPUArrays.default_rgn(CuArray)` to generate random numbers for a `CuArray` and found that it can only generate a maximum of 256 random numbers at one time. Starting form 257th, it’s the repeat of the first 256 numbers. Below is an example:\n```using CUDA, Random\nusing CUDA: GPUArrays\na = zeros(256*3) |&gt; CuArray\nrand!(GPUArrays.default_rng(CuArray), a)\na[1:256] == a[257:512] # true\na[1:256] == a[513:end] # true```\nI’m wondering is there a way to avoid this?","user":"U019U0QCF7F","ts":"1614615646.069100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7Oks","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I use "},{"type":"text","text":"GPUArrays.default_rgn(CuArray)","style":{"code":true}},{"type":"text","text":" to generate random numbers for a "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":" and found that it can only generate a maximum of 256 random numbers at one time. Starting form 257th, it’s the repeat of the first 256 numbers. Below is an example:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using CUDA, Random\nusing CUDA: GPUArrays\na = zeros(256*3) |> CuArray\nrand!(GPUArrays.default_rng(CuArray), a)\na[1:256] == a[257:512] # true\na[1:256] == a[513:end] # true"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I’m wondering is there a way to avoid this?"}]}]}]},{"client_msg_id":"e777fe10-f572-4a87-9d0a-2cc2cd42a3c7","type":"message","text":"<@U019U0QCF7F> Not sure about `GPUArrays.default_rgn` but why not use `Base.rand` and `Base.randn`? Could just do\n```a = rand(256*3) |&gt; CuArray```","user":"UEP056STX","ts":"1614616926.070000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Gj00","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U019U0QCF7F"},{"type":"text","text":" Not sure about "},{"type":"text","text":"GPUArrays.default_rgn","style":{"code":true}},{"type":"text","text":" but why not use "},{"type":"text","text":"Base.rand","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"Base.randn","style":{"code":true}},{"type":"text","text":"? Could just do\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"a = rand(256*3) |> CuArray"}]}]}],"thread_ts":"1614616926.070000","reply_count":1,"reply_users_count":1,"latest_reply":"1614618632.070500","reply_users":["U019U0QCF7F"],"subscribed":false},{"client_msg_id":"a7e1e627-b461-41a0-af1e-9dadfec34f3a","type":"message","text":"Currently the only really robust way to do multi-GPU is with separate processes controlling each GPU. One downside is that transferring memory between GPUs has to go through the CPU as its serialized/deserialized by Julia (unless I'm missing something). Is there any other way to do transfers between GPUs? Ideally what I'm looking for is something like the pseudo-code:\n```device!(0)\nx = unified_memory(CuArray(...))\nx_handle = some_sort_of_handle(x)\n\npmap(workers()) do i\n    device!(i)\n    local_x = CuArray(...)\n    copy_to!(local_x, x_handle)\n    ...\nend```\nI started reading about CUDA MPS but I don't know enough if its the right thing to devote more time to understanding, or if it can be used from CUDA.jl at all. I'm also aware of CUDA-aware MPI, but since in my case most of my work is from Jupyter notebooks, I don't think it can really work. I've also been playing alot lately with threads instead of processes, but that doesn't seem stable enough to rely on quite yet. Curious if there's some other (easy?) option with processes I'm missing?","user":"UUMJUCYRK","ts":"1614672449.074900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gLvc3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Currently the only really robust way to do multi-GPU is with separate processes controlling each GPU. One downside is that transferring memory between GPUs has to go through the CPU as its serialized/deserialized by Julia (unless I'm missing something). Is there any other way to do transfers between GPUs? Ideally what I'm looking for is something like the pseudo-code:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"device!(0)\nx = unified_memory(CuArray(...))\nx_handle = some_sort_of_handle(x)\n\npmap(workers()) do i\n    device!(i)\n    local_x = CuArray(...)\n    copy_to!(local_x, x_handle)\n    ...\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I started reading about CUDA MPS but I don't know enough if its the right thing to devote more time to understanding, or if it can be used from CUDA.jl at all. I'm also aware of CUDA-aware MPI, but since in my case most of my work is from Jupyter notebooks, I don't think it can really work. I've also been playing alot lately with threads instead of processes, but that doesn't seem stable enough to rely on quite yet. Curious if there's some other (easy?) option with processes I'm missing?"}]}]}],"thread_ts":"1614672449.074900","reply_count":1,"reply_users_count":1,"latest_reply":"1614672901.075000","reply_users":["UMY1LV01G"],"subscribed":false},{"client_msg_id":"3b998b4d-8ffb-44b4-b9e2-d9fafeb5ed17","type":"message","text":"<https://www.phoronix.com/scan.php?page=news_item&amp;px=Intel-2021-LLVM-SPIR-V-Backend> no more llvm-spirv translator and easy Vulkan codegen in GPUCompiler, perhaps?","user":"UMY1LV01G","ts":"1614732027.081300","team":"T68168MUP","attachments":[{"title":"Intel Looking To Upstream A Proper SPIR-V Compute Back-End For LLVM - Phoronix","title_link":"https://www.phoronix.com/scan.php?page=news_item&px=Intel-2021-LLVM-SPIR-V-Backend","text":"Phoronix is the leading technology website for Linux hardware reviews, open-source news, Linux benchmarks, open-source benchmarks, and computer hardware tests.","fallback":"Intel Looking To Upstream A Proper SPIR-V Compute Back-End For LLVM - Phoronix","from_url":"https://www.phoronix.com/scan.php?page=news_item&px=Intel-2021-LLVM-SPIR-V-Backend","service_icon":"https://www.phoronix.com/apple-touch-icon-57x57.png","service_name":"phoronix.com","id":1,"original_url":"https://www.phoronix.com/scan.php?page=news_item&amp;px=Intel-2021-LLVM-SPIR-V-Backend"}],"blocks":[{"type":"rich_text","block_id":"5f9Nn","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://www.phoronix.com/scan.php?page=news_item&px=Intel-2021-LLVM-SPIR-V-Backend"},{"type":"text","text":" no more llvm-spirv translator and easy Vulkan codegen in GPUCompiler, perhaps?"}]}]}]},{"client_msg_id":"1d5bc48f-64e7-450e-b002-0ccf36b7c5a1","type":"message","text":"are enums, defined and used on the host, supported on the device via Kernel Abstractions?","user":"U0178LR5K7F","ts":"1614742571.082600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RY+T","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"are enums, defined and used on the host, supported on the device via Kernel Abstractions?"}]}]}]},{"client_msg_id":"fa8c3550-6ec8-4da3-847f-820a1564dbbc","type":"message","text":"Enums are just constants so yeah","user":"U67BJLYCS","ts":"1614745417.083000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"y2I","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Enums are just constants so yeah"}]}]}]},{"client_msg_id":"fd700671-6c27-457e-8628-44093645f215","type":"message","text":"I am working on isolating the problem to get a MWE but this is the error I am seeing\n```ERROR: GPUCompiler.jl encountered an unexpected internal error.\nPlease file an issue attaching the following information, including the backtrace,\nas well as a reproducible example (if possible).\n\nInternalCompilerError: length(frames) == 1, at /home/aleonard31/.julia/packages/GPUCompiler/uTpNx/src/validation.jl:246```\nif I comment out a line using an enum as an argument I do not see a error","user":"U0178LR5K7F","ts":"1614748130.084700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6JI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am working on isolating the problem to get a MWE but this is the error I am seeing\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: GPUCompiler.jl encountered an unexpected internal error.\nPlease file an issue attaching the following information, including the backtrace,\nas well as a reproducible example (if possible).\n\nInternalCompilerError: length(frames) == 1, at /home/aleonard31/.julia/packages/GPUCompiler/uTpNx/src/validation.jl:246"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"if I comment out a line using an enum as an argument I do not see a error"}]}]}]},{"client_msg_id":"e9489e86-f532-47de-b8e2-b97826d7ce9b","type":"message","text":"I have a Flux model that is a bit too big to run on one GPU. I could split it into an encoder and decoder part, and the data that would need to be sent between them could potentially be quite small, like a 1000^2 matrix at the most. What should I be using to sync data back and forth?","user":"U6C8RR26P","ts":"1614785827.088600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Wh6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a Flux model that is a bit too big to run on one GPU. I could split it into an encoder and decoder part, and the data that would need to be sent between them could potentially be quite small, like a 1000^2 matrix at the most. What should I be using to sync data back and forth?"}]}]}]},{"client_msg_id":"c0a8ec57-5608-46e5-a6d6-474f28700608","type":"message","text":"Hi, I am broadcasting a `CuArray` over a function that has some trig functions in it and I am seeing\n```┌ Warning: calls to Base intrinsics might be GPU incompatible\n│   exception =\n│    You called sin(x::T) where T&lt;:Union{Float32, Float64} in Base.Math at special/trig.jl:29, maybe you intended to call sin(x::Float32) in CUDA at /home/aleonard31/.julia/packages/CUDA/wTQsK/src/device/intrinsics/math.jl:13 instead?\n│    Stacktrace:\n│     [1] sin at special/trig.jl:29\n│     [2] broadcast_kernel at /home/aleonard31/.julia/packages/GPUArrays/WV76E/src/host/broadcast.jl:59```\nis there a way to handle this without changing the sin/cos to CUDA.sin/CUDA.cos?","user":"U0178LR5K7F","ts":"1614797995.090100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2al","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I am broadcasting a "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":" over a function that has some trig functions in it and I am seeing\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"┌ Warning: calls to Base intrinsics might be GPU incompatible\n│   exception =\n│    You called sin(x::T) where T<:Union{Float32, Float64} in Base.Math at special/trig.jl:29, maybe you intended to call sin(x::Float32) in CUDA at /home/aleonard31/.julia/packages/CUDA/wTQsK/src/device/intrinsics/math.jl:13 instead?\n│    Stacktrace:\n│     [1] sin at special/trig.jl:29\n│     [2] broadcast_kernel at /home/aleonard31/.julia/packages/GPUArrays/WV76E/src/host/broadcast.jl:59"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"is there a way to handle this without changing the sin/cos to CUDA.sin/CUDA.cos?"}]}]}],"thread_ts":"1614797995.090100","reply_count":3,"reply_users_count":2,"latest_reply":"1614798487.090800","reply_users":["U0178LR5K7F","U6A0PD8CR"],"subscribed":false},{"client_msg_id":"507b3049-3add-4363-a55a-388a74e55b76","type":"message","text":"Pytorch 1.8 has ROCm support apparently","user":"UDGT4PM41","ts":"1614949390.094500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Hth9u","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Pytorch 1.8 has ROCm support apparently"}]}]}]},{"client_msg_id":"e797ef8d-74a6-406c-b405-8250132c4865","type":"message","text":"Do GPU libraries use something like `GC.@preserve`?","user":"UCLGS1HML","ts":"1614954013.095600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0LNV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do GPU libraries use something like "},{"type":"text","text":"GC.@preserve","style":{"code":true}},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"85ff17f6-1851-4ea7-b94b-01b77fb8b428","type":"message","text":"generally not, since GPU operations are stream-ordered so the free can't happen before e.g. the kernel finished executing. but we'll probably have to be more careful about that in the future.","user":"U68A3ASP9","ts":"1614954202.096500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dHIr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"generally not, since GPU operations are stream-ordered so the free can't happen before e.g. the kernel finished executing. but we'll probably have to be more careful about that in the future."}]}]}]},{"client_msg_id":"9f26bb81-66d7-40d7-a74b-b4537631fb04","type":"message","text":"Thanks! I’m looking through the CUDA.jl docs and didn’t find anything obvious.\n\n&gt;  but we’ll probably have to be more careful about that in the future\nDoes this mean that you will have your own thing like `CUDA.@preserve x op(pointer(x))` for CUDA’s\nown garbage collector or does this just mean that you plan on strictly ensuring garbage collection\ndoesn’t occur within a kernel?","user":"UCLGS1HML","ts":"1614955090.096800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bYy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks! I’m looking through the CUDA.jl docs and didn’t find anything obvious.\n\n"}]},{"type":"rich_text_quote","elements":[{"type":"text","text":" but we’ll probably have to be more careful about that in the future"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nDoes this mean that you will have your own thing like "},{"type":"text","text":"CUDA.@preserve x op(pointer(x))","style":{"code":true}},{"type":"text","text":" for CUDA’s\nown garbage collector or does this just mean that you plan on strictly ensuring garbage collection\ndoesn’t occur within a kernel?"}]}]}]},{"client_msg_id":"c5046b5b-9a2c-49c8-845b-91955fd94a9a","type":"message","text":"CUDA.jl doesn't have its own garbage collector, on the host we use Julia's GC and in kernels we currently don't really support dynamic allocations. and currently GC can trigger during kernel execution, but that will just enqueue a `free` operation which can only execute after the kernel is done, so I haven't thought of an API to deal with the scenario where that would lead to bad results.","user":"U68A3ASP9","ts":"1614957356.099800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pIQ+r","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CUDA.jl doesn't have its own garbage collector, on the host we use Julia's GC and in kernels we currently don't really support dynamic allocations. and currently GC can trigger during kernel execution, but that will just enqueue a "},{"type":"text","text":"free","style":{"code":true}},{"type":"text","text":" operation which can only execute after the kernel is done, so I haven't thought of an API to deal with the scenario where that would lead to bad results."}]}]}]},{"client_msg_id":"86db38d0-6b4e-4407-ab74-96f21e4f5084","type":"message","text":"We’re trying to figure out a generic interface for this sort of thing in ArrayInterface.jl and would like it to eventually be useful for GPUs too (<https://github.com/JuliaArrays/ArrayInterface.jl/issues/104>\n, <https://github.com/JuliaArrays/ArrayInterface.jl/issues/130>).\nThe idea is that we could have something that is one step back from `Libc.malloc` and do something like...\n\n```buffer = allocate(...)\nptr = pointer(buffer)\nGC.@preserve buffer f(ptr)  # `f` is the actually function body that users care about\ndereference(buffer)```\nThere’s a lot of abstraction here (`ptr` could still be carrying info about the size, strides, etc.).\nI think we’ll need to figure out exactly what info needs to be propagated between methods before this thing really materializes though.","user":"UCLGS1HML","ts":"1614957702.100100","team":"T68168MUP","edited":{"user":"UCLGS1HML","ts":"1614957750.000000"},"blocks":[{"type":"rich_text","block_id":"RckLZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We’re trying to figure out a generic interface for this sort of thing in ArrayInterface.jl and would like it to eventually be useful for GPUs too ("},{"type":"link","url":"https://github.com/JuliaArrays/ArrayInterface.jl/issues/104"},{"type":"text","text":"\n, "},{"type":"link","url":"https://github.com/JuliaArrays/ArrayInterface.jl/issues/130"},{"type":"text","text":").\nThe idea is that we could have something that is one step back from "},{"type":"text","text":"Libc.malloc","style":{"code":true}},{"type":"text","text":" and do something like...\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"buffer = allocate(...)\nptr = pointer(buffer)\nGC.@preserve buffer f(ptr)  # `f` is the actually function body that users care about\ndereference(buffer)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"There’s a lot of abstraction here ("},{"type":"text","text":"ptr","style":{"code":true}},{"type":"text","text":" could still be carrying info about the size, strides, etc.).\nI think we’ll need to figure out exactly what info needs to be propagated between methods before this thing really materializes though."}]}]}]},{"client_msg_id":"b5ff0c13-ed9c-4e2c-be98-4feef14a5e72","type":"message","text":"oh, interesting. that might be useful for the GPU stack indeed; CUDA.jl's current Buffer abstraction is pretty ad hoc. it does need to deal with a variety of types of buffers though (some typed, others untyped, etc), so it might be hard to generalize.","user":"U68A3ASP9","ts":"1614957978.101800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"XOWc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"oh, interesting. that might be useful for the GPU stack indeed; CUDA.jl's current Buffer abstraction is pretty ad hoc. it does need to deal with a variety of types of buffers though (some typed, others untyped, etc), so it might be hard to generalize."}]}]}]},{"client_msg_id":"9175dc27-500d-4907-a109-76c32b580365","type":"message","text":"most of that is in here: <https://github.com/JuliaGPU/CUDA.jl/blob/master/lib/cudadrv/memory.jl>","user":"U68A3ASP9","ts":"1614958001.102000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6xd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"most of that is in here: "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/blob/master/lib/cudadrv/memory.jl"}]}]}]},{"client_msg_id":"5eb1c431-6f30-40c9-8f15-018d376da239","type":"message","text":"oneAPI's is here: <https://github.com/JuliaGPU/oneAPI.jl/blob/master/lib/level-zero/memory.jl>. pretty similar, but as always with some subtle differences.","user":"U68A3ASP9","ts":"1614958073.102900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3Rcj0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"oneAPI's is here: "},{"type":"link","url":"https://github.com/JuliaGPU/oneAPI.jl/blob/master/lib/level-zero/memory.jl"},{"type":"text","text":". pretty similar, but as always with some subtle differences."}]}]}]},{"client_msg_id":"665bf21d-a566-409d-9d49-aeb406c7ab03","type":"message","text":"Thanks! I’ll take a look at those.\n\n&gt; it does need to deal with a variety of types of buffers though\nI anticipate there needing to be a lot of wiggle room here.\nJust in terms of CPUs we need completely unique buffers that manage mutability/immutability and static/dynamic/variable sizes.\nThis has the potential to be a very deep interface so we’re just trying to get the most superficial and generic methods in place.\nHopefully this will let us hack on the more intricate details for a long time (optimizing GC, threading, bit masks etc.) while allowing users a consistent experience.","user":"UCLGS1HML","ts":"1614958530.104300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2VLE8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks! I’ll take a look at those.\n\n"}]},{"type":"rich_text_quote","elements":[{"type":"text","text":"it does need to deal with a variety of types of buffers though"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nI anticipate there needing to be a lot of wiggle room here.\nJust in terms of CPUs we need completely unique buffers that manage mutability/immutability and static/dynamic/variable sizes.\nThis has the potential to be a very deep interface so we’re just trying to get the most superficial and generic methods in place.\nHopefully this will let us hack on the more intricate details for a long time (optimizing GC, threading, bit masks etc.) while allowing users a consistent experience."}]}]}]},{"client_msg_id":"71318651-33ac-4e6f-ab97-03070dc7cefe","type":"message","text":"Is there an option to restrict the maximum number of registers used in a kernel like -maxrregcount option for nvcc?","user":"U01FXSDEXN3","ts":"1614959005.105400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Wg7X2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there an option to restrict the maximum number of registers used in a kernel like -maxrregcount option for nvcc?"}]}]}]},{"client_msg_id":"fae17c2c-e70b-4fc9-a5eb-6e9b6be8c39c","type":"message","text":"yes, see <https://juliagpu.github.io/CUDA.jl/stable/api/compiler/#CUDA.cufunction>. you can pass these args to `@cuda`","user":"U68A3ASP9","ts":"1614959061.105700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wpD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yes, see "},{"type":"link","url":"https://juliagpu.github.io/CUDA.jl/stable/api/compiler/#CUDA.cufunction"},{"type":"text","text":". you can pass these args to "},{"type":"text","text":"@cuda","style":{"code":true}}]}]}]},{"client_msg_id":"8db22a36-e7c1-406a-992f-7185ffdda52d","type":"message","text":"Thanks!","user":"U01FXSDEXN3","ts":"1614959090.105900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Gii","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks!"}]}]}]},{"client_msg_id":"544c6a15-cbc7-49f2-ad60-a689f8a41921","type":"message","text":"when creating kernels with KernelAbstractions, does KernelAbstractions figure out on its own whether there are  enough blocks on the GPU to fill the `ndrange` (and otherwise each \"physical block\" sequentially works on several blocks in the partition), or is it up to each individual kernel to deal with this?","user":"U6BJ9E351","ts":"1614961295.108100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"v3i","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"when creating kernels with KernelAbstractions, does KernelAbstractions figure out on its own whether there are  enough blocks on the GPU to fill the "},{"type":"text","text":"ndrange","style":{"code":true}},{"type":"text","text":" (and otherwise each \"physical block\" sequentially works on several blocks in the partition), or is it up to each individual kernel to deal with this?"}]}]}]},{"client_msg_id":"f9e3c26f-bab3-4ba5-a617-f043687467cf","type":"message","text":"You can oversubscribe","user":"U67BJLYCS","ts":"1614963149.108400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UaWp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You can oversubscribe"}]}]}]},{"client_msg_id":"81c3fb80-3199-4ed1-9471-9c07fad4008c","type":"message","text":"There are some limits with how big a CUDA grid can be","user":"U67BJLYCS","ts":"1614963182.109000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TSb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"There are some limits with how big a CUDA grid can be"}]}]}]},{"client_msg_id":"ae697332-1f13-45a6-b8aa-2652930efd40","type":"message","text":"cool! I'm getting used to the `ndrange` system and it's actually pretty nice that it handles a fair amount of stuff automatically. Slightly unrelated, I also find it nice that I can use `groupsize` longer than 3, say `(1, 1, 32, 32)`, and it doesn't complain: it can simplify indexing a lot in some cases.","user":"U6BJ9E351","ts":"1614963806.111600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MvQU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"cool! I'm getting used to the "},{"type":"text","text":"ndrange","style":{"code":true}},{"type":"text","text":" system and it's actually pretty nice that it handles a fair amount of stuff automatically. Slightly unrelated, I also find it nice that I can use "},{"type":"text","text":"groupsize","style":{"code":true}},{"type":"text","text":" longer than 3, say "},{"type":"text","text":"(1, 1, 32, 32)","style":{"code":true}},{"type":"text","text":", and it doesn't complain: it can simplify indexing a lot in some cases."}]}]}]},{"client_msg_id":"374e4763-5294-46fa-97e6-b64901859c5f","type":"message","text":"<@U68A3ASP9>, <@U67BJLYCS>, <@U7THT3TM3> can someone enable buildkite for <https://github.com/JuliaFolds/UnionArrays.jl>?","user":"UC7AF7NSU","ts":"1614980627.111900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"f5IQ=","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":", "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":", "},{"type":"user","user_id":"U7THT3TM3"},{"type":"text","text":" can someone enable buildkite for "},{"type":"link","url":"https://github.com/JuliaFolds/UnionArrays.jl"},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"02a8d058-25ff-4259-99cf-8498840d47bf","type":"message","text":"(UnionArrays.jl is my shot at trying to create Union eltype support for GPU arrays without explicitly coding for GPU arrays)","user":"UC7AF7NSU","ts":"1614980633.112100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Kgg+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(UnionArrays.jl is my shot at trying to create Union eltype support for GPU arrays without explicitly coding for GPU arrays)"}]}]}],"thread_ts":"1614980633.112100","reply_count":1,"reply_users_count":1,"latest_reply":"1614980764.112200","reply_users":["UC7AF7NSU"],"subscribed":false},{"client_msg_id":"25c5a0a2-b636-48ae-b73d-dffb1cedf426","type":"message","text":"I want to do \"partially\" batched matrix multiplication i.e. A*B but where A is n x p x b and B is p x d . I can repeat B and use batched_mul but it seems stupid, does \"partial\" batched_mul exist and has a name ?","user":"UKJSNT1QR","ts":"1615129529.114000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KgFs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I want to do \"partially\" batched matrix multiplication i.e. A*B but where A is n x p x b and B is p x d . I can repeat B and use batched_mul but it seems stupid, does \"partial\" batched_mul exist and has a name ?"}]}]}]},{"client_msg_id":"6b83a675-dacd-4cae-9f16-d2e643b43d90","type":"message","text":"Disregard that, this is actually already taken care by batched_mul :smile:","user":"UKJSNT1QR","ts":"1615129988.114500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ONX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Disregard that, this is actually already taken care by batched_mul "},{"type":"emoji","name":"smile"}]}]}]},{"client_msg_id":"6e40474b-c890-4819-97ab-39a1fe920de8","type":"message","text":"Hi, I'm new to using the GPU in julia computations. When I install `CUDA.jl` and try to use it, I get `CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE`. Any ideas why that would happen? I'm on a somewhat-offline cluster that has 1080Tis with CUDA11.0 installed","user":"U01PJ63E11N","ts":"1615154622.116200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6ku3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I'm new to using the GPU in julia computations. When I install "},{"type":"text","text":"CUDA.jl","style":{"code":true}},{"type":"text","text":" and try to use it, I get "},{"type":"text","text":"CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE","style":{"code":true}},{"type":"text","text":". Any ideas why that would happen? I'm on a somewhat-offline cluster that has 1080Tis with CUDA11.0 installed"}]}]}]},{"client_msg_id":"4fc743e4-9ac1-4a17-86a2-586d0676a013","type":"message","text":"I just had an interesting bug. I wonder if it's a well-known pitfall. I had a CUDA kernel like this\n\n```function kernel(...)\n    acc = basecase(...)  # acc can be of different type for different thread\n    combine(acc, ...)    # uses sync_threads() in a loop\nend```\nand now I'm trying to make it work for the case where `acc` is a small `Union`.\n\nSince a method dispatch is a branch after everything is inlined, the above code tries to synchronize threads that are in different branches and hence deadlocks (or at least that's my theory for why it didn't work). It's obvious in hindsight but I had no idea when I was first writing the code. (<https://github.com/JuliaFolds/FoldsCUDA.jl/pull/49/commits/6fee3d172c68dc58b813f6b7e61a58470ace5b01#diff-d8ec7579b0bb38bbb8581baf2a6398e4b19d7859e670f009a6058a62a29b8ba4R198-R202>)\n\nAlso, this makes me wonder if union splitting can potentially introduce a deadlock, if the compiler is written for sequential programs in mind. (cc <@U67BJLYCS>)","user":"UC7AF7NSU","ts":"1615169106.116600","team":"T68168MUP","edited":{"user":"UC7AF7NSU","ts":"1615169488.000000"},"blocks":[{"type":"rich_text","block_id":"0eMeb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I just had an interesting bug. I wonder if it's a well-known pitfall. I had a CUDA kernel like this\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function kernel(...)\n    acc = basecase(...)  # acc can be of different type for different thread\n    combine(acc, ...)    # uses sync_threads() in a loop\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nand now I'm trying to make it work for the case where "},{"type":"text","text":"acc","style":{"code":true}},{"type":"text","text":" is a small "},{"type":"text","text":"Union","style":{"code":true}},{"type":"text","text":".\n\nSince a method dispatch is a branch after everything is inlined, the above code tries to synchronize threads that are in different branches and hence deadlocks (or at least that's my theory for why it didn't work). It's obvious in hindsight but I had no idea when I was first writing the code. ("},{"type":"link","url":"https://github.com/JuliaFolds/FoldsCUDA.jl/pull/49/commits/6fee3d172c68dc58b813f6b7e61a58470ace5b01#diff-d8ec7579b0bb38bbb8581baf2a6398e4b19d7859e670f009a6058a62a29b8ba4R198-R202"},{"type":"text","text":")\n\nAlso, this makes me wonder if union splitting can potentially introduce a deadlock, if the compiler is written for sequential programs in mind. (cc "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":")"}]}]}],"thread_ts":"1615169106.116600","reply_count":1,"reply_users_count":1,"latest_reply":"1615169623.116800","reply_users":["UC7AF7NSU"],"subscribed":false},{"client_msg_id":"27793dbd-6c1a-4ea6-a47d-b0ee2d8c54ef","type":"message","text":"When you get `isbits` errors, how do you interpret the stacktrace to find the culprit ?\n\nBecause I'm just seeing `CuArrays` here (and wrappers of CuArrays, I assume one of them is not GPU friendly)\n\n```ERROR: GPU compilation of kernel broadcast_kernel(CUDA.CuKernelContext, CuDeviceArray{Float32,1,1}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(Zygote.accum),Tuple{Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Base.ReshapedArray{Float32,1,NNlib.BatchedTranspose{Float32,CuArray{Float32,3}},Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}},Tuple{Bool},Tuple{Int64}}}}, Int64) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(Zygote.accum),Tuple{Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Base.ReshapedArray{Float32,1,NNlib.BatchedTranspose{Float32,CuArray{Float32,3}},Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}},Tuple{Bool},Tuple{Int64}}}}, which is not isbits:\n  .args is of type Tuple{Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Base.ReshapedArray{Float32,1,NNlib.BatchedTranspose{Float32,CuArray{Float32,3}},Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}},Tuple{Bool},Tuple{Int64}}} which is not isbits.\n    .2 is of type Base.Broadcast.Extruded{Base.ReshapedArray{Float32,1,NNlib.BatchedTranspose{Float32,CuArray{Float32,3}},Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}},Tuple{Bool},Tuple{Int64}} which is not isbits.\n      .x is of type Base.ReshapedArray{Float32,1,NNlib.BatchedTranspose{Float32,CuArray{Float32,3}},Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}} which is not isbits.\n        .parent is of type NNlib.BatchedTranspose{Float32,CuArray{Float32,3}} which is not isbits.\n          .parent is of type CuArray{Float32,3} which is not isbits.\n            .ctx is of type CuContext which is not isbits.```","user":"UKJSNT1QR","ts":"1615280938.001200","team":"T68168MUP","edited":{"user":"UKJSNT1QR","ts":"1615280978.000000"},"blocks":[{"type":"rich_text","block_id":"mDLgb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"When you get "},{"type":"text","text":"isbits","style":{"code":true}},{"type":"text","text":" errors, how do you interpret the stacktrace to find the culprit ?\n\nBecause I'm just seeing "},{"type":"text","text":"CuArrays","style":{"code":true}},{"type":"text","text":" here (and wrappers of CuArrays, I assume one of them is not GPU friendly)\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: GPU compilation of kernel broadcast_kernel(CUDA.CuKernelContext, CuDeviceArray{Float32,1,1}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(Zygote.accum),Tuple{Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Base.ReshapedArray{Float32,1,NNlib.BatchedTranspose{Float32,CuArray{Float32,3}},Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}},Tuple{Bool},Tuple{Int64}}}}, Int64) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(Zygote.accum),Tuple{Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Base.ReshapedArray{Float32,1,NNlib.BatchedTranspose{Float32,CuArray{Float32,3}},Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}},Tuple{Bool},Tuple{Int64}}}}, which is not isbits:\n  .args is of type Tuple{Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Base.ReshapedArray{Float32,1,NNlib.BatchedTranspose{Float32,CuArray{Float32,3}},Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}},Tuple{Bool},Tuple{Int64}}} which is not isbits.\n    .2 is of type Base.Broadcast.Extruded{Base.ReshapedArray{Float32,1,NNlib.BatchedTranspose{Float32,CuArray{Float32,3}},Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}},Tuple{Bool},Tuple{Int64}} which is not isbits.\n      .x is of type Base.ReshapedArray{Float32,1,NNlib.BatchedTranspose{Float32,CuArray{Float32,3}},Tuple{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64}}} which is not isbits.\n        .parent is of type NNlib.BatchedTranspose{Float32,CuArray{Float32,3}} which is not isbits.\n          .parent is of type CuArray{Float32,3} which is not isbits.\n            .ctx is of type CuContext which is not isbits."}]}]}]},{"client_msg_id":"707fe220-a928-4d9f-b75e-262f3f80cfaa","type":"message","text":"CuArray can't exist in a kernel, that should have been converted to a CuDeviceArray","user":"U68A3ASP9","ts":"1615281815.001800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"C3tsB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CuArray can't exist in a kernel, that should have been converted to a CuDeviceArray"}]}]}]},{"client_msg_id":"936a915b-1bf7-405c-9d9b-e84608ede785","type":"message","text":"we use Adapt.jl for that, which doesn't udnerstand NNLib.BatchedTranspose here","user":"U68A3ASP9","ts":"1615281830.002200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"R4WmR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"we use Adapt.jl for that, which doesn't udnerstand NNLib.BatchedTranspose here"}]}]}]},{"client_msg_id":"8aeab2ce-0f1d-428f-8672-47e7c07d08bb","type":"message","text":"Ok got it, seems like `batched_transpose` causes many headaches, as the doc for `NNlib.batched_transpose` says `PermutedDimsArray` is more widely supported I'll ask the Flux devs to just add a depracation warning if it makes sense to","user":"UKJSNT1QR","ts":"1615282750.003900","team":"T68168MUP","edited":{"user":"UKJSNT1QR","ts":"1615282788.000000"},"blocks":[{"type":"rich_text","block_id":"fN9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ok got it, seems like "},{"type":"text","text":"batched_transpose","style":{"code":true}},{"type":"text","text":" causes many headaches, as the doc for "},{"type":"text","text":"NNlib.batched_transpose","style":{"code":true}},{"type":"text","text":" says "},{"type":"text","text":"PermutedDimsArray","style":{"code":true}},{"type":"text","text":" is more widely supported I'll ask the Flux devs to just add a depracation warning if it makes sense to"}]}]}]},{"client_msg_id":"2f3fbba1-255f-4fd2-bf38-bb6dcb05179a","type":"message","text":"I've been writing reduce for type-changing accumulator and now I get to the point that I can run some transducers with non-trivial state type transition like `Transducers.ReducePartitionBy` on GPU (which, in turn, can be used for group reduce operation on sorted tables): <https://github.com/JuliaFolds/FoldsCUDA.jl/pull/49>","user":"UC7AF7NSU","ts":"1615327125.005200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"000Fx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I've been writing reduce for type-changing accumulator and now I get to the point that I can run some transducers with non-trivial state type transition like "},{"type":"text","text":"Transducers.ReducePartitionBy","style":{"code":true}},{"type":"text","text":" on GPU (which, in turn, can be used for group reduce operation on sorted tables): "},{"type":"link","url":"https://github.com/JuliaFolds/FoldsCUDA.jl/pull/49"}]}]}]},{"client_msg_id":"507c6164-971e-41ac-b13a-54f7d3d7bb1b","type":"message","text":"I feel I'm becoming so good at begging the Julia compiler to union splitting things :joy:","user":"UC7AF7NSU","ts":"1615327140.005500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UxsGt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I feel I'm becoming so good at begging the Julia compiler to union splitting things "},{"type":"emoji","name":"joy"}]}]}]},{"client_msg_id":"afd780f3-1c5e-4bca-9032-af9dc9b34e2a","type":"message","text":"But, since my approach for manual \"forced\" union splitting is mostly based on the continuation-passing style which could hurt performance on GPU  (due to thread divergence), I wonder if we can enable different optimizations/lowering of small Union. Can we have a custom pass on Julia's SSA IR in GPUCompiler? (cc <@U68A3ASP9> <@U67BJLYCS>)","user":"UC7AF7NSU","ts":"1615327192.006300","team":"T68168MUP","edited":{"user":"UC7AF7NSU","ts":"1615327245.000000"},"blocks":[{"type":"rich_text","block_id":"zIHl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But, since my approach for manual \"forced\" union splitting is mostly based on the continuation-passing style which could hurt performance on GPU  (due to thread divergence), I wonder if we can enable different optimizations/lowering of small Union. Can we have a custom pass on Julia's SSA IR in GPUCompiler? (cc "},{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":" "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":")"}]}]}],"thread_ts":"1615327192.006300","reply_count":3,"reply_users_count":2,"latest_reply":"1615328312.007000","reply_users":["U6A0PD8CR","U67BJLYCS"],"subscribed":false},{"type":"message","subtype":"channel_join","ts":"1615336275.011000","user":"U01K2JB9GPJ","text":"<@U01K2JB9GPJ> has joined the channel","inviter":"UDGT4PM41"},{"client_msg_id":"9623b7d4-1f67-4f2f-9321-3d9a01e27f57","type":"message","text":"Hi, I just found an odd error `access to undefined reference` . It occurred when my GPU memory overflow and after that,  even I reclaim enough memory, the error would still appeared when I tried any GPU-related commands such as very simple `CUDA.memory_status()`   . I was wondering how to fix it.","user":"U01977X150R","ts":"1615337872.015700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6t0a","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I just found an odd error"},{"type":"text","text":" access to undefined reference","style":{"code":true}},{"type":"text","text":" . It occurred when my GPU memory overflow and after that,  even I reclaim enough memory, the error would still appeared when I tried any GPU-related commands such as very simple "},{"type":"text","text":"CUDA.memory_status() ","style":{"code":true}},{"type":"text","text":"  . I was wondering how to fix it."}]}]}],"thread_ts":"1615337872.015700","reply_count":1,"reply_users_count":1,"latest_reply":"1615339979.016200","reply_users":["U6A0PD8CR"],"subscribed":false},{"client_msg_id":"8008f771-d67a-477a-88d9-99705c8c01e0","type":"message","text":"I looked at some of the meeting notes for the last ML Ecosystem coordination call and there was some talk of KA replacing vendor specific kernels. Has there been a change in the philosophy for KA?\n\nLast I understood it wasn't meant to reach full performance potential of the hardware, but instead trade off some of that?","user":"UDGT4PM41","ts":"1615399093.018700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"U3/i","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I looked at some of the meeting notes for the last ML Ecosystem coordination call and there was some talk of KA replacing vendor specific kernels. Has there been a change in the philosophy for KA?\n\nLast I understood it wasn't meant to reach full performance potential of the hardware, but instead trade off some of that?"}]}]}]},{"client_msg_id":"2a24ca73-c087-4b2e-8449-f5f721ede884","type":"message","text":"KA hasn't changed its portability goals, but users may not want to use it for CPU implementations because it's not currently close to optimal performance (because of loss of vectorization)","user":"U6A0PD8CR","ts":"1615403041.019900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"qWHP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"KA hasn't changed its portability goals, but users may not want to use it for CPU implementations because it's not currently close to optimal performance (because of loss of vectorization)"}]}]}],"reactions":[{"name":"+1","users":["UDGT4PM41"],"count":1}]},{"client_msg_id":"284db8d2-2d61-4397-b45f-b46aea632883","type":"message","text":"Yup, that bullet was meant to say what Julian just said :slightly_smiling_face:","user":"UH9KWTTD3","ts":"1615410220.020400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nyc3Y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yup, that bullet was meant to say what Julian just said "},{"type":"emoji","name":"slightly_smiling_face"}]}]}]},{"client_msg_id":"41adf65b-96af-4516-aa42-89d0f64e0019","type":"message","text":"Can anyone point me to how displaying a `CuArray` works when scalar indexing is not allowed?","user":"UH9KWTTD3","ts":"1615486119.021900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kLcVt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can anyone point me to how displaying a "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":" works when scalar indexing is not allowed?"}]}]}]},{"client_msg_id":"ab409ae6-85bb-42b4-bcea-d569058eaac3","type":"message","text":"Trying to fix <https://github.com/FluxML/Flux.jl/issues/1532> and it is weirdly not as simple as I thought","user":"UH9KWTTD3","ts":"1615486156.022400","team":"T68168MUP","edited":{"user":"UH9KWTTD3","ts":"1615486162.000000"},"blocks":[{"type":"rich_text","block_id":"ilXIt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Trying to fix "},{"type":"link","url":"https://github.com/FluxML/Flux.jl/issues/1532"},{"type":"text","text":" and it is weirdly not as simple as I thought"}]}]}]},{"client_msg_id":"f603b10b-4a8d-4b04-baff-9320e5274427","type":"message","text":"It problably calls `@cuprint` or something related to it","user":"U01C3624SGJ","ts":"1615486283.023800","team":"T68168MUP","edited":{"user":"U01C3624SGJ","ts":"1615486303.000000"},"blocks":[{"type":"rich_text","block_id":"ioWk6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It problably calls "},{"type":"text","text":"@cuprint","style":{"code":true}},{"type":"text","text":" or something related to it"}]}]}]},{"client_msg_id":"fd245a18-40a3-4065-9eae-04e83c7d5877","type":"message","text":"Then again, checking the stacktrace it doesnt go through the the CUDA print function","user":"U01C3624SGJ","ts":"1615486488.025500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hvA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Then again, checking the stacktrace it doesnt go through the the CUDA print function"}]}]}]},{"client_msg_id":"a0ebe643-8691-43f2-b944-907b96805281","type":"message","text":"Since it is just printing you could temporarily allow it, or just download the data to the host?","user":"U67D54KS8","ts":"1615486507.026100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"w9A=t","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Since it is just printing you could temporarily allow it, or just download the data to the host?"}]}]}]},{"client_msg_id":"53098f4f-fcb5-4a05-a89d-1a499e8813b4","type":"message","text":"Yeah downloading to the host is what I did. I just wanted to see if there was something else I could be doing, but it sounds like it is more complex than it is worth. Thanks!","user":"UH9KWTTD3","ts":"1615486568.027000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"G53mX","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah downloading to the host is what I did. I just wanted to see if there was something else I could be doing, but it sounds like it is more complex than it is worth. Thanks!"}]}]}]},{"client_msg_id":"6042dced-4309-4075-bc52-32db2df2bd6e","type":"message","text":"<https://github.com/JuliaGPU/GPUArrays.jl/blob/7f68400531cf06a0b3e3424f94f5e25e8a8a05c1/src/host/abstractarray.jl#L47-L59>","user":"U68A3ASP9","ts":"1615491533.027300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j8R=B","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/JuliaGPU/GPUArrays.jl/blob/7f68400531cf06a0b3e3424f94f5e25e8a8a05c1/src/host/abstractarray.jl#L47-L59"}]}]}]},{"client_msg_id":"43523251-6c8f-48f3-9161-1f28bedee547","type":"message","text":"for quick hacks you can just put `@allowscalar` in front of an expression","user":"U68A3ASP9","ts":"1615491567.027900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"69UA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"for quick hacks you can just put "},{"type":"text","text":"@allowscalar","style":{"code":true}},{"type":"text","text":" in front of an expression"}]}]}],"reactions":[{"name":"+1::skin-tone-5","users":["UH9KWTTD3"],"count":1}]},{"type":"message","text":"how do I figure out what went wrong here?","files":[{"id":"F01RQQAQEF2","created":1615517849,"timestamp":1615517849,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"UPK2KJ95Y","editable":false,"size":15264,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01RQQAQEF2/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01RQQAQEF2/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01RQQAQEF2-db51092581/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01RQQAQEF2-db51092581/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01RQQAQEF2-db51092581/image_360.png","thumb_360_w":360,"thumb_360_h":62,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01RQQAQEF2-db51092581/image_480.png","thumb_480_w":480,"thumb_480_h":82,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01RQQAQEF2-db51092581/image_160.png","original_w":655,"original_h":112,"thumb_tiny":"AwAIADCnwSOQfwxSnp2pi9ad/DTJHGk49utKegpvf8aAHd6a3T/69L3pD92gD//Z","permalink":"https://julialang.slack.com/files/UPK2KJ95Y/F01RQQAQEF2/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01RQQAQEF2-751b5c969a","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"Cqw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"how do I figure out what went wrong here?"}]}]}],"user":"UPK2KJ95Y","display_as_bot":false,"ts":"1615517862.028500"},{"client_msg_id":"f5f3f147-5f12-483b-a02a-7b33a8f40588","type":"message","text":"Run with `julia -g2`","user":"U67BJLYCS","ts":"1615518008.029400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2Ioq2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Run with "},{"type":"text","text":"julia -g2","style":{"code":true}}]}]}]},{"client_msg_id":"e0c86411-2cb7-4cec-a97a-67ffccd85142","type":"message","text":"Sorry for the high-level question, but I’m curious if there are any block-sparse GPU kernels / support in any of the various julia packages.\n\nThe closest thing I found were bindings for CUSPARSE, which is not block-sparse, unfortunately.","user":"U01QUBGB6JJ","ts":"1615524054.031300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Js/C","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Sorry for the high-level question, but I’m curious if there are any block-sparse GPU kernels / support in any of the various julia packages.\n\nThe closest thing I found were bindings for CUSPARSE, which is not block-sparse, unfortunately."}]}]}]},{"type":"message","text":"This is equivalent to `for x_index in 2:101, y_index in 2:101`  right?","files":[{"id":"F01RDRT55C1","created":1615526878,"timestamp":1615526878,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"UPK2KJ95Y","editable":false,"size":12433,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01RDRT55C1/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01RDRT55C1/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01RDRT55C1-225affd499/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01RDRT55C1-225affd499/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01RDRT55C1-225affd499/image_360.png","thumb_360_w":360,"thumb_360_h":57,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01RDRT55C1-225affd499/image_480.png","thumb_480_w":480,"thumb_480_h":76,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01RDRT55C1-225affd499/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01RDRT55C1-225affd499/image_720.png","thumb_720_w":720,"thumb_720_h":113,"original_w":756,"original_h":119,"thumb_tiny":"AwAHADDQLDmkBHWkPU0g+7TsA4sDShh/kUwUg/iosBJkE9vypwIFRD71PosB/9k=","permalink":"https://julialang.slack.com/files/UPK2KJ95Y/F01RDRT55C1/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01RDRT55C1-de791bc6de","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"j0Sn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This is equivalent to "},{"type":"text","text":"for x_index in 2:101, y_index in 2:101  ","style":{"code":true}},{"type":"text","text":"right?"}]}]}],"user":"UPK2KJ95Y","display_as_bot":false,"ts":"1615526950.032100"},{"client_msg_id":"9837b173-e4ec-4382-92a7-24dc847faa10","type":"message","text":"I have a complex gitlab-ci workflow which needs to be ported to buildkite. Is there any guide for a one-to-one conversion.\n\nParticularly I used .gitlab-ci.yml to point another repo which had all the github-bot functions and gitlab-ci parameters, and pass the variables.\n``` include: path/to/another/repo\nvariables: A\n           B```\n (Is making a plugin the only way?)","user":"UN5FQHFNY","ts":"1615552949.034700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1fS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a complex gitlab-ci workflow which needs to be ported to buildkite. Is there any guide for a one-to-one conversion.\n\nParticularly I used .gitlab-ci.yml to point another repo which had all the github-bot functions and gitlab-ci parameters, and pass the variables.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":" include: path/to/another/repo\nvariables: A\n           B"}]},{"type":"rich_text_section","elements":[{"type":"text","text":" (Is making a plugin the only way?)"}]}]}]},{"client_msg_id":"0d640423-7ab9-4f43-8f8d-25e188cf0f79","type":"message","text":"Do any of you know what/where I need to start to make a kernel implementation using Julia as fast as nvcc implementation? I have two different kernel implementations, one is written in C using nvcc and the other in Julia. They are almost the same, except inevitable changes such as a pointer to array in C being implemented using CuDeviceArray in Julia. They were launched using the same kernel configuration. Currently, nvcc implementation is 6 times faster than Julia, and I would like to reduce this performance gap.\n\nIt seems that generated code is very different. In the case of nvcc, the maximum number of registers was 190 but it was 250 for Julia. When I used nvcc, I set `maxrregcount`  to 96 for better occupancy and saw about 30% improvement. I tried similar things for Julia implementation by trying different values of `maxregs` but with no luck. Any help will be greatly appreciated.","user":"U01FXSDEXN3","ts":"1615569301.043900","team":"T68168MUP","edited":{"user":"U01FXSDEXN3","ts":"1615569513.000000"},"blocks":[{"type":"rich_text","block_id":"vZb51","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Do any of you know what/where I need to start to make a kernel implementation using Julia as fast as nvcc implementation? I have two different kernel implementations, one is written in C using nvcc and the other in Julia. They are almost the same, except inevitable changes such as a pointer to array in C being implemented using CuDeviceArray in Julia. They were launched using the same kernel configuration. Currently, nvcc implementation is 6 times faster than Julia, and I would like to reduce this performance gap.\n\nIt seems that generated code is very different. In the case of nvcc, the maximum number of registers was 190 but it was 250 for Julia. When I used nvcc, I set "},{"type":"text","text":"maxrregcount","style":{"code":true}},{"type":"text","text":"  to 96 for better occupancy and saw about 30% improvement. I tried similar things for Julia implementation by trying different values of "},{"type":"text","text":"maxregs","style":{"code":true}},{"type":"text","text":" but with no luck. Any help will be greatly appreciated."}]}]}],"thread_ts":"1615569301.043900","reply_count":3,"reply_users_count":2,"latest_reply":"1615570183.044800","reply_users":["U68A3ASP9","U01FXSDEXN3"],"subscribed":false},{"client_msg_id":"b464d63d-1348-4b29-8620-0d55ee2fa92a","type":"message","text":"Hi there. I've run unto an issue with `LowerTriangular` and `CuArray`s:\n```julia&gt; L = LowerTriangular(CUDA.rand(3,3));\njulia&gt; L * CUDA.rand(3)\nERROR: ArgumentError: cannot take the CPU address of a CuArray{Float32,2}\nStacktrace:\n [1] unsafe_convert(::Type{Ptr{Float32}}, ::CuArray{Float32,2}) at /home/vargonis/.julia/packages/CUDA/YeS8q/src/array.jl:211\n [2] trmv!(::Char, ::Char, ::Char, ::CuArray{Float32,2}, ::CuArray{Float32,1}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/blas.jl:1171\n [3] lmul!(::LowerTriangular{Float32,CuArray{Float32,2}}, ::CuArray{Float32,1}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/triangular.jl:737\n [4] *(::LowerTriangular{Float32,CuArray{Float32,2}}, ::CuArray{Float32,1}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/triangular.jl:2003\n [5] top-level scope at REPL[8]:1```\nIt's funny though that `L * L'`, for instance, works fine. Any workaround suggestion, beyond dropping `LowerTriangular`? I'm using it because it is what `cholesky` gives me. I tried using `L.data` but that actually contains stuff above the diagonal.","user":"ULDCM1P9P","ts":"1615669767.050700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/R+","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi there. I've run unto an issue with "},{"type":"text","text":"LowerTriangular","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":"s:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> L = LowerTriangular(CUDA.rand(3,3));\njulia> L * CUDA.rand(3)\nERROR: ArgumentError: cannot take the CPU address of a CuArray{Float32,2}\nStacktrace:\n [1] unsafe_convert(::Type{Ptr{Float32}}, ::CuArray{Float32,2}) at /home/vargonis/.julia/packages/CUDA/YeS8q/src/array.jl:211\n [2] trmv!(::Char, ::Char, ::Char, ::CuArray{Float32,2}, ::CuArray{Float32,1}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/blas.jl:1171\n [3] lmul!(::LowerTriangular{Float32,CuArray{Float32,2}}, ::CuArray{Float32,1}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/triangular.jl:737\n [4] *(::LowerTriangular{Float32,CuArray{Float32,2}}, ::CuArray{Float32,1}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/triangular.jl:2003\n [5] top-level scope at REPL[8]:1"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"It's funny though that "},{"type":"text","text":"L * L'","style":{"code":true}},{"type":"text","text":", for instance, works fine. Any workaround suggestion, beyond dropping "},{"type":"text","text":"LowerTriangular","style":{"code":true}},{"type":"text","text":"? I'm using it because it is what "},{"type":"text","text":"cholesky","style":{"code":true}},{"type":"text","text":" gives me. I tried using "},{"type":"text","text":"L.data","style":{"code":true}},{"type":"text","text":" but that actually contains stuff above the diagonal."}]}]}]},{"client_msg_id":"c5cf7f2a-6344-44e2-a3bf-8fb6308247f9","type":"message","text":"Nvidia : <https://twitter.com/blelbach/status/1371137901258448906?s=19|https://twitter.com/blelbach/status/1371137901258448906?s=19>","user":"UDGT4PM41","ts":"1615740090.051400","team":"T68168MUP","attachments":[{"fallback":"<https://twitter.com/blelbach|@blelbach>: <https://twitter.com/AriKatz20|@AriKatz20> <https://twitter.com/attila_f_feher|@attila_f_feher> <https://twitter.com/_joaogui1|@_joaogui1> <https://twitter.com/ClaymorePT|@ClaymorePT> <https://twitter.com/datametrician|@datametrician> You claimed earlier that Julia is a 10x-100x improvement.\n\nConvince me that it's even a 2x improvement and I'd make the investment!","ts":1615739724,"author_name":"Bryce Adelstein Lelbach","author_link":"https://twitter.com/blelbach/status/1371137901258448906","author_icon":"https://pbs.twimg.com/profile_images/1150126498743037952/oHongNvu_normal.jpg","author_subname":"@blelbach","text":"<https://twitter.com/AriKatz20|@AriKatz20> <https://twitter.com/attila_f_feher|@attila_f_feher> <https://twitter.com/_joaogui1|@_joaogui1> <https://twitter.com/ClaymorePT|@ClaymorePT> <https://twitter.com/datametrician|@datametrician> You claimed earlier that Julia is a 10x-100x improvement.\n\nConvince me that it's even a 2x improvement and I'd make the investment!","service_name":"twitter","service_url":"https://twitter.com/","from_url":"https://twitter.com/blelbach/status/1371137901258448906?s=19","id":1,"original_url":"https://twitter.com/blelbach/status/1371137901258448906?s=19","footer":"Twitter","footer_icon":"https://a.slack-edge.com/80588/img/services/twitter_pixel_snapped_32.png"}],"blocks":[{"type":"rich_text","block_id":"2CDb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Nvidia : "},{"type":"link","url":"https://twitter.com/blelbach/status/1371137901258448906?s=19","text":"https://twitter.com/blelbach/status/1371137901258448906?s=19"}]}]}],"reactions":[{"name":"gpu","users":["UGU761DU2"],"count":1}]},{"client_msg_id":"2a2288be-3177-452d-9df1-35c9f05f10f2","type":"message","text":"Ok, but you're literally arguing with the chair of the US C++ committee ;)","user":"U674T3KB3","ts":"1615740812.052800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uSl/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ok, but you're literally arguing with the chair of the US C++ committee ;)"}]}]}]},{"type":"message","subtype":"thread_broadcast","text":"Yea, that explains all the C++ stanning. I just want to say that I really never intended to convey any sort of antagonism at all towards NVIDIA or python, and I understand that it's important not to overdo things like this. I left one offhand comment (following another one about Julia) and got nerdsniped into debating technical stuff and ecosystem development. Never meant to make it into any sort of language war, which is what seems to be perceived","user":"UDGT4PM41","ts":"1615743660.052900","thread_ts":"1615740812.052800","root":{"client_msg_id":"2a2288be-3177-452d-9df1-35c9f05f10f2","type":"message","text":"Ok, but you're literally arguing with the chair of the US C++ committee ;)","user":"U674T3KB3","ts":"1615740812.052800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uSl/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ok, but you're literally arguing with the chair of the US C++ committee ;)"}]}]}],"thread_ts":"1615740812.052800","reply_count":2,"reply_users_count":2,"latest_reply":"1615743739.053200","reply_users":["UDGT4PM41","U674T3KB3"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"l3u2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yea, that explains all the C++ stanning. I just want to say that I really never intended to convey any sort of antagonism at all towards NVIDIA or python, and I understand that it's important not to overdo things like this. I left one offhand comment (following another one about Julia) and got nerdsniped into debating technical stuff and ecosystem development. Never meant to make it into any sort of language war, which is what seems to be perceived"}]}]}],"client_msg_id":"6530d24a-ce0e-41db-bae9-d5e4947a3e10"},{"client_msg_id":"ce5550ea-d60b-446f-897e-8cf51cf04cb7","type":"message","text":"My GPU’s compute capability is less than the requirement set by latest stable CUDA.jl (3.5 &lt; 5.0) but `test CUDA` has  8531/8536 test passing. If I run into compatibility errors are they going to be insidious like the array I’m mutating is going to turn into garbage or will it be more obvious like the program throws an error?","user":"U01122ME7C5","ts":"1615759973.056400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"D/xd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"My GPU’s compute capability is less than the requirement set by latest stable CUDA.jl (3.5 < 5.0) but "},{"type":"text","text":"test CUDA","style":{"code":true}},{"type":"text","text":" has  8531/8536 test passing. If I run into compatibility errors are they going to be insidious like the array I’m mutating is going to turn into garbage or will it be more obvious like the program throws an error?"}]}]}]},{"client_msg_id":"b2ade93c-5c96-4e22-88aa-878a42c9007c","type":"message","text":"no, it's going to be very obvious errors. for now, there's actually not much relying on &gt;sm_35, so don't worry about it.","user":"U68A3ASP9","ts":"1615791586.058300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OL/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"no, it's going to be very obvious errors. for now, there's actually not much relying on >sm_35, so don't worry about it."}]}]}]},{"client_msg_id":"9f399c11-63e8-48cb-a73d-3e7fc1da7e5c","type":"message","text":"Reminder: GPU BoF/call in ~90 minutes -- 16:00 UTC -- for all your GPU questions, ...\n<https://mit.zoom.us/j/92410737378?pwd=MWkrSkIzMEtRWkZHdm1XQmpNbm90UT09>","user":"U68A3ASP9","ts":"1615819190.060600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DBU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Reminder: GPU BoF/call in ~90 minutes -- 16:00 UTC -- for all your GPU questions, ...\n"},{"type":"link","url":"https://mit.zoom.us/j/92410737378?pwd=MWkrSkIzMEtRWkZHdm1XQmpNbm90UT09"}]}]}]},{"client_msg_id":"b0923123-4a58-4d03-bd78-70e3e6660231","type":"message","text":"Is there a way to see ptxas info of each device function? I have a kernel, and it calls some number of device functions. I would like to see ptxas information, such as number of registers, stack frame, and spill stores, of each device function, similar to the output obtained from nvcc compiled with '-Xptxas=-v' flag.","user":"U01FXSDEXN3","ts":"1615840443.064900","team":"T68168MUP","edited":{"user":"U01FXSDEXN3","ts":"1615840481.000000"},"blocks":[{"type":"rich_text","block_id":"twz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to see ptxas info of each device function? I have a kernel, and it calls some number of device functions. I would like to see ptxas information, such as number of registers, stack frame, and spill stores, of each device function, similar to the output obtained from nvcc compiled with '-Xptxas=-v' flag."}]}]}]},{"client_msg_id":"f314336b-344c-4b74-a7e9-e27c15958ae1","type":"message","text":"`JULIA_DEBUG=CUDA`","user":"U67BJLYCS","ts":"1615840768.065300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QjN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"JULIA_DEBUG=CUDA","style":{"code":true}}]}]}]},{"client_msg_id":"66d0860f-8528-4d9e-a38c-74aa81ed877b","type":"message","text":"should do that","user":"U67BJLYCS","ts":"1615840772.065500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"O0sj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"should do that"}]}]}]},{"client_msg_id":"26541daf-5df1-437b-a695-0bfd1e884cc1","type":"message","text":"Thanks!","user":"U01FXSDEXN3","ts":"1615841111.065700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DTqs4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks!"}]}]}]},{"client_msg_id":"7c06c005-d043-421f-820c-44fb4c8555b2","type":"message","text":"IIRC","user":"U67BJLYCS","ts":"1615841232.065900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RkpN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"IIRC"}]}]}]},{"client_msg_id":"8a466748-595b-4ade-86c8-b9bd1e4eef0b","type":"message","text":"If it doesn't come back and complain","user":"U67BJLYCS","ts":"1615841242.066300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PYH9o","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If it doesn't come back and complain"}]}]}]}]}