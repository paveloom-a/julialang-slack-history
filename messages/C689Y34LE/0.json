{"cursor": 2, "messages": [{"client_msg_id":"8dcd4c88-8ee7-40f0-a495-454c45b48f9a","type":"message","text":"I was just slightly scared to see that e.g. `2 .* unified_memory_array` returns another unified memory array, but I suppose the result will be physically on whatever GPU computed it and from that point on theres no overhead for that GPU to do other things with this array?","user":"UUMJUCYRK","ts":"1611738155.063800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Zi8t","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I was just slightly scared to see that e.g. "},{"type":"text","text":"2 .* unified_memory_array","style":{"code":true}},{"type":"text","text":" returns another unified memory array, but I suppose the result will be physically on whatever GPU computed it and from that point on theres no overhead for that GPU to do other things with this array?"}]}]}]},{"client_msg_id":"f478c383-9923-469c-83fb-c1f5db4f74ef","type":"message","text":"&lt;sm_60 is OK as long as you have peer to peer, if not it'll keep the array in CPU memory and use slow PCI-E reads","user":"U68A3ASP9","ts":"1611738363.064400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"An5h","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"<sm_60 is OK as long as you have peer to peer, if not it'll keep the array in CPU memory and use slow PCI-E reads"}]}]}]},{"client_msg_id":"83f871ae-29dd-4053-88d5-e3a7a9cbe0db","type":"message","text":"&gt; sm_60 is stricly better because then your array, or the pages you're working with, will transparantly migrate to the GPU that is processing them","user":"U68A3ASP9","ts":"1611738385.065000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QbRr+","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"sm_60 is stricly better because then your array, or the pages you're working with, will transparantly migrate to the GPU that is processing them"}]}]}]},{"client_msg_id":"be5904a8-a2d3-4312-9859-ee20c057dbc0","type":"message","text":"&gt; I was just slightly scared to see that e.g. 2 .* unified_memory_array returns another unified memory array,\nCuArray doesn't currently really support unified memory, so these allocations will be tied to a device, and not be unified","user":"U68A3ASP9","ts":"1611738430.065700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/rBQ","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"I was just slightly scared to see that e.g. 2 .* unified_memory_array returns another unified memory array,"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"CuArray doesn't currently really support unified memory, so these allocations will be tied to a device, and not be unified"}]}]}]},{"type":"message","text":"do you mean that just the constructor doesn't support unified? once allocated, doesn't the fact that e.g. here I can access `y` from device 1 mean the result was in unified memory?","files":[{"id":"F01LQ6QPECQ","created":1611738630,"timestamp":1611738630,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"UUMJUCYRK","editable":false,"size":65370,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01LQ6QPECQ/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01LQ6QPECQ/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01LQ6QPECQ-e6ef08d6bb/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01LQ6QPECQ-e6ef08d6bb/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01LQ6QPECQ-e6ef08d6bb/image_360.png","thumb_360_w":360,"thumb_360_h":219,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01LQ6QPECQ-e6ef08d6bb/image_480.png","thumb_480_w":480,"thumb_480_h":292,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01LQ6QPECQ-e6ef08d6bb/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01LQ6QPECQ-e6ef08d6bb/image_720.png","thumb_720_w":720,"thumb_720_h":438,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01LQ6QPECQ-e6ef08d6bb/image_800.png","thumb_800_w":800,"thumb_800_h":486,"original_w":954,"original_h":580,"thumb_tiny":"AwAdADDQ/GgfWlxznH6UuaADn1o59qM0ZoAOfajmjNGaAG4+bP8AWncegpvO7rxTsfWgAzRmlooATNGaWigD/9k=","permalink":"https://julialang.slack.com/files/UUMJUCYRK/F01LQ6QPECQ/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01LQ6QPECQ-1d7571f0de","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"GkY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"do you mean that just the constructor doesn't support unified? once allocated, doesn't the fact that e.g. here I can access "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" from device 1 mean the result was in unified memory?"}]}]}],"user":"UUMJUCYRK","display_as_bot":false,"ts":"1611738650.066600"},{"client_msg_id":"c8683e18-dc9c-4f5e-839c-c158310612f5","type":"message","text":"huh","user":"U68A3ASP9","ts":"1611739191.067000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zVU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"huh"}]}]}]},{"client_msg_id":"f15c8593-25c6-4a32-a6d3-66051d56e93f","type":"message","text":"`CUDA.is_managed` returns `false`, so I'm surprised this doesn't trigger an illegal memory access","user":"U68A3ASP9","ts":"1611739500.067400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9sGo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CUDA.is_managed","style":{"code":true}},{"type":"text","text":" returns "},{"type":"text","text":"false","style":{"code":true}},{"type":"text","text":", so I'm surprised this doesn't trigger an illegal memory access"}]}]}]},{"client_msg_id":"c59411d0-d7d0-48f8-bf74-7497765a852b","type":"message","text":"I guess with UVA, `memcpy` knows the device of origin and so you don't need to switch devices","user":"U68A3ASP9","ts":"1611739549.068000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"C4h","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I guess with UVA, "},{"type":"text","text":"memcpy","style":{"code":true}},{"type":"text","text":" knows the device of origin and so you don't need to switch devices"}]}]}]},{"client_msg_id":"e86c3fd1-d0ea-44d2-b5bc-e8453ca8d66e","type":"message","text":"The relevant `memcpy` here is the one to copy `y` to CPU to print it? Indeed `2 .* y` is an illegal memory access. In any case, that all makes sense to me then I think, and its also all I need (I don't need to later work on `y` from a different GPU)","user":"UUMJUCYRK","ts":"1611739974.069400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LeuON","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The relevant "},{"type":"text","text":"memcpy","style":{"code":true}},{"type":"text","text":" here is the one to copy "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" to CPU to print it? Indeed "},{"type":"text","text":"2 .* y","style":{"code":true}},{"type":"text","text":" is an illegal memory access. In any case, that all makes sense to me then I think, and its also all I need (I don't need to later work on "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" from a different GPU)"}]}]}]},{"client_msg_id":"df1b4296-9439-4c3b-b223-ee6b2956050b","type":"message","text":"yeah correct","user":"U68A3ASP9","ts":"1611739990.069600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PNlMq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yeah correct"}]}]}]},{"client_msg_id":"d1b163ff-f5b9-4ab3-99ff-a07f4f52aa17","type":"message","text":"if you want to test out unified memory for more, I'd recommend trying to switch the CUDA.jl allocator to unified memory altogether","user":"U68A3ASP9","ts":"1611740014.070200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5hc5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"if you want to test out unified memory for more, I'd recommend trying to switch the CUDA.jl allocator to unified memory altogether"}]}]}]},{"client_msg_id":"10b8a7f7-32a1-4b5f-9fdf-66addd5a7c70","type":"message","text":"is that a built in option or do you mean hacking it?","user":"UUMJUCYRK","ts":"1611740313.070400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"q=Fx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is that a built in option or do you mean hacking it?"}]}]}]},{"client_msg_id":"5e9573f0-03b1-4580-872f-a35436daf000","type":"message","text":"hacking it, it should be a tiny change","user":"U68A3ASP9","ts":"1611740506.070600","team":"T68168MUP","edited":{"user":"U68A3ASP9","ts":"1611740509.000000"},"blocks":[{"type":"rich_text","block_id":"FZRO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hacking it, it should be a tiny change"}]}]}],"thread_ts":"1611740506.070600","reply_count":1,"reply_users_count":1,"latest_reply":"1611740514.070800","reply_users":["U68A3ASP9"],"subscribed":false,"reactions":[{"name":"+1","users":["UUMJUCYRK"],"count":1}]},{"client_msg_id":"3cae42dc-94ca-449f-8412-9cf029a38774","type":"message","text":"cool, will play around with it, thanks!","user":"UUMJUCYRK","ts":"1611740540.071300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5rZGI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"cool, will play around with it, thanks!"}]}]}]},{"client_msg_id":"537cccc4-f743-4dfb-af41-7eade5e02eaa","type":"message","text":"Does `diagm(cu(rand(10)))` make a CuArray? (I was trying to debug something via CI :cry:  and something was making an Array…)","user":"UD0NS8PDF","ts":"1611777449.072200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ddKYo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does "},{"type":"text","text":"diagm(cu(rand(10)))","style":{"code":true}},{"type":"text","text":" make a CuArray? (I was trying to debug something via CI "},{"type":"emoji","name":"cry"},{"type":"text","text":"  and something was making an Array…)"}]}]}]},{"client_msg_id":"c2591968-7408-4897-94c8-7b4d951f2b64","type":"message","text":"```ulia&gt; diagm(cu(rand(10)))\n┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`\n└ @ GPUArrays ~/.julia/packages/GPUArrays/WV76E/src/host/indexing.jl:43\n10×10 Array{Float32,2}:```","user":"U6N6VQE30","ts":"1611780003.072400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"w+t","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ulia> diagm(cu(rand(10)))\n┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`\n└ @ GPUArrays ~/.julia/packages/GPUArrays/WV76E/src/host/indexing.jl:43\n10×10 Array{Float32,2}:"}]}]}],"thread_ts":"1611780003.072400","reply_count":2,"reply_users_count":1,"latest_reply":"1611780740.072900","reply_users":["UD0NS8PDF"],"subscribed":false},{"client_msg_id":"c8f8ffa2-1fe8-473c-82ff-ff6de609bcdd","type":"message","text":"I got a dreaded device compatibility error when updating CUDA.\n```┌ Warning: Your Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 &lt; 5.0).\n│ Some functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n└ @ CUDA ~/.julia/packages/CUDA/wTQsK/src/state.jl:251```\nIs there a table mapping GPUs/compute versions to CUDA versions, or should I just be doing binary search to find the latest CUDA version that doesn't throw this error?","user":"U73ACR3TQ","ts":"1611795078.074200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kr9Y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I got a dreaded device compatibility error when updating CUDA.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"┌ Warning: Your Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n│ Some functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n└ @ CUDA ~/.julia/packages/CUDA/wTQsK/src/state.jl:251"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a table mapping GPUs/compute versions to CUDA versions, or should I just be doing binary search to find the latest CUDA version that doesn't throw this error?"}]}]}]},{"client_msg_id":"1b6dfb38-547e-4b21-a50f-8c9425cbe42a","type":"message","text":"that's a warning, not an error.","user":"U68A3ASP9","ts":"1611818365.074700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UXY1k","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"that's a warning, not an error."}]}]}]},{"client_msg_id":"e6ae833d-b8ad-4b96-b1e8-57ffebe81fbe","type":"message","text":"I should probably add a table to the README, but the incompatibility doesn't matter much right now. you should be fine.","user":"U68A3ASP9","ts":"1611818452.075300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PDG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I should probably add a table to the README, but the incompatibility doesn't matter much right now. you should be fine."}]}]}]},{"client_msg_id":"7228d4e6-8a6f-48ad-88aa-f095f76785e2","type":"message","text":"I would like to ask, as BLAS.nrm2 is the BLAS function for calculating the 2-norm on the cpu, for CuArrays it should be CUBLAS.nrm2 right?","user":"UTDSTSANP","ts":"1611842910.076700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lbxf/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I would like to ask, as BLAS.nrm2 is the BLAS function for calculating the 2-norm on the cpu, for CuArrays it should be CUBLAS.nrm2 right?"}]}]}],"thread_ts":"1611842910.076700","reply_count":1,"reply_users_count":1,"latest_reply":"1611843474.077600","reply_users":["U68A3ASP9"],"subscribed":false},{"client_msg_id":"128d2141-37f0-492e-98a1-89a911a4afdb","type":"message","text":"Does anyone know why random number generator with fixed seeds are so slow?\n```julia&gt; e = zeros(Float32, 100000, 100);\n\njulia&gt; e2 = cu(e);\n\njulia&gt; seed = rand(UInt64);\n\njulia&gt; @btime Random.seed!(seed);rand!(e);\n  10.654 μs (2 allocations: 112 bytes)\n\njulia&gt; @btime CUDA.seed!(seed);rand!(e2);\n  6.114 ms (18 allocations: 51.84 KiB)```\nLike that a 1000x factor slower","user":"U010WA4SZK5","ts":"1611843143.077500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iaj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does anyone know why random number generator with fixed seeds are so slow?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> e = zeros(Float32, 100000, 100);\n\njulia> e2 = cu(e);\n\njulia> seed = rand(UInt64);\n\njulia> @btime Random.seed!(seed);rand!(e);\n  10.654 μs (2 allocations: 112 bytes)\n\njulia> @btime CUDA.seed!(seed);rand!(e2);\n  6.114 ms (18 allocations: 51.84 KiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Like that a 1000x factor slower"}]}]}]},{"client_msg_id":"ea41910a-1458-46b5-a522-38aee7b3477c","type":"message","text":"Digression: in @btime, why do we put a $ when passing the parameters in the function? I tried both with and without putting the $ and it gives the same benchmarks literally","user":"UTDSTSANP","ts":"1611843650.079300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sMNC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Digression: in @btime, why do we put a $ when passing the parameters in the function? I tried both with and without putting the $ and it gives the same benchmarks literally"}]}]}]},{"client_msg_id":"a4e87888-ec50-44b1-89ca-d9cee8b69117","type":"message","text":"interpolation to avoid overhead of accessing global values, see the BenchmarkTools.jl README","user":"U68A3ASP9","ts":"1611843711.079900","team":"T68168MUP","edited":{"user":"U68A3ASP9","ts":"1611843716.000000"},"blocks":[{"type":"rich_text","block_id":"9d8T","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"interpolation to avoid overhead of accessing global values, see the BenchmarkTools.jl README"}]}]}],"thread_ts":"1611843711.079900","reply_count":1,"reply_users_count":1,"latest_reply":"1611843752.080100","reply_users":["UTDSTSANP"],"subscribed":false},{"client_msg_id":"cdec4cef-5262-4c52-b411-ce547441b795","type":"message","text":"What are `DenseCuArray` s and `StridedCuArray`s ? I just can't seem to find any documentation about these datatypes, could someone help please?","user":"UTDSTSANP","ts":"1611844867.083600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j1uH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What are "},{"type":"text","text":"DenseCuArray","style":{"code":true}},{"type":"text","text":" s and "},{"type":"text","text":"StridedCuArray","style":{"code":true}},{"type":"text","text":"s ? I just can't seem to find any documentation about these datatypes, could someone help please?"}]}]}]},{"client_msg_id":"59ccc1a1-9503-4467-9e35-5e18b66a9e57","type":"message","text":"unions for contiguous vs strided memory, mirrorring Base's StridedArray (which is basically a non-contiguous view)","user":"U68A3ASP9","ts":"1611845307.084600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"N/gO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"unions for contiguous vs strided memory, mirrorring Base's StridedArray (which is basically a non-contiguous view)"}]}]}]},{"client_msg_id":"899ecde0-d963-442c-a43f-5d7d3380023c","type":"message","text":"<https://julialang.slack.com/archives/C680MM7D4/p1611872031254800|https://julialang.slack.com/archives/C680MM7D4/p1611872031254800>","user":"UDGT4PM41","ts":"1611872391.086600","team":"T68168MUP","attachments":[{"from_url":"https://julialang.slack.com/archives/C680MM7D4/p1611872031254800","fallback":"[January 28th, 2021 2:13 PM] arikatzpro: Can someone please eli5 parallel scan?","ts":"1611872031.254800","author_id":"UDGT4PM41","author_subname":"Ari Katz","channel_id":"C680MM7D4","channel_name":"random","is_msg_unfurl":true,"text":"Can someone please eli5 parallel scan?","author_name":"Ari Katz","author_link":"https://julialang.slack.com/team/UDGT4PM41","author_icon":"https://secure.gravatar.com/avatar/a9e84ba6e7b9db667ae3371c11a07dfe.jpg?s=48&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0022-48.png","mrkdwn_in":["text"],"id":1,"original_url":"https://julialang.slack.com/archives/C680MM7D4/p1611872031254800","footer":"Posted in #random"}],"blocks":[{"type":"rich_text","block_id":"shlM9","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://julialang.slack.com/archives/C680MM7D4/p1611872031254800","text":"https://julialang.slack.com/archives/C680MM7D4/p1611872031254800"}]}]}]},{"client_msg_id":"9ce2f14e-99d0-4a91-a81c-b83beb0f2012","type":"message","text":"Does there exist a method to find out the absolute value element-wise in a CuArray?","user":"UTDSTSANP","ts":"1611901521.087800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xQZS=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does there exist a method to find out the absolute value element-wise in a CuArray?"}]}]}]},{"client_msg_id":"7f2a6b57-80c3-4a7b-835b-34244df18ac0","type":"message","text":"are you just looking for `abs.(u)`?","user":"U69BL50BF","ts":"1611901572.088200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GiM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"are you just looking for "},{"type":"text","text":"abs.(u)","style":{"code":true}},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"f20d6658-4c40-4008-8bb2-d877f9a4d1d2","type":"message","text":"That was nice. I thought I would again encounter the scalar getindex error. Thank you Chris!","user":"UTDSTSANP","ts":"1611902030.088800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rd65C","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That was nice. I thought I would again encounter the scalar getindex error. Thank you Chris!"}]}]}]},{"client_msg_id":"65f748fd-10b6-40f7-8918-8461b49cbf82","type":"message","text":"Basic operation like `c = a .+ b` failed. Both a and b are small cuda arrays.\n\nsignal (11): Segmentation fault\nin expression starting at REPL[4]:1\nfesetenv at ./julia-1.6.0-beta1/bin/../lib/julia/libopenlibm.so (unknown line)\nunknown function (ip: 0x7f2049e7cf)\nAllocations: 44366399 (Pool: 44353014; Big: 13385); GC: 49\nSegmentation fault (core dumped)\n\nJulia : 1.6.0 beta\nCUDA : v2.6.1 installed through package manager.\n\nConfiguration aarch64. NVIDIA AGX hardware.\nIs this expected ? Given the uncommon configuration relative to regular x86-64 pc.","user":"UKREUAYEM","ts":"1611984331.090100","team":"T68168MUP","edited":{"user":"UKREUAYEM","ts":"1611985601.000000"},"blocks":[{"type":"rich_text","block_id":"vMGv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Basic operation like "},{"type":"text","text":"c = a .+ b ","style":{"code":true}},{"type":"text","text":"failed. Both a and b are small cuda arrays.\n\nsignal (11): Segmentation fault\nin expression starting at REPL[4]:1\nfesetenv at ./julia-1.6.0-beta1/bin/../lib/julia/libopenlibm.so (unknown line)\nunknown function (ip: 0x7f2049e7cf)\nAllocations: 44366399 (Pool: 44353014; Big: 13385); GC: 49\nSegmentation fault (core dumped)\n\nJulia : 1.6.0 beta\nCUDA : v2.6.1 installed through package manager.\n\nConfiguration aarch64. NVIDIA AGX hardware.\nIs this expected ? Given the uncommon configuration relative to regular x86-64 pc."}]}]}]},{"client_msg_id":"a2a20885-0453-4edd-84b2-7bdefdb989f2","type":"message","text":"Hm, I thought I had fixed that: <https://github.com/JuliaLang/julia/issues/38427>\nwhere do you get openlibm from? Is this a custom Julia build?\neither way, it's a bug in Julia, so you can re-open that issue or file a new one if it persists.","user":"U68A3ASP9","ts":"1612017297.091800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jsUd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hm, I thought I had fixed that: "},{"type":"link","url":"https://github.com/JuliaLang/julia/issues/38427"},{"type":"text","text":"\nwhere do you get openlibm from? Is this a custom Julia build?\neither way, it's a bug in Julia, so you can re-open that issue or file a new one if it persists."}]}]}],"thread_ts":"1612017297.091800","reply_count":1,"reply_users_count":1,"latest_reply":"1612024709.091900","reply_users":["UKREUAYEM"],"subscribed":false},{"client_msg_id":"29629506-d933-48cf-998d-498b4dc5eb6a","type":"message","text":"I'm playing around with KernelAbstractions,and am liking it a lot so far. I have a basic doubt as to whether something is possible. I understand that in a kernel, the function is run asynchronously and in parallel on all the indices in the `ndrange`.\n\nImagine that the result is not completely independent from the order, but to compute the kernel on a given index, I necessarily need to have computed some other indices before (the overall dependency structure will be some directed acyclic graph). Is there a way to tell KernelAbstractions to follow this order (which still allows for a lot of parallelism) or should one do a manual layering, do a kernel on a chunk, wait on the result, do another kernel, and so on?","user":"U6BJ9E351","ts":"1612126021.096900","team":"T68168MUP","edited":{"user":"U6BJ9E351","ts":"1612126290.000000"},"blocks":[{"type":"rich_text","block_id":"L19Tj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm playing around with KernelAbstractions,and am liking it a lot so far. I have a basic doubt as to whether something is possible. I understand that in a kernel, the function is run asynchronously and in parallel on all the indices in the "},{"type":"text","text":"ndrange","style":{"code":true}},{"type":"text","text":".\n\nImagine that the result is not completely independent from the order, but to compute the kernel on a given index, I necessarily need to have computed some other indices before (the overall dependency structure will be some directed acyclic graph). Is there a way to tell KernelAbstractions to follow this order (which still allows for a lot of parallelism) or should one do a manual layering, do a kernel on a chunk, wait on the result, do another kernel, and so on?"}]}]}],"thread_ts":"1612126021.096900","reply_count":5,"reply_users_count":2,"latest_reply":"1612173224.103700","reply_users":["UC7AF7NSU","U6BJ9E351"],"subscribed":false},{"client_msg_id":"49f6f6b6-7182-43f6-9f72-266b7409c517","type":"message","text":"You would need to define it as a operation of multiple kernels","user":"U67BJLYCS","ts":"1612126917.097900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Sg0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You would need to define it as a operation of multiple kernels"}]}]}]},{"client_msg_id":"cf104ba8-35c1-4afc-be4b-9d57be40f8b1","type":"message","text":"Or I need to think real hard and add dynamic parallelism like CUDA has","user":"U67BJLYCS","ts":"1612126948.098700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"XzIv0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Or I need to think real hard and add dynamic parallelism like CUDA has"}]}]}]},{"client_msg_id":"ad51cebb-c618-43e4-99d1-646708fd719a","type":"message","text":"ah, I see, so this (<https://juliagpu.github.io/CUDA.jl/dev/api/kernel/#Dynamic-parallelism>) is the low-level building block one would use to achieve that? I think it'd be useful to have a more high-level DAG-based formalism implemented on top of it, but I have no idea how easy / hard that is","user":"U6BJ9E351","ts":"1612128322.100200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rMI8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ah, I see, so this ("},{"type":"link","url":"https://juliagpu.github.io/CUDA.jl/dev/api/kernel/#Dynamic-parallelism"},{"type":"text","text":") is the low-level building block one would use to achieve that? I think it'd be useful to have a more high-level DAG-based formalism implemented on top of it, but I have no idea how easy / hard that is"}]}]}],"thread_ts":"1612128322.100200","reply_count":19,"reply_users_count":3,"latest_reply":"1612187366.105900","reply_users":["U6A0PD8CR","UC7AF7NSU","U6BJ9E351"],"subscribed":false},{"client_msg_id":"1949e392-13d8-4d41-bd98-9821d429a299","type":"message","text":"Could dagger lower to that ?","user":"UDGT4PM41","ts":"1612133871.100700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vMV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Could dagger lower to that ?"}]}]}],"thread_ts":"1612133871.100700","reply_count":1,"reply_users_count":1,"latest_reply":"1612142439.100800","reply_users":["U6A0PD8CR"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"Not sure what kind of DAGs you are talking about but is it possible to express it as a single kernel (even with some indexing tricks)? If so, maybe that's possible to formalize them as iterator combinators (like `zip`/`product`) and define a fold on it.","user":"UC7AF7NSU","ts":"1612145604.101600","thread_ts":"1612126021.096900","root":{"client_msg_id":"29629506-d933-48cf-998d-498b4dc5eb6a","type":"message","text":"I'm playing around with KernelAbstractions,and am liking it a lot so far. I have a basic doubt as to whether something is possible. I understand that in a kernel, the function is run asynchronously and in parallel on all the indices in the `ndrange`.\n\nImagine that the result is not completely independent from the order, but to compute the kernel on a given index, I necessarily need to have computed some other indices before (the overall dependency structure will be some directed acyclic graph). Is there a way to tell KernelAbstractions to follow this order (which still allows for a lot of parallelism) or should one do a manual layering, do a kernel on a chunk, wait on the result, do another kernel, and so on?","user":"U6BJ9E351","ts":"1612126021.096900","team":"T68168MUP","edited":{"user":"U6BJ9E351","ts":"1612126290.000000"},"blocks":[{"type":"rich_text","block_id":"L19Tj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm playing around with KernelAbstractions,and am liking it a lot so far. I have a basic doubt as to whether something is possible. I understand that in a kernel, the function is run asynchronously and in parallel on all the indices in the "},{"type":"text","text":"ndrange","style":{"code":true}},{"type":"text","text":".\n\nImagine that the result is not completely independent from the order, but to compute the kernel on a given index, I necessarily need to have computed some other indices before (the overall dependency structure will be some directed acyclic graph). Is there a way to tell KernelAbstractions to follow this order (which still allows for a lot of parallelism) or should one do a manual layering, do a kernel on a chunk, wait on the result, do another kernel, and so on?"}]}]}],"thread_ts":"1612126021.096900","reply_count":5,"reply_users_count":2,"latest_reply":"1612173224.103700","reply_users":["UC7AF7NSU","U6BJ9E351"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"U4TB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not sure what kind of DAGs you are talking about but is it possible to express it as a single kernel (even with some indexing tricks)? If so, maybe that's possible to formalize them as iterator combinators (like "},{"type":"text","text":"zip","style":{"code":true}},{"type":"text","text":"/"},{"type":"text","text":"product","style":{"code":true}},{"type":"text","text":") and define a fold on it."}]}]}],"client_msg_id":"eb905366-e52f-4d0f-923c-a9c307238570"},{"client_msg_id":"46b8c28b-972f-4595-8432-a0ae675630ac","type":"message","text":"Timely reminder JuliaGPU office hours in 2h","user":"U67BJLYCS","ts":"1612191191.106500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=jU4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Timely reminder JuliaGPU office hours in 2h"}]}]}]},{"client_msg_id":"673be344-c4e7-444a-849c-ea385a7bead4","type":"message","text":"happening now","user":"U67BJLYCS","ts":"1612199536.106700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bY9+L","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"happening now"}]}]}]},{"client_msg_id":"63e0c827-1577-4762-8faa-d6ef8531adb3","type":"message","text":"hey I tried checking out the new stream/task functionality using <https://github.com/JuliaGPU/CUDA.jl/pull/662#issuecomment-765273035|this example> but I'm not seeing the same behavior - mine looks like it's all running in one stream. Why might that be?Screen Shot 2021-02-03 at 5.00.24 PM","user":"U01G39CC63F","ts":"1612389717.109700","team":"T68168MUP","edited":{"user":"U01G39CC63F","ts":"1612389743.000000"},"blocks":[{"type":"rich_text","block_id":"pvsg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hey I tried checking out the new stream/task functionality using "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/pull/662#issuecomment-765273035","text":"this example"},{"type":"text","text":" but I'm not seeing the same behavior - mine looks like it's all running in one stream. Why might that be?Screen Shot 2021-02-03 at 5.00.24 PM"}]}]}],"thread_ts":"1612389717.109700","reply_count":3,"reply_users_count":2,"latest_reply":"1612420878.112700","reply_users":["U01G39CC63F","U68A3ASP9"],"subscribed":false},{"client_msg_id":"db0d29ab-6f1b-4b52-8311-f2e2c8a0103f","type":"message","text":"<@U68A3ASP9> I was looking for the `whitelist` in `GPUCompiler`, but it seems to have disappeared between the latest release and master (and I can’t even find where it happened)","user":"U67G3QRJM","ts":"1612391646.112000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xre","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":" I was looking for the "},{"type":"text","text":"whitelist","style":{"code":true}},{"type":"text","text":" in "},{"type":"text","text":"GPUCompiler","style":{"code":true}},{"type":"text","text":", but it seems to have disappeared between the latest release and master (and I can’t even find where it happened)"}]}]}],"thread_ts":"1612391646.112000","reply_count":4,"reply_users_count":2,"latest_reply":"1612423047.113200","reply_users":["U67G3QRJM","U68A3ASP9"],"subscribed":false},{"client_msg_id":"5014175e-2ea2-497c-9d96-796b7a274a3f","type":"message","text":"```ERROR: CUDA error: a PTX JIT compilation failed (code 218, ERROR_INVALID_PTX)\nptxas application ptx input, line 2440; error   : Entry function '_Z27julia_broadcast_kernel_994815CuKernelContext8SubArrayI4DualIv7Float64Li15EELi1E13CuDeviceArrayIS1_IvS2_Li15EELi1ELi1EE5TupleIS3_I5Int64Li1ELi1EEELifalseEE11BroadcastedIvS4_I5OneToIS5_EE8_897_898IS1_IvS2_Li15EEES4_I8ExtrudedIS0_IS2_Li1ES3_IS2_Li1ELi1EES4_IS3_IS5_Li1ELi1EEELifalseEES4_I4BoolES4_IS5_EES4_I8PartialsILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_EEEES5_' uses too much parameter space (0x1958 bytes, 0x1100 max).\nptxas fatal   : Ptx assembly aborted due to errors```\nDoes this ring a bell to anyone? I have no idea what the parameter space is but it looks like a compiler limit of sorts. I use this function <https://github.com/JuliaDiff/ForwardDiff.jl/blob/a884ddd1f71f54db765cf41e6ecc5a38fd6db70e/src/apiutils.jl#L82>\n\nFor small `N` it seems to produce the expected result. With larger `N` (here `N=53`) this seems to break. A way to circumvent it is to write a kernel instead of using the ForwardDiff API.","user":"ULL3KSGBS","ts":"1612541798.119600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vGIza","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: CUDA error: a PTX JIT compilation failed (code 218, ERROR_INVALID_PTX)\nptxas application ptx input, line 2440; error   : Entry function '_Z27julia_broadcast_kernel_994815CuKernelContext8SubArrayI4DualIv7Float64Li15EELi1E13CuDeviceArrayIS1_IvS2_Li15EELi1ELi1EE5TupleIS3_I5Int64Li1ELi1EEELifalseEE11BroadcastedIvS4_I5OneToIS5_EE8_897_898IS1_IvS2_Li15EEES4_I8ExtrudedIS0_IS2_Li1ES3_IS2_Li1ELi1EES4_IS3_IS5_Li1ELi1EEELifalseEES4_I4BoolES4_IS5_EES4_I8PartialsILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_EEEES5_' uses too much parameter space (0x1958 bytes, 0x1100 max).\nptxas fatal   : Ptx assembly aborted due to errors"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Does this ring a bell to anyone? I have no idea what the parameter space is but it looks like a compiler limit of sorts. I use this function "},{"type":"link","url":"https://github.com/JuliaDiff/ForwardDiff.jl/blob/a884ddd1f71f54db765cf41e6ecc5a38fd6db70e/src/apiutils.jl#L82"},{"type":"text","text":"\n\nFor small "},{"type":"text","text":"N","style":{"code":true}},{"type":"text","text":" it seems to produce the expected result. With larger "},{"type":"text","text":"N","style":{"code":true}},{"type":"text","text":" (here "},{"type":"text","text":"N=53","style":{"code":true}},{"type":"text","text":") this seems to break. A way to circumvent it is to write a kernel instead of using the ForwardDiff API."}]}]}]},{"client_msg_id":"4cc24d95-3f47-497c-8691-fc717e077887","type":"message","text":"you're using a huge tuple as argument","user":"U68A3ASP9","ts":"1612542114.119900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YrWv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you're using a huge tuple as argument"}]}]}]},{"client_msg_id":"1f922629-8318-43de-b29c-2f9f5db6624d","type":"message","text":"bits types passed to kernels are passed by value, not by reference, and there's a limit: 4K IIRC","user":"U68A3ASP9","ts":"1612542135.120400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"epBTR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"bits types passed to kernels are passed by value, not by reference, and there's a limit: 4K IIRC"}]}]}]},{"client_msg_id":"56e18ee3-7e89-4ebd-9c34-4439c789d5fb","type":"message","text":"you could pass the value as a single-valued array. Ref could work too, but isn't supported very well, so might result in other issues.","user":"U68A3ASP9","ts":"1612542202.121000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SEwP=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you could pass the value as a single-valued array. Ref could work too, but isn't supported very well, so might result in other issues."}]}]}]},{"client_msg_id":"d0bd9c18-efd1-4a1d-8ab5-6be97fdd47c7","type":"message","text":"<@U68A3ASP9> <@U67BJLYCS> <@U7THT3TM3> how do I set up buildkite? I think I set up buildkite and github for my account/organization but <https://buildkite.com/organizations/julialang/repository-providers> says \"Page not found\" for me. I'm guessing you need to add my repository?\n\nI'd like to use it in <https://github.com/JuliaFolds/FoldsCUDA.jl> and maybe at some point in <https://github.com/tkf/UnionArrays.jl>","user":"UC7AF7NSU","ts":"1612681159.121500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j+DpH","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":" "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":" "},{"type":"user","user_id":"U7THT3TM3"},{"type":"text","text":" how do I set up buildkite? I think I set up buildkite and github for my account/organization but "},{"type":"link","url":"https://buildkite.com/organizations/julialang/repository-providers"},{"type":"text","text":" says \"Page not found\" for me. I'm guessing you need to add my repository?\n\nI'd like to use it in "},{"type":"link","url":"https://github.com/JuliaFolds/FoldsCUDA.jl"},{"type":"text","text":" and maybe at some point in "},{"type":"link","url":"https://github.com/tkf/UnionArrays.jl"}]}]}]},{"client_msg_id":"bd8fa02b-ee2a-484a-9b69-ed2ddbf3db1f","type":"message","text":"You'll need to temporarily add a Buildkite admin as an owner of the organization that owns the repo (JuliaFolds, in this case). Once the setup is done, you can convert them from an owner back to a regular member.","user":"U7THT3TM3","ts":"1612681398.122500","team":"T68168MUP","edited":{"user":"U7THT3TM3","ts":"1612681446.000000"},"blocks":[{"type":"rich_text","block_id":"Yib","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You'll need to temporarily add a Buildkite admin as an owner of the organization that owns the repo (JuliaFolds, in this case). Once the setup is done, you can convert them from an owner back to a regular member."}]}]}]},{"client_msg_id":"83bf9804-6d6c-4912-99c2-f07ef418015f","type":"message","text":"Also, Buildkite is really inconvenient to use for repos owned by users. I'd recommend moving UnionArrays.jl to the JuliaFolds org. Then the setup is easy.","user":"U7THT3TM3","ts":"1612681436.123300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/Qc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Also, Buildkite is really inconvenient to use for repos owned by users. I'd recommend moving UnionArrays.jl to the JuliaFolds org. Then the setup is easy."}]}]}]},{"client_msg_id":"8dec640e-8eb1-4f30-af59-3fbf210fd4d1","type":"message","text":"Thanks! I invited you as an owner and moved <https://github.com/tkf/UnionArrays.jl|UnionArrays.jl> there","user":"UC7AF7NSU","ts":"1612683588.123800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xbJSL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks! I invited you as an owner and moved "},{"type":"link","url":"https://github.com/tkf/UnionArrays.jl","text":"UnionArrays.jl"},{"type":"text","text":" there"}]}]}]},{"client_msg_id":"38e5f80e-bd41-447c-9e82-69b24033acb5","type":"message","text":"<https://github.com/JuliaFolds/FoldsCUDA.jl/pull/39>","user":"U7THT3TM3","ts":"1612684479.124000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"cauv","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/JuliaFolds/FoldsCUDA.jl/pull/39"}]}]}]},{"client_msg_id":"562e1372-e07b-4728-a66e-782644da4d14","type":"message","text":"You can convert me back to a regular member now BTW","user":"U7THT3TM3","ts":"1612684493.124300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"enPY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You can convert me back to a regular member now BTW"}]}]}]},{"client_msg_id":"bd0b75f2-14b2-4442-811e-b24ccb3f279b","type":"message","text":"Wow, you created the config. Thanks a lot!","user":"UC7AF7NSU","ts":"1612684540.124500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GWT3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Wow, you created the config. Thanks a lot!"}]}]}]},{"client_msg_id":"0c22c7cb-62a6-46ef-9314-09254d13c8de","type":"message","text":"I didn't set up coverage. If you want coverage, follow the instructions here: <https://github.com/JuliaGPU/buildkite/blob/main/README.md#using-secrets>","user":"U7THT3TM3","ts":"1612684589.125100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xBMtd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I didn't set up coverage. If you want coverage, follow the instructions here: "},{"type":"link","url":"https://github.com/JuliaGPU/buildkite/blob/main/README.md#using-secrets"}]}]}]},{"client_msg_id":"c609595a-60c9-4782-837f-10e3dcd6fc4b","type":"message","text":"gotcha","user":"UC7AF7NSU","ts":"1612684688.125300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VEU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"gotcha"}]}]}]},{"client_msg_id":"9edb20a0-08de-4485-a999-d2cb0eacbec6","type":"message","text":"Am I supposed to be able to see <https://buildkite.com/julialang/foldscuda-dot-jl/builds/4>? It says \"page not found\" for me. The link is from GitHub's build status","user":"UC7AF7NSU","ts":"1612684705.125700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7H=jy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Am I supposed to be able to see "},{"type":"link","url":"https://buildkite.com/julialang/foldscuda-dot-jl/builds/4"},{"type":"text","text":"? It says \"page not found\" for me. The link is from GitHub's build status"}]}]}]},{"client_msg_id":"25F54228-1EEF-4FA3-8642-3DEEFAED3B3E","type":"message","text":"Ah, I forgot to make the pipeline public.","user":"U7THT3TM3","ts":"1612685109.126300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"o7E","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah, I forgot to make the pipeline public."}]}]}]},{"client_msg_id":"9A27B939-C20E-4B14-87C9-BE33C70B8DDA","type":"message","text":"Okay, I made the pipeline public.","user":"U7THT3TM3","ts":"1612685115.126600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UMY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Okay, I made the pipeline public."}]}]}]},{"client_msg_id":"338575D7-58F0-4377-B992-3419818B43E5","type":"message","text":"Try it now.","user":"U7THT3TM3","ts":"1612685119.126800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=iw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Try it now."}]}]}]},{"client_msg_id":"1938cfad-7c93-47aa-b311-bdd2e2c13731","type":"message","text":"Works now!","user":"UC7AF7NSU","ts":"1612685169.127000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LlS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Works now!"}]}]}],"reactions":[{"name":"+1","users":["U7THT3TM3"],"count":1}]},{"client_msg_id":"3aa2f81e-e493-4407-9647-44242535cb5a","type":"message","text":"<@U7THT3TM3> what would need to be done to finish setting up buildkite here: <https://github.com/jump-dev/SCS.jl/pull/202> (I'm not the owner of the repo, I just contributed GPU/CUDA capabilities) cc: <@U014UPHHPM0>","user":"UHBF252VC","ts":"1612707847.128600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uSfJo","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U7THT3TM3"},{"type":"text","text":" what would need to be done to finish setting up buildkite here: "},{"type":"link","url":"https://github.com/jump-dev/SCS.jl/pull/202"},{"type":"text","text":" (I'm not the owner of the repo, I just contributed GPU/CUDA capabilities) cc: "},{"type":"user","user_id":"U014UPHHPM0"}]}]}]}]}