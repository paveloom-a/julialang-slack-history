{"cursor": 0, "messages": [{"client_msg_id":"e88fbf86-1f69-4fe9-a15a-1dd9cbc530db","type":"message","text":"Can we get GPU CI configured for ForwardDiff?","user":"UEN48T0BT","ts":"1607967804.492700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wJ+Fl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can we get GPU CI configured for ForwardDiff?"}]}]}],"thread_ts":"1607967804.492700","reply_count":7,"reply_users_count":3,"latest_reply":"1608042439.006900","reply_users":["U68A3ASP9","UEN48T0BT","U680THK2S"],"subscribed":false},{"client_msg_id":"49ccd126-3237-4e13-b1b9-e4f463016ebd","type":"message","text":"<@U68A3ASP9>, I've been very slowly trying to understand how Nvidia Optix could work with Julia/CUDA.jl. So far I've understood that Optix has a C API to where one sends \"programs\" (ray generation, intersection) that are no more than PTX code (literally a string) compiled from functions with specific signatures. The functions have to have specific name prefixes, like `__raygen__` and receive no parameters. The input of such functions has to be done through global variables, e.g., `.const .align 8 .b8 params[24];`, and through calls to a device API defined in asm in C headers, e.g., `call (%r1), _optix_get_launch_index_x, ();`.   <@U68A3ASP9> can you give some hint whether any of this 3 means may pose a big problem to current CUDA.jl workings?","user":"U6CCK2SCV","ts":"1607973616.001100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xQ1m","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":", I've been very slowly trying to understand how Nvidia Optix could work with Julia/CUDA.jl. So far I've understood that Optix has a C API to where one sends \"programs\" (ray generation, intersection) that are no more than PTX code (literally a string) compiled from functions with specific signatures. The functions have to have specific name prefixes, like "},{"type":"text","text":"__raygen__","style":{"code":true}},{"type":"text","text":" and receive no parameters. The input of such functions has to be done through global variables, e.g., "},{"type":"text","text":".const .align 8 .b8 params[24];","style":{"code":true}},{"type":"text","text":", and through calls to a device API defined in asm in C headers, e.g., "},{"type":"text","text":"call (%r1), _optix_get_launch_index_x, ();","style":{"code":true}},{"type":"text","text":".   "},{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":" can you give some hint whether any of this 3 means may pose a big problem to current CUDA.jl workings?"}]}]}],"thread_ts":"1607973616.001100","reply_count":5,"reply_users_count":2,"latest_reply":"1607977510.006200","reply_users":["U6CCK2SCV","U68A3ASP9"],"subscribed":false},{"client_msg_id":"a04f7484-ae4e-4d7b-96df-3508a7bd15b9","type":"message","text":"Hey, what’s the status of sparse operations using `CUDA.jl` ? In particular, is there any way to efficiently evaluate a kronecker product of a dense CuArray and sparse CuArray?","user":"UPM0H43C7","ts":"1607974672.003300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"W2u","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey, what’s the status of sparse operations using "},{"type":"text","text":"CUDA.jl","style":{"code":true}},{"type":"text","text":" ? In particular, is there any way to efficiently evaluate a kronecker product of a dense CuArray and sparse CuArray?"}]}]}],"thread_ts":"1607974672.003300","reply_count":34,"reply_users_count":6,"latest_reply":"1608088028.019800","reply_users":["U68A3ASP9","UPM0H43C7","UDGT4PM41","U6A0PD8CR","U67BJLYCS","UH9KWTTD3"],"subscribed":false},{"client_msg_id":"f1adf01f-e383-484d-bb96-d722801552ad","type":"message","text":"I would like to run a large number of iterations (order 10^13 or more) of a relatively small sized simulation, for every iteration I need a bunch of random numbers which can't be pre computed and allocated on the cpu due to size. I was wondering if there is a way to use CUDA, since I believe kernel functions are pretty limited in terms of what can be done inside them.","user":"UU2E6M753","ts":"1608058751.012100","team":"T68168MUP","edited":{"user":"UU2E6M753","ts":"1608059241.000000"},"blocks":[{"type":"rich_text","block_id":"q9O5U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I would like to run a large number of iterations (order 10^13 or more) of a relatively small sized simulation, for every iteration I need a bunch of random numbers which can't be pre computed and allocated on the cpu due to size. I was wondering if there is a way to use CUDA, since I believe kernel functions are pretty limited in terms of what can be done inside them."}]}]}],"thread_ts":"1608058751.012100","reply_count":8,"reply_users_count":2,"latest_reply":"1608192672.037800","reply_users":["UU2E6M753","U68A3ASP9"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"What would it take to be able to write kernels on sparse matrices?","user":"UDGT4PM41","ts":"1608059346.012300","thread_ts":"1607974672.003300","root":{"client_msg_id":"a04f7484-ae4e-4d7b-96df-3508a7bd15b9","type":"message","text":"Hey, what’s the status of sparse operations using `CUDA.jl` ? In particular, is there any way to efficiently evaluate a kronecker product of a dense CuArray and sparse CuArray?","user":"UPM0H43C7","ts":"1607974672.003300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"W2u","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey, what’s the status of sparse operations using "},{"type":"text","text":"CUDA.jl","style":{"code":true}},{"type":"text","text":" ? In particular, is there any way to efficiently evaluate a kronecker product of a dense CuArray and sparse CuArray?"}]}]}],"thread_ts":"1607974672.003300","reply_count":34,"reply_users_count":6,"latest_reply":"1608088028.019800","reply_users":["U68A3ASP9","UPM0H43C7","UDGT4PM41","U6A0PD8CR","U67BJLYCS","UH9KWTTD3"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"OPd=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What would it take to be able to write kernels on sparse matrices?"}]}]}],"client_msg_id":"ebd29528-6442-486b-a11a-c68d038bfab8"},{"client_msg_id":"a3fdc5df-a77a-40b9-be49-e87d8bc8291f","type":"message","text":"I have an AMD GPU on a windows device with WSL2. Has anyone tried to run AMDGPU.jl on wsl2? If so, what are the necessary components to be installed?\n\nAre there any other ways to use an AMD GPU without WSL2 on windows? Is there a device agnostic binding available, that trades off a bit on performance, but can still be used?","user":"U01GL3BUV0W","ts":"1608095693.024700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kd5v","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have an AMD GPU on a windows device with WSL2. Has anyone tried to run AMDGPU.jl on wsl2? If so, what are the necessary components to be installed?\n\nAre there any other ways to use an AMD GPU without WSL2 on windows? Is there a device agnostic binding available, that trades off a bit on performance, but can still be used?"}]}]}],"thread_ts":"1608095693.024700","reply_count":3,"reply_users_count":1,"latest_reply":"1608157682.035900","reply_users":["U6A0PD8CR"],"subscribed":false},{"client_msg_id":"438e9cd3-3080-4776-8810-2992d35ce739","type":"message","text":"As far as I understand it AMDGPU.jl strictly requires ROCm (i.e., <https://github.com/RadeonOpenCompute/ROCm>) and as far as I know ROCm still only exists for Linux","user":"UGU761DU2","ts":"1608104331.026500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"k+JBT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"As far as I understand it AMDGPU.jl strictly requires ROCm (i.e., "},{"type":"link","url":"https://github.com/RadeonOpenCompute/ROCm"},{"type":"text","text":") and as far as I know ROCm still only exists for Linux"}]}]}]},{"client_msg_id":"652b9b94-1f63-436f-9338-0d79379b52ec","type":"message","text":"And it appears to be somewhat unclear whether WSL2 is Linux enough for ROCm, <https://github.com/RadeonOpenCompute/ROCm/issues/794>","user":"UGU761DU2","ts":"1608104464.027900","team":"T68168MUP","edited":{"user":"UGU761DU2","ts":"1608104537.000000"},"blocks":[{"type":"rich_text","block_id":"R5Ik","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And it appears to be somewhat unclear whether WSL2 is Linux enough for ROCm, "},{"type":"link","url":"https://github.com/RadeonOpenCompute/ROCm/issues/794"}]}]}],"thread_ts":"1608104464.027900","reply_count":10,"reply_users_count":3,"latest_reply":"1608213834.044900","reply_users":["U01GL3BUV0W","U66SGEWAC","U6A0PD8CR"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"What about other packages? OpenCL.jl has really poor docs, Vulkan.jl has none.","user":"U01GL3BUV0W","ts":"1608111087.028100","thread_ts":"1608104464.027900","root":{"client_msg_id":"652b9b94-1f63-436f-9338-0d79379b52ec","type":"message","text":"And it appears to be somewhat unclear whether WSL2 is Linux enough for ROCm, <https://github.com/RadeonOpenCompute/ROCm/issues/794>","user":"UGU761DU2","ts":"1608104464.027900","team":"T68168MUP","edited":{"user":"UGU761DU2","ts":"1608104537.000000"},"blocks":[{"type":"rich_text","block_id":"R5Ik","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And it appears to be somewhat unclear whether WSL2 is Linux enough for ROCm, "},{"type":"link","url":"https://github.com/RadeonOpenCompute/ROCm/issues/794"}]}]}],"thread_ts":"1608104464.027900","reply_count":10,"reply_users_count":3,"latest_reply":"1608213834.044900","reply_users":["U01GL3BUV0W","U66SGEWAC","U6A0PD8CR"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"G9Iyt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What about other packages? OpenCL.jl has really poor docs, Vulkan.jl has none."}]}]}],"client_msg_id":"bfdb725b-5105-4131-b5c3-12bf5311408f"},{"client_msg_id":"abf5d974-f6f5-4408-a4ae-29286296aff0","type":"message","text":"I think no GPU library runs on WSL2 out of the box right now - the very first thing that seems to be slowly working is CUDA on WSL2, but last time I checked it was in beta:\n<https://ubuntu.com/blog/getting-started-with-cuda-on-ubuntu-on-wsl-2>\nYou need to be on insider built etc... Pretty sure nothing like that exists for amd yet","user":"U66SGEWAC","ts":"1608115292.030600","team":"T68168MUP","attachments":[{"service_name":"Ubuntu","title":"Getting started with CUDA on Ubuntu on WSL 2 | Ubuntu","title_link":"https://ubuntu.com/blog/getting-started-with-cuda-on-ubuntu-on-wsl-2","text":"At Build 2020 Microsoft announced support for GPU compute on Windows Subsystem for Linux 2. Ubuntu is the leading Linux distribution for WSL and a sponsor of WSLConf. Canonical, the publisher of Ubuntu, provides enterprise support for Ubuntu on WSL through Ubuntu Advantage. This guide will walk early adopters through the steps on turning […]","fallback":"Ubuntu: Getting started with CUDA on Ubuntu on WSL 2 | Ubuntu","image_url":"https://ubuntu.com/wp-content/uploads/7332/191122_NvidiaStopsCUDAmacOS.jpg","from_url":"https://ubuntu.com/blog/getting-started-with-cuda-on-ubuntu-on-wsl-2","image_width":500,"image_height":250,"image_bytes":21952,"service_icon":"https://assets.ubuntu.com/v1/17b68252-apple-touch-icon-180x180-precomposed-ubuntu.png","id":1,"original_url":"https://ubuntu.com/blog/getting-started-with-cuda-on-ubuntu-on-wsl-2"}],"blocks":[{"type":"rich_text","block_id":"ZQe6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think no GPU library runs on WSL2 out of the box right now - the very first thing that seems to be slowly working is CUDA on WSL2, but last time I checked it was in beta:\n"},{"type":"link","url":"https://ubuntu.com/blog/getting-started-with-cuda-on-ubuntu-on-wsl-2"},{"type":"text","text":"\nYou need to be on insider built etc... Pretty sure nothing like that exists for amd yet"}]}]}],"reactions":[{"name":"+1","users":["U01GL3BUV0W"],"count":1}]},{"type":"message","subtype":"thread_broadcast","text":"So I actually put some time and looked at OpenCL, and it works, but you have to write the kernel yourself, unlike in CUDA where you can wrote simple Julia code.","user":"U01GL3BUV0W","ts":"1608116323.030800","thread_ts":"1608104464.027900","root":{"client_msg_id":"652b9b94-1f63-436f-9338-0d79379b52ec","type":"message","text":"And it appears to be somewhat unclear whether WSL2 is Linux enough for ROCm, <https://github.com/RadeonOpenCompute/ROCm/issues/794>","user":"UGU761DU2","ts":"1608104464.027900","team":"T68168MUP","edited":{"user":"UGU761DU2","ts":"1608104537.000000"},"blocks":[{"type":"rich_text","block_id":"R5Ik","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And it appears to be somewhat unclear whether WSL2 is Linux enough for ROCm, "},{"type":"link","url":"https://github.com/RadeonOpenCompute/ROCm/issues/794"}]}]}],"thread_ts":"1608104464.027900","reply_count":10,"reply_users_count":3,"latest_reply":"1608213834.044900","reply_users":["U01GL3BUV0W","U66SGEWAC","U6A0PD8CR"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"vIrh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So I actually put some time and looked at OpenCL, and it works, but you have to write the kernel yourself, unlike in CUDA where you can wrote simple Julia code."}]}]}],"client_msg_id":"7ab5e318-c0eb-451f-9dfe-85480068f071"},{"client_msg_id":"ff40dff6-05b1-443c-a664-a357587e907a","type":"message","text":"Isn't there a \"GPUArray\" package that allows you to run things on the CPU to check if your code is affected by allowscalar etc?","user":"U6CJRSR63","ts":"1608129684.032200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PWmQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Isn't there a \"GPUArray\" package that allows you to run things on the CPU to check if your code is affected by allowscalar etc?"}]}]}],"thread_ts":"1608129684.032200","reply_count":6,"reply_users_count":3,"latest_reply":"1608157277.035300","reply_users":["UEN48T0BT","U6CJRSR63","U6A0PD8CR"],"subscribed":false},{"client_msg_id":"f5eb3679-86e6-4650-af35-884b6570fe04","type":"message","text":"I thought there was, but can't remember which one","user":"U6CJRSR63","ts":"1608129695.032500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dXy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I thought there was, but can't remember which one"}]}]}]},{"client_msg_id":"5d5a3138-f560-46c6-bcd2-0016411d9d63","type":"message","text":"GPUArrays.jl has an JLArray reference implementation that executes on the CPU. but it generally isn't fleshed out enough to be usable with realistic applications; it's currently mostly intended to run the GPUArrays test suite on CI.","user":"U68A3ASP9","ts":"1608130757.033800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"x8Bc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"GPUArrays.jl has an JLArray reference implementation that executes on the CPU. but it generally isn't fleshed out enough to be usable with realistic applications; it's currently mostly intended to run the GPUArrays test suite on CI."}]}]}],"reactions":[{"name":"+1","users":["U7THT3TM3"],"count":1}]},{"client_msg_id":"af14252e-8be5-42d4-a242-63d6256d5078","type":"message","text":"Okay, thanks","user":"U6CJRSR63","ts":"1608132080.034000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vnu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Okay, thanks"}]}]}]},{"client_msg_id":"dc388ba5-c663-4722-8370-91194e88a4e2","type":"message","text":"just got tripped up by this:\n```julia&gt; C = CUDA.rand(1, 1, 1, 1);\n\njulia&gt; view(C, :, :, :, 1)\n1×1×1 CuArray{Float32,3}:\n[:, :, 1] =\n 0.3129622\n\njulia&gt; selectdim(C, 4, 1)\n1×1×1 view(::CuArray{Float32,4}, :, :, :, 1) with eltype Float32:\n[:, :, 1] =\n 0.3129622```","user":"U6BJ9E351","ts":"1608209932.038300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PC/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"just got tripped up by this:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> C = CUDA.rand(1, 1, 1, 1);\n\njulia> view(C, :, :, :, 1)\n1×1×1 CuArray{Float32,3}:\n[:, :, 1] =\n 0.3129622\n\njulia> selectdim(C, 4, 1)\n1×1×1 view(::CuArray{Float32,4}, :, :, :, 1) with eltype Float32:\n[:, :, 1] =\n 0.3129622"}]}]}]},{"client_msg_id":"0d52e0db-5ce1-48ed-b05b-fab6dbe23135","type":"message","text":"for my use cases, it is much better to get the vanilla `CuArray` , so I agree with the idea that a contiguous view (not sure if the term is clear, I mean one where the data is stored contiguously) is just a `CuArray`, but I think it's odd that the two things differ","user":"U6BJ9E351","ts":"1608210031.039800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iknB0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"for my use cases, it is much better to get the vanilla "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":" , so I agree with the idea that a contiguous view (not sure if the term is clear, I mean one where the data is stored contiguously) is just a "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":", but I think it's odd that the two things differ"}]}]}]},{"client_msg_id":"adcdd7e5-e06a-46ec-b707-d76948927687","type":"message","text":"(related, if I get something that I know is stored contiguously, can I say \"treat this lump of memory as a `CuArray` with this size?)","user":"U6BJ9E351","ts":"1608210163.040600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+Zj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(related, if I get something that I know is stored contiguously, can I say \"treat this lump of memory as a "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":" with this size?)"}]}]}],"thread_ts":"1608210163.040600","reply_count":7,"reply_users_count":2,"latest_reply":"1608211559.044100","reply_users":["U68A3ASP9","U6BJ9E351"],"subscribed":false},{"client_msg_id":"71905dc4-07a3-4b52-b3f8-938f45689d1a","type":"message","text":"it's an optimization. I agree it isn't particularly nice, but when I started using SubArray for contiguous views too (we have Base.FastContiguousSubArray to determine if those are contiguous) package load times regressed significantly","user":"U68A3ASP9","ts":"1608210288.041500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"R5aU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it's an optimization. I agree it isn't particularly nice, but when I started using SubArray for contiguous views too (we have Base.FastContiguousSubArray to determine if those are contiguous) package load times regressed significantly"}]}]}]},{"client_msg_id":"1250eea6-9f91-4e9d-91a6-4fd9f3126bc3","type":"message","text":"because of complicated unions everywhere","user":"U68A3ASP9","ts":"1608210301.041700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=lp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"because of complicated unions everywhere"}]}]}]},{"client_msg_id":"8e83bcdd-9f0a-4802-97e0-29a62c302299","type":"message","text":"ok, I'll just go with `view` then. The problem is (I think) that I was also reshaping the thing afterwards, but `view` + `reshape` works fine.","user":"U6BJ9E351","ts":"1608210729.042900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jFW3Y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ok, I'll just go with "},{"type":"text","text":"view","style":{"code":true}},{"type":"text","text":" then. The problem is (I think) that I was also reshaping the thing afterwards, but "},{"type":"text","text":"view","style":{"code":true}},{"type":"text","text":" + "},{"type":"text","text":"reshape","style":{"code":true}},{"type":"text","text":" works fine."}]}]}]},{"client_msg_id":"0c59d1d9-d2bb-4b5f-b276-275c8da52679","type":"message","text":"Is there a general way to get the suitable version of function `f` that runs on array `c`? I'd rather avoid to special case CUDA. Is the \"general way\" something like `Broadcast.broadcasted(f, c).f` ? I don't like accessing a private field, but couldn't figure out another way","user":"U6BJ9E351","ts":"1608222377.046900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jQMQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a general way to get the suitable version of function "},{"type":"text","text":"f","style":{"code":true}},{"type":"text","text":" that runs on array "},{"type":"text","text":"c","style":{"code":true}},{"type":"text","text":"? I'd rather avoid to special case CUDA. Is the \"general way\" something like "},{"type":"text","text":"Broadcast.broadcasted(f, c).f","style":{"code":true}},{"type":"text","text":" ? I don't like accessing a private field, but couldn't figure out another way"}]}]}],"thread_ts":"1608222377.046900","reply_count":3,"reply_users_count":2,"latest_reply":"1608307125.053900","reply_users":["U68A3ASP9","U6BJ9E351"],"subscribed":false},{"client_msg_id":"a84adfba-7175-4a4d-9594-5956272c583e","type":"message","text":"Thanks to whoever mentioned KernelAbstractions! Not sure if the devs hang out in here, but I was wondering if there's a good way to handle multiple async events. I want to fire off a bunch of kernels (no data dependencies) then do a `collect` on all of them","user":"U9ZEWU9T9","ts":"1608263669.048400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Qc6P6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks to whoever mentioned KernelAbstractions! Not sure if the devs hang out in here, but I was wondering if there's a good way to handle multiple async events. I want to fire off a bunch of kernels (no data dependencies) then do a "},{"type":"text","text":"collect","style":{"code":true}},{"type":"text","text":" on all of them"}]}]}]},{"client_msg_id":"fd97e496-cb46-49ca-a636-76250d5ef354","type":"message","text":"Multievent","user":"U67BJLYCS","ts":"1608266785.048800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xXnB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Multievent"}]}]}]},{"client_msg_id":"bb68d037-aebb-4fba-b91f-999cf0beb733","type":"message","text":"Allows you to merge multiple events and merge them into one","user":"U67BJLYCS","ts":"1608267056.049500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/H5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Allows you to merge multiple events and merge them into one"}]}]}]},{"client_msg_id":"3a1d0356-65af-414e-a3d9-5b783440a430","type":"message","text":"```    events = []\n    for layer in 1:layers\n        push!(events, kernel_func!(cuda_grids, layer))\n    end\n    \n    # \"collect\" tasks\n    wait(MultiEvent(Tuple(events)))```","user":"U9ZEWU9T9","ts":"1608273459.050000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"f4P","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"    events = []\n    for layer in 1:layers\n        push!(events, kernel_func!(cuda_grids, layer))\n    end\n    \n    # \"collect\" tasks\n    wait(MultiEvent(Tuple(events)))"}]}]}]},{"client_msg_id":"59f5dc86-25a8-4ad2-85f7-160ff7071837","type":"message","text":"Not sure if that's the most ergonomic way to do things, but it seems to work","user":"U9ZEWU9T9","ts":"1608273487.050800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YkQo3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not sure if that's the most ergonomic way to do things, but it seems to work"}]}]}]},{"client_msg_id":"7fde09c2-b281-4a06-9550-23572059cb76","type":"message","text":"Is there a convenient way to do `map(f, axes(a, 2))`, but which produces a GPUArray if `a` is a `GPUArray`?","user":"UM30MT6RF","ts":"1608301804.052200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"XuQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a convenient way to do "},{"type":"text","text":"map(f, axes(a, 2))","style":{"code":true}},{"type":"text","text":", but which produces a GPUArray if "},{"type":"text","text":"a","style":{"code":true}},{"type":"text","text":" is a "},{"type":"text","text":"GPUArray","style":{"code":true}},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"b3183025-834d-4961-bd66-71c96563b56b","type":"message","text":"Perhaps `gpu_call` is what I need?","user":"UM30MT6RF","ts":"1608302053.052700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yuRB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Perhaps "},{"type":"text","text":"gpu_call","style":{"code":true}},{"type":"text","text":" is what I need?"}]}]}]},{"client_msg_id":"35a3e6d8-59c8-4e4e-a805-fd72198b7046","type":"message","text":"Ah, preallocating `similar` and then doing `map!` should work for my usecase, but it would be nice if the resulting eltype wouldn't have to be specified manually","user":"UM30MT6RF","ts":"1608302605.053800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Xr5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah, preallocating "},{"type":"text","text":"similar","style":{"code":true}},{"type":"text","text":" and then doing "},{"type":"text","text":"map!","style":{"code":true}},{"type":"text","text":" should work for my usecase, but it would be nice if the resulting eltype wouldn't have to be specified manually"}]}]}]},{"client_msg_id":"9dc8981c-f040-4c9a-a7f6-efcb8db83184","type":"message","text":"`something.(f.(axes(a, 2)), @view a[1,:])`? :grimacing:","user":"U6740K1SP","ts":"1608310738.054700","team":"T68168MUP","edited":{"user":"U6740K1SP","ts":"1608310825.000000"},"blocks":[{"type":"rich_text","block_id":"K6z7V","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"something.(f.(axes(a, 2)), @view a[1,:])","style":{"code":true}},{"type":"text","text":"? "},{"type":"emoji","name":"grimacing"}]}]}],"thread_ts":"1608310738.054700","reply_count":2,"reply_users_count":2,"latest_reply":"1608311702.055200","reply_users":["UM30MT6RF","U6740K1SP"],"subscribed":false},{"client_msg_id":"a749bba5-0972-4daf-b73d-b62d47046931","type":"message","text":"Is there a way to pass structs wrapping a `CuArray` to a `cufunction`? I want to do something like this:\n```julia&gt; cf(x) = (x.x[1] = 1f0; nothing)\ncf (generic function with 1 method)\n\njulia&gt; struct B{T}; x::T end\n\njulia&gt; x = B(cu([0f0, 0f0]))\nB{CuArray{Float32,1}}(Float32[0.0, 0.0])\n\njulia&gt; @cuda cf(x)\nERROR: GPU compilation of kernel cf(B{CuArray{Float32,1}}) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 2 to your kernel function is of type B{CuArray{Float32,1}}, which is not isbits:\n  .x is of type CuArray{Float32,1} which is not isbits.\n    .ctx is of type CuContext which is not isbits.\n\n\nStacktrace:\n [1] check_invocation(::GPUCompiler.CompilerJob, ::LLVM.Function) at /local/scratch/ssd/sschaub/.julia/packages/GPUCompiler/uTpNx/src/validation.jl:68\n[...]```","user":"UM30MT6RF","ts":"1608328657.056700","team":"T68168MUP","edited":{"user":"UM30MT6RF","ts":"1608328676.000000"},"blocks":[{"type":"rich_text","block_id":"qtTA1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to pass structs wrapping a "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":" to a "},{"type":"text","text":"cufunction","style":{"code":true}},{"type":"text","text":"? I want to do something like this:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> cf(x) = (x.x[1] = 1f0; nothing)\ncf (generic function with 1 method)\n\njulia> struct B{T}; x::T end\n\njulia> x = B(cu([0f0, 0f0]))\nB{CuArray{Float32,1}}(Float32[0.0, 0.0])\n\njulia> @cuda cf(x)\nERROR: GPU compilation of kernel cf(B{CuArray{Float32,1}}) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 2 to your kernel function is of type B{CuArray{Float32,1}}, which is not isbits:\n  .x is of type CuArray{Float32,1} which is not isbits.\n    .ctx is of type CuContext which is not isbits.\n\n\nStacktrace:\n [1] check_invocation(::GPUCompiler.CompilerJob, ::LLVM.Function) at /local/scratch/ssd/sschaub/.julia/packages/GPUCompiler/uTpNx/src/validation.jl:68\n[...]"}]}]}]},{"client_msg_id":"f4fc8eed-c94b-4c5d-ba90-bb84a2e9720e","type":"message","text":"Adapt.jl","user":"U67BJLYCS","ts":"1608340460.057000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rRk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Adapt.jl"}]}]}]},{"client_msg_id":"60c03838-bf1f-4ff2-96d2-1a4b4d30e453","type":"message","text":"For all your auto-conversion needs","user":"U67BJLYCS","ts":"1608340470.057400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ojbw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"For all your auto-conversion needs"}]}]}]},{"client_msg_id":"1131f4d4-2b64-4480-aeaf-37e808264e8b","type":"message","text":"Nice, thanks for the tip! I don't quite understand how yet, but this does the trick:\n```Adapt.adapt_structure(T, x::B) = B(adapt(T, x.x))```","user":"UM30MT6RF","ts":"1608341496.058600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"g8fd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Nice, thanks for the tip! I don't quite understand how yet, but this does the trick:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Adapt.adapt_structure(T, x::B) = B(adapt(T, x.x))"}]}]}]},{"client_msg_id":"c998b7d7-9cff-45d5-afed-6ce0717baf1e","type":"message","text":"Is this a missing method for `mul!(::StridedCuVector, ...)`?\n```julia&gt; mul!(view(cu(rand(Float32, 2, 2)), 1, :), cu(rand(Float32, 2, 2)), cu(rand(Float32, 2)))\nERROR: ArgumentError: cannot take the CPU address of a CuArray{Float32,2}\nStacktrace:\n [1] unsafe_convert(::Type{Ptr{Float32}}, ::CuArray{Float32,2}) at /local/scratch/ssd/sschaub/.julia/packages/CUDA/YeS8q/src/array.jl:211\n [2] gemv!(::Char, ::Float32, ::CuArray{Float32,2}, ::CuArray{Float32,1}, ::Float32, ::SubArray{Float32,1,CuArray{Float32,2},Tuple{Int64,Base.Slice{Base.OneTo{Int64}}},true}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/blas.jl:626\n [3] gemv!(::SubArray{Float32,1,CuArray{Float32,2},Tuple{Int64,Base.Slice{Base.OneTo{Int64}}},true}, ::Char, ::CuArray{Float32,2}, ::CuArray{Float32,1}, ::Bool, ::Bool) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl:470\n [4] mul! at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl:66 [inlined]\n [5] mul!(::SubArray{Float32,1,CuArray{Float32,2},Tuple{Int64,Base.Slice{Base.OneTo{Int64}}},true}, ::CuArray{Float32,2}, ::CuArray{Float32,1}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl:208\n [6] top-level scope at REPL[24]:1```","user":"UM30MT6RF","ts":"1608398855.059700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zQURv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is this a missing method for "},{"type":"text","text":"mul!(::StridedCuVector, ...)","style":{"code":true}},{"type":"text","text":"?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> mul!(view(cu(rand(Float32, 2, 2)), 1, :), cu(rand(Float32, 2, 2)), cu(rand(Float32, 2)))\nERROR: ArgumentError: cannot take the CPU address of a CuArray{Float32,2}\nStacktrace:\n [1] unsafe_convert(::Type{Ptr{Float32}}, ::CuArray{Float32,2}) at /local/scratch/ssd/sschaub/.julia/packages/CUDA/YeS8q/src/array.jl:211\n [2] gemv!(::Char, ::Float32, ::CuArray{Float32,2}, ::CuArray{Float32,1}, ::Float32, ::SubArray{Float32,1,CuArray{Float32,2},Tuple{Int64,Base.Slice{Base.OneTo{Int64}}},true}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/blas.jl:626\n [3] gemv!(::SubArray{Float32,1,CuArray{Float32,2},Tuple{Int64,Base.Slice{Base.OneTo{Int64}}},true}, ::Char, ::CuArray{Float32,2}, ::CuArray{Float32,1}, ::Bool, ::Bool) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl:470\n [4] mul! at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl:66 [inlined]\n [5] mul!(::SubArray{Float32,1,CuArray{Float32,2},Tuple{Int64,Base.Slice{Base.OneTo{Int64}}},true}, ::CuArray{Float32,2}, ::CuArray{Float32,1}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl:208\n [6] top-level scope at REPL[24]:1"}]}]}],"thread_ts":"1608398855.059700","reply_count":8,"reply_users_count":3,"latest_reply":"1608406026.061800","reply_users":["U6A0PD8CR","U68A3ASP9","UM30MT6RF"],"subscribed":false},{"client_msg_id":"16163de0-40e9-4f23-ba89-cb872a6c20e7","type":"message","text":"I posted a somewhat-convoluted GPU question on Discourse re: finding the indices of the two largest elements in an array <https://discourse.julialang.org/t/finding-index-and-value-of-largest-two-elements-in-a-cuarray/>","user":"UB2QRMQPN","ts":"1608422773.063200","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"Finding index and value of largest two elements in a CuArray","title_link":"https://discourse.julialang.org/t/finding-index-and-value-of-largest-two-elements-in-a-cuarray/","text":"I need to find the indices and values of the two largest elements in a 2D array. I whipped up a foldl-based solution for the CPU that’s much faster than Base.findmax and Base.partialsortperm, but it runs like molasses on the GPU compared to CUDA.findmax. function findmax2(prev, curr) i, v = curr v &gt; prev.first.v &amp;&amp; return (first=(; i, v), second=prev.first) v &gt; prev.second.v &amp;&amp; return (first=prev.first, second=(; i, v)) return prev end julia&gt; dA = CUDA.rand(512, 512); cA = Arra...","fallback":"JuliaLang: Finding index and value of largest two elements in a CuArray","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","ts":1608416181,"from_url":"https://discourse.julialang.org/t/finding-index-and-value-of-largest-two-elements-in-a-cuarray/","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/finding-index-and-value-of-largest-two-elements-in-a-cuarray/"}],"blocks":[{"type":"rich_text","block_id":"w6Y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I posted a somewhat-convoluted GPU question on Discourse re: finding the indices of the two largest elements in an array "},{"type":"link","url":"https://discourse.julialang.org/t/finding-index-and-value-of-largest-two-elements-in-a-cuarray/"}]}]}]},{"client_msg_id":"84913ea5-5fd2-4b57-927b-556a7d317dcc","type":"message","text":"You can do it on a CPU with a straightforward `foldl`, but the same approach on the GPU leads to slooooow scalar indexing (although I'm not sure where it happens - there aren't any obvious `getindex` calls)","user":"UB2QRMQPN","ts":"1608422918.064800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iVGKa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You can do it on a CPU with a straightforward "},{"type":"text","text":"foldl","style":{"code":true}},{"type":"text","text":", but the same approach on the GPU leads to slooooow scalar indexing (although I'm not sure where it happens - there aren't any obvious "},{"type":"text","text":"getindex","style":{"code":true}},{"type":"text","text":" calls)"}]}]}],"thread_ts":"1608422918.064800","reply_count":7,"reply_users_count":4,"latest_reply":"1608524190.068300","reply_users":["UCZ7VBGUD","UB2QRMQPN","UDGT4PM41","UC7AF7NSU"],"subscribed":false},{"client_msg_id":"03b079cb-fa53-4610-b0c9-50d2bd2a4ab1","type":"message","text":"<https://twitter.com/dougallj/status/1339934291929694210?s=19|https://twitter.com/dougallj/status/1339934291929694210?s=19>","user":"UDGT4PM41","ts":"1608462833.067000","team":"T68168MUP","attachments":[{"fallback":"<https://twitter.com/dougallj|@dougallj>: I started reversing Apple AMX, an undocumented 64-bit ARM instruction set extension for a matrix coprocessor, used by the Accelerate framework on the M1 (and A13+). My (incomplete) IDA/Hex-Rays plugin and notes so far: <https://gist.github.com/dougallj/7a75a3be1ec69ca550e7c36dc75e0d6f>","ts":1608300204,"author_name":"Dougall","author_link":"https://twitter.com/dougallj/status/1339934291929694210","author_icon":"https://pbs.twimg.com/profile_images/1272453689253351424/JEhZXeHE_normal.jpg","author_subname":"@dougallj","text":"I started reversing Apple AMX, an undocumented 64-bit ARM instruction set extension for a matrix coprocessor, used by the Accelerate framework on the M1 (and A13+). My (incomplete) IDA/Hex-Rays plugin and notes so far: <https://gist.github.com/dougallj/7a75a3be1ec69ca550e7c36dc75e0d6f>","service_name":"twitter","service_url":"https://twitter.com/","from_url":"https://twitter.com/dougallj/status/1339934291929694210?s=19","id":1,"original_url":"https://twitter.com/dougallj/status/1339934291929694210?s=19","footer":"Twitter","footer_icon":"https://a.slack-edge.com/80588/img/services/twitter_pixel_snapped_32.png"}],"blocks":[{"type":"rich_text","block_id":"9cL","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://twitter.com/dougallj/status/1339934291929694210?s=19","text":"https://twitter.com/dougallj/status/1339934291929694210?s=19"}]}]}],"reactions":[{"name":"apple","users":["UGU761DU2"],"count":1}]},{"client_msg_id":"bfbf37f4-e2d1-4f41-8da0-7a8db8eac011","type":"message","text":"Off topic, but I noticed one of the mesa devs is playing around with implementing an OpenCL userspace in Rust: <https://gitlab.freedesktop.org/karolherbst/mesa/-/tree/rusticl/src/gallium/frontends/rusticl>","user":"UMY1LV01G","ts":"1608487465.067900","team":"T68168MUP","edited":{"user":"UMY1LV01G","ts":"1608487504.000000"},"attachments":[{"service_name":"GitLab","title":"src/gallium/frontends/rusticl · rusticl · Karol Herbst / mesa","title_link":"https://gitlab.freedesktop.org/karolherbst/mesa/-/tree/rusticl/src/gallium/frontends/rusticl","text":"Mesa 3D graphics library","fallback":"GitLab: src/gallium/frontends/rusticl · rusticl · Karol Herbst / mesa","thumb_url":"https://gitlab.freedesktop.org/uploads/-/system/project/avatar/2064/gears.png","from_url":"https://gitlab.freedesktop.org/karolherbst/mesa/-/tree/rusticl/src/gallium/frontends/rusticl","thumb_width":773,"thumb_height":773,"service_icon":"https://gitlab.freedesktop.org/assets/touch-icon-iphone-5a9cee0e8a51212e70b90c87c12f382c428870c0ff67d1eb034d884b78d2dae7.png","id":1,"original_url":"https://gitlab.freedesktop.org/karolherbst/mesa/-/tree/rusticl/src/gallium/frontends/rusticl"}],"blocks":[{"type":"rich_text","block_id":"qElp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Off topic, but I noticed one of the mesa devs is playing around with implementing an OpenCL userspace in Rust: "},{"type":"link","url":"https://gitlab.freedesktop.org/karolherbst/mesa/-/tree/rusticl/src/gallium/frontends/rusticl"}]}]}]},{"client_msg_id":"03147d10-8b96-44bf-9e14-85362afa6dfd","type":"message","text":"Does any package or functions exist already which promote array storage? Something roughly like:\n```promote_storage(x::Array,   y::Array)   = (x,y)\npromote_storage(x::CuArray, y::CuArray) = (x,y)\npromote_storage(x::Array,   y::CuArray) = (cu(x),y)\npromote_storage(x::CuArray, y::Array)   = (x,cu(y))```","user":"UUMJUCYRK","ts":"1608585918.071400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Myq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does any package or functions exist already which promote array storage? Something roughly like:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"promote_storage(x::Array,   y::Array)   = (x,y)\npromote_storage(x::CuArray, y::CuArray) = (x,y)\npromote_storage(x::Array,   y::CuArray) = (cu(x),y)\npromote_storage(x::CuArray, y::Array)   = (x,cu(y))"}]}]}]},{"client_msg_id":"4223b09a-43bb-4a9c-8391-eca1b7d24375","type":"message","text":"If Adapt.jl doesn't already do that, it might be a nice idea for a PR.","user":"U8D9768Q6","ts":"1608586011.072100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KNTDS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If Adapt.jl doesn't already do that, it might be a nice idea for a PR."}]}]}],"thread_ts":"1608586011.072100","reply_count":1,"reply_users_count":1,"latest_reply":"1608586103.073100","reply_users":["U7THT3TM3"],"subscribed":false,"reactions":[{"name":"+1","users":["U7THT3TM3"],"count":1}]},{"client_msg_id":"8ed45261-3c76-47de-8752-90a240011f07","type":"message","text":"AFAIK it doesn't","user":"UUMJUCYRK","ts":"1608586091.072500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"njxgg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"AFAIK it doesn't"}]}]}]},{"client_msg_id":"F9096864-DBBD-4B39-A419-0423F7E5DDC3","type":"message","text":"A PR to Adapt.jl sounds great","user":"U7THT3TM3","ts":"1608586114.073600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sNs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"A PR to Adapt.jl sounds great"}]}]}]},{"client_msg_id":"a6689b22-7f1a-4bde-bc5f-a347cc4ea295","type":"message","text":"Hi there. I'm trying to figure out where to find an `im2col` for `CuArrays`, but with no luck yet :confused:, any hint?","user":"ULDCM1P9P","ts":"1608638446.076300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vx5s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi there. I'm trying to figure out where to find an "},{"type":"text","text":"im2col","style":{"code":true}},{"type":"text","text":" for "},{"type":"text","text":"CuArrays","style":{"code":true}},{"type":"text","text":", but with no luck yet "},{"type":"emoji","name":"confused"},{"type":"text","text":", any hint?"}]}]}]},{"client_msg_id":"faccc763-5483-4d8d-a637-b94235ffca93","type":"message","text":"`Flux.flatten` might work, alternatively, you can probably just use `reshape`","user":"UM30MT6RF","ts":"1608638801.077100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6Du","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Flux.flatten","style":{"code":true}},{"type":"text","text":" might work, alternatively, you can probably just use "},{"type":"text","text":"reshape","style":{"code":true}}]}]}]},{"client_msg_id":"8de2effa-93bc-417c-bea5-1e97810dfc90","type":"message","text":"Well, the operation I want is not just a reshape, it's rather about having a (reshaped) view as windows, typically used for simple implementation of convolutions","user":"ULDCM1P9P","ts":"1608638947.078400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yMq=v","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Well, the operation I want is not just a reshape, it's rather about having a (reshaped) view as windows, typically used for simple implementation of convolutions"}]}]}],"thread_ts":"1608638947.078400","reply_count":14,"reply_users_count":3,"latest_reply":"1608641357.081200","reply_users":["ULDCM1P9P","UM30MT6RF","UBVE598BC"],"subscribed":false},{"client_msg_id":"9a775179-f258-4582-8d12-8d18bcc08b1f","type":"message","text":"So, I am running CUDA.jl tests and some of them are failing. Should I ignore them? If not how do I follow up?","user":"UPK2KJ95Y","ts":"1608656070.082100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"E89ux","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So, I am running CUDA.jl tests and some of them are failing. Should I ignore them? If not how do I follow up?"}]}]}]},{"client_msg_id":"74cf21db-5696-48b6-9ec6-200e8e6467f5","type":"message","text":"Based on that this takes forever, I'm guessing the following is not optimized into a single kernel call?\n```x = cu(rand(256,4,256,4))\nmapslices(mean, x, dims=(1,3))```\nIs there an equivalent that would be fast, without writing my own kernel?","user":"UUMJUCYRK","ts":"1608758480.085600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"P36Ft","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Based on that this takes forever, I'm guessing the following is not optimized into a single kernel call?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"x = cu(rand(256,4,256,4))\nmapslices(mean, x, dims=(1,3))"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Is there an equivalent that would be fast, without writing my own kernel?"}]}]}]},{"client_msg_id":"b854bdc1-0375-456b-b675-d05f4e3b9cef","type":"message","text":"Answer: `mean(x, dims=(1,3))`","user":"UUMJUCYRK","ts":"1608759930.085900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=1gdD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Answer: "},{"type":"text","text":"mean(x, dims=(1,3))","style":{"code":true}}]}]}]},{"client_msg_id":"e0dde9fc-1eea-4297-b317-2f7bd2855c42","type":"message","text":"The following function works on the CPU, but I try to get it well-parallelized on the GPU.\nHere, `m` is and 2 dimensional array of floats; `x`, `y` and `s` are floats ...\n\nI tried to use broadcasting with `CUDA.@sync`, but since `f` is called with elements from both `offsetx` and `offsety`, this doesn't work.\n\nHow do I get such code run efficiently on a CUDA GPU?   Thanks.\n\n`mold! = function( m, x, y, s)` \n    `offsetx = Float32.( 1 : size( mold)[ 1]) .- x`\n    `offsety = Float32.( 1 : size( mold)[ 2]) .- y`\n\n    `for i in CartesianIndices( m)` \n        `m[ i] = f( offsetx[ i[1]], offsety[ i[2]], s)` \n    `end`\n`end`\n\nCall: `mold!( m, 12.34, 56.78, 9.0)`","user":"U015WE9UU0H","ts":"1608761114.087200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"i9OS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The following function works on the CPU, but I try to get it well-parallelized on the GPU.\nHere, "},{"type":"text","text":"m","style":{"code":true}},{"type":"text","text":" is and 2 dimensional array of floats; "},{"type":"text","text":"x","style":{"code":true}},{"type":"text","text":", "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"s","style":{"code":true}},{"type":"text","text":" are floats ...\n\nI tried to use broadcasting with "},{"type":"text","text":"CUDA.@sync","style":{"code":true}},{"type":"text","text":", but since "},{"type":"text","text":"f","style":{"code":true}},{"type":"text","text":" is called with elements from both "},{"type":"text","text":"offsetx","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"offsety","style":{"code":true}},{"type":"text","text":", this doesn't work.\n\nHow do I get such code run efficiently on a CUDA GPU?   Thanks.\n\n"},{"type":"text","text":"mold! = function( m, x, y, s) ","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    offsetx = Float32.( 1 : size( mold)[ 1]) .- x","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    offsety = Float32.( 1 : size( mold)[ 2]) .- y","style":{"code":true}},{"type":"text","text":"\n\n"},{"type":"text","text":"    for i in CartesianIndices( m) ","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"        m[ i] = f( offsetx[ i[1]], offsety[ i[2]], s) ","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    end","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"end","style":{"code":true}},{"type":"text","text":"\n\nCall: "},{"type":"text","text":"mold!( m, 12.34, 56.78, 9.0)","style":{"code":true}}]}]}]},{"client_msg_id":"A9336DD4-4B68-432D-AA5F-8CB2755A5FE5","type":"message","text":"Brand new to CUDA.jl, and I was looking at subsetting an array using a Boolean index. Is this possible for a CuArray? Struggling to find any documentation on the topic ","user":"U017XL92LJG","ts":"1608863457.089300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Dm2A","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Brand new to CUDA.jl, and I was looking at subsetting an array using a Boolean index. Is this possible for a CuArray? Struggling to find any documentation on the topic "}]}]}]},{"client_msg_id":"0de321aa-2f90-4730-a0c4-5811fc051f93","type":"message","text":"Any ideas on the prospects of running Julia on the new Apple M1 GPU and neural engine? I couldn't find this discussed anywhere.","user":"U85JBUGGP","ts":"1608916537.090600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0vr4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Any ideas on the prospects of running Julia on the new Apple M1 GPU and neural engine? I couldn't find this discussed anywhere."}]}]}]},{"client_msg_id":"b186d532-22eb-45c2-93a7-f3c465bd67f3","type":"message","text":"`cudaGraphicsResource` `cudaGraphicsGLRegisterBuffer` etc aren't currently wrapped in CUDA.jl, are they?","user":"U66SGEWAC","ts":"1609095267.094800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Xk0A","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"cudaGraphicsResource","style":{"code":true}},{"type":"text","text":" "},{"type":"text","text":"cudaGraphicsGLRegisterBuffer","style":{"code":true}},{"type":"text","text":" etc aren't currently wrapped in CUDA.jl, are they?"}]}]}]},{"client_msg_id":"1fd12799-6126-4514-b336-036f71d14645","type":"message","text":"Out of curiosity, is it likely that KernelAbstractions.jl will be working on Julia 1.6 by the time 1.6 is released?","user":"U7THT3TM3","ts":"1609297541.096900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nUfi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Out of curiosity, is it likely that KernelAbstractions.jl will be working on Julia 1.6 by the time 1.6 is released?"}]}]}]},{"client_msg_id":"ebd6dccd-6eaa-4126-a789-81499f8b0b8d","type":"message","text":"Yes","user":"U67BJLYCS","ts":"1609300461.097100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QHH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes"}]}]}]},{"client_msg_id":"a6492591-f3dc-42be-91ad-54da73fe0044","type":"message","text":"Quite likely","user":"U67BJLYCS","ts":"1609300484.097300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fzXx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Quite likely"}]}]}],"reactions":[{"name":"+1","users":["U7THT3TM3"],"count":1}]},{"client_msg_id":"defccd65-9e93-4c2f-b405-238305991f94","type":"message","text":"I have a C program that uses CuFFT to perform long FFts on 8 bit (signed integer) data samples.  CuFFT doesn't support 8 bit integers natively, but to conserve GPU memory the program uses CuFFT callbacks to feed it normalized floats that are read though a texture memory object backed by an input buffer containing the 8-bit samples.  This texture memory object is setup using the \"CUDA Runtime API\" `cudaTextureDesc` structure, which has a `readMode` field that can be set to `cudaReadModeNormalizedFloat` to get a \"free\" on-the-fly conversion from 8 bit signed integer to normalized float, `[-1, +1]`.\n\nI'd like to replicate this functionality with CUDA.jl, but I've encountered several roadblocks:\n\n1. CUDA.jl uses the \"CUDA Driver API\" and the texture object setup is different enough that there is no distinct equivalent to  `readMode=cudaReadModeNormalizedFloat`.  The CUDA docs are not too clear on this, but it seems like maybe just leaving out the `CU_TRSF_READ_AS_INTEGER` flag might suffice?  Currently that flag is always set if an integer type is backing the texture object, but that could be made selectable with a new keyword argument (with backward compatible default value) to the `CuTexture` constructor.\n2. Even if that does enable the equivalent behavior in the texture object, it seems that CUDA.jl texture fetches always returns integer values if the data type backing the texture object is of an integer type.  This precludes any use of the texture memory for this free int-to-float conversion, but it's not clear to me how to make it possible to selectively enable/disable this aspect of CUDA.jl's texture fetch functionality.\n3. The `cufftXtSetCallback()` function is not exposed by CUDA.jl.  This would be easy enough to add, but it's not clear how one could create the callback functions and get pointers to them.  Maybe via `CUDA.dynamic_cufunction`?\n4. Given the above items, I'm contemplating taking the memory hit and expanding the input data to 32 (or 16?) bit floats.  Has anyone used the (not yet wrapped) NPP functions?  They seem fairly straightforward to use via `ccall()`, but I wonder whether they offer enough added performance over CUDA.jl conversion functions to be worth the extra calling effort.\nThanks for any feedback!","user":"U01FKQQ7J0J","ts":"1609316596.124800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"aXQ5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a C program that uses CuFFT to perform long FFts on 8 bit (signed integer) data samples.  CuFFT doesn't support 8 bit integers natively, but to conserve GPU memory the program uses CuFFT callbacks to feed it normalized floats that are read though a texture memory object backed by an input buffer containing the 8-bit samples.  This texture memory object is setup using the \"CUDA Runtime API\" "},{"type":"text","text":"cudaTextureDesc","style":{"code":true}},{"type":"text","text":" structure, which has a "},{"type":"text","text":"readMode","style":{"code":true}},{"type":"text","text":" field that can be set to "},{"type":"text","text":"cudaReadModeNormalizedFloat","style":{"code":true}},{"type":"text","text":" to get a \"free\" on-the-fly conversion from 8 bit signed integer to normalized float, "},{"type":"text","text":"[-1, +1]","style":{"code":true}},{"type":"text","text":".\n\nI'd like to replicate this functionality with CUDA.jl, but I've encountered several roadblocks:\n\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CUDA.jl uses the \"CUDA Driver API\" and the texture object setup is different enough that there is no distinct equivalent to  "},{"type":"text","text":"readMode=cudaReadModeNormalizedFloat","style":{"code":true}},{"type":"text","text":".  The CUDA docs are not too clear on this, but it seems like maybe just leaving out the "},{"type":"text","text":"CU_TRSF_READ_AS_INTEGER","style":{"code":true}},{"type":"text","text":" flag might suffice?  Currently that flag is always set if an integer type is backing the texture object, but that could be made selectable with a new keyword argument (with backward compatible default value) to the "},{"type":"text","text":"CuTexture","style":{"code":true}},{"type":"text","text":" constructor."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Even if that does enable the equivalent behavior in the texture object, it seems that CUDA.jl texture fetches always returns integer values if the data type backing the texture object is of an integer type.  This precludes any use of the texture memory for this free int-to-float conversion, but it's not clear to me how to make it possible to selectively enable/disable this aspect of CUDA.jl's texture fetch functionality."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"The "},{"type":"text","text":"cufftXtSetCallback()","style":{"code":true}},{"type":"text","text":" function is not exposed by CUDA.jl.  This would be easy enough to add, but it's not clear how one could create the callback functions and get pointers to them.  Maybe via "},{"type":"text","text":"CUDA.dynamic_cufunction","style":{"code":true}},{"type":"text","text":"?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Given the above items, I'm contemplating taking the memory hit and expanding the input data to 32 (or 16?) bit floats.  Has anyone used the (not yet wrapped) NPP functions?  They seem fairly straightforward to use via "},{"type":"text","text":"ccall()","style":{"code":true}},{"type":"text","text":", but I wonder whether they offer enough added performance over CUDA.jl conversion functions to be worth the extra calling effort."}]}],"style":"ordered","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"\nThanks for any feedback!"}]}]}]},{"client_msg_id":"a216ec06-e1d2-4a56-952d-11adc9195c08","type":"message","text":"Hi,\n\nI can’t multiply a CSR sparse matrix anymore. This used to work with `CuArrays.jl`, is there a workaround?\n```julia&gt; Jpo_gpu * CuArray(orbitguess_f)\nERROR: MethodError: no method matching mv!(::Char, ::Float64, ::CUDA.CUSPARSE.CuSparseMatrixCSR{Float64}, ::CuArray{Float64,1}, ::Float64, ::CuArray{Float64,1}, ::Char)\nClosest candidates are:\n  mv!(::Char, ::Float64, ::CUDA.CUSPARSE.CuSparseMatrixBSR{Float64}, ::CuArray{Float64,1}, ::Float64, ::CuArray{Float64,1}, ::Char) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/wrappers.jl:193\n  mv!(::Char, ::Complex{Float64}, ::CUDA.CUSPARSE.CuSparseMatrixBSR{Complex{Float64}}, ::CuArray{Complex{Float64},1}, ::Complex{Float64}, ::CuArray{Complex{Float64},1}, ::Char) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/wrappers.jl:193\n  mv!(::Char, ::Complex{Float32}, ::CUDA.CUSPARSE.CuSparseMatrixBSR{Complex{Float32}}, ::CuArray{Complex{Float32},1}, ::Complex{Float32}, ::CuArray{Complex{Float32},1}, ::Char) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/wrappers.jl:193\n  ...\nStacktrace:\n [1] mul!(::CuArray{Float64,1}, ::CUDA.CUSPARSE.CuSparseMatrixCSR{Float64}, ::CuArray{Float64,1}) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/interfaces.jl:12\n [2] *(::CUDA.CUSPARSE.CuSparseMatrixCSR{Float64}, ::CuArray{Float64,1}) at /home/rveltz/julia/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/matmul.jl:51\n [3] top-level scope at REPL[1946]:1\n [4] eval(::Module, ::Any) at ./boot.jl:331\n [5] eval_user_input(::Any, ::REPL.REPLBackend) at /home/rveltz/julia/usr/share/julia/stdlib/v1.4/REPL/src/REPL.jl:86\n [6] run_backend(::REPL.REPLBackend) at /home/rveltz/.julia/packages/Revise/ucYAZ/src/Revise.jl:1184\n [7] top-level scope at REPL[1785]:0```","user":"U7GQE9JP9","ts":"1609324486.126000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ilrh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi,\n\nI can’t multiply a CSR sparse matrix anymore. This used to work with "},{"type":"text","text":"CuArrays.jl","style":{"code":true}},{"type":"text","text":", is there a workaround?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> Jpo_gpu * CuArray(orbitguess_f)\nERROR: MethodError: no method matching mv!(::Char, ::Float64, ::CUDA.CUSPARSE.CuSparseMatrixCSR{Float64}, ::CuArray{Float64,1}, ::Float64, ::CuArray{Float64,1}, ::Char)\nClosest candidates are:\n  mv!(::Char, ::Float64, ::CUDA.CUSPARSE.CuSparseMatrixBSR{Float64}, ::CuArray{Float64,1}, ::Float64, ::CuArray{Float64,1}, ::Char) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/wrappers.jl:193\n  mv!(::Char, ::Complex{Float64}, ::CUDA.CUSPARSE.CuSparseMatrixBSR{Complex{Float64}}, ::CuArray{Complex{Float64},1}, ::Complex{Float64}, ::CuArray{Complex{Float64},1}, ::Char) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/wrappers.jl:193\n  mv!(::Char, ::Complex{Float32}, ::CUDA.CUSPARSE.CuSparseMatrixBSR{Complex{Float32}}, ::CuArray{Complex{Float32},1}, ::Complex{Float32}, ::CuArray{Complex{Float32},1}, ::Char) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/wrappers.jl:193\n  ...\nStacktrace:\n [1] mul!(::CuArray{Float64,1}, ::CUDA.CUSPARSE.CuSparseMatrixCSR{Float64}, ::CuArray{Float64,1}) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/interfaces.jl:12\n [2] *(::CUDA.CUSPARSE.CuSparseMatrixCSR{Float64}, ::CuArray{Float64,1}) at /home/rveltz/julia/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/matmul.jl:51\n [3] top-level scope at REPL[1946]:1\n [4] eval(::Module, ::Any) at ./boot.jl:331\n [5] eval_user_input(::Any, ::REPL.REPLBackend) at /home/rveltz/julia/usr/share/julia/stdlib/v1.4/REPL/src/REPL.jl:86\n [6] run_backend(::REPL.REPLBackend) at /home/rveltz/.julia/packages/Revise/ucYAZ/src/Revise.jl:1184\n [7] top-level scope at REPL[1785]:0"}]}]}]}]}