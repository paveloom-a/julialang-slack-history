{"cursor": 0, "messages": [{"client_msg_id":"e88fbf86-1f69-4fe9-a15a-1dd9cbc530db","type":"message","text":"Can we get GPU CI configured for ForwardDiff?","user":"UEN48T0BT","ts":"1607967804.492700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wJ+Fl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can we get GPU CI configured for ForwardDiff?"}]}]}],"thread_ts":"1607967804.492700","reply_count":7,"reply_users_count":3,"latest_reply":"1608042439.006900","reply_users":["U68A3ASP9","UEN48T0BT","U680THK2S"],"subscribed":false},{"client_msg_id":"49ccd126-3237-4e13-b1b9-e4f463016ebd","type":"message","text":"<@U68A3ASP9>, I've been very slowly trying to understand how Nvidia Optix could work with Julia/CUDA.jl. So far I've understood that Optix has a C API to where one sends \"programs\" (ray generation, intersection) that are no more than PTX code (literally a string) compiled from functions with specific signatures. The functions have to have specific name prefixes, like `__raygen__` and receive no parameters. The input of such functions has to be done through global variables, e.g., `.const .align 8 .b8 params[24];`, and through calls to a device API defined in asm in C headers, e.g., `call (%r1), _optix_get_launch_index_x, ();`.   <@U68A3ASP9> can you give some hint whether any of this 3 means may pose a big problem to current CUDA.jl workings?","user":"U6CCK2SCV","ts":"1607973616.001100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xQ1m","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":", I've been very slowly trying to understand how Nvidia Optix could work with Julia/CUDA.jl. So far I've understood that Optix has a C API to where one sends \"programs\" (ray generation, intersection) that are no more than PTX code (literally a string) compiled from functions with specific signatures. The functions have to have specific name prefixes, like "},{"type":"text","text":"__raygen__","style":{"code":true}},{"type":"text","text":" and receive no parameters. The input of such functions has to be done through global variables, e.g., "},{"type":"text","text":".const .align 8 .b8 params[24];","style":{"code":true}},{"type":"text","text":", and through calls to a device API defined in asm in C headers, e.g., "},{"type":"text","text":"call (%r1), _optix_get_launch_index_x, ();","style":{"code":true}},{"type":"text","text":".   "},{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":" can you give some hint whether any of this 3 means may pose a big problem to current CUDA.jl workings?"}]}]}],"thread_ts":"1607973616.001100","reply_count":5,"reply_users_count":2,"latest_reply":"1607977510.006200","reply_users":["U6CCK2SCV","U68A3ASP9"],"subscribed":false},{"client_msg_id":"a04f7484-ae4e-4d7b-96df-3508a7bd15b9","type":"message","text":"Hey, what’s the status of sparse operations using `CUDA.jl` ? In particular, is there any way to efficiently evaluate a kronecker product of a dense CuArray and sparse CuArray?","user":"UPM0H43C7","ts":"1607974672.003300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"W2u","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey, what’s the status of sparse operations using "},{"type":"text","text":"CUDA.jl","style":{"code":true}},{"type":"text","text":" ? In particular, is there any way to efficiently evaluate a kronecker product of a dense CuArray and sparse CuArray?"}]}]}],"thread_ts":"1607974672.003300","reply_count":34,"reply_users_count":6,"latest_reply":"1608088028.019800","reply_users":["U68A3ASP9","UPM0H43C7","UDGT4PM41","U6A0PD8CR","U67BJLYCS","UH9KWTTD3"],"subscribed":false},{"client_msg_id":"f1adf01f-e383-484d-bb96-d722801552ad","type":"message","text":"I would like to run a large number of iterations (order 10^13 or more) of a relatively small sized simulation, for every iteration I need a bunch of random numbers which can't be pre computed and allocated on the cpu due to size. I was wondering if there is a way to use CUDA, since I believe kernel functions are pretty limited in terms of what can be done inside them.","user":"UU2E6M753","ts":"1608058751.012100","team":"T68168MUP","edited":{"user":"UU2E6M753","ts":"1608059241.000000"},"blocks":[{"type":"rich_text","block_id":"q9O5U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I would like to run a large number of iterations (order 10^13 or more) of a relatively small sized simulation, for every iteration I need a bunch of random numbers which can't be pre computed and allocated on the cpu due to size. I was wondering if there is a way to use CUDA, since I believe kernel functions are pretty limited in terms of what can be done inside them."}]}]}],"thread_ts":"1608058751.012100","reply_count":8,"reply_users_count":2,"latest_reply":"1608192672.037800","reply_users":["UU2E6M753","U68A3ASP9"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"What would it take to be able to write kernels on sparse matrices?","user":"UDGT4PM41","ts":"1608059346.012300","thread_ts":"1607974672.003300","root":{"client_msg_id":"a04f7484-ae4e-4d7b-96df-3508a7bd15b9","type":"message","text":"Hey, what’s the status of sparse operations using `CUDA.jl` ? In particular, is there any way to efficiently evaluate a kronecker product of a dense CuArray and sparse CuArray?","user":"UPM0H43C7","ts":"1607974672.003300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"W2u","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey, what’s the status of sparse operations using "},{"type":"text","text":"CUDA.jl","style":{"code":true}},{"type":"text","text":" ? In particular, is there any way to efficiently evaluate a kronecker product of a dense CuArray and sparse CuArray?"}]}]}],"thread_ts":"1607974672.003300","reply_count":34,"reply_users_count":6,"latest_reply":"1608088028.019800","reply_users":["U68A3ASP9","UPM0H43C7","UDGT4PM41","U6A0PD8CR","U67BJLYCS","UH9KWTTD3"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"OPd=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What would it take to be able to write kernels on sparse matrices?"}]}]}],"client_msg_id":"ebd29528-6442-486b-a11a-c68d038bfab8"},{"client_msg_id":"a3fdc5df-a77a-40b9-be49-e87d8bc8291f","type":"message","text":"I have an AMD GPU on a windows device with WSL2. Has anyone tried to run AMDGPU.jl on wsl2? If so, what are the necessary components to be installed?\n\nAre there any other ways to use an AMD GPU without WSL2 on windows? Is there a device agnostic binding available, that trades off a bit on performance, but can still be used?","user":"U01GL3BUV0W","ts":"1608095693.024700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kd5v","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have an AMD GPU on a windows device with WSL2. Has anyone tried to run AMDGPU.jl on wsl2? If so, what are the necessary components to be installed?\n\nAre there any other ways to use an AMD GPU without WSL2 on windows? Is there a device agnostic binding available, that trades off a bit on performance, but can still be used?"}]}]}],"thread_ts":"1608095693.024700","reply_count":3,"reply_users_count":1,"latest_reply":"1608157682.035900","reply_users":["U6A0PD8CR"],"subscribed":false},{"client_msg_id":"438e9cd3-3080-4776-8810-2992d35ce739","type":"message","text":"As far as I understand it AMDGPU.jl strictly requires ROCm (i.e., <https://github.com/RadeonOpenCompute/ROCm>) and as far as I know ROCm still only exists for Linux","user":"UGU761DU2","ts":"1608104331.026500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"k+JBT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"As far as I understand it AMDGPU.jl strictly requires ROCm (i.e., "},{"type":"link","url":"https://github.com/RadeonOpenCompute/ROCm"},{"type":"text","text":") and as far as I know ROCm still only exists for Linux"}]}]}]},{"client_msg_id":"652b9b94-1f63-436f-9338-0d79379b52ec","type":"message","text":"And it appears to be somewhat unclear whether WSL2 is Linux enough for ROCm, <https://github.com/RadeonOpenCompute/ROCm/issues/794>","user":"UGU761DU2","ts":"1608104464.027900","team":"T68168MUP","edited":{"user":"UGU761DU2","ts":"1608104537.000000"},"blocks":[{"type":"rich_text","block_id":"R5Ik","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And it appears to be somewhat unclear whether WSL2 is Linux enough for ROCm, "},{"type":"link","url":"https://github.com/RadeonOpenCompute/ROCm/issues/794"}]}]}],"thread_ts":"1608104464.027900","reply_count":10,"reply_users_count":3,"latest_reply":"1608213834.044900","reply_users":["U01GL3BUV0W","U66SGEWAC","U6A0PD8CR"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"What about other packages? OpenCL.jl has really poor docs, Vulkan.jl has none.","user":"U01GL3BUV0W","ts":"1608111087.028100","thread_ts":"1608104464.027900","root":{"client_msg_id":"652b9b94-1f63-436f-9338-0d79379b52ec","type":"message","text":"And it appears to be somewhat unclear whether WSL2 is Linux enough for ROCm, <https://github.com/RadeonOpenCompute/ROCm/issues/794>","user":"UGU761DU2","ts":"1608104464.027900","team":"T68168MUP","edited":{"user":"UGU761DU2","ts":"1608104537.000000"},"blocks":[{"type":"rich_text","block_id":"R5Ik","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And it appears to be somewhat unclear whether WSL2 is Linux enough for ROCm, "},{"type":"link","url":"https://github.com/RadeonOpenCompute/ROCm/issues/794"}]}]}],"thread_ts":"1608104464.027900","reply_count":10,"reply_users_count":3,"latest_reply":"1608213834.044900","reply_users":["U01GL3BUV0W","U66SGEWAC","U6A0PD8CR"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"G9Iyt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What about other packages? OpenCL.jl has really poor docs, Vulkan.jl has none."}]}]}],"client_msg_id":"bfdb725b-5105-4131-b5c3-12bf5311408f"},{"client_msg_id":"abf5d974-f6f5-4408-a4ae-29286296aff0","type":"message","text":"I think no GPU library runs on WSL2 out of the box right now - the very first thing that seems to be slowly working is CUDA on WSL2, but last time I checked it was in beta:\n<https://ubuntu.com/blog/getting-started-with-cuda-on-ubuntu-on-wsl-2>\nYou need to be on insider built etc... Pretty sure nothing like that exists for amd yet","user":"U66SGEWAC","ts":"1608115292.030600","team":"T68168MUP","attachments":[{"service_name":"Ubuntu","title":"Getting started with CUDA on Ubuntu on WSL 2 | Ubuntu","title_link":"https://ubuntu.com/blog/getting-started-with-cuda-on-ubuntu-on-wsl-2","text":"At Build 2020 Microsoft announced support for GPU compute on Windows Subsystem for Linux 2. Ubuntu is the leading Linux distribution for WSL and a sponsor of WSLConf. Canonical, the publisher of Ubuntu, provides enterprise support for Ubuntu on WSL through Ubuntu Advantage. This guide will walk early adopters through the steps on turning […]","fallback":"Ubuntu: Getting started with CUDA on Ubuntu on WSL 2 | Ubuntu","image_url":"https://ubuntu.com/wp-content/uploads/7332/191122_NvidiaStopsCUDAmacOS.jpg","from_url":"https://ubuntu.com/blog/getting-started-with-cuda-on-ubuntu-on-wsl-2","image_width":500,"image_height":250,"image_bytes":21952,"service_icon":"https://assets.ubuntu.com/v1/17b68252-apple-touch-icon-180x180-precomposed-ubuntu.png","id":1,"original_url":"https://ubuntu.com/blog/getting-started-with-cuda-on-ubuntu-on-wsl-2"}],"blocks":[{"type":"rich_text","block_id":"ZQe6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think no GPU library runs on WSL2 out of the box right now - the very first thing that seems to be slowly working is CUDA on WSL2, but last time I checked it was in beta:\n"},{"type":"link","url":"https://ubuntu.com/blog/getting-started-with-cuda-on-ubuntu-on-wsl-2"},{"type":"text","text":"\nYou need to be on insider built etc... Pretty sure nothing like that exists for amd yet"}]}]}],"reactions":[{"name":"+1","users":["U01GL3BUV0W"],"count":1}]},{"type":"message","subtype":"thread_broadcast","text":"So I actually put some time and looked at OpenCL, and it works, but you have to write the kernel yourself, unlike in CUDA where you can wrote simple Julia code.","user":"U01GL3BUV0W","ts":"1608116323.030800","thread_ts":"1608104464.027900","root":{"client_msg_id":"652b9b94-1f63-436f-9338-0d79379b52ec","type":"message","text":"And it appears to be somewhat unclear whether WSL2 is Linux enough for ROCm, <https://github.com/RadeonOpenCompute/ROCm/issues/794>","user":"UGU761DU2","ts":"1608104464.027900","team":"T68168MUP","edited":{"user":"UGU761DU2","ts":"1608104537.000000"},"blocks":[{"type":"rich_text","block_id":"R5Ik","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And it appears to be somewhat unclear whether WSL2 is Linux enough for ROCm, "},{"type":"link","url":"https://github.com/RadeonOpenCompute/ROCm/issues/794"}]}]}],"thread_ts":"1608104464.027900","reply_count":10,"reply_users_count":3,"latest_reply":"1608213834.044900","reply_users":["U01GL3BUV0W","U66SGEWAC","U6A0PD8CR"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"vIrh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So I actually put some time and looked at OpenCL, and it works, but you have to write the kernel yourself, unlike in CUDA where you can wrote simple Julia code."}]}]}],"client_msg_id":"7ab5e318-c0eb-451f-9dfe-85480068f071"},{"client_msg_id":"ff40dff6-05b1-443c-a664-a357587e907a","type":"message","text":"Isn't there a \"GPUArray\" package that allows you to run things on the CPU to check if your code is affected by allowscalar etc?","user":"U6CJRSR63","ts":"1608129684.032200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PWmQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Isn't there a \"GPUArray\" package that allows you to run things on the CPU to check if your code is affected by allowscalar etc?"}]}]}],"thread_ts":"1608129684.032200","reply_count":6,"reply_users_count":3,"latest_reply":"1608157277.035300","reply_users":["UEN48T0BT","U6CJRSR63","U6A0PD8CR"],"subscribed":false},{"client_msg_id":"f5eb3679-86e6-4650-af35-884b6570fe04","type":"message","text":"I thought there was, but can't remember which one","user":"U6CJRSR63","ts":"1608129695.032500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dXy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I thought there was, but can't remember which one"}]}]}]},{"client_msg_id":"5d5a3138-f560-46c6-bcd2-0016411d9d63","type":"message","text":"GPUArrays.jl has an JLArray reference implementation that executes on the CPU. but it generally isn't fleshed out enough to be usable with realistic applications; it's currently mostly intended to run the GPUArrays test suite on CI.","user":"U68A3ASP9","ts":"1608130757.033800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"x8Bc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"GPUArrays.jl has an JLArray reference implementation that executes on the CPU. but it generally isn't fleshed out enough to be usable with realistic applications; it's currently mostly intended to run the GPUArrays test suite on CI."}]}]}],"reactions":[{"name":"+1","users":["U7THT3TM3"],"count":1}]},{"client_msg_id":"af14252e-8be5-42d4-a242-63d6256d5078","type":"message","text":"Okay, thanks","user":"U6CJRSR63","ts":"1608132080.034000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vnu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Okay, thanks"}]}]}]},{"client_msg_id":"dc388ba5-c663-4722-8370-91194e88a4e2","type":"message","text":"just got tripped up by this:\n```julia&gt; C = CUDA.rand(1, 1, 1, 1);\n\njulia&gt; view(C, :, :, :, 1)\n1×1×1 CuArray{Float32,3}:\n[:, :, 1] =\n 0.3129622\n\njulia&gt; selectdim(C, 4, 1)\n1×1×1 view(::CuArray{Float32,4}, :, :, :, 1) with eltype Float32:\n[:, :, 1] =\n 0.3129622```","user":"U6BJ9E351","ts":"1608209932.038300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PC/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"just got tripped up by this:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> C = CUDA.rand(1, 1, 1, 1);\n\njulia> view(C, :, :, :, 1)\n1×1×1 CuArray{Float32,3}:\n[:, :, 1] =\n 0.3129622\n\njulia> selectdim(C, 4, 1)\n1×1×1 view(::CuArray{Float32,4}, :, :, :, 1) with eltype Float32:\n[:, :, 1] =\n 0.3129622"}]}]}]},{"client_msg_id":"0d52e0db-5ce1-48ed-b05b-fab6dbe23135","type":"message","text":"for my use cases, it is much better to get the vanilla `CuArray` , so I agree with the idea that a contiguous view (not sure if the term is clear, I mean one where the data is stored contiguously) is just a `CuArray`, but I think it's odd that the two things differ","user":"U6BJ9E351","ts":"1608210031.039800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iknB0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"for my use cases, it is much better to get the vanilla "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":" , so I agree with the idea that a contiguous view (not sure if the term is clear, I mean one where the data is stored contiguously) is just a "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":", but I think it's odd that the two things differ"}]}]}]},{"client_msg_id":"adcdd7e5-e06a-46ec-b707-d76948927687","type":"message","text":"(related, if I get something that I know is stored contiguously, can I say \"treat this lump of memory as a `CuArray` with this size?)","user":"U6BJ9E351","ts":"1608210163.040600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+Zj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(related, if I get something that I know is stored contiguously, can I say \"treat this lump of memory as a "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":" with this size?)"}]}]}],"thread_ts":"1608210163.040600","reply_count":7,"reply_users_count":2,"latest_reply":"1608211559.044100","reply_users":["U68A3ASP9","U6BJ9E351"],"subscribed":false},{"client_msg_id":"71905dc4-07a3-4b52-b3f8-938f45689d1a","type":"message","text":"it's an optimization. I agree it isn't particularly nice, but when I started using SubArray for contiguous views too (we have Base.FastContiguousSubArray to determine if those are contiguous) package load times regressed significantly","user":"U68A3ASP9","ts":"1608210288.041500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"R5aU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it's an optimization. I agree it isn't particularly nice, but when I started using SubArray for contiguous views too (we have Base.FastContiguousSubArray to determine if those are contiguous) package load times regressed significantly"}]}]}]},{"client_msg_id":"1250eea6-9f91-4e9d-91a6-4fd9f3126bc3","type":"message","text":"because of complicated unions everywhere","user":"U68A3ASP9","ts":"1608210301.041700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=lp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"because of complicated unions everywhere"}]}]}]},{"client_msg_id":"8e83bcdd-9f0a-4802-97e0-29a62c302299","type":"message","text":"ok, I'll just go with `view` then. The problem is (I think) that I was also reshaping the thing afterwards, but `view` + `reshape` works fine.","user":"U6BJ9E351","ts":"1608210729.042900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jFW3Y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ok, I'll just go with "},{"type":"text","text":"view","style":{"code":true}},{"type":"text","text":" then. The problem is (I think) that I was also reshaping the thing afterwards, but "},{"type":"text","text":"view","style":{"code":true}},{"type":"text","text":" + "},{"type":"text","text":"reshape","style":{"code":true}},{"type":"text","text":" works fine."}]}]}]},{"client_msg_id":"0c59d1d9-d2bb-4b5f-b276-275c8da52679","type":"message","text":"Is there a general way to get the suitable version of function `f` that runs on array `c`? I'd rather avoid to special case CUDA. Is the \"general way\" something like `Broadcast.broadcasted(f, c).f` ? I don't like accessing a private field, but couldn't figure out another way","user":"U6BJ9E351","ts":"1608222377.046900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jQMQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a general way to get the suitable version of function "},{"type":"text","text":"f","style":{"code":true}},{"type":"text","text":" that runs on array "},{"type":"text","text":"c","style":{"code":true}},{"type":"text","text":"? I'd rather avoid to special case CUDA. Is the \"general way\" something like "},{"type":"text","text":"Broadcast.broadcasted(f, c).f","style":{"code":true}},{"type":"text","text":" ? I don't like accessing a private field, but couldn't figure out another way"}]}]}],"thread_ts":"1608222377.046900","reply_count":3,"reply_users_count":2,"latest_reply":"1608307125.053900","reply_users":["U68A3ASP9","U6BJ9E351"],"subscribed":false},{"client_msg_id":"a84adfba-7175-4a4d-9594-5956272c583e","type":"message","text":"Thanks to whoever mentioned KernelAbstractions! Not sure if the devs hang out in here, but I was wondering if there's a good way to handle multiple async events. I want to fire off a bunch of kernels (no data dependencies) then do a `collect` on all of them","user":"U9ZEWU9T9","ts":"1608263669.048400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Qc6P6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks to whoever mentioned KernelAbstractions! Not sure if the devs hang out in here, but I was wondering if there's a good way to handle multiple async events. I want to fire off a bunch of kernels (no data dependencies) then do a "},{"type":"text","text":"collect","style":{"code":true}},{"type":"text","text":" on all of them"}]}]}]},{"client_msg_id":"fd97e496-cb46-49ca-a636-76250d5ef354","type":"message","text":"Multievent","user":"U67BJLYCS","ts":"1608266785.048800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xXnB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Multievent"}]}]}]},{"client_msg_id":"bb68d037-aebb-4fba-b91f-999cf0beb733","type":"message","text":"Allows you to merge multiple events and merge them into one","user":"U67BJLYCS","ts":"1608267056.049500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/H5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Allows you to merge multiple events and merge them into one"}]}]}]},{"client_msg_id":"3a1d0356-65af-414e-a3d9-5b783440a430","type":"message","text":"```    events = []\n    for layer in 1:layers\n        push!(events, kernel_func!(cuda_grids, layer))\n    end\n    \n    # \"collect\" tasks\n    wait(MultiEvent(Tuple(events)))```","user":"U9ZEWU9T9","ts":"1608273459.050000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"f4P","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"    events = []\n    for layer in 1:layers\n        push!(events, kernel_func!(cuda_grids, layer))\n    end\n    \n    # \"collect\" tasks\n    wait(MultiEvent(Tuple(events)))"}]}]}]},{"client_msg_id":"59f5dc86-25a8-4ad2-85f7-160ff7071837","type":"message","text":"Not sure if that's the most ergonomic way to do things, but it seems to work","user":"U9ZEWU9T9","ts":"1608273487.050800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YkQo3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not sure if that's the most ergonomic way to do things, but it seems to work"}]}]}]},{"client_msg_id":"7fde09c2-b281-4a06-9550-23572059cb76","type":"message","text":"Is there a convenient way to do `map(f, axes(a, 2))`, but which produces a GPUArray if `a` is a `GPUArray`?","user":"UM30MT6RF","ts":"1608301804.052200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"XuQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a convenient way to do "},{"type":"text","text":"map(f, axes(a, 2))","style":{"code":true}},{"type":"text","text":", but which produces a GPUArray if "},{"type":"text","text":"a","style":{"code":true}},{"type":"text","text":" is a "},{"type":"text","text":"GPUArray","style":{"code":true}},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"b3183025-834d-4961-bd66-71c96563b56b","type":"message","text":"Perhaps `gpu_call` is what I need?","user":"UM30MT6RF","ts":"1608302053.052700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yuRB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Perhaps "},{"type":"text","text":"gpu_call","style":{"code":true}},{"type":"text","text":" is what I need?"}]}]}]},{"client_msg_id":"35a3e6d8-59c8-4e4e-a805-fd72198b7046","type":"message","text":"Ah, preallocating `similar` and then doing `map!` should work for my usecase, but it would be nice if the resulting eltype wouldn't have to be specified manually","user":"UM30MT6RF","ts":"1608302605.053800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Xr5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah, preallocating "},{"type":"text","text":"similar","style":{"code":true}},{"type":"text","text":" and then doing "},{"type":"text","text":"map!","style":{"code":true}},{"type":"text","text":" should work for my usecase, but it would be nice if the resulting eltype wouldn't have to be specified manually"}]}]}]},{"client_msg_id":"9dc8981c-f040-4c9a-a7f6-efcb8db83184","type":"message","text":"`something.(f.(axes(a, 2)), @view a[1,:])`? :grimacing:","user":"U6740K1SP","ts":"1608310738.054700","team":"T68168MUP","edited":{"user":"U6740K1SP","ts":"1608310825.000000"},"blocks":[{"type":"rich_text","block_id":"K6z7V","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"something.(f.(axes(a, 2)), @view a[1,:])","style":{"code":true}},{"type":"text","text":"? "},{"type":"emoji","name":"grimacing"}]}]}],"thread_ts":"1608310738.054700","reply_count":2,"reply_users_count":2,"latest_reply":"1608311702.055200","reply_users":["UM30MT6RF","U6740K1SP"],"subscribed":false},{"client_msg_id":"a749bba5-0972-4daf-b73d-b62d47046931","type":"message","text":"Is there a way to pass structs wrapping a `CuArray` to a `cufunction`? I want to do something like this:\n```julia&gt; cf(x) = (x.x[1] = 1f0; nothing)\ncf (generic function with 1 method)\n\njulia&gt; struct B{T}; x::T end\n\njulia&gt; x = B(cu([0f0, 0f0]))\nB{CuArray{Float32,1}}(Float32[0.0, 0.0])\n\njulia&gt; @cuda cf(x)\nERROR: GPU compilation of kernel cf(B{CuArray{Float32,1}}) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 2 to your kernel function is of type B{CuArray{Float32,1}}, which is not isbits:\n  .x is of type CuArray{Float32,1} which is not isbits.\n    .ctx is of type CuContext which is not isbits.\n\n\nStacktrace:\n [1] check_invocation(::GPUCompiler.CompilerJob, ::LLVM.Function) at /local/scratch/ssd/sschaub/.julia/packages/GPUCompiler/uTpNx/src/validation.jl:68\n[...]```","user":"UM30MT6RF","ts":"1608328657.056700","team":"T68168MUP","edited":{"user":"UM30MT6RF","ts":"1608328676.000000"},"blocks":[{"type":"rich_text","block_id":"qtTA1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to pass structs wrapping a "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":" to a "},{"type":"text","text":"cufunction","style":{"code":true}},{"type":"text","text":"? I want to do something like this:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> cf(x) = (x.x[1] = 1f0; nothing)\ncf (generic function with 1 method)\n\njulia> struct B{T}; x::T end\n\njulia> x = B(cu([0f0, 0f0]))\nB{CuArray{Float32,1}}(Float32[0.0, 0.0])\n\njulia> @cuda cf(x)\nERROR: GPU compilation of kernel cf(B{CuArray{Float32,1}}) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 2 to your kernel function is of type B{CuArray{Float32,1}}, which is not isbits:\n  .x is of type CuArray{Float32,1} which is not isbits.\n    .ctx is of type CuContext which is not isbits.\n\n\nStacktrace:\n [1] check_invocation(::GPUCompiler.CompilerJob, ::LLVM.Function) at /local/scratch/ssd/sschaub/.julia/packages/GPUCompiler/uTpNx/src/validation.jl:68\n[...]"}]}]}]},{"client_msg_id":"f4fc8eed-c94b-4c5d-ba90-bb84a2e9720e","type":"message","text":"Adapt.jl","user":"U67BJLYCS","ts":"1608340460.057000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rRk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Adapt.jl"}]}]}]},{"client_msg_id":"60c03838-bf1f-4ff2-96d2-1a4b4d30e453","type":"message","text":"For all your auto-conversion needs","user":"U67BJLYCS","ts":"1608340470.057400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ojbw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"For all your auto-conversion needs"}]}]}]},{"client_msg_id":"1131f4d4-2b64-4480-aeaf-37e808264e8b","type":"message","text":"Nice, thanks for the tip! I don't quite understand how yet, but this does the trick:\n```Adapt.adapt_structure(T, x::B) = B(adapt(T, x.x))```","user":"UM30MT6RF","ts":"1608341496.058600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"g8fd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Nice, thanks for the tip! I don't quite understand how yet, but this does the trick:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Adapt.adapt_structure(T, x::B) = B(adapt(T, x.x))"}]}]}]},{"client_msg_id":"c998b7d7-9cff-45d5-afed-6ce0717baf1e","type":"message","text":"Is this a missing method for `mul!(::StridedCuVector, ...)`?\n```julia&gt; mul!(view(cu(rand(Float32, 2, 2)), 1, :), cu(rand(Float32, 2, 2)), cu(rand(Float32, 2)))\nERROR: ArgumentError: cannot take the CPU address of a CuArray{Float32,2}\nStacktrace:\n [1] unsafe_convert(::Type{Ptr{Float32}}, ::CuArray{Float32,2}) at /local/scratch/ssd/sschaub/.julia/packages/CUDA/YeS8q/src/array.jl:211\n [2] gemv!(::Char, ::Float32, ::CuArray{Float32,2}, ::CuArray{Float32,1}, ::Float32, ::SubArray{Float32,1,CuArray{Float32,2},Tuple{Int64,Base.Slice{Base.OneTo{Int64}}},true}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/blas.jl:626\n [3] gemv!(::SubArray{Float32,1,CuArray{Float32,2},Tuple{Int64,Base.Slice{Base.OneTo{Int64}}},true}, ::Char, ::CuArray{Float32,2}, ::CuArray{Float32,1}, ::Bool, ::Bool) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl:470\n [4] mul! at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl:66 [inlined]\n [5] mul!(::SubArray{Float32,1,CuArray{Float32,2},Tuple{Int64,Base.Slice{Base.OneTo{Int64}}},true}, ::CuArray{Float32,2}, ::CuArray{Float32,1}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl:208\n [6] top-level scope at REPL[24]:1```","user":"UM30MT6RF","ts":"1608398855.059700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zQURv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is this a missing method for "},{"type":"text","text":"mul!(::StridedCuVector, ...)","style":{"code":true}},{"type":"text","text":"?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> mul!(view(cu(rand(Float32, 2, 2)), 1, :), cu(rand(Float32, 2, 2)), cu(rand(Float32, 2)))\nERROR: ArgumentError: cannot take the CPU address of a CuArray{Float32,2}\nStacktrace:\n [1] unsafe_convert(::Type{Ptr{Float32}}, ::CuArray{Float32,2}) at /local/scratch/ssd/sschaub/.julia/packages/CUDA/YeS8q/src/array.jl:211\n [2] gemv!(::Char, ::Float32, ::CuArray{Float32,2}, ::CuArray{Float32,1}, ::Float32, ::SubArray{Float32,1,CuArray{Float32,2},Tuple{Int64,Base.Slice{Base.OneTo{Int64}}},true}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/blas.jl:626\n [3] gemv!(::SubArray{Float32,1,CuArray{Float32,2},Tuple{Int64,Base.Slice{Base.OneTo{Int64}}},true}, ::Char, ::CuArray{Float32,2}, ::CuArray{Float32,1}, ::Bool, ::Bool) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl:470\n [4] mul! at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl:66 [inlined]\n [5] mul!(::SubArray{Float32,1,CuArray{Float32,2},Tuple{Int64,Base.Slice{Base.OneTo{Int64}}},true}, ::CuArray{Float32,2}, ::CuArray{Float32,1}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl:208\n [6] top-level scope at REPL[24]:1"}]}]}],"thread_ts":"1608398855.059700","reply_count":8,"reply_users_count":3,"latest_reply":"1608406026.061800","reply_users":["U6A0PD8CR","U68A3ASP9","UM30MT6RF"],"subscribed":false},{"client_msg_id":"16163de0-40e9-4f23-ba89-cb872a6c20e7","type":"message","text":"I posted a somewhat-convoluted GPU question on Discourse re: finding the indices of the two largest elements in an array <https://discourse.julialang.org/t/finding-index-and-value-of-largest-two-elements-in-a-cuarray/>","user":"UB2QRMQPN","ts":"1608422773.063200","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"Finding index and value of largest two elements in a CuArray","title_link":"https://discourse.julialang.org/t/finding-index-and-value-of-largest-two-elements-in-a-cuarray/","text":"I need to find the indices and values of the two largest elements in a 2D array. I whipped up a foldl-based solution for the CPU that’s much faster than Base.findmax and Base.partialsortperm, but it runs like molasses on the GPU compared to CUDA.findmax. function findmax2(prev, curr) i, v = curr v &gt; prev.first.v &amp;&amp; return (first=(; i, v), second=prev.first) v &gt; prev.second.v &amp;&amp; return (first=prev.first, second=(; i, v)) return prev end julia&gt; dA = CUDA.rand(512, 512); cA = Arra...","fallback":"JuliaLang: Finding index and value of largest two elements in a CuArray","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","ts":1608416181,"from_url":"https://discourse.julialang.org/t/finding-index-and-value-of-largest-two-elements-in-a-cuarray/","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/finding-index-and-value-of-largest-two-elements-in-a-cuarray/"}],"blocks":[{"type":"rich_text","block_id":"w6Y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I posted a somewhat-convoluted GPU question on Discourse re: finding the indices of the two largest elements in an array "},{"type":"link","url":"https://discourse.julialang.org/t/finding-index-and-value-of-largest-two-elements-in-a-cuarray/"}]}]}]},{"client_msg_id":"84913ea5-5fd2-4b57-927b-556a7d317dcc","type":"message","text":"You can do it on a CPU with a straightforward `foldl`, but the same approach on the GPU leads to slooooow scalar indexing (although I'm not sure where it happens - there aren't any obvious `getindex` calls)","user":"UB2QRMQPN","ts":"1608422918.064800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iVGKa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You can do it on a CPU with a straightforward "},{"type":"text","text":"foldl","style":{"code":true}},{"type":"text","text":", but the same approach on the GPU leads to slooooow scalar indexing (although I'm not sure where it happens - there aren't any obvious "},{"type":"text","text":"getindex","style":{"code":true}},{"type":"text","text":" calls)"}]}]}],"thread_ts":"1608422918.064800","reply_count":7,"reply_users_count":4,"latest_reply":"1608524190.068300","reply_users":["UCZ7VBGUD","UB2QRMQPN","UDGT4PM41","UC7AF7NSU"],"subscribed":false},{"client_msg_id":"03b079cb-fa53-4610-b0c9-50d2bd2a4ab1","type":"message","text":"<https://twitter.com/dougallj/status/1339934291929694210?s=19|https://twitter.com/dougallj/status/1339934291929694210?s=19>","user":"UDGT4PM41","ts":"1608462833.067000","team":"T68168MUP","attachments":[{"fallback":"<https://twitter.com/dougallj|@dougallj>: I started reversing Apple AMX, an undocumented 64-bit ARM instruction set extension for a matrix coprocessor, used by the Accelerate framework on the M1 (and A13+). My (incomplete) IDA/Hex-Rays plugin and notes so far: <https://gist.github.com/dougallj/7a75a3be1ec69ca550e7c36dc75e0d6f>","ts":1608300204,"author_name":"Dougall","author_link":"https://twitter.com/dougallj/status/1339934291929694210","author_icon":"https://pbs.twimg.com/profile_images/1272453689253351424/JEhZXeHE_normal.jpg","author_subname":"@dougallj","text":"I started reversing Apple AMX, an undocumented 64-bit ARM instruction set extension for a matrix coprocessor, used by the Accelerate framework on the M1 (and A13+). My (incomplete) IDA/Hex-Rays plugin and notes so far: <https://gist.github.com/dougallj/7a75a3be1ec69ca550e7c36dc75e0d6f>","service_name":"twitter","service_url":"https://twitter.com/","from_url":"https://twitter.com/dougallj/status/1339934291929694210?s=19","id":1,"original_url":"https://twitter.com/dougallj/status/1339934291929694210?s=19","footer":"Twitter","footer_icon":"https://a.slack-edge.com/80588/img/services/twitter_pixel_snapped_32.png"}],"blocks":[{"type":"rich_text","block_id":"9cL","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://twitter.com/dougallj/status/1339934291929694210?s=19","text":"https://twitter.com/dougallj/status/1339934291929694210?s=19"}]}]}],"reactions":[{"name":"apple","users":["UGU761DU2"],"count":1}]},{"client_msg_id":"bfbf37f4-e2d1-4f41-8da0-7a8db8eac011","type":"message","text":"Off topic, but I noticed one of the mesa devs is playing around with implementing an OpenCL userspace in Rust: <https://gitlab.freedesktop.org/karolherbst/mesa/-/tree/rusticl/src/gallium/frontends/rusticl>","user":"UMY1LV01G","ts":"1608487465.067900","team":"T68168MUP","edited":{"user":"UMY1LV01G","ts":"1608487504.000000"},"attachments":[{"service_name":"GitLab","title":"src/gallium/frontends/rusticl · rusticl · Karol Herbst / mesa","title_link":"https://gitlab.freedesktop.org/karolherbst/mesa/-/tree/rusticl/src/gallium/frontends/rusticl","text":"Mesa 3D graphics library","fallback":"GitLab: src/gallium/frontends/rusticl · rusticl · Karol Herbst / mesa","thumb_url":"https://gitlab.freedesktop.org/uploads/-/system/project/avatar/2064/gears.png","from_url":"https://gitlab.freedesktop.org/karolherbst/mesa/-/tree/rusticl/src/gallium/frontends/rusticl","thumb_width":773,"thumb_height":773,"service_icon":"https://gitlab.freedesktop.org/assets/touch-icon-iphone-5a9cee0e8a51212e70b90c87c12f382c428870c0ff67d1eb034d884b78d2dae7.png","id":1,"original_url":"https://gitlab.freedesktop.org/karolherbst/mesa/-/tree/rusticl/src/gallium/frontends/rusticl"}],"blocks":[{"type":"rich_text","block_id":"qElp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Off topic, but I noticed one of the mesa devs is playing around with implementing an OpenCL userspace in Rust: "},{"type":"link","url":"https://gitlab.freedesktop.org/karolherbst/mesa/-/tree/rusticl/src/gallium/frontends/rusticl"}]}]}]},{"client_msg_id":"03147d10-8b96-44bf-9e14-85362afa6dfd","type":"message","text":"Does any package or functions exist already which promote array storage? Something roughly like:\n```promote_storage(x::Array,   y::Array)   = (x,y)\npromote_storage(x::CuArray, y::CuArray) = (x,y)\npromote_storage(x::Array,   y::CuArray) = (cu(x),y)\npromote_storage(x::CuArray, y::Array)   = (x,cu(y))```","user":"UUMJUCYRK","ts":"1608585918.071400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Myq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does any package or functions exist already which promote array storage? Something roughly like:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"promote_storage(x::Array,   y::Array)   = (x,y)\npromote_storage(x::CuArray, y::CuArray) = (x,y)\npromote_storage(x::Array,   y::CuArray) = (cu(x),y)\npromote_storage(x::CuArray, y::Array)   = (x,cu(y))"}]}]}]},{"client_msg_id":"4223b09a-43bb-4a9c-8391-eca1b7d24375","type":"message","text":"If Adapt.jl doesn't already do that, it might be a nice idea for a PR.","user":"U8D9768Q6","ts":"1608586011.072100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KNTDS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If Adapt.jl doesn't already do that, it might be a nice idea for a PR."}]}]}],"thread_ts":"1608586011.072100","reply_count":1,"reply_users_count":1,"latest_reply":"1608586103.073100","reply_users":["U7THT3TM3"],"subscribed":false,"reactions":[{"name":"+1","users":["U7THT3TM3"],"count":1}]},{"client_msg_id":"8ed45261-3c76-47de-8752-90a240011f07","type":"message","text":"AFAIK it doesn't","user":"UUMJUCYRK","ts":"1608586091.072500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"njxgg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"AFAIK it doesn't"}]}]}]},{"client_msg_id":"F9096864-DBBD-4B39-A419-0423F7E5DDC3","type":"message","text":"A PR to Adapt.jl sounds great","user":"U7THT3TM3","ts":"1608586114.073600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sNs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"A PR to Adapt.jl sounds great"}]}]}]},{"client_msg_id":"a6689b22-7f1a-4bde-bc5f-a347cc4ea295","type":"message","text":"Hi there. I'm trying to figure out where to find an `im2col` for `CuArrays`, but with no luck yet :confused:, any hint?","user":"ULDCM1P9P","ts":"1608638446.076300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vx5s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi there. I'm trying to figure out where to find an "},{"type":"text","text":"im2col","style":{"code":true}},{"type":"text","text":" for "},{"type":"text","text":"CuArrays","style":{"code":true}},{"type":"text","text":", but with no luck yet "},{"type":"emoji","name":"confused"},{"type":"text","text":", any hint?"}]}]}]},{"client_msg_id":"faccc763-5483-4d8d-a637-b94235ffca93","type":"message","text":"`Flux.flatten` might work, alternatively, you can probably just use `reshape`","user":"UM30MT6RF","ts":"1608638801.077100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6Du","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Flux.flatten","style":{"code":true}},{"type":"text","text":" might work, alternatively, you can probably just use "},{"type":"text","text":"reshape","style":{"code":true}}]}]}]},{"client_msg_id":"8de2effa-93bc-417c-bea5-1e97810dfc90","type":"message","text":"Well, the operation I want is not just a reshape, it's rather about having a (reshaped) view as windows, typically used for simple implementation of convolutions","user":"ULDCM1P9P","ts":"1608638947.078400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yMq=v","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Well, the operation I want is not just a reshape, it's rather about having a (reshaped) view as windows, typically used for simple implementation of convolutions"}]}]}],"thread_ts":"1608638947.078400","reply_count":14,"reply_users_count":3,"latest_reply":"1608641357.081200","reply_users":["ULDCM1P9P","UM30MT6RF","UBVE598BC"],"subscribed":false},{"client_msg_id":"9a775179-f258-4582-8d12-8d18bcc08b1f","type":"message","text":"So, I am running CUDA.jl tests and some of them are failing. Should I ignore them? If not how do I follow up?","user":"UPK2KJ95Y","ts":"1608656070.082100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"E89ux","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So, I am running CUDA.jl tests and some of them are failing. Should I ignore them? If not how do I follow up?"}]}]}]},{"client_msg_id":"74cf21db-5696-48b6-9ec6-200e8e6467f5","type":"message","text":"Based on that this takes forever, I'm guessing the following is not optimized into a single kernel call?\n```x = cu(rand(256,4,256,4))\nmapslices(mean, x, dims=(1,3))```\nIs there an equivalent that would be fast, without writing my own kernel?","user":"UUMJUCYRK","ts":"1608758480.085600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"P36Ft","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Based on that this takes forever, I'm guessing the following is not optimized into a single kernel call?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"x = cu(rand(256,4,256,4))\nmapslices(mean, x, dims=(1,3))"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Is there an equivalent that would be fast, without writing my own kernel?"}]}]}]},{"client_msg_id":"b854bdc1-0375-456b-b675-d05f4e3b9cef","type":"message","text":"Answer: `mean(x, dims=(1,3))`","user":"UUMJUCYRK","ts":"1608759930.085900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=1gdD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Answer: "},{"type":"text","text":"mean(x, dims=(1,3))","style":{"code":true}}]}]}]},{"client_msg_id":"e0dde9fc-1eea-4297-b317-2f7bd2855c42","type":"message","text":"The following function works on the CPU, but I try to get it well-parallelized on the GPU.\nHere, `m` is and 2 dimensional array of floats; `x`, `y` and `s` are floats ...\n\nI tried to use broadcasting with `CUDA.@sync`, but since `f` is called with elements from both `offsetx` and `offsety`, this doesn't work.\n\nHow do I get such code run efficiently on a CUDA GPU?   Thanks.\n\n`mold! = function( m, x, y, s)` \n    `offsetx = Float32.( 1 : size( mold)[ 1]) .- x`\n    `offsety = Float32.( 1 : size( mold)[ 2]) .- y`\n\n    `for i in CartesianIndices( m)` \n        `m[ i] = f( offsetx[ i[1]], offsety[ i[2]], s)` \n    `end`\n`end`\n\nCall: `mold!( m, 12.34, 56.78, 9.0)`","user":"U015WE9UU0H","ts":"1608761114.087200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"i9OS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The following function works on the CPU, but I try to get it well-parallelized on the GPU.\nHere, "},{"type":"text","text":"m","style":{"code":true}},{"type":"text","text":" is and 2 dimensional array of floats; "},{"type":"text","text":"x","style":{"code":true}},{"type":"text","text":", "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"s","style":{"code":true}},{"type":"text","text":" are floats ...\n\nI tried to use broadcasting with "},{"type":"text","text":"CUDA.@sync","style":{"code":true}},{"type":"text","text":", but since "},{"type":"text","text":"f","style":{"code":true}},{"type":"text","text":" is called with elements from both "},{"type":"text","text":"offsetx","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"offsety","style":{"code":true}},{"type":"text","text":", this doesn't work.\n\nHow do I get such code run efficiently on a CUDA GPU?   Thanks.\n\n"},{"type":"text","text":"mold! = function( m, x, y, s) ","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    offsetx = Float32.( 1 : size( mold)[ 1]) .- x","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    offsety = Float32.( 1 : size( mold)[ 2]) .- y","style":{"code":true}},{"type":"text","text":"\n\n"},{"type":"text","text":"    for i in CartesianIndices( m) ","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"        m[ i] = f( offsetx[ i[1]], offsety[ i[2]], s) ","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    end","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"end","style":{"code":true}},{"type":"text","text":"\n\nCall: "},{"type":"text","text":"mold!( m, 12.34, 56.78, 9.0)","style":{"code":true}}]}]}]},{"client_msg_id":"A9336DD4-4B68-432D-AA5F-8CB2755A5FE5","type":"message","text":"Brand new to CUDA.jl, and I was looking at subsetting an array using a Boolean index. Is this possible for a CuArray? Struggling to find any documentation on the topic ","user":"U017XL92LJG","ts":"1608863457.089300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Dm2A","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Brand new to CUDA.jl, and I was looking at subsetting an array using a Boolean index. Is this possible for a CuArray? Struggling to find any documentation on the topic "}]}]}]},{"client_msg_id":"0de321aa-2f90-4730-a0c4-5811fc051f93","type":"message","text":"Any ideas on the prospects of running Julia on the new Apple M1 GPU and neural engine? I couldn't find this discussed anywhere.","user":"U85JBUGGP","ts":"1608916537.090600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0vr4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Any ideas on the prospects of running Julia on the new Apple M1 GPU and neural engine? I couldn't find this discussed anywhere."}]}]}]},{"client_msg_id":"b186d532-22eb-45c2-93a7-f3c465bd67f3","type":"message","text":"`cudaGraphicsResource` `cudaGraphicsGLRegisterBuffer` etc aren't currently wrapped in CUDA.jl, are they?","user":"U66SGEWAC","ts":"1609095267.094800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Xk0A","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"cudaGraphicsResource","style":{"code":true}},{"type":"text","text":" "},{"type":"text","text":"cudaGraphicsGLRegisterBuffer","style":{"code":true}},{"type":"text","text":" etc aren't currently wrapped in CUDA.jl, are they?"}]}]}]},{"client_msg_id":"1fd12799-6126-4514-b336-036f71d14645","type":"message","text":"Out of curiosity, is it likely that KernelAbstractions.jl will be working on Julia 1.6 by the time 1.6 is released?","user":"U7THT3TM3","ts":"1609297541.096900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nUfi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Out of curiosity, is it likely that KernelAbstractions.jl will be working on Julia 1.6 by the time 1.6 is released?"}]}]}]},{"client_msg_id":"ebd6dccd-6eaa-4126-a789-81499f8b0b8d","type":"message","text":"Yes","user":"U67BJLYCS","ts":"1609300461.097100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QHH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes"}]}]}]},{"client_msg_id":"a6492591-f3dc-42be-91ad-54da73fe0044","type":"message","text":"Quite likely","user":"U67BJLYCS","ts":"1609300484.097300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fzXx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Quite likely"}]}]}],"reactions":[{"name":"+1","users":["U7THT3TM3"],"count":1}]},{"client_msg_id":"defccd65-9e93-4c2f-b405-238305991f94","type":"message","text":"I have a C program that uses CuFFT to perform long FFts on 8 bit (signed integer) data samples.  CuFFT doesn't support 8 bit integers natively, but to conserve GPU memory the program uses CuFFT callbacks to feed it normalized floats that are read though a texture memory object backed by an input buffer containing the 8-bit samples.  This texture memory object is setup using the \"CUDA Runtime API\" `cudaTextureDesc` structure, which has a `readMode` field that can be set to `cudaReadModeNormalizedFloat` to get a \"free\" on-the-fly conversion from 8 bit signed integer to normalized float, `[-1, +1]`.\n\nI'd like to replicate this functionality with CUDA.jl, but I've encountered several roadblocks:\n\n1. CUDA.jl uses the \"CUDA Driver API\" and the texture object setup is different enough that there is no distinct equivalent to  `readMode=cudaReadModeNormalizedFloat`.  The CUDA docs are not too clear on this, but it seems like maybe just leaving out the `CU_TRSF_READ_AS_INTEGER` flag might suffice?  Currently that flag is always set if an integer type is backing the texture object, but that could be made selectable with a new keyword argument (with backward compatible default value) to the `CuTexture` constructor.\n2. Even if that does enable the equivalent behavior in the texture object, it seems that CUDA.jl texture fetches always returns integer values if the data type backing the texture object is of an integer type.  This precludes any use of the texture memory for this free int-to-float conversion, but it's not clear to me how to make it possible to selectively enable/disable this aspect of CUDA.jl's texture fetch functionality.\n3. The `cufftXtSetCallback()` function is not exposed by CUDA.jl.  This would be easy enough to add, but it's not clear how one could create the callback functions and get pointers to them.  Maybe via `CUDA.dynamic_cufunction`?\n4. Given the above items, I'm contemplating taking the memory hit and expanding the input data to 32 (or 16?) bit floats.  Has anyone used the (not yet wrapped) NPP functions?  They seem fairly straightforward to use via `ccall()`, but I wonder whether they offer enough added performance over CUDA.jl conversion functions to be worth the extra calling effort.\nThanks for any feedback!","user":"U01FKQQ7J0J","ts":"1609316596.124800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"aXQ5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a C program that uses CuFFT to perform long FFts on 8 bit (signed integer) data samples.  CuFFT doesn't support 8 bit integers natively, but to conserve GPU memory the program uses CuFFT callbacks to feed it normalized floats that are read though a texture memory object backed by an input buffer containing the 8-bit samples.  This texture memory object is setup using the \"CUDA Runtime API\" "},{"type":"text","text":"cudaTextureDesc","style":{"code":true}},{"type":"text","text":" structure, which has a "},{"type":"text","text":"readMode","style":{"code":true}},{"type":"text","text":" field that can be set to "},{"type":"text","text":"cudaReadModeNormalizedFloat","style":{"code":true}},{"type":"text","text":" to get a \"free\" on-the-fly conversion from 8 bit signed integer to normalized float, "},{"type":"text","text":"[-1, +1]","style":{"code":true}},{"type":"text","text":".\n\nI'd like to replicate this functionality with CUDA.jl, but I've encountered several roadblocks:\n\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CUDA.jl uses the \"CUDA Driver API\" and the texture object setup is different enough that there is no distinct equivalent to  "},{"type":"text","text":"readMode=cudaReadModeNormalizedFloat","style":{"code":true}},{"type":"text","text":".  The CUDA docs are not too clear on this, but it seems like maybe just leaving out the "},{"type":"text","text":"CU_TRSF_READ_AS_INTEGER","style":{"code":true}},{"type":"text","text":" flag might suffice?  Currently that flag is always set if an integer type is backing the texture object, but that could be made selectable with a new keyword argument (with backward compatible default value) to the "},{"type":"text","text":"CuTexture","style":{"code":true}},{"type":"text","text":" constructor."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Even if that does enable the equivalent behavior in the texture object, it seems that CUDA.jl texture fetches always returns integer values if the data type backing the texture object is of an integer type.  This precludes any use of the texture memory for this free int-to-float conversion, but it's not clear to me how to make it possible to selectively enable/disable this aspect of CUDA.jl's texture fetch functionality."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"The "},{"type":"text","text":"cufftXtSetCallback()","style":{"code":true}},{"type":"text","text":" function is not exposed by CUDA.jl.  This would be easy enough to add, but it's not clear how one could create the callback functions and get pointers to them.  Maybe via "},{"type":"text","text":"CUDA.dynamic_cufunction","style":{"code":true}},{"type":"text","text":"?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Given the above items, I'm contemplating taking the memory hit and expanding the input data to 32 (or 16?) bit floats.  Has anyone used the (not yet wrapped) NPP functions?  They seem fairly straightforward to use via "},{"type":"text","text":"ccall()","style":{"code":true}},{"type":"text","text":", but I wonder whether they offer enough added performance over CUDA.jl conversion functions to be worth the extra calling effort."}]}],"style":"ordered","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"\nThanks for any feedback!"}]}]}]},{"client_msg_id":"a216ec06-e1d2-4a56-952d-11adc9195c08","type":"message","text":"Hi,\n\nI can’t multiply a CSR sparse matrix anymore. This used to work with `CuArrays.jl`, is there a workaround?\n```julia&gt; Jpo_gpu * CuArray(orbitguess_f)\nERROR: MethodError: no method matching mv!(::Char, ::Float64, ::CUDA.CUSPARSE.CuSparseMatrixCSR{Float64}, ::CuArray{Float64,1}, ::Float64, ::CuArray{Float64,1}, ::Char)\nClosest candidates are:\n  mv!(::Char, ::Float64, ::CUDA.CUSPARSE.CuSparseMatrixBSR{Float64}, ::CuArray{Float64,1}, ::Float64, ::CuArray{Float64,1}, ::Char) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/wrappers.jl:193\n  mv!(::Char, ::Complex{Float64}, ::CUDA.CUSPARSE.CuSparseMatrixBSR{Complex{Float64}}, ::CuArray{Complex{Float64},1}, ::Complex{Float64}, ::CuArray{Complex{Float64},1}, ::Char) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/wrappers.jl:193\n  mv!(::Char, ::Complex{Float32}, ::CUDA.CUSPARSE.CuSparseMatrixBSR{Complex{Float32}}, ::CuArray{Complex{Float32},1}, ::Complex{Float32}, ::CuArray{Complex{Float32},1}, ::Char) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/wrappers.jl:193\n  ...\nStacktrace:\n [1] mul!(::CuArray{Float64,1}, ::CUDA.CUSPARSE.CuSparseMatrixCSR{Float64}, ::CuArray{Float64,1}) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/interfaces.jl:12\n [2] *(::CUDA.CUSPARSE.CuSparseMatrixCSR{Float64}, ::CuArray{Float64,1}) at /home/rveltz/julia/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/matmul.jl:51\n [3] top-level scope at REPL[1946]:1\n [4] eval(::Module, ::Any) at ./boot.jl:331\n [5] eval_user_input(::Any, ::REPL.REPLBackend) at /home/rveltz/julia/usr/share/julia/stdlib/v1.4/REPL/src/REPL.jl:86\n [6] run_backend(::REPL.REPLBackend) at /home/rveltz/.julia/packages/Revise/ucYAZ/src/Revise.jl:1184\n [7] top-level scope at REPL[1785]:0```","user":"U7GQE9JP9","ts":"1609324486.126000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ilrh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi,\n\nI can’t multiply a CSR sparse matrix anymore. This used to work with "},{"type":"text","text":"CuArrays.jl","style":{"code":true}},{"type":"text","text":", is there a workaround?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> Jpo_gpu * CuArray(orbitguess_f)\nERROR: MethodError: no method matching mv!(::Char, ::Float64, ::CUDA.CUSPARSE.CuSparseMatrixCSR{Float64}, ::CuArray{Float64,1}, ::Float64, ::CuArray{Float64,1}, ::Char)\nClosest candidates are:\n  mv!(::Char, ::Float64, ::CUDA.CUSPARSE.CuSparseMatrixBSR{Float64}, ::CuArray{Float64,1}, ::Float64, ::CuArray{Float64,1}, ::Char) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/wrappers.jl:193\n  mv!(::Char, ::Complex{Float64}, ::CUDA.CUSPARSE.CuSparseMatrixBSR{Complex{Float64}}, ::CuArray{Complex{Float64},1}, ::Complex{Float64}, ::CuArray{Complex{Float64},1}, ::Char) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/wrappers.jl:193\n  mv!(::Char, ::Complex{Float32}, ::CUDA.CUSPARSE.CuSparseMatrixBSR{Complex{Float32}}, ::CuArray{Complex{Float32},1}, ::Complex{Float32}, ::CuArray{Complex{Float32},1}, ::Char) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/wrappers.jl:193\n  ...\nStacktrace:\n [1] mul!(::CuArray{Float64,1}, ::CUDA.CUSPARSE.CuSparseMatrixCSR{Float64}, ::CuArray{Float64,1}) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/interfaces.jl:12\n [2] *(::CUDA.CUSPARSE.CuSparseMatrixCSR{Float64}, ::CuArray{Float64,1}) at /home/rveltz/julia/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/matmul.jl:51\n [3] top-level scope at REPL[1946]:1\n [4] eval(::Module, ::Any) at ./boot.jl:331\n [5] eval_user_input(::Any, ::REPL.REPLBackend) at /home/rveltz/julia/usr/share/julia/stdlib/v1.4/REPL/src/REPL.jl:86\n [6] run_backend(::REPL.REPLBackend) at /home/rveltz/.julia/packages/Revise/ucYAZ/src/Revise.jl:1184\n [7] top-level scope at REPL[1785]:0"}]}]}]},{"client_msg_id":"911822ad-64ec-4277-a8f4-f3744a731e48","type":"message","text":"There wouldn't happen to be a library I can use for Kahan Summation on GPU from Julia, would there?","user":"UUMJUCYRK","ts":"1609371720.132000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WMQuO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"There wouldn't happen to be a library I can use for Kahan Summation on GPU from Julia, would there?"}]}]}]},{"client_msg_id":"63ab213c-ab84-4266-9b50-1db6aa90f8e7","type":"message","text":"What's the easiest way to download the CUDA artifacts?","user":"U7THT3TM3","ts":"1609578933.133000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GTK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What's the easiest way to download the CUDA artifacts?"}]}]}]},{"client_msg_id":"5e6ef995-b1a0-4b9c-905c-f907e8752bba","type":"message","text":"I.e., if I just do this: `using CUDA`, it doesn't download the artifacts, because they are lazy","user":"U7THT3TM3","ts":"1609578956.133700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=dC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I.e., if I just do this: "},{"type":"text","text":"using CUDA","style":{"code":true}},{"type":"text","text":", it doesn't download the artifacts, because they are lazy"}]}]}]},{"client_msg_id":"097d9aa0-431b-4dba-b139-8920948088ea","type":"message","text":"The artifacts get downloaded the first time I actually do something with CUDA","user":"U7THT3TM3","ts":"1609578969.134100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lm0S0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The artifacts get downloaded the first time I actually do something with CUDA"}]}]}]},{"client_msg_id":"335302b7-d168-47cd-a18c-08f1669a0890","type":"message","text":"But is there a way I can trigger the download manually, before I do any computation?","user":"U7THT3TM3","ts":"1609578995.134700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YHkA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But is there a way I can trigger the download manually, before I do any computation?"}]}]}]},{"client_msg_id":"63ece1c4-587d-41f1-9725-57a47e04ca7f","type":"message","text":"Is there a way to do matrix multiplication with cuBLAS and store the result by overwriting an existing matrix?\n\nI tried this:\n```CUDA.CUBLAS.gemm!(C, A, B)```\nWhere `A`, `B`, and `C` are `CuArray`, but I get a MethodError:\n```julia&gt; CUDA.CUBLAS.gemm!(C, A, B)\nERROR: MethodError: no method matching gemm!(::CuArray{Float64,2}, ::CuArray{Float64,2}, ::CuArray{Float64,2})\nStacktrace:\n [1] top-level scope at REPL[44]:1```","user":"U7THT3TM3","ts":"1609583317.136500","team":"T68168MUP","edited":{"user":"U7THT3TM3","ts":"1609583358.000000"},"blocks":[{"type":"rich_text","block_id":"tjo7X","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to do matrix multiplication with cuBLAS and store the result by overwriting an existing matrix?\n\nI tried this:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"CUDA.CUBLAS.gemm!(C, A, B)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Where "},{"type":"text","text":"A","style":{"code":true}},{"type":"text","text":", "},{"type":"text","text":"B","style":{"code":true}},{"type":"text","text":", and "},{"type":"text","text":"C","style":{"code":true}},{"type":"text","text":" are "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":", but I get a MethodError:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> CUDA.CUBLAS.gemm!(C, A, B)\nERROR: MethodError: no method matching gemm!(::CuArray{Float64,2}, ::CuArray{Float64,2}, ::CuArray{Float64,2})\nStacktrace:\n [1] top-level scope at REPL[44]:1"}]}]}],"thread_ts":"1609583317.136500","reply_count":5,"reply_users_count":2,"latest_reply":"1609586063.137700","reply_users":["UM30MT6RF","U7THT3TM3"],"subscribed":false},{"client_msg_id":"e985aef3-d285-4c08-843b-71256958aea2","type":"message","text":"Whats the current recommended way to deal with broadcasting CuArrays over Base.sin without having to have two copies of your code and make one of them CUDA.sin?","user":"UUMJUCYRK","ts":"1609619621.141000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RUie","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Whats the current recommended way to deal with broadcasting CuArrays over Base.sin without having to have two copies of your code and make one of them CUDA.sin?"}]}]}]},{"client_msg_id":"3179bb55-20bd-4906-9999-a082086024a5","type":"message","text":"SPIR-V kernel support just landed in Clover: <https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/2078>","user":"UMY1LV01G","ts":"1609700726.148200","team":"T68168MUP","attachments":[{"service_name":"GitLab","title":"clover: support IL programs (!2078) · Merge Requests · Mesa / mesa","title_link":"https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/2078","text":"The extension cl_khr_il_program and OpenCL 2.1 added functionalities to provide kernels as intermediate languages rather than OpenCL C. This merge request adds the infrastructure for it, as well...","fallback":"GitLab: clover: support IL programs (!2078) · Merge Requests · Mesa / mesa","thumb_url":"https://gitlab.freedesktop.org/uploads/-/system/project/avatar/176/gears.png","fields":[{"title":"Author","value":"Pierre Moreau","short":true},{"title":"Assignee","value":"Marge Bot","short":true}],"from_url":"https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/2078","thumb_width":773,"thumb_height":773,"service_icon":"https://gitlab.freedesktop.org/assets/touch-icon-iphone-5a9cee0e8a51212e70b90c87c12f382c428870c0ff67d1eb034d884b78d2dae7.png","id":1,"original_url":"https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/2078"}],"blocks":[{"type":"rich_text","block_id":"18h2d","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"SPIR-V kernel support just landed in Clover: "},{"type":"link","url":"https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/2078"}]}]}],"reactions":[{"name":"cool","users":["UDSU53PEG"],"count":1}]},{"client_msg_id":"20f060f0-582e-4763-9bd6-ebea723a0bc0","type":"message","text":"I wonder if it would support GPUCompiler output OOTB...","user":"UMY1LV01G","ts":"1609701304.148900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0OcFd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I wonder if it would support GPUCompiler output OOTB..."}]}]}]},{"client_msg_id":"4e4e4a8d-0a2d-40fb-a693-ae48ddc3d415","type":"message","text":"GPU office hours in an hour!","user":"U67BJLYCS","ts":"1609775650.150000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5kI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"GPU office hours in an hour!"}]}]}]},{"client_msg_id":"e0725d8f-458b-4e2d-8f72-9feaa54e4c8a","type":"message","text":"since people probably won't be upgrading to 1.6 immediately, I'm creating an release of CUDA.jl for 1.5 with most changes from the master branch backported to it: <https://github.com/JuliaGPU/CUDA.jl/pull/623>\nnow would be a good time for testing, or to create a PR if you want a specific feature/bugfix to be part of that release.","user":"U68A3ASP9","ts":"1609832816.151800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"X9O0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"since people probably won't be upgrading to 1.6 immediately, I'm creating an release of CUDA.jl for 1.5 with most changes from the master branch backported to it: "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/pull/623"},{"type":"text","text":"\nnow would be a good time for testing, or to create a PR if you want a specific feature/bugfix to be part of that release."}]}]}]},{"client_msg_id":"545257b8-8805-465d-b4d5-1c7ad650f630","type":"message","text":"<https://rosenzweig.io/blog/asahi-gpu-part-1.html>","user":"UDGT4PM41","ts":"1610046156.153800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hFnZ","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://rosenzweig.io/blog/asahi-gpu-part-1.html"}]}]}],"thread_ts":"1610046156.153800","reply_count":1,"reply_users_count":1,"latest_reply":"1610047108.154500","reply_users":["U6A0PD8CR"],"subscribed":false},{"client_msg_id":"da211520-4a12-41aa-82a0-f47b9eead1e9","type":"message","text":"\"Dissecting the Apple M1 GPU, part I\"","user":"UDGT4PM41","ts":"1610046183.154400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1qfk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"\"Dissecting the Apple M1 GPU, part I\""}]}]}]},{"client_msg_id":"ebbafe9b-dbb9-4b10-a66f-c5106c07a7b0","type":"message","text":"Any ideas for getting more output from a `CUDA.KernelException(CUDA.CuDevice(0))`?","user":"UM30MT6RF","ts":"1610049227.155300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WSML","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Any ideas for getting more output from a "},{"type":"text","text":"CUDA.KernelException(CUDA.CuDevice(0))","style":{"code":true}},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"06034694-ece4-4899-aa84-ee2af46f6d22","type":"message","text":"that's the host part, there should be a device print before that","user":"U68A3ASP9","ts":"1610049491.155900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gmI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"that's the host part, there should be a device print before that"}]}]}]},{"client_msg_id":"816d950e-4cb0-45de-8ce3-7afd0019c5da","type":"message","text":"unless you're running with `-g0`","user":"U68A3ASP9","ts":"1610049497.156100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YTn+D","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"unless you're running with "},{"type":"text","text":"-g0","style":{"code":true}}]}]}],"thread_ts":"1610049497.156100","reply_count":14,"reply_users_count":2,"latest_reply":"1610050910.160100","reply_users":["UM30MT6RF","U68A3ASP9"],"subscribed":false},{"client_msg_id":"63fc4c2e-6c8c-4c25-894f-9f606e0350a5","type":"message","text":"I get a backtrace, but not more:\n```ERROR: CUDA.KernelException(CUDA.CuDevice(0))\nStacktrace:\n  [1] check_exceptions()\n    @ CUDA /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/src/compiler/exceptions.jl:94\n  [2] prepare_cuda_call()\n    @ CUDA /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/src/state.jl:85\n  [3] initialize_api()\n    @ CUDA /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/lib/cudadrv/error.jl:92\n  [4] macro expansion\n    @ /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/lib/cudadrv/libcuda.jl:975 [inlined]\n  [5] macro expansion\n    @ /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/lib/cudadrv/error.jl:102 [inlined]\n  [6] cuStreamWaitEvent\n    @ /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/lib/utils/call.jl:26 [inlined]\n  [7] wait\n    @ /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/lib/cudadrv/events.jl:77 [inlined]\n  [8] wait\n    @ /local/scratch/ssd/sschaub/.julia/packages/KernelAbstractions/z2oKi/src/backends/cuda.jl:74 [inlined]\n  [9] #68#𝒜𝒸𝓉!\n    @ /local/scratch/ssd/sschaub/.julia/packages/Tullio/u3myB/src/macro.jl:1169 [inlined]\n [10] #68#𝒜𝒸𝓉!\n[...]```","user":"UM30MT6RF","ts":"1610049850.156900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ypmw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I get a backtrace, but not more:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: CUDA.KernelException(CUDA.CuDevice(0))\nStacktrace:\n  [1] check_exceptions()\n    @ CUDA /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/src/compiler/exceptions.jl:94\n  [2] prepare_cuda_call()\n    @ CUDA /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/src/state.jl:85\n  [3] initialize_api()\n    @ CUDA /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/lib/cudadrv/error.jl:92\n  [4] macro expansion\n    @ /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/lib/cudadrv/libcuda.jl:975 [inlined]\n  [5] macro expansion\n    @ /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/lib/cudadrv/error.jl:102 [inlined]\n  [6] cuStreamWaitEvent\n    @ /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/lib/utils/call.jl:26 [inlined]\n  [7] wait\n    @ /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/lib/cudadrv/events.jl:77 [inlined]\n  [8] wait\n    @ /local/scratch/ssd/sschaub/.julia/packages/KernelAbstractions/z2oKi/src/backends/cuda.jl:74 [inlined]\n  [9] #68#𝒜𝒸𝓉!\n    @ /local/scratch/ssd/sschaub/.julia/packages/Tullio/u3myB/src/macro.jl:1169 [inlined]\n [10] #68#𝒜𝒸𝓉!\n[...]"}]}]}]},{"client_msg_id":"2dfb2eb5-7d16-4521-8536-626c640dde3b","type":"message","text":"Hi all,\nI'm looking to use a GPU CG method with an ILU preconditioner. I'm currently using a CPU CG method through IterativeSolvers and ILU through *<https://github.com/haampie/IncompleteLU.jl|IncompleteLU.jl>* . The CG method from IterativeSolvers is easily extended to GPUs however, some work is required on IncompleteLU.jl to implement it on a GPU. I think only `forward_substitution` and `backward_substitution` methods are required to extend this. I've tried using `sv2!` with little success. Can anyone give me a hand?","user":"U01CZ5M383E","ts":"1610065370.164500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mxao1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi all,\nI'm looking to use a GPU CG method with an ILU preconditioner. I'm currently using a CPU CG method through IterativeSolvers and ILU through "},{"type":"link","url":"https://github.com/haampie/IncompleteLU.jl","text":"IncompleteLU.jl","style":{"bold":true}},{"type":"text","text":" . The CG method from IterativeSolvers is easily extended to GPUs however, some work is required on IncompleteLU.jl to implement it on a GPU. I think only "},{"type":"text","text":"forward_substitution","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"backward_substitution","style":{"code":true}},{"type":"text","text":" methods are required to extend this. I've tried using "},{"type":"text","text":"sv2!","style":{"code":true}},{"type":"text","text":" with little success. Can anyone give me a hand?"}]}]}]},{"client_msg_id":"bf6d7784-cd26-4954-b000-f24e5654e325","type":"message","text":"I have also tried built-in CUDA ILU with no success (`ilu02`)","user":"U01CZ5M383E","ts":"1610065586.165000","team":"T68168MUP","edited":{"user":"U01CZ5M383E","ts":"1610065611.000000"},"blocks":[{"type":"rich_text","block_id":"0B4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have also tried built-in CUDA ILU with no success ("},{"type":"text","text":"ilu02","style":{"code":true}},{"type":"text","text":")"}]}]}]},{"client_msg_id":"ff01381b-91bf-4877-9671-30732952b395","type":"message","text":"Yeah, I have never tried that myself. In the end it's just 2 sparse triangular matrices L and U that should be copied to the GPU. I can't recall why IncompleteLU implements the forward &amp; backward solves itself and doesn't use UnitLowerTriangular and such. I'm also not aware if there is a sparse triangular solver provided by cusparse you could use","user":"U6N6VQE30","ts":"1610067848.172700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sgvT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah, I have never tried that myself. In the end it's just 2 sparse triangular matrices L and U that should be copied to the GPU. I can't recall why IncompleteLU implements the forward & backward solves itself and doesn't use UnitLowerTriangular and such. I'm also not aware if there is a sparse triangular solver provided by cusparse you could use"}]}]}]},{"client_msg_id":"5d0b04f1-2e19-4e8b-b7ca-7fc6343ac1cc","type":"message","text":"I really like EllipsisNotation.jl, but unfortunately it looks like it causes CuArray views that would otherwise not need a wrapper to acquire one, which screws up some of my downstream code:\n```julia&gt; using CUDA, EllipsisNotation\n\njulia&gt; x = cu(rand(10));\n\njulia&gt; view(x, 1:2)\n2-element CuArray{Float32,1}:\n 0.06164578\n 0.95716375\n\njulia&gt; view(x, 1:2, ..)\n2-element view(::CuArray{Float32,1}, 1:2) with eltype Float32:\n 0.06164578\n 0.95716375```\nNot knowing which repo is the right one file an Issue in, figured Id ask here, any ideas?","user":"UUMJUCYRK","ts":"1610075375.175900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5d4V","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I really like EllipsisNotation.jl, but unfortunately it looks like it causes CuArray views that would otherwise not need a wrapper to acquire one, which screws up some of my downstream code:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> using CUDA, EllipsisNotation\n\njulia> x = cu(rand(10));\n\njulia> view(x, 1:2)\n2-element CuArray{Float32,1}:\n 0.06164578\n 0.95716375\n\njulia> view(x, 1:2, ..)\n2-element view(::CuArray{Float32,1}, 1:2) with eltype Float32:\n 0.06164578\n 0.95716375"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Not knowing which repo is the right one file an Issue in, figured Id ask here, any ideas?"}]}]}]},{"client_msg_id":"f3336360-3275-4bc2-84cb-f8ff05b7cdc0","type":"message","text":"EllipsisNotation.jl probably needs overloads for `view` with GPUs, since it's specialized in CUDA.jl","user":"U69BL50BF","ts":"1610078238.176500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jLXp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"EllipsisNotation.jl probably needs overloads for "},{"type":"text","text":"view","style":{"code":true}},{"type":"text","text":" with GPUs, since it's specialized in CUDA.jl"}]}]}]},{"client_msg_id":"D2ED59B0-5BD8-47C7-99C8-08AC89B90202","type":"message","text":"Does the master branch of CUDA.jl work on Julia 1.6-nightly?","user":"U7THT3TM3","ts":"1610101498.177500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DJw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does the master branch of CUDA.jl work on Julia 1.6-nightly?"}]}]}]},{"client_msg_id":"834EC604-66F2-45ED-99FE-C800677B5141","type":"message","text":"If so, would be great to get a release","user":"U7THT3TM3","ts":"1610101543.177800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YpE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If so, would be great to get a release"}]}]}]},{"client_msg_id":"2BB84F2E-069D-4181-98B4-F1CA429155DA","type":"message","text":"Context: <https://github.com/JuliaGPU/KernelAbstractions.jl/pull/177#issuecomment-756655092|https://github.com/JuliaGPU/KernelAbstractions.jl/pull/177#issuecomment-756655092>","user":"U7THT3TM3","ts":"1610101574.178100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OVsfM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Context: "},{"type":"link","url":"https://github.com/JuliaGPU/KernelAbstractions.jl/pull/177#issuecomment-756655092","text":"https://github.com/JuliaGPU/KernelAbstractions.jl/pull/177#issuecomment-756655092"}]}]}]},{"client_msg_id":"f28f30c3-fcdf-46b6-860a-eb3b70ddd94c","type":"message","text":"I'm first going to release a version for 1.5, wasn't planning on doing 1.6 anytime soon. why not use a Manifest and use CUDA.jl by git sha?","user":"U68A3ASP9","ts":"1610105415.178600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3OTw2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm first going to release a version for 1.5, wasn't planning on doing 1.6 anytime soon. why not use a Manifest and use CUDA.jl by git sha?"}]}]}]},{"client_msg_id":"4716143A-94E0-4377-BFB0-0034A5403970","type":"message","text":"CUDA master requires 1.6, right?\n\nIf we check in a Manifest that uses CUDA master, then this pins all CI jobs to the package versions in that Manifest.","user":"U7THT3TM3","ts":"1610105512.179800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"77F/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CUDA master requires 1.6, right?\n"},{"type":"text","text":"\n"},{"type":"text","text":"If we check in a Manifest that uses CUDA master, then this pins all CI jobs to the package versions in that Manifest."}]}]}]},{"client_msg_id":"E5453593-2F1B-46D6-BD60-4840DDA52B50","type":"message","text":"And that Manifest can't be used on Julia 1.5.","user":"U7THT3TM3","ts":"1610105536.180500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=Omp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And that Manifest can't be used on Julia 1.5."}]}]}]},{"client_msg_id":"3702b2ab-8001-4166-b30d-2abc338812b2","type":"message","text":"ah, right","user":"U68A3ASP9","ts":"1610105542.180700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oIqDE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ah, right"}]}]}]},{"client_msg_id":"b9658741-74ac-4005-8349-e4a6a92a716f","type":"message","text":"that's an annoying limitation of manifests, tbh","user":"U68A3ASP9","ts":"1610105576.182000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uyGm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"that's an annoying limitation of manifests, tbh"}]}]}]},{"client_msg_id":"E4CB6468-A6F4-480E-A297-B7FCCF50E9CF","type":"message","text":"I had an idea once that we should have separate Manifests for different Julia versions. Because manifests are not portable between Julia versions","user":"U7THT3TM3","ts":"1610105579.182200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"d4DH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I had an idea once that we should have separate Manifests for different Julia versions. Because manifests are not portable between Julia versions"}]}]}]},{"client_msg_id":"4CC6EF7B-4E39-4352-97DF-3DF878B86EA0","type":"message","text":"So you'd have \n\n`Manifest.1.5.toml`\n`Manifest.1.6.toml`\n\nEtc","user":"U7THT3TM3","ts":"1610105592.182600","team":"T68168MUP","edited":{"user":"U7THT3TM3","ts":"1610105671.000000"},"blocks":[{"type":"rich_text","block_id":"p=Wcy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So you'd have \n\n"},{"type":"text","text":"Manifest.1.5.toml","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"Manifest.1.6.toml","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"\nEtc"}]}]}],"thread_ts":"1610105592.182600","reply_count":4,"reply_users_count":2,"latest_reply":"1610105970.188900","reply_users":["U68A3ASP9","U7THT3TM3"],"subscribed":false},{"client_msg_id":"1943F412-7D34-4EC6-B2CA-D5FF09BA4D11","type":"message","text":"And then Julia would load the matching manifest","user":"U7THT3TM3","ts":"1610105604.183000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"w=L9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And then Julia would load the matching manifest"}]}]}]},{"client_msg_id":"1ED0583B-A051-404D-A6E8-CD08317414F8","type":"message","text":"The response to this idea was mixed. ","user":"U7THT3TM3","ts":"1610105617.183400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"i=9KW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The response to this idea was mixed. "}]}]}]},{"client_msg_id":"919F1DBD-838D-4D33-8DB4-C45710B1D57A","type":"message","text":"So probably it won't be adopted.","user":"U7THT3TM3","ts":"1610105648.184200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WRs7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So probably it won't be adopted."}]}]}]},{"client_msg_id":"D4CC9596-003F-497A-8CC3-9544E1F3E2DC","type":"message","text":"Here's what we could do for now. In the buildkite file (<https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/.buildkite/pipeline.yml|https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/.buildkite/pipeline.yml>), after the \"JuliaCI/julia#v0.6\" and before the \"JuliaCI/julia-test#v0.3\", I would add a line that looks like this:\n\n`julia -e 'using Pkg; pkgstr\"] add CUDA#master\"` ","user":"U7THT3TM3","ts":"1610105883.186500","team":"T68168MUP","edited":{"user":"U7THT3TM3","ts":"1610105905.000000"},"blocks":[{"type":"rich_text","block_id":"g3T","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Here's what we could do for now. In the buildkite file ("},{"type":"link","url":"https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/.buildkite/pipeline.yml","text":"https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/.buildkite/pipeline.yml"},{"type":"text","text":"), after the \"JuliaCI/julia#v0.6\" and before the \"JuliaCI/julia-test#v0.3\", I would add a line that looks like this:\n\n"},{"type":"text","text":"julia -e 'using Pkg; pkgstr\"] add CUDA#m","style":{"code":true}},{"type":"text","text":"aster","style":{"code":true}},{"type":"text","text":"\"","style":{"code":true}},{"type":"text","text":" "}]}]}]},{"client_msg_id":"A38E6345-19F5-4C6D-A8EE-7D030F8639E1","type":"message","text":"Is it possible to run arbitrary commands in the buildkite pipeline?","user":"U7THT3TM3","ts":"1610105894.187000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UyOYN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is it possible to run arbitrary commands in the buildkite pipeline?"}]}]}]},{"client_msg_id":"e8b8fd32-4395-417c-8471-2ef36322d895","type":"message","text":"no. there's only a single command, the plugins add hooks to execute before that. so this command would be executed after having instantiated, and override the actual test command","user":"U68A3ASP9","ts":"1610106058.189900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"H/FZL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"no. there's only a single command, the plugins add hooks to execute before that. so this command would be executed after having instantiated, and override the actual test command"}]}]}]},{"client_msg_id":"cf685244-9a39-43e1-bf23-161b34dc3309","type":"message","text":"but I'll create a release, nobody is using 1.6 anyway","user":"U68A3ASP9","ts":"1610106075.190300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"XJ1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"but I'll create a release, nobody is using 1.6 anyway"}]}]}],"reactions":[{"name":"heart","users":["U7THT3TM3"],"count":1}]},{"client_msg_id":"FA732284-EA69-4CC2-961D-2ECE5BC39C99","type":"message","text":"Long term, it might actually be worth adding the \"per-Julia-version manifests\" feature to Buildkite at least","user":"U7THT3TM3","ts":"1610106116.191400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Q8/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Long term, it might actually be worth adding the \"per-Julia-version manifests\" feature to Buildkite at least"}]}]}]},{"client_msg_id":"14A6185C-9597-4767-8646-415DB182588C","type":"message","text":"We can mull it over for a while","user":"U7THT3TM3","ts":"1610106131.191800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gwia","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We can mull it over for a while"}]}]}]},{"client_msg_id":"30a84e08-61c6-4a22-8e66-7103fc5379ed","type":"message","text":"Has anyone looked into using the newly added `reinterpret(reshape, ...)` on `CuArrays`? I am running into issues with KernelAbstractions and am wondering whether that might be a CUDA issue: <https://github.com/JuliaGPU/KernelAbstractions.jl/issues/179#issuecomment-756714700>","user":"UM30MT6RF","ts":"1610111815.194200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uy2oO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Has anyone looked into using the newly added "},{"type":"text","text":"reinterpret(reshape, ...)","style":{"code":true}},{"type":"text","text":" on "},{"type":"text","text":"CuArrays","style":{"code":true}},{"type":"text","text":"? I am running into issues with KernelAbstractions and am wondering whether that might be a CUDA issue: "},{"type":"link","url":"https://github.com/JuliaGPU/KernelAbstractions.jl/issues/179#issuecomment-756714700"}]}]}]},{"client_msg_id":"1f1b622d-db78-4124-9069-6491bee77213","type":"message","text":"Maybe the best solution here would be to produce a `CuArray` as well instead of a wrapper?","user":"UM30MT6RF","ts":"1610111935.195300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"l/Lm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Maybe the best solution here would be to produce a "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":" as well instead of a wrapper?"}]}]}]},{"client_msg_id":"2e074ac1-d506-47c8-9e28-a673743020c5","type":"message","text":"Hi all,\nDoes anyone have any ideas on how to fix this code so that the GPU output matches the CPU CG + CPU `\\`? On Julia 1.5 with CUDA 2.0.2\n```using CUDA, CUDA.CUSPARSE, CUDA.CUSOLVER\nusing LinearAlgebra\nusing SparseArrays\nusing IncompleteLU\nusing IterativeSolvers\nCUDA.allowscalar(false)\n\nval = sprand(200,200,0.05);\nA_cpu = val*val'\nb_cpu = rand(200)\n\nA_gpu = CuSparseMatrixCSC(A_cpu)\nb_gpu = CuArray(b_cpu)\n\nPrecilu = ilu(A_cpu, τ = 3.0)\n\nimport Base: ldiv!\nfunction LinearAlgebra.ldiv!(y::CuArray, P::LUgpu, x::CuArray)\n  copyto!(y, x)\n  sv2!('N', 'L', 1.0, P, y, 'O')\n  sv2!('N', 'U', 1.0, P, y, 'O')\n  return y\nend\n\nfunction LinearAlgebra.ldiv!(P::LUgpu, x::CuArray)\n  sv2!('N', 'L', 1.0, P, x, 'O')\n  sv2!('N', 'U', 1.0, P, x, 'O')\n  return x\nend\n\n# function LinearAlgebra.ldiv!(_lu::LUperso, rhs::CUDA.CuArray)\n# \t_x = UpperTriangular(_lu.Ut) \\ (LowerTriangular(_lu.L) \\ rhs)\n# \trhs .= vec(_x)\n# \t# CUDA.unsafe_free!(_x)\n# \trhs\n# end\n#\n# function LinearAlgebra.ldiv!(yrhs::CuArray,_lu::LUperso, rhs::CuArray)\n# \tcopyto!(yrhs,rhs)\n# \t_x = UpperTriangular(_lu.Ut) \\ (LowerTriangular(_lu.L) \\ rhs)\n# \trhs .= vec(_x)\n# \t# CUDA.unsafe_free!(_x)\n# \trhs\n# end\n\nstruct LUgpu\n\tL\n\tUt\t# transpose of U in LU decomposition\nend\n\n\nP = LUperso(LowerTriangular(CuSparseMatrixCSC(I+Precilu.L)), UpperTriangular(CuSparseMatrixCSC(sparse(Precilu.U'))));\nval = cg(A_gpu,b_gpu,verbose=true,Pl=P,tol=10^-7,maxiter=1000)\n\nP_cpu = ilu(A_cpu,τ=3.0)\nval = cg(A_cpu,b_cpu;verbose=true,Pl=P_cpu,tol=10^-7,maxiter=1000)\nA_cpu\\b_cpu ```","user":"U01CZ5M383E","ts":"1610153168.199800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"p9E","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi all,\nDoes anyone have any ideas on how to fix this code so that the GPU output matches the CPU CG + CPU "},{"type":"text","text":"\\","style":{"code":true}},{"type":"text","text":"? On Julia 1.5 with CUDA 2.0.2\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using CUDA, CUDA.CUSPARSE, CUDA.CUSOLVER\nusing LinearAlgebra\nusing SparseArrays\nusing IncompleteLU\nusing IterativeSolvers\nCUDA.allowscalar(false)\n\nval = sprand(200,200,0.05);\nA_cpu = val*val'\nb_cpu = rand(200)\n\nA_gpu = CuSparseMatrixCSC(A_cpu)\nb_gpu = CuArray(b_cpu)\n\nPrecilu = ilu(A_cpu, τ = 3.0)\n\nimport Base: ldiv!\nfunction LinearAlgebra.ldiv!(y::CuArray, P::LUgpu, x::CuArray)\n  copyto!(y, x)\n  sv2!('N', 'L', 1.0, P, y, 'O')\n  sv2!('N', 'U', 1.0, P, y, 'O')\n  return y\nend\n\nfunction LinearAlgebra.ldiv!(P::LUgpu, x::CuArray)\n  sv2!('N', 'L', 1.0, P, x, 'O')\n  sv2!('N', 'U', 1.0, P, x, 'O')\n  return x\nend\n\n# function LinearAlgebra.ldiv!(_lu::LUperso, rhs::CUDA.CuArray)\n# \t_x = UpperTriangular(_lu.Ut) \\ (LowerTriangular(_lu.L) \\ rhs)\n# \trhs .= vec(_x)\n# \t# CUDA.unsafe_free!(_x)\n# \trhs\n# end\n#\n# function LinearAlgebra.ldiv!(yrhs::CuArray,_lu::LUperso, rhs::CuArray)\n# \tcopyto!(yrhs,rhs)\n# \t_x = UpperTriangular(_lu.Ut) \\ (LowerTriangular(_lu.L) \\ rhs)\n# \trhs .= vec(_x)\n# \t# CUDA.unsafe_free!(_x)\n# \trhs\n# end\n\nstruct LUgpu\n\tL\n\tUt\t# transpose of U in LU decomposition\nend\n\n\nP = LUperso(LowerTriangular(CuSparseMatrixCSC(I+Precilu.L)), UpperTriangular(CuSparseMatrixCSC(sparse(Precilu.U'))));\nval = cg(A_gpu,b_gpu,verbose=true,Pl=P,tol=10^-7,maxiter=1000)\n\nP_cpu = ilu(A_cpu,τ=3.0)\nval = cg(A_cpu,b_cpu;verbose=true,Pl=P_cpu,tol=10^-7,maxiter=1000)\nA_cpu\\b_cpu "}]}]}]},{"client_msg_id":"94898089-1969-4F52-B660-A03E7B41E7AA","type":"message","text":"Is the newly released CUDA.jl version 2.4.0 meant to be compatible with Julia 1.6?\n\nThe compat entry is `julia = \"~1.5\"`, which unfortunately excludes Julia 1.6.","user":"U7THT3TM3","ts":"1610241890.201500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KiE6B","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is the newly released CUDA.jl version 2.4.0 meant to be compatible with Julia 1.6?\n"},{"type":"text","text":"\n"},{"type":"text","text":"The compat entry is "},{"type":"text","text":"julia = \"~1.5\"","style":{"code":true}},{"type":"text","text":", which unfortunately excludes Julia 1.6."}]}]}]},{"client_msg_id":"6862B773-446C-48C4-BB16-6E8EA985263E","type":"message","text":"Oh I see... CUDA.jl 2.4.0 was the release for Julia 1.5","user":"U7THT3TM3","ts":"1610241927.202100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"s7ZHW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oh I see... CUDA.jl 2.4.0 was the release for Julia 1.5"}]}]}]},{"client_msg_id":"2B8E2C0A-E732-4363-8188-21F63C1FCE88","type":"message","text":"CUDA.jl 2.5.0 will be for Julia 1.6, but we are having trouble registering in because of Registrator","user":"U7THT3TM3","ts":"1610241947.202800","team":"T68168MUP","edited":{"user":"U7THT3TM3","ts":"1610241951.000000"},"blocks":[{"type":"rich_text","block_id":"WXQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CUDA.jl 2.5.0 will be for Julia 1.6, but we are having trouble registering in because of Registrator"}]}]}]},{"client_msg_id":"ab803024-8cdb-439c-bbe8-a6d28bbb6447","type":"message","text":"what docker or singularity container should I use? is there a 1.5 image?","user":"UAH43TMUN","ts":"1610346131.204800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wMl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"what docker or singularity container should I use? is there a 1.5 image?"}]}]}]},{"client_msg_id":"f07fd6a4-3821-4e5c-b009-4a2635a35489","type":"message","text":"Hi, I am trying to use `CUDA.cudaDeviceGetAttribute()` on `CUDA.cudaDevAttrMaxThreadsPerBlock` but how do I feed the first argument?\n\n```function cudaDeviceGetAttribute(value, attr, device)\n    ccall(\"extern cudaDeviceGetAttribute\", llvmcall, cudaError_t,\n          (Ptr{Cint}, cudaDeviceAttr, Cint),\n          value, attr, device)\nend```\ni.e. I'm wondering how to get a pointer for the first argument:\n`CUDA.cudaDeviceGetAttribute(????, CUDA.cudaDevAttrMaxThreadsPerBlock, Cint(0))`","user":"UG1KB4NLD","ts":"1610384752.207300","team":"T68168MUP","edited":{"user":"UG1KB4NLD","ts":"1610384973.000000"},"blocks":[{"type":"rich_text","block_id":"Us7n","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I am trying to use "},{"type":"text","text":"CUDA.cudaDeviceGetAttribute()","style":{"code":true}},{"type":"text","text":" on "},{"type":"text","text":"CUDA.cudaDevAttrMaxThreadsPerBlock","style":{"code":true}},{"type":"text","text":" but how do I feed the first argument?\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function cudaDeviceGetAttribute(value, attr, device)\n    ccall(\"extern cudaDeviceGetAttribute\", llvmcall, cudaError_t,\n          (Ptr{Cint}, cudaDeviceAttr, Cint),\n          value, attr, device)\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"i.e. I'm wondering how to get a pointer for the first argument:\n"},{"type":"text","text":"CUDA.cudaDeviceGetAttribute(????, CUDA.cudaDevAttrMaxThreadsPerBlock, Cint(0))","style":{"code":true}}]}]}],"thread_ts":"1610384752.207300","reply_count":15,"reply_users_count":3,"latest_reply":"1610387814.210400","reply_users":["U68A3ASP9","UG1KB4NLD","U67BJLYCS"],"subscribed":false},{"client_msg_id":"e8f569e6-af81-4788-9719-f86178c06ec4","type":"message","text":"To update the `CUdevice_attribute_enum`, I\n1. `cd /res/wrap`\n2. `julia`\n3. `] activated .`\n4. `include(\"wrap.jl\")`\n\nbut the output feels like it's not installing the latest version of everything, since it installs according to the local `Project.toml` and `Manifest.toml` files. Should I modify these two files (feels tidious) or update the packages later by `] up` later (but it is taking too much time)? Are there other methods?\n\nThe output now (BTW the downloads for both `CUDA` and `CUDA_full` were stuck at 17.5%. The first ctrl+c makes the download continue to run, the second ctrl+c makes it abort):\n```(@v1.7) pkg&gt; activate .\n  Activating environment at `~/.julia/dev/CUDA/res/wrap/Project.toml`\n\n\nStacktrace:\n [1] _require(pkg::Base.PkgId)\n   @ Base ./loading.jl:986\n [2] require(uuidkey::Base.PkgId)\n   @ Base ./loading.jl:910\n [3] require(into::Module, mod::Symbol)\n   @ Base ./loading.jl:897\n [4] include(fname::String)\n   @ Base.MainInclude ./client.jl:451\n [5] top-level scope\n   @ REPL[3]:1\nin expression starting at /home/qyu/.julia/dev/CUDA/res/wrap/wrap.jl:370\n\n(wrap) pkg&gt; instantiate\n   Installed Clang_jll ──────────── v9.0.1+4\n   Installed OrderedCollections ─── v1.3.0\n   Installed CUDNN_CUDA110_jll ──── v8.0.2+0\n   Installed CUDA_full_jll ──────── v11.0.3+0\n   Installed CUTENSOR_CUDA110_jll ─ v1.2.0+0\n   Installed CUDA_jll ───────────── v11.0.2+1\n   Installed Clang ──────────────── v0.12.0\n Downloading artifact: CUDA\n^C┌ Error: curl_multi_remove_handle: 1                    ]  17.5 %\n└ @ Downloads.Curl /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.7/Do  Downloaded artifact: CUDA\n  Downloaded artifact: CUDA\n  Downloaded artifact: CUTENSOR_CUDA110\n  Downloaded artifact: CUDNN_CUDA110\n  Downloaded artifact: Clang\n Downloading artifact: CUDA_full\n^C┌ Error: curl_multi_remove_handle: 1                    ]  17.5 %\n  Downloaded artifact: CUDA_full\n Downloading artifact: CUDA_full\njulia: src/unix/poll.c:117: uv_poll_start: Assertion `!(((handle)-&gt;flags &amp; (UV_HANDLE_CLOSING | UV_HANDLE_CLOSED)) != 0)' failed.\n\nsignal (6): Aborted```","user":"UG1KB4NLD","ts":"1610402295.215100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SXd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"To update the "},{"type":"text","text":"CUdevice_attribute_enum","style":{"code":true}},{"type":"text","text":", I\n1. "},{"type":"text","text":"cd /res/wrap","style":{"code":true}},{"type":"text","text":"\n2. "},{"type":"text","text":"julia","style":{"code":true}},{"type":"text","text":"\n3. "},{"type":"text","text":"] activated .","style":{"code":true}},{"type":"text","text":"\n4. "},{"type":"text","text":"include(\"wrap.jl\")","style":{"code":true}},{"type":"text","text":"\n\nbut the output feels like it's not installing the latest version of everything, since it installs according to the local "},{"type":"text","text":"Project.toml","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"Manifest.toml","style":{"code":true}},{"type":"text","text":" files. Should I modify these two files (feels tidious) or update the packages later by "},{"type":"text","text":"] up","style":{"code":true}},{"type":"text","text":" later (but it is taking too much time)? Are there other methods?\n\nThe output now (BTW the downloads for both "},{"type":"text","text":"CUDA","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"CUDA_full","style":{"code":true}},{"type":"text","text":" were stuck at 17.5%. The first ctrl+c makes the download continue to run, the second ctrl+c makes it abort):\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"(@v1.7) pkg> activate .\n  Activating environment at `~/.julia/dev/CUDA/res/wrap/Project.toml`\n\n\nStacktrace:\n [1] _require(pkg::Base.PkgId)\n   @ Base ./loading.jl:986\n [2] require(uuidkey::Base.PkgId)\n   @ Base ./loading.jl:910\n [3] require(into::Module, mod::Symbol)\n   @ Base ./loading.jl:897\n [4] include(fname::String)\n   @ Base.MainInclude ./client.jl:451\n [5] top-level scope\n   @ REPL[3]:1\nin expression starting at /home/qyu/.julia/dev/CUDA/res/wrap/wrap.jl:370\n\n(wrap) pkg> instantiate\n   Installed Clang_jll ──────────── v9.0.1+4\n   Installed OrderedCollections ─── v1.3.0\n   Installed CUDNN_CUDA110_jll ──── v8.0.2+0\n   Installed CUDA_full_jll ──────── v11.0.3+0\n   Installed CUTENSOR_CUDA110_jll ─ v1.2.0+0\n   Installed CUDA_jll ───────────── v11.0.2+1\n   Installed Clang ──────────────── v0.12.0\n Downloading artifact: CUDA\n^C┌ Error: curl_multi_remove_handle: 1                    ]  17.5 %\n└ @ Downloads.Curl /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.7/Do  Downloaded artifact: CUDA\n  Downloaded artifact: CUDA\n  Downloaded artifact: CUTENSOR_CUDA110\n  Downloaded artifact: CUDNN_CUDA110\n  Downloaded artifact: Clang\n Downloading artifact: CUDA_full\n^C┌ Error: curl_multi_remove_handle: 1                    ]  17.5 %\n  Downloaded artifact: CUDA_full\n Downloading artifact: CUDA_full\njulia: src/unix/poll.c:117: uv_poll_start: Assertion `!(((handle)->flags & (UV_HANDLE_CLOSING | UV_HANDLE_CLOSED)) != 0)' failed.\n\nsignal (6): Aborted"}]}]}]},{"client_msg_id":"84ac4e82-eb38-498c-ab50-a6942e8e31c5","type":"message","text":"Is there a way to get the element out of a dimension-0 CuArray without triggering a scalar indexing warning/error? Eg\n```julia&gt; CUDA.allowscalar(false)\n\njulia&gt; x = CuArray{Float64,0}(undef)\n 0-dimensional CuArray{Float64,0}:\n0.0\n\njulia&gt; x[1] # error\n\njulia&gt; x[] # error```\n","user":"UUMJUCYRK","ts":"1610413181.218700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"R/t=I","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to get the element out of a dimension-0 CuArray without triggering a scalar indexing warning/error? Eg\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> CUDA.allowscalar(false)\n\njulia> x = CuArray{Float64,0}(undef)\n 0-dimensional CuArray{Float64,0}:\n0.0\n\njulia> x[1] # error\n\njulia> x[] # error"}]},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"3ab42ffd-4275-4298-ac9e-987d906760ee","type":"message","text":"Alternatively, some way to sum over specified dims of an array and get back a scalar if they happen to be _all_ of the dims? I was trying,\n```julia&gt; x = cu(rand(2,2));\n\njulia&gt; dropdims(sum(x, dims=(1,2)), dims=(1,2))\n 0-dimensional CuArray{Float32,0}:\n2.363763```\nObviously you can do `sum(x)` but here I need to specify the dims, just that sometimes its all of them and in those cases I want the scalar","user":"UUMJUCYRK","ts":"1610413442.220300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"urx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Alternatively, some way to sum over specified dims of an array and get back a scalar if they happen to be "},{"type":"text","text":"all ","style":{"italic":true}},{"type":"text","text":"of the dims? I was trying,\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> x = cu(rand(2,2));\n\njulia> dropdims(sum(x, dims=(1,2)), dims=(1,2))\n 0-dimensional CuArray{Float32,0}:\n2.363763"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Obviously you can do "},{"type":"text","text":"sum(x)","style":{"code":true}},{"type":"text","text":" but here I need to specify the dims, just that sometimes its all of them and in those cases I want the scalar"}]}]}]},{"client_msg_id":"33edf21d-639a-4dd4-a417-ced4ea2fbfed","type":"message","text":"Ah.. `CUDA.@allowscalar`","user":"UUMJUCYRK","ts":"1610416650.220600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ErB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah.. "},{"type":"text","text":"CUDA.@allowscalar","style":{"code":true}}]}]}]},{"client_msg_id":"7dcee73d-6e53-4b1e-9b7e-f865b855ba01","type":"message","text":"Is there a quick way to disable the GPU, so I can test my code without it? Perhaps through an environment variable?","user":"U01EK81V5GF","ts":"1610454806.225000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Uyw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a quick way to disable the GPU, so I can test my code without it? Perhaps through an environment variable?"}]}]}]},{"client_msg_id":"bd2837af-2faf-4aad-8565-73939879e754","type":"message","text":"`CUDA_VISIBLE_DEVICES=-1`","user":"U68A3ASP9","ts":"1610455600.225300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oAk6p","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CUDA_VISIBLE_DEVICES=-1","style":{"code":true}}]}]}]}]}