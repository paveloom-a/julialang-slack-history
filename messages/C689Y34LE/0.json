{"cursor": 0, "messages": [{"client_msg_id":"e88fbf86-1f69-4fe9-a15a-1dd9cbc530db","type":"message","text":"Can we get GPU CI configured for ForwardDiff?","user":"UEN48T0BT","ts":"1607967804.492700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wJ+Fl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Can we get GPU CI configured for ForwardDiff?"}]}]}],"thread_ts":"1607967804.492700","reply_count":7,"reply_users_count":3,"latest_reply":"1608042439.006900","reply_users":["U68A3ASP9","UEN48T0BT","U680THK2S"],"subscribed":false},{"client_msg_id":"49ccd126-3237-4e13-b1b9-e4f463016ebd","type":"message","text":"<@U68A3ASP9>, I've been very slowly trying to understand how Nvidia Optix could work with Julia/CUDA.jl. So far I've understood that Optix has a C API to where one sends \"programs\" (ray generation, intersection) that are no more than PTX code (literally a string) compiled from functions with specific signatures. The functions have to have specific name prefixes, like `__raygen__` and receive no parameters. The input of such functions has to be done through global variables, e.g., `.const .align 8 .b8 params[24];`, and through calls to a device API defined in asm in C headers, e.g., `call (%r1), _optix_get_launch_index_x, ();`.   <@U68A3ASP9> can you give some hint whether any of this 3 means may pose a big problem to current CUDA.jl workings?","user":"U6CCK2SCV","ts":"1607973616.001100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xQ1m","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":", I've been very slowly trying to understand how Nvidia Optix could work with Julia/CUDA.jl. So far I've understood that Optix has a C API to where one sends \"programs\" (ray generation, intersection) that are no more than PTX code (literally a string) compiled from functions with specific signatures. The functions have to have specific name prefixes, like "},{"type":"text","text":"__raygen__","style":{"code":true}},{"type":"text","text":" and receive no parameters. The input of such functions has to be done through global variables, e.g., "},{"type":"text","text":".const .align 8 .b8 params[24];","style":{"code":true}},{"type":"text","text":", and through calls to a device API defined in asm in C headers, e.g., "},{"type":"text","text":"call (%r1), _optix_get_launch_index_x, ();","style":{"code":true}},{"type":"text","text":".   "},{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":" can you give some hint whether any of this 3 means may pose a big problem to current CUDA.jl workings?"}]}]}],"thread_ts":"1607973616.001100","reply_count":5,"reply_users_count":2,"latest_reply":"1607977510.006200","reply_users":["U6CCK2SCV","U68A3ASP9"],"subscribed":false},{"client_msg_id":"a04f7484-ae4e-4d7b-96df-3508a7bd15b9","type":"message","text":"Hey, what’s the status of sparse operations using `CUDA.jl` ? In particular, is there any way to efficiently evaluate a kronecker product of a dense CuArray and sparse CuArray?","user":"UPM0H43C7","ts":"1607974672.003300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"W2u","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey, what’s the status of sparse operations using "},{"type":"text","text":"CUDA.jl","style":{"code":true}},{"type":"text","text":" ? In particular, is there any way to efficiently evaluate a kronecker product of a dense CuArray and sparse CuArray?"}]}]}],"thread_ts":"1607974672.003300","reply_count":34,"reply_users_count":6,"latest_reply":"1608088028.019800","reply_users":["U68A3ASP9","UPM0H43C7","UDGT4PM41","U6A0PD8CR","U67BJLYCS","UH9KWTTD3"],"subscribed":false},{"client_msg_id":"f1adf01f-e383-484d-bb96-d722801552ad","type":"message","text":"I would like to run a large number of iterations (order 10^13 or more) of a relatively small sized simulation, for every iteration I need a bunch of random numbers which can't be pre computed and allocated on the cpu due to size. I was wondering if there is a way to use CUDA, since I believe kernel functions are pretty limited in terms of what can be done inside them.","user":"UU2E6M753","ts":"1608058751.012100","team":"T68168MUP","edited":{"user":"UU2E6M753","ts":"1608059241.000000"},"blocks":[{"type":"rich_text","block_id":"q9O5U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I would like to run a large number of iterations (order 10^13 or more) of a relatively small sized simulation, for every iteration I need a bunch of random numbers which can't be pre computed and allocated on the cpu due to size. I was wondering if there is a way to use CUDA, since I believe kernel functions are pretty limited in terms of what can be done inside them."}]}]}],"thread_ts":"1608058751.012100","reply_count":8,"reply_users_count":2,"latest_reply":"1608192672.037800","reply_users":["UU2E6M753","U68A3ASP9"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"What would it take to be able to write kernels on sparse matrices?","user":"UDGT4PM41","ts":"1608059346.012300","thread_ts":"1607974672.003300","root":{"client_msg_id":"a04f7484-ae4e-4d7b-96df-3508a7bd15b9","type":"message","text":"Hey, what’s the status of sparse operations using `CUDA.jl` ? In particular, is there any way to efficiently evaluate a kronecker product of a dense CuArray and sparse CuArray?","user":"UPM0H43C7","ts":"1607974672.003300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"W2u","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey, what’s the status of sparse operations using "},{"type":"text","text":"CUDA.jl","style":{"code":true}},{"type":"text","text":" ? In particular, is there any way to efficiently evaluate a kronecker product of a dense CuArray and sparse CuArray?"}]}]}],"thread_ts":"1607974672.003300","reply_count":34,"reply_users_count":6,"latest_reply":"1608088028.019800","reply_users":["U68A3ASP9","UPM0H43C7","UDGT4PM41","U6A0PD8CR","U67BJLYCS","UH9KWTTD3"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"OPd=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What would it take to be able to write kernels on sparse matrices?"}]}]}],"client_msg_id":"ebd29528-6442-486b-a11a-c68d038bfab8"},{"client_msg_id":"a3fdc5df-a77a-40b9-be49-e87d8bc8291f","type":"message","text":"I have an AMD GPU on a windows device with WSL2. Has anyone tried to run AMDGPU.jl on wsl2? If so, what are the necessary components to be installed?\n\nAre there any other ways to use an AMD GPU without WSL2 on windows? Is there a device agnostic binding available, that trades off a bit on performance, but can still be used?","user":"U01GL3BUV0W","ts":"1608095693.024700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kd5v","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have an AMD GPU on a windows device with WSL2. Has anyone tried to run AMDGPU.jl on wsl2? If so, what are the necessary components to be installed?\n\nAre there any other ways to use an AMD GPU without WSL2 on windows? Is there a device agnostic binding available, that trades off a bit on performance, but can still be used?"}]}]}],"thread_ts":"1608095693.024700","reply_count":3,"reply_users_count":1,"latest_reply":"1608157682.035900","reply_users":["U6A0PD8CR"],"subscribed":false},{"client_msg_id":"438e9cd3-3080-4776-8810-2992d35ce739","type":"message","text":"As far as I understand it AMDGPU.jl strictly requires ROCm (i.e., <https://github.com/RadeonOpenCompute/ROCm>) and as far as I know ROCm still only exists for Linux","user":"UGU761DU2","ts":"1608104331.026500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"k+JBT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"As far as I understand it AMDGPU.jl strictly requires ROCm (i.e., "},{"type":"link","url":"https://github.com/RadeonOpenCompute/ROCm"},{"type":"text","text":") and as far as I know ROCm still only exists for Linux"}]}]}]},{"client_msg_id":"652b9b94-1f63-436f-9338-0d79379b52ec","type":"message","text":"And it appears to be somewhat unclear whether WSL2 is Linux enough for ROCm, <https://github.com/RadeonOpenCompute/ROCm/issues/794>","user":"UGU761DU2","ts":"1608104464.027900","team":"T68168MUP","edited":{"user":"UGU761DU2","ts":"1608104537.000000"},"blocks":[{"type":"rich_text","block_id":"R5Ik","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And it appears to be somewhat unclear whether WSL2 is Linux enough for ROCm, "},{"type":"link","url":"https://github.com/RadeonOpenCompute/ROCm/issues/794"}]}]}],"thread_ts":"1608104464.027900","reply_count":10,"reply_users_count":3,"latest_reply":"1608213834.044900","reply_users":["U01GL3BUV0W","U66SGEWAC","U6A0PD8CR"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"What about other packages? OpenCL.jl has really poor docs, Vulkan.jl has none.","user":"U01GL3BUV0W","ts":"1608111087.028100","thread_ts":"1608104464.027900","root":{"client_msg_id":"652b9b94-1f63-436f-9338-0d79379b52ec","type":"message","text":"And it appears to be somewhat unclear whether WSL2 is Linux enough for ROCm, <https://github.com/RadeonOpenCompute/ROCm/issues/794>","user":"UGU761DU2","ts":"1608104464.027900","team":"T68168MUP","edited":{"user":"UGU761DU2","ts":"1608104537.000000"},"blocks":[{"type":"rich_text","block_id":"R5Ik","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And it appears to be somewhat unclear whether WSL2 is Linux enough for ROCm, "},{"type":"link","url":"https://github.com/RadeonOpenCompute/ROCm/issues/794"}]}]}],"thread_ts":"1608104464.027900","reply_count":10,"reply_users_count":3,"latest_reply":"1608213834.044900","reply_users":["U01GL3BUV0W","U66SGEWAC","U6A0PD8CR"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"G9Iyt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What about other packages? OpenCL.jl has really poor docs, Vulkan.jl has none."}]}]}],"client_msg_id":"bfdb725b-5105-4131-b5c3-12bf5311408f"},{"client_msg_id":"abf5d974-f6f5-4408-a4ae-29286296aff0","type":"message","text":"I think no GPU library runs on WSL2 out of the box right now - the very first thing that seems to be slowly working is CUDA on WSL2, but last time I checked it was in beta:\n<https://ubuntu.com/blog/getting-started-with-cuda-on-ubuntu-on-wsl-2>\nYou need to be on insider built etc... Pretty sure nothing like that exists for amd yet","user":"U66SGEWAC","ts":"1608115292.030600","team":"T68168MUP","attachments":[{"service_name":"Ubuntu","title":"Getting started with CUDA on Ubuntu on WSL 2 | Ubuntu","title_link":"https://ubuntu.com/blog/getting-started-with-cuda-on-ubuntu-on-wsl-2","text":"At Build 2020 Microsoft announced support for GPU compute on Windows Subsystem for Linux 2. Ubuntu is the leading Linux distribution for WSL and a sponsor of WSLConf. Canonical, the publisher of Ubuntu, provides enterprise support for Ubuntu on WSL through Ubuntu Advantage. This guide will walk early adopters through the steps on turning […]","fallback":"Ubuntu: Getting started with CUDA on Ubuntu on WSL 2 | Ubuntu","image_url":"https://ubuntu.com/wp-content/uploads/7332/191122_NvidiaStopsCUDAmacOS.jpg","from_url":"https://ubuntu.com/blog/getting-started-with-cuda-on-ubuntu-on-wsl-2","image_width":500,"image_height":250,"image_bytes":21952,"service_icon":"https://assets.ubuntu.com/v1/17b68252-apple-touch-icon-180x180-precomposed-ubuntu.png","id":1,"original_url":"https://ubuntu.com/blog/getting-started-with-cuda-on-ubuntu-on-wsl-2"}],"blocks":[{"type":"rich_text","block_id":"ZQe6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think no GPU library runs on WSL2 out of the box right now - the very first thing that seems to be slowly working is CUDA on WSL2, but last time I checked it was in beta:\n"},{"type":"link","url":"https://ubuntu.com/blog/getting-started-with-cuda-on-ubuntu-on-wsl-2"},{"type":"text","text":"\nYou need to be on insider built etc... Pretty sure nothing like that exists for amd yet"}]}]}],"reactions":[{"name":"+1","users":["U01GL3BUV0W"],"count":1}]},{"type":"message","subtype":"thread_broadcast","text":"So I actually put some time and looked at OpenCL, and it works, but you have to write the kernel yourself, unlike in CUDA where you can wrote simple Julia code.","user":"U01GL3BUV0W","ts":"1608116323.030800","thread_ts":"1608104464.027900","root":{"client_msg_id":"652b9b94-1f63-436f-9338-0d79379b52ec","type":"message","text":"And it appears to be somewhat unclear whether WSL2 is Linux enough for ROCm, <https://github.com/RadeonOpenCompute/ROCm/issues/794>","user":"UGU761DU2","ts":"1608104464.027900","team":"T68168MUP","edited":{"user":"UGU761DU2","ts":"1608104537.000000"},"blocks":[{"type":"rich_text","block_id":"R5Ik","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And it appears to be somewhat unclear whether WSL2 is Linux enough for ROCm, "},{"type":"link","url":"https://github.com/RadeonOpenCompute/ROCm/issues/794"}]}]}],"thread_ts":"1608104464.027900","reply_count":10,"reply_users_count":3,"latest_reply":"1608213834.044900","reply_users":["U01GL3BUV0W","U66SGEWAC","U6A0PD8CR"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"vIrh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So I actually put some time and looked at OpenCL, and it works, but you have to write the kernel yourself, unlike in CUDA where you can wrote simple Julia code."}]}]}],"client_msg_id":"7ab5e318-c0eb-451f-9dfe-85480068f071"},{"client_msg_id":"ff40dff6-05b1-443c-a664-a357587e907a","type":"message","text":"Isn't there a \"GPUArray\" package that allows you to run things on the CPU to check if your code is affected by allowscalar etc?","user":"U6CJRSR63","ts":"1608129684.032200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PWmQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Isn't there a \"GPUArray\" package that allows you to run things on the CPU to check if your code is affected by allowscalar etc?"}]}]}],"thread_ts":"1608129684.032200","reply_count":6,"reply_users_count":3,"latest_reply":"1608157277.035300","reply_users":["UEN48T0BT","U6CJRSR63","U6A0PD8CR"],"subscribed":false},{"client_msg_id":"f5eb3679-86e6-4650-af35-884b6570fe04","type":"message","text":"I thought there was, but can't remember which one","user":"U6CJRSR63","ts":"1608129695.032500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dXy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I thought there was, but can't remember which one"}]}]}]},{"client_msg_id":"5d5a3138-f560-46c6-bcd2-0016411d9d63","type":"message","text":"GPUArrays.jl has an JLArray reference implementation that executes on the CPU. but it generally isn't fleshed out enough to be usable with realistic applications; it's currently mostly intended to run the GPUArrays test suite on CI.","user":"U68A3ASP9","ts":"1608130757.033800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"x8Bc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"GPUArrays.jl has an JLArray reference implementation that executes on the CPU. but it generally isn't fleshed out enough to be usable with realistic applications; it's currently mostly intended to run the GPUArrays test suite on CI."}]}]}],"reactions":[{"name":"+1","users":["U7THT3TM3"],"count":1}]},{"client_msg_id":"af14252e-8be5-42d4-a242-63d6256d5078","type":"message","text":"Okay, thanks","user":"U6CJRSR63","ts":"1608132080.034000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vnu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Okay, thanks"}]}]}]},{"client_msg_id":"dc388ba5-c663-4722-8370-91194e88a4e2","type":"message","text":"just got tripped up by this:\n```julia&gt; C = CUDA.rand(1, 1, 1, 1);\n\njulia&gt; view(C, :, :, :, 1)\n1×1×1 CuArray{Float32,3}:\n[:, :, 1] =\n 0.3129622\n\njulia&gt; selectdim(C, 4, 1)\n1×1×1 view(::CuArray{Float32,4}, :, :, :, 1) with eltype Float32:\n[:, :, 1] =\n 0.3129622```","user":"U6BJ9E351","ts":"1608209932.038300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PC/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"just got tripped up by this:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> C = CUDA.rand(1, 1, 1, 1);\n\njulia> view(C, :, :, :, 1)\n1×1×1 CuArray{Float32,3}:\n[:, :, 1] =\n 0.3129622\n\njulia> selectdim(C, 4, 1)\n1×1×1 view(::CuArray{Float32,4}, :, :, :, 1) with eltype Float32:\n[:, :, 1] =\n 0.3129622"}]}]}]},{"client_msg_id":"0d52e0db-5ce1-48ed-b05b-fab6dbe23135","type":"message","text":"for my use cases, it is much better to get the vanilla `CuArray` , so I agree with the idea that a contiguous view (not sure if the term is clear, I mean one where the data is stored contiguously) is just a `CuArray`, but I think it's odd that the two things differ","user":"U6BJ9E351","ts":"1608210031.039800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iknB0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"for my use cases, it is much better to get the vanilla "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":" , so I agree with the idea that a contiguous view (not sure if the term is clear, I mean one where the data is stored contiguously) is just a "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":", but I think it's odd that the two things differ"}]}]}]},{"client_msg_id":"adcdd7e5-e06a-46ec-b707-d76948927687","type":"message","text":"(related, if I get something that I know is stored contiguously, can I say \"treat this lump of memory as a `CuArray` with this size?)","user":"U6BJ9E351","ts":"1608210163.040600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+Zj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"(related, if I get something that I know is stored contiguously, can I say \"treat this lump of memory as a "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":" with this size?)"}]}]}],"thread_ts":"1608210163.040600","reply_count":7,"reply_users_count":2,"latest_reply":"1608211559.044100","reply_users":["U68A3ASP9","U6BJ9E351"],"subscribed":false},{"client_msg_id":"71905dc4-07a3-4b52-b3f8-938f45689d1a","type":"message","text":"it's an optimization. I agree it isn't particularly nice, but when I started using SubArray for contiguous views too (we have Base.FastContiguousSubArray to determine if those are contiguous) package load times regressed significantly","user":"U68A3ASP9","ts":"1608210288.041500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"R5aU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"it's an optimization. I agree it isn't particularly nice, but when I started using SubArray for contiguous views too (we have Base.FastContiguousSubArray to determine if those are contiguous) package load times regressed significantly"}]}]}]},{"client_msg_id":"1250eea6-9f91-4e9d-91a6-4fd9f3126bc3","type":"message","text":"because of complicated unions everywhere","user":"U68A3ASP9","ts":"1608210301.041700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=lp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"because of complicated unions everywhere"}]}]}]},{"client_msg_id":"8e83bcdd-9f0a-4802-97e0-29a62c302299","type":"message","text":"ok, I'll just go with `view` then. The problem is (I think) that I was also reshaping the thing afterwards, but `view` + `reshape` works fine.","user":"U6BJ9E351","ts":"1608210729.042900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jFW3Y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ok, I'll just go with "},{"type":"text","text":"view","style":{"code":true}},{"type":"text","text":" then. The problem is (I think) that I was also reshaping the thing afterwards, but "},{"type":"text","text":"view","style":{"code":true}},{"type":"text","text":" + "},{"type":"text","text":"reshape","style":{"code":true}},{"type":"text","text":" works fine."}]}]}]},{"client_msg_id":"0c59d1d9-d2bb-4b5f-b276-275c8da52679","type":"message","text":"Is there a general way to get the suitable version of function `f` that runs on array `c`? I'd rather avoid to special case CUDA. Is the \"general way\" something like `Broadcast.broadcasted(f, c).f` ? I don't like accessing a private field, but couldn't figure out another way","user":"U6BJ9E351","ts":"1608222377.046900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jQMQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a general way to get the suitable version of function "},{"type":"text","text":"f","style":{"code":true}},{"type":"text","text":" that runs on array "},{"type":"text","text":"c","style":{"code":true}},{"type":"text","text":"? I'd rather avoid to special case CUDA. Is the \"general way\" something like "},{"type":"text","text":"Broadcast.broadcasted(f, c).f","style":{"code":true}},{"type":"text","text":" ? I don't like accessing a private field, but couldn't figure out another way"}]}]}],"thread_ts":"1608222377.046900","reply_count":3,"reply_users_count":2,"latest_reply":"1608307125.053900","reply_users":["U68A3ASP9","U6BJ9E351"],"subscribed":false},{"client_msg_id":"a84adfba-7175-4a4d-9594-5956272c583e","type":"message","text":"Thanks to whoever mentioned KernelAbstractions! Not sure if the devs hang out in here, but I was wondering if there's a good way to handle multiple async events. I want to fire off a bunch of kernels (no data dependencies) then do a `collect` on all of them","user":"U9ZEWU9T9","ts":"1608263669.048400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Qc6P6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks to whoever mentioned KernelAbstractions! Not sure if the devs hang out in here, but I was wondering if there's a good way to handle multiple async events. I want to fire off a bunch of kernels (no data dependencies) then do a "},{"type":"text","text":"collect","style":{"code":true}},{"type":"text","text":" on all of them"}]}]}]},{"client_msg_id":"fd97e496-cb46-49ca-a636-76250d5ef354","type":"message","text":"Multievent","user":"U67BJLYCS","ts":"1608266785.048800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xXnB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Multievent"}]}]}]},{"client_msg_id":"bb68d037-aebb-4fba-b91f-999cf0beb733","type":"message","text":"Allows you to merge multiple events and merge them into one","user":"U67BJLYCS","ts":"1608267056.049500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/H5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Allows you to merge multiple events and merge them into one"}]}]}]},{"client_msg_id":"3a1d0356-65af-414e-a3d9-5b783440a430","type":"message","text":"```    events = []\n    for layer in 1:layers\n        push!(events, kernel_func!(cuda_grids, layer))\n    end\n    \n    # \"collect\" tasks\n    wait(MultiEvent(Tuple(events)))```","user":"U9ZEWU9T9","ts":"1608273459.050000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"f4P","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"    events = []\n    for layer in 1:layers\n        push!(events, kernel_func!(cuda_grids, layer))\n    end\n    \n    # \"collect\" tasks\n    wait(MultiEvent(Tuple(events)))"}]}]}]},{"client_msg_id":"59f5dc86-25a8-4ad2-85f7-160ff7071837","type":"message","text":"Not sure if that's the most ergonomic way to do things, but it seems to work","user":"U9ZEWU9T9","ts":"1608273487.050800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YkQo3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not sure if that's the most ergonomic way to do things, but it seems to work"}]}]}]},{"client_msg_id":"7fde09c2-b281-4a06-9550-23572059cb76","type":"message","text":"Is there a convenient way to do `map(f, axes(a, 2))`, but which produces a GPUArray if `a` is a `GPUArray`?","user":"UM30MT6RF","ts":"1608301804.052200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"XuQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a convenient way to do "},{"type":"text","text":"map(f, axes(a, 2))","style":{"code":true}},{"type":"text","text":", but which produces a GPUArray if "},{"type":"text","text":"a","style":{"code":true}},{"type":"text","text":" is a "},{"type":"text","text":"GPUArray","style":{"code":true}},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"b3183025-834d-4961-bd66-71c96563b56b","type":"message","text":"Perhaps `gpu_call` is what I need?","user":"UM30MT6RF","ts":"1608302053.052700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yuRB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Perhaps "},{"type":"text","text":"gpu_call","style":{"code":true}},{"type":"text","text":" is what I need?"}]}]}]},{"client_msg_id":"35a3e6d8-59c8-4e4e-a805-fd72198b7046","type":"message","text":"Ah, preallocating `similar` and then doing `map!` should work for my usecase, but it would be nice if the resulting eltype wouldn't have to be specified manually","user":"UM30MT6RF","ts":"1608302605.053800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Xr5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah, preallocating "},{"type":"text","text":"similar","style":{"code":true}},{"type":"text","text":" and then doing "},{"type":"text","text":"map!","style":{"code":true}},{"type":"text","text":" should work for my usecase, but it would be nice if the resulting eltype wouldn't have to be specified manually"}]}]}]},{"client_msg_id":"9dc8981c-f040-4c9a-a7f6-efcb8db83184","type":"message","text":"`something.(f.(axes(a, 2)), @view a[1,:])`? :grimacing:","user":"U6740K1SP","ts":"1608310738.054700","team":"T68168MUP","edited":{"user":"U6740K1SP","ts":"1608310825.000000"},"blocks":[{"type":"rich_text","block_id":"K6z7V","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"something.(f.(axes(a, 2)), @view a[1,:])","style":{"code":true}},{"type":"text","text":"? "},{"type":"emoji","name":"grimacing"}]}]}],"thread_ts":"1608310738.054700","reply_count":2,"reply_users_count":2,"latest_reply":"1608311702.055200","reply_users":["UM30MT6RF","U6740K1SP"],"subscribed":false},{"client_msg_id":"a749bba5-0972-4daf-b73d-b62d47046931","type":"message","text":"Is there a way to pass structs wrapping a `CuArray` to a `cufunction`? I want to do something like this:\n```julia&gt; cf(x) = (x.x[1] = 1f0; nothing)\ncf (generic function with 1 method)\n\njulia&gt; struct B{T}; x::T end\n\njulia&gt; x = B(cu([0f0, 0f0]))\nB{CuArray{Float32,1}}(Float32[0.0, 0.0])\n\njulia&gt; @cuda cf(x)\nERROR: GPU compilation of kernel cf(B{CuArray{Float32,1}}) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 2 to your kernel function is of type B{CuArray{Float32,1}}, which is not isbits:\n  .x is of type CuArray{Float32,1} which is not isbits.\n    .ctx is of type CuContext which is not isbits.\n\n\nStacktrace:\n [1] check_invocation(::GPUCompiler.CompilerJob, ::LLVM.Function) at /local/scratch/ssd/sschaub/.julia/packages/GPUCompiler/uTpNx/src/validation.jl:68\n[...]```","user":"UM30MT6RF","ts":"1608328657.056700","team":"T68168MUP","edited":{"user":"UM30MT6RF","ts":"1608328676.000000"},"blocks":[{"type":"rich_text","block_id":"qtTA1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to pass structs wrapping a "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":" to a "},{"type":"text","text":"cufunction","style":{"code":true}},{"type":"text","text":"? I want to do something like this:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> cf(x) = (x.x[1] = 1f0; nothing)\ncf (generic function with 1 method)\n\njulia> struct B{T}; x::T end\n\njulia> x = B(cu([0f0, 0f0]))\nB{CuArray{Float32,1}}(Float32[0.0, 0.0])\n\njulia> @cuda cf(x)\nERROR: GPU compilation of kernel cf(B{CuArray{Float32,1}}) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 2 to your kernel function is of type B{CuArray{Float32,1}}, which is not isbits:\n  .x is of type CuArray{Float32,1} which is not isbits.\n    .ctx is of type CuContext which is not isbits.\n\n\nStacktrace:\n [1] check_invocation(::GPUCompiler.CompilerJob, ::LLVM.Function) at /local/scratch/ssd/sschaub/.julia/packages/GPUCompiler/uTpNx/src/validation.jl:68\n[...]"}]}]}]},{"client_msg_id":"f4fc8eed-c94b-4c5d-ba90-bb84a2e9720e","type":"message","text":"Adapt.jl","user":"U67BJLYCS","ts":"1608340460.057000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rRk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Adapt.jl"}]}]}]},{"client_msg_id":"60c03838-bf1f-4ff2-96d2-1a4b4d30e453","type":"message","text":"For all your auto-conversion needs","user":"U67BJLYCS","ts":"1608340470.057400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ojbw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"For all your auto-conversion needs"}]}]}]},{"client_msg_id":"1131f4d4-2b64-4480-aeaf-37e808264e8b","type":"message","text":"Nice, thanks for the tip! I don't quite understand how yet, but this does the trick:\n```Adapt.adapt_structure(T, x::B) = B(adapt(T, x.x))```","user":"UM30MT6RF","ts":"1608341496.058600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"g8fd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Nice, thanks for the tip! I don't quite understand how yet, but this does the trick:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"Adapt.adapt_structure(T, x::B) = B(adapt(T, x.x))"}]}]}]},{"client_msg_id":"c998b7d7-9cff-45d5-afed-6ce0717baf1e","type":"message","text":"Is this a missing method for `mul!(::StridedCuVector, ...)`?\n```julia&gt; mul!(view(cu(rand(Float32, 2, 2)), 1, :), cu(rand(Float32, 2, 2)), cu(rand(Float32, 2)))\nERROR: ArgumentError: cannot take the CPU address of a CuArray{Float32,2}\nStacktrace:\n [1] unsafe_convert(::Type{Ptr{Float32}}, ::CuArray{Float32,2}) at /local/scratch/ssd/sschaub/.julia/packages/CUDA/YeS8q/src/array.jl:211\n [2] gemv!(::Char, ::Float32, ::CuArray{Float32,2}, ::CuArray{Float32,1}, ::Float32, ::SubArray{Float32,1,CuArray{Float32,2},Tuple{Int64,Base.Slice{Base.OneTo{Int64}}},true}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/blas.jl:626\n [3] gemv!(::SubArray{Float32,1,CuArray{Float32,2},Tuple{Int64,Base.Slice{Base.OneTo{Int64}}},true}, ::Char, ::CuArray{Float32,2}, ::CuArray{Float32,1}, ::Bool, ::Bool) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl:470\n [4] mul! at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl:66 [inlined]\n [5] mul!(::SubArray{Float32,1,CuArray{Float32,2},Tuple{Int64,Base.Slice{Base.OneTo{Int64}}},true}, ::CuArray{Float32,2}, ::CuArray{Float32,1}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl:208\n [6] top-level scope at REPL[24]:1```","user":"UM30MT6RF","ts":"1608398855.059700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zQURv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is this a missing method for "},{"type":"text","text":"mul!(::StridedCuVector, ...)","style":{"code":true}},{"type":"text","text":"?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> mul!(view(cu(rand(Float32, 2, 2)), 1, :), cu(rand(Float32, 2, 2)), cu(rand(Float32, 2)))\nERROR: ArgumentError: cannot take the CPU address of a CuArray{Float32,2}\nStacktrace:\n [1] unsafe_convert(::Type{Ptr{Float32}}, ::CuArray{Float32,2}) at /local/scratch/ssd/sschaub/.julia/packages/CUDA/YeS8q/src/array.jl:211\n [2] gemv!(::Char, ::Float32, ::CuArray{Float32,2}, ::CuArray{Float32,1}, ::Float32, ::SubArray{Float32,1,CuArray{Float32,2},Tuple{Int64,Base.Slice{Base.OneTo{Int64}}},true}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/blas.jl:626\n [3] gemv!(::SubArray{Float32,1,CuArray{Float32,2},Tuple{Int64,Base.Slice{Base.OneTo{Int64}}},true}, ::Char, ::CuArray{Float32,2}, ::CuArray{Float32,1}, ::Bool, ::Bool) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl:470\n [4] mul! at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl:66 [inlined]\n [5] mul!(::SubArray{Float32,1,CuArray{Float32,2},Tuple{Int64,Base.Slice{Base.OneTo{Int64}}},true}, ::CuArray{Float32,2}, ::CuArray{Float32,1}) at /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl:208\n [6] top-level scope at REPL[24]:1"}]}]}],"thread_ts":"1608398855.059700","reply_count":8,"reply_users_count":3,"latest_reply":"1608406026.061800","reply_users":["U6A0PD8CR","U68A3ASP9","UM30MT6RF"],"subscribed":false},{"client_msg_id":"16163de0-40e9-4f23-ba89-cb872a6c20e7","type":"message","text":"I posted a somewhat-convoluted GPU question on Discourse re: finding the indices of the two largest elements in an array <https://discourse.julialang.org/t/finding-index-and-value-of-largest-two-elements-in-a-cuarray/>","user":"UB2QRMQPN","ts":"1608422773.063200","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"Finding index and value of largest two elements in a CuArray","title_link":"https://discourse.julialang.org/t/finding-index-and-value-of-largest-two-elements-in-a-cuarray/","text":"I need to find the indices and values of the two largest elements in a 2D array. I whipped up a foldl-based solution for the CPU that’s much faster than Base.findmax and Base.partialsortperm, but it runs like molasses on the GPU compared to CUDA.findmax. function findmax2(prev, curr) i, v = curr v &gt; prev.first.v &amp;&amp; return (first=(; i, v), second=prev.first) v &gt; prev.second.v &amp;&amp; return (first=prev.first, second=(; i, v)) return prev end julia&gt; dA = CUDA.rand(512, 512); cA = Arra...","fallback":"JuliaLang: Finding index and value of largest two elements in a CuArray","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","ts":1608416181,"from_url":"https://discourse.julialang.org/t/finding-index-and-value-of-largest-two-elements-in-a-cuarray/","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/finding-index-and-value-of-largest-two-elements-in-a-cuarray/"}],"blocks":[{"type":"rich_text","block_id":"w6Y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I posted a somewhat-convoluted GPU question on Discourse re: finding the indices of the two largest elements in an array "},{"type":"link","url":"https://discourse.julialang.org/t/finding-index-and-value-of-largest-two-elements-in-a-cuarray/"}]}]}]},{"client_msg_id":"84913ea5-5fd2-4b57-927b-556a7d317dcc","type":"message","text":"You can do it on a CPU with a straightforward `foldl`, but the same approach on the GPU leads to slooooow scalar indexing (although I'm not sure where it happens - there aren't any obvious `getindex` calls)","user":"UB2QRMQPN","ts":"1608422918.064800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iVGKa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You can do it on a CPU with a straightforward "},{"type":"text","text":"foldl","style":{"code":true}},{"type":"text","text":", but the same approach on the GPU leads to slooooow scalar indexing (although I'm not sure where it happens - there aren't any obvious "},{"type":"text","text":"getindex","style":{"code":true}},{"type":"text","text":" calls)"}]}]}],"thread_ts":"1608422918.064800","reply_count":7,"reply_users_count":4,"latest_reply":"1608524190.068300","reply_users":["UCZ7VBGUD","UB2QRMQPN","UDGT4PM41","UC7AF7NSU"],"subscribed":false},{"client_msg_id":"03b079cb-fa53-4610-b0c9-50d2bd2a4ab1","type":"message","text":"<https://twitter.com/dougallj/status/1339934291929694210?s=19|https://twitter.com/dougallj/status/1339934291929694210?s=19>","user":"UDGT4PM41","ts":"1608462833.067000","team":"T68168MUP","attachments":[{"fallback":"<https://twitter.com/dougallj|@dougallj>: I started reversing Apple AMX, an undocumented 64-bit ARM instruction set extension for a matrix coprocessor, used by the Accelerate framework on the M1 (and A13+). My (incomplete) IDA/Hex-Rays plugin and notes so far: <https://gist.github.com/dougallj/7a75a3be1ec69ca550e7c36dc75e0d6f>","ts":1608300204,"author_name":"Dougall","author_link":"https://twitter.com/dougallj/status/1339934291929694210","author_icon":"https://pbs.twimg.com/profile_images/1272453689253351424/JEhZXeHE_normal.jpg","author_subname":"@dougallj","text":"I started reversing Apple AMX, an undocumented 64-bit ARM instruction set extension for a matrix coprocessor, used by the Accelerate framework on the M1 (and A13+). My (incomplete) IDA/Hex-Rays plugin and notes so far: <https://gist.github.com/dougallj/7a75a3be1ec69ca550e7c36dc75e0d6f>","service_name":"twitter","service_url":"https://twitter.com/","from_url":"https://twitter.com/dougallj/status/1339934291929694210?s=19","id":1,"original_url":"https://twitter.com/dougallj/status/1339934291929694210?s=19","footer":"Twitter","footer_icon":"https://a.slack-edge.com/80588/img/services/twitter_pixel_snapped_32.png"}],"blocks":[{"type":"rich_text","block_id":"9cL","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://twitter.com/dougallj/status/1339934291929694210?s=19","text":"https://twitter.com/dougallj/status/1339934291929694210?s=19"}]}]}],"reactions":[{"name":"apple","users":["UGU761DU2"],"count":1}]},{"client_msg_id":"bfbf37f4-e2d1-4f41-8da0-7a8db8eac011","type":"message","text":"Off topic, but I noticed one of the mesa devs is playing around with implementing an OpenCL userspace in Rust: <https://gitlab.freedesktop.org/karolherbst/mesa/-/tree/rusticl/src/gallium/frontends/rusticl>","user":"UMY1LV01G","ts":"1608487465.067900","team":"T68168MUP","edited":{"user":"UMY1LV01G","ts":"1608487504.000000"},"attachments":[{"service_name":"GitLab","title":"src/gallium/frontends/rusticl · rusticl · Karol Herbst / mesa","title_link":"https://gitlab.freedesktop.org/karolherbst/mesa/-/tree/rusticl/src/gallium/frontends/rusticl","text":"Mesa 3D graphics library","fallback":"GitLab: src/gallium/frontends/rusticl · rusticl · Karol Herbst / mesa","thumb_url":"https://gitlab.freedesktop.org/uploads/-/system/project/avatar/2064/gears.png","from_url":"https://gitlab.freedesktop.org/karolherbst/mesa/-/tree/rusticl/src/gallium/frontends/rusticl","thumb_width":773,"thumb_height":773,"service_icon":"https://gitlab.freedesktop.org/assets/touch-icon-iphone-5a9cee0e8a51212e70b90c87c12f382c428870c0ff67d1eb034d884b78d2dae7.png","id":1,"original_url":"https://gitlab.freedesktop.org/karolherbst/mesa/-/tree/rusticl/src/gallium/frontends/rusticl"}],"blocks":[{"type":"rich_text","block_id":"qElp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Off topic, but I noticed one of the mesa devs is playing around with implementing an OpenCL userspace in Rust: "},{"type":"link","url":"https://gitlab.freedesktop.org/karolherbst/mesa/-/tree/rusticl/src/gallium/frontends/rusticl"}]}]}]},{"client_msg_id":"03147d10-8b96-44bf-9e14-85362afa6dfd","type":"message","text":"Does any package or functions exist already which promote array storage? Something roughly like:\n```promote_storage(x::Array,   y::Array)   = (x,y)\npromote_storage(x::CuArray, y::CuArray) = (x,y)\npromote_storage(x::Array,   y::CuArray) = (cu(x),y)\npromote_storage(x::CuArray, y::Array)   = (x,cu(y))```","user":"UUMJUCYRK","ts":"1608585918.071400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Myq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does any package or functions exist already which promote array storage? Something roughly like:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"promote_storage(x::Array,   y::Array)   = (x,y)\npromote_storage(x::CuArray, y::CuArray) = (x,y)\npromote_storage(x::Array,   y::CuArray) = (cu(x),y)\npromote_storage(x::CuArray, y::Array)   = (x,cu(y))"}]}]}]},{"client_msg_id":"4223b09a-43bb-4a9c-8391-eca1b7d24375","type":"message","text":"If Adapt.jl doesn't already do that, it might be a nice idea for a PR.","user":"U8D9768Q6","ts":"1608586011.072100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KNTDS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If Adapt.jl doesn't already do that, it might be a nice idea for a PR."}]}]}],"thread_ts":"1608586011.072100","reply_count":1,"reply_users_count":1,"latest_reply":"1608586103.073100","reply_users":["U7THT3TM3"],"subscribed":false,"reactions":[{"name":"+1","users":["U7THT3TM3"],"count":1}]},{"client_msg_id":"8ed45261-3c76-47de-8752-90a240011f07","type":"message","text":"AFAIK it doesn't","user":"UUMJUCYRK","ts":"1608586091.072500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"njxgg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"AFAIK it doesn't"}]}]}]},{"client_msg_id":"F9096864-DBBD-4B39-A419-0423F7E5DDC3","type":"message","text":"A PR to Adapt.jl sounds great","user":"U7THT3TM3","ts":"1608586114.073600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sNs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"A PR to Adapt.jl sounds great"}]}]}]},{"client_msg_id":"a6689b22-7f1a-4bde-bc5f-a347cc4ea295","type":"message","text":"Hi there. I'm trying to figure out where to find an `im2col` for `CuArrays`, but with no luck yet :confused:, any hint?","user":"ULDCM1P9P","ts":"1608638446.076300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vx5s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi there. I'm trying to figure out where to find an "},{"type":"text","text":"im2col","style":{"code":true}},{"type":"text","text":" for "},{"type":"text","text":"CuArrays","style":{"code":true}},{"type":"text","text":", but with no luck yet "},{"type":"emoji","name":"confused"},{"type":"text","text":", any hint?"}]}]}]},{"client_msg_id":"faccc763-5483-4d8d-a637-b94235ffca93","type":"message","text":"`Flux.flatten` might work, alternatively, you can probably just use `reshape`","user":"UM30MT6RF","ts":"1608638801.077100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6Du","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Flux.flatten","style":{"code":true}},{"type":"text","text":" might work, alternatively, you can probably just use "},{"type":"text","text":"reshape","style":{"code":true}}]}]}]},{"client_msg_id":"8de2effa-93bc-417c-bea5-1e97810dfc90","type":"message","text":"Well, the operation I want is not just a reshape, it's rather about having a (reshaped) view as windows, typically used for simple implementation of convolutions","user":"ULDCM1P9P","ts":"1608638947.078400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yMq=v","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Well, the operation I want is not just a reshape, it's rather about having a (reshaped) view as windows, typically used for simple implementation of convolutions"}]}]}],"thread_ts":"1608638947.078400","reply_count":14,"reply_users_count":3,"latest_reply":"1608641357.081200","reply_users":["ULDCM1P9P","UM30MT6RF","UBVE598BC"],"subscribed":false},{"client_msg_id":"9a775179-f258-4582-8d12-8d18bcc08b1f","type":"message","text":"So, I am running CUDA.jl tests and some of them are failing. Should I ignore them? If not how do I follow up?","user":"UPK2KJ95Y","ts":"1608656070.082100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"E89ux","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So, I am running CUDA.jl tests and some of them are failing. Should I ignore them? If not how do I follow up?"}]}]}]},{"client_msg_id":"74cf21db-5696-48b6-9ec6-200e8e6467f5","type":"message","text":"Based on that this takes forever, I'm guessing the following is not optimized into a single kernel call?\n```x = cu(rand(256,4,256,4))\nmapslices(mean, x, dims=(1,3))```\nIs there an equivalent that would be fast, without writing my own kernel?","user":"UUMJUCYRK","ts":"1608758480.085600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"P36Ft","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Based on that this takes forever, I'm guessing the following is not optimized into a single kernel call?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"x = cu(rand(256,4,256,4))\nmapslices(mean, x, dims=(1,3))"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Is there an equivalent that would be fast, without writing my own kernel?"}]}]}]},{"client_msg_id":"b854bdc1-0375-456b-b675-d05f4e3b9cef","type":"message","text":"Answer: `mean(x, dims=(1,3))`","user":"UUMJUCYRK","ts":"1608759930.085900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=1gdD","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Answer: "},{"type":"text","text":"mean(x, dims=(1,3))","style":{"code":true}}]}]}]},{"client_msg_id":"e0dde9fc-1eea-4297-b317-2f7bd2855c42","type":"message","text":"The following function works on the CPU, but I try to get it well-parallelized on the GPU.\nHere, `m` is and 2 dimensional array of floats; `x`, `y` and `s` are floats ...\n\nI tried to use broadcasting with `CUDA.@sync`, but since `f` is called with elements from both `offsetx` and `offsety`, this doesn't work.\n\nHow do I get such code run efficiently on a CUDA GPU?   Thanks.\n\n`mold! = function( m, x, y, s)` \n    `offsetx = Float32.( 1 : size( mold)[ 1]) .- x`\n    `offsety = Float32.( 1 : size( mold)[ 2]) .- y`\n\n    `for i in CartesianIndices( m)` \n        `m[ i] = f( offsetx[ i[1]], offsety[ i[2]], s)` \n    `end`\n`end`\n\nCall: `mold!( m, 12.34, 56.78, 9.0)`","user":"U015WE9UU0H","ts":"1608761114.087200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"i9OS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The following function works on the CPU, but I try to get it well-parallelized on the GPU.\nHere, "},{"type":"text","text":"m","style":{"code":true}},{"type":"text","text":" is and 2 dimensional array of floats; "},{"type":"text","text":"x","style":{"code":true}},{"type":"text","text":", "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"s","style":{"code":true}},{"type":"text","text":" are floats ...\n\nI tried to use broadcasting with "},{"type":"text","text":"CUDA.@sync","style":{"code":true}},{"type":"text","text":", but since "},{"type":"text","text":"f","style":{"code":true}},{"type":"text","text":" is called with elements from both "},{"type":"text","text":"offsetx","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"offsety","style":{"code":true}},{"type":"text","text":", this doesn't work.\n\nHow do I get such code run efficiently on a CUDA GPU?   Thanks.\n\n"},{"type":"text","text":"mold! = function( m, x, y, s) ","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    offsetx = Float32.( 1 : size( mold)[ 1]) .- x","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    offsety = Float32.( 1 : size( mold)[ 2]) .- y","style":{"code":true}},{"type":"text","text":"\n\n"},{"type":"text","text":"    for i in CartesianIndices( m) ","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"        m[ i] = f( offsetx[ i[1]], offsety[ i[2]], s) ","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"    end","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"end","style":{"code":true}},{"type":"text","text":"\n\nCall: "},{"type":"text","text":"mold!( m, 12.34, 56.78, 9.0)","style":{"code":true}}]}]}]},{"client_msg_id":"A9336DD4-4B68-432D-AA5F-8CB2755A5FE5","type":"message","text":"Brand new to CUDA.jl, and I was looking at subsetting an array using a Boolean index. Is this possible for a CuArray? Struggling to find any documentation on the topic ","user":"U017XL92LJG","ts":"1608863457.089300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Dm2A","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Brand new to CUDA.jl, and I was looking at subsetting an array using a Boolean index. Is this possible for a CuArray? Struggling to find any documentation on the topic "}]}]}]},{"client_msg_id":"0de321aa-2f90-4730-a0c4-5811fc051f93","type":"message","text":"Any ideas on the prospects of running Julia on the new Apple M1 GPU and neural engine? I couldn't find this discussed anywhere.","user":"U85JBUGGP","ts":"1608916537.090600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0vr4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Any ideas on the prospects of running Julia on the new Apple M1 GPU and neural engine? I couldn't find this discussed anywhere."}]}]}]},{"client_msg_id":"b186d532-22eb-45c2-93a7-f3c465bd67f3","type":"message","text":"`cudaGraphicsResource` `cudaGraphicsGLRegisterBuffer` etc aren't currently wrapped in CUDA.jl, are they?","user":"U66SGEWAC","ts":"1609095267.094800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Xk0A","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"cudaGraphicsResource","style":{"code":true}},{"type":"text","text":" "},{"type":"text","text":"cudaGraphicsGLRegisterBuffer","style":{"code":true}},{"type":"text","text":" etc aren't currently wrapped in CUDA.jl, are they?"}]}]}]},{"client_msg_id":"1fd12799-6126-4514-b336-036f71d14645","type":"message","text":"Out of curiosity, is it likely that KernelAbstractions.jl will be working on Julia 1.6 by the time 1.6 is released?","user":"U7THT3TM3","ts":"1609297541.096900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nUfi","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Out of curiosity, is it likely that KernelAbstractions.jl will be working on Julia 1.6 by the time 1.6 is released?"}]}]}]},{"client_msg_id":"ebd6dccd-6eaa-4126-a789-81499f8b0b8d","type":"message","text":"Yes","user":"U67BJLYCS","ts":"1609300461.097100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QHH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yes"}]}]}]},{"client_msg_id":"a6492591-f3dc-42be-91ad-54da73fe0044","type":"message","text":"Quite likely","user":"U67BJLYCS","ts":"1609300484.097300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fzXx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Quite likely"}]}]}],"reactions":[{"name":"+1","users":["U7THT3TM3"],"count":1}]},{"client_msg_id":"defccd65-9e93-4c2f-b405-238305991f94","type":"message","text":"I have a C program that uses CuFFT to perform long FFts on 8 bit (signed integer) data samples.  CuFFT doesn't support 8 bit integers natively, but to conserve GPU memory the program uses CuFFT callbacks to feed it normalized floats that are read though a texture memory object backed by an input buffer containing the 8-bit samples.  This texture memory object is setup using the \"CUDA Runtime API\" `cudaTextureDesc` structure, which has a `readMode` field that can be set to `cudaReadModeNormalizedFloat` to get a \"free\" on-the-fly conversion from 8 bit signed integer to normalized float, `[-1, +1]`.\n\nI'd like to replicate this functionality with CUDA.jl, but I've encountered several roadblocks:\n\n1. CUDA.jl uses the \"CUDA Driver API\" and the texture object setup is different enough that there is no distinct equivalent to  `readMode=cudaReadModeNormalizedFloat`.  The CUDA docs are not too clear on this, but it seems like maybe just leaving out the `CU_TRSF_READ_AS_INTEGER` flag might suffice?  Currently that flag is always set if an integer type is backing the texture object, but that could be made selectable with a new keyword argument (with backward compatible default value) to the `CuTexture` constructor.\n2. Even if that does enable the equivalent behavior in the texture object, it seems that CUDA.jl texture fetches always returns integer values if the data type backing the texture object is of an integer type.  This precludes any use of the texture memory for this free int-to-float conversion, but it's not clear to me how to make it possible to selectively enable/disable this aspect of CUDA.jl's texture fetch functionality.\n3. The `cufftXtSetCallback()` function is not exposed by CUDA.jl.  This would be easy enough to add, but it's not clear how one could create the callback functions and get pointers to them.  Maybe via `CUDA.dynamic_cufunction`?\n4. Given the above items, I'm contemplating taking the memory hit and expanding the input data to 32 (or 16?) bit floats.  Has anyone used the (not yet wrapped) NPP functions?  They seem fairly straightforward to use via `ccall()`, but I wonder whether they offer enough added performance over CUDA.jl conversion functions to be worth the extra calling effort.\nThanks for any feedback!","user":"U01FKQQ7J0J","ts":"1609316596.124800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"aXQ5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have a C program that uses CuFFT to perform long FFts on 8 bit (signed integer) data samples.  CuFFT doesn't support 8 bit integers natively, but to conserve GPU memory the program uses CuFFT callbacks to feed it normalized floats that are read though a texture memory object backed by an input buffer containing the 8-bit samples.  This texture memory object is setup using the \"CUDA Runtime API\" "},{"type":"text","text":"cudaTextureDesc","style":{"code":true}},{"type":"text","text":" structure, which has a "},{"type":"text","text":"readMode","style":{"code":true}},{"type":"text","text":" field that can be set to "},{"type":"text","text":"cudaReadModeNormalizedFloat","style":{"code":true}},{"type":"text","text":" to get a \"free\" on-the-fly conversion from 8 bit signed integer to normalized float, "},{"type":"text","text":"[-1, +1]","style":{"code":true}},{"type":"text","text":".\n\nI'd like to replicate this functionality with CUDA.jl, but I've encountered several roadblocks:\n\n"}]},{"type":"rich_text_list","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CUDA.jl uses the \"CUDA Driver API\" and the texture object setup is different enough that there is no distinct equivalent to  "},{"type":"text","text":"readMode=cudaReadModeNormalizedFloat","style":{"code":true}},{"type":"text","text":".  The CUDA docs are not too clear on this, but it seems like maybe just leaving out the "},{"type":"text","text":"CU_TRSF_READ_AS_INTEGER","style":{"code":true}},{"type":"text","text":" flag might suffice?  Currently that flag is always set if an integer type is backing the texture object, but that could be made selectable with a new keyword argument (with backward compatible default value) to the "},{"type":"text","text":"CuTexture","style":{"code":true}},{"type":"text","text":" constructor."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Even if that does enable the equivalent behavior in the texture object, it seems that CUDA.jl texture fetches always returns integer values if the data type backing the texture object is of an integer type.  This precludes any use of the texture memory for this free int-to-float conversion, but it's not clear to me how to make it possible to selectively enable/disable this aspect of CUDA.jl's texture fetch functionality."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"The "},{"type":"text","text":"cufftXtSetCallback()","style":{"code":true}},{"type":"text","text":" function is not exposed by CUDA.jl.  This would be easy enough to add, but it's not clear how one could create the callback functions and get pointers to them.  Maybe via "},{"type":"text","text":"CUDA.dynamic_cufunction","style":{"code":true}},{"type":"text","text":"?"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Given the above items, I'm contemplating taking the memory hit and expanding the input data to 32 (or 16?) bit floats.  Has anyone used the (not yet wrapped) NPP functions?  They seem fairly straightforward to use via "},{"type":"text","text":"ccall()","style":{"code":true}},{"type":"text","text":", but I wonder whether they offer enough added performance over CUDA.jl conversion functions to be worth the extra calling effort."}]}],"style":"ordered","indent":0},{"type":"rich_text_section","elements":[{"type":"text","text":"\nThanks for any feedback!"}]}]}]},{"client_msg_id":"a216ec06-e1d2-4a56-952d-11adc9195c08","type":"message","text":"Hi,\n\nI can’t multiply a CSR sparse matrix anymore. This used to work with `CuArrays.jl`, is there a workaround?\n```julia&gt; Jpo_gpu * CuArray(orbitguess_f)\nERROR: MethodError: no method matching mv!(::Char, ::Float64, ::CUDA.CUSPARSE.CuSparseMatrixCSR{Float64}, ::CuArray{Float64,1}, ::Float64, ::CuArray{Float64,1}, ::Char)\nClosest candidates are:\n  mv!(::Char, ::Float64, ::CUDA.CUSPARSE.CuSparseMatrixBSR{Float64}, ::CuArray{Float64,1}, ::Float64, ::CuArray{Float64,1}, ::Char) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/wrappers.jl:193\n  mv!(::Char, ::Complex{Float64}, ::CUDA.CUSPARSE.CuSparseMatrixBSR{Complex{Float64}}, ::CuArray{Complex{Float64},1}, ::Complex{Float64}, ::CuArray{Complex{Float64},1}, ::Char) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/wrappers.jl:193\n  mv!(::Char, ::Complex{Float32}, ::CUDA.CUSPARSE.CuSparseMatrixBSR{Complex{Float32}}, ::CuArray{Complex{Float32},1}, ::Complex{Float32}, ::CuArray{Complex{Float32},1}, ::Char) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/wrappers.jl:193\n  ...\nStacktrace:\n [1] mul!(::CuArray{Float64,1}, ::CUDA.CUSPARSE.CuSparseMatrixCSR{Float64}, ::CuArray{Float64,1}) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/interfaces.jl:12\n [2] *(::CUDA.CUSPARSE.CuSparseMatrixCSR{Float64}, ::CuArray{Float64,1}) at /home/rveltz/julia/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/matmul.jl:51\n [3] top-level scope at REPL[1946]:1\n [4] eval(::Module, ::Any) at ./boot.jl:331\n [5] eval_user_input(::Any, ::REPL.REPLBackend) at /home/rveltz/julia/usr/share/julia/stdlib/v1.4/REPL/src/REPL.jl:86\n [6] run_backend(::REPL.REPLBackend) at /home/rveltz/.julia/packages/Revise/ucYAZ/src/Revise.jl:1184\n [7] top-level scope at REPL[1785]:0```","user":"U7GQE9JP9","ts":"1609324486.126000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ilrh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi,\n\nI can’t multiply a CSR sparse matrix anymore. This used to work with "},{"type":"text","text":"CuArrays.jl","style":{"code":true}},{"type":"text","text":", is there a workaround?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> Jpo_gpu * CuArray(orbitguess_f)\nERROR: MethodError: no method matching mv!(::Char, ::Float64, ::CUDA.CUSPARSE.CuSparseMatrixCSR{Float64}, ::CuArray{Float64,1}, ::Float64, ::CuArray{Float64,1}, ::Char)\nClosest candidates are:\n  mv!(::Char, ::Float64, ::CUDA.CUSPARSE.CuSparseMatrixBSR{Float64}, ::CuArray{Float64,1}, ::Float64, ::CuArray{Float64,1}, ::Char) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/wrappers.jl:193\n  mv!(::Char, ::Complex{Float64}, ::CUDA.CUSPARSE.CuSparseMatrixBSR{Complex{Float64}}, ::CuArray{Complex{Float64},1}, ::Complex{Float64}, ::CuArray{Complex{Float64},1}, ::Char) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/wrappers.jl:193\n  mv!(::Char, ::Complex{Float32}, ::CUDA.CUSPARSE.CuSparseMatrixBSR{Complex{Float32}}, ::CuArray{Complex{Float32},1}, ::Complex{Float32}, ::CuArray{Complex{Float32},1}, ::Char) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/wrappers.jl:193\n  ...\nStacktrace:\n [1] mul!(::CuArray{Float64,1}, ::CUDA.CUSPARSE.CuSparseMatrixCSR{Float64}, ::CuArray{Float64,1}) at /home/rveltz/.julia/packages/CUDA/dZvbp/lib/cusparse/interfaces.jl:12\n [2] *(::CUDA.CUSPARSE.CuSparseMatrixCSR{Float64}, ::CuArray{Float64,1}) at /home/rveltz/julia/usr/share/julia/stdlib/v1.4/LinearAlgebra/src/matmul.jl:51\n [3] top-level scope at REPL[1946]:1\n [4] eval(::Module, ::Any) at ./boot.jl:331\n [5] eval_user_input(::Any, ::REPL.REPLBackend) at /home/rveltz/julia/usr/share/julia/stdlib/v1.4/REPL/src/REPL.jl:86\n [6] run_backend(::REPL.REPLBackend) at /home/rveltz/.julia/packages/Revise/ucYAZ/src/Revise.jl:1184\n [7] top-level scope at REPL[1785]:0"}]}]}]},{"client_msg_id":"911822ad-64ec-4277-a8f4-f3744a731e48","type":"message","text":"There wouldn't happen to be a library I can use for Kahan Summation on GPU from Julia, would there?","user":"UUMJUCYRK","ts":"1609371720.132000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WMQuO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"There wouldn't happen to be a library I can use for Kahan Summation on GPU from Julia, would there?"}]}]}]},{"client_msg_id":"63ab213c-ab84-4266-9b50-1db6aa90f8e7","type":"message","text":"What's the easiest way to download the CUDA artifacts?","user":"U7THT3TM3","ts":"1609578933.133000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GTK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What's the easiest way to download the CUDA artifacts?"}]}]}]},{"client_msg_id":"5e6ef995-b1a0-4b9c-905c-f907e8752bba","type":"message","text":"I.e., if I just do this: `using CUDA`, it doesn't download the artifacts, because they are lazy","user":"U7THT3TM3","ts":"1609578956.133700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=dC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I.e., if I just do this: "},{"type":"text","text":"using CUDA","style":{"code":true}},{"type":"text","text":", it doesn't download the artifacts, because they are lazy"}]}]}]},{"client_msg_id":"097d9aa0-431b-4dba-b139-8920948088ea","type":"message","text":"The artifacts get downloaded the first time I actually do something with CUDA","user":"U7THT3TM3","ts":"1609578969.134100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lm0S0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The artifacts get downloaded the first time I actually do something with CUDA"}]}]}]},{"client_msg_id":"335302b7-d168-47cd-a18c-08f1669a0890","type":"message","text":"But is there a way I can trigger the download manually, before I do any computation?","user":"U7THT3TM3","ts":"1609578995.134700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YHkA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But is there a way I can trigger the download manually, before I do any computation?"}]}]}]},{"client_msg_id":"63ece1c4-587d-41f1-9725-57a47e04ca7f","type":"message","text":"Is there a way to do matrix multiplication with cuBLAS and store the result by overwriting an existing matrix?\n\nI tried this:\n```CUDA.CUBLAS.gemm!(C, A, B)```\nWhere `A`, `B`, and `C` are `CuArray`, but I get a MethodError:\n```julia&gt; CUDA.CUBLAS.gemm!(C, A, B)\nERROR: MethodError: no method matching gemm!(::CuArray{Float64,2}, ::CuArray{Float64,2}, ::CuArray{Float64,2})\nStacktrace:\n [1] top-level scope at REPL[44]:1```","user":"U7THT3TM3","ts":"1609583317.136500","team":"T68168MUP","edited":{"user":"U7THT3TM3","ts":"1609583358.000000"},"blocks":[{"type":"rich_text","block_id":"tjo7X","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to do matrix multiplication with cuBLAS and store the result by overwriting an existing matrix?\n\nI tried this:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"CUDA.CUBLAS.gemm!(C, A, B)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Where "},{"type":"text","text":"A","style":{"code":true}},{"type":"text","text":", "},{"type":"text","text":"B","style":{"code":true}},{"type":"text","text":", and "},{"type":"text","text":"C","style":{"code":true}},{"type":"text","text":" are "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":", but I get a MethodError:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> CUDA.CUBLAS.gemm!(C, A, B)\nERROR: MethodError: no method matching gemm!(::CuArray{Float64,2}, ::CuArray{Float64,2}, ::CuArray{Float64,2})\nStacktrace:\n [1] top-level scope at REPL[44]:1"}]}]}],"thread_ts":"1609583317.136500","reply_count":5,"reply_users_count":2,"latest_reply":"1609586063.137700","reply_users":["UM30MT6RF","U7THT3TM3"],"subscribed":false},{"client_msg_id":"e985aef3-d285-4c08-843b-71256958aea2","type":"message","text":"Whats the current recommended way to deal with broadcasting CuArrays over Base.sin without having to have two copies of your code and make one of them CUDA.sin?","user":"UUMJUCYRK","ts":"1609619621.141000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"RUie","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Whats the current recommended way to deal with broadcasting CuArrays over Base.sin without having to have two copies of your code and make one of them CUDA.sin?"}]}]}]},{"client_msg_id":"3179bb55-20bd-4906-9999-a082086024a5","type":"message","text":"SPIR-V kernel support just landed in Clover: <https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/2078>","user":"UMY1LV01G","ts":"1609700726.148200","team":"T68168MUP","attachments":[{"service_name":"GitLab","title":"clover: support IL programs (!2078) · Merge Requests · Mesa / mesa","title_link":"https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/2078","text":"The extension cl_khr_il_program and OpenCL 2.1 added functionalities to provide kernels as intermediate languages rather than OpenCL C. This merge request adds the infrastructure for it, as well...","fallback":"GitLab: clover: support IL programs (!2078) · Merge Requests · Mesa / mesa","thumb_url":"https://gitlab.freedesktop.org/uploads/-/system/project/avatar/176/gears.png","fields":[{"title":"Author","value":"Pierre Moreau","short":true},{"title":"Assignee","value":"Marge Bot","short":true}],"from_url":"https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/2078","thumb_width":773,"thumb_height":773,"service_icon":"https://gitlab.freedesktop.org/assets/touch-icon-iphone-5a9cee0e8a51212e70b90c87c12f382c428870c0ff67d1eb034d884b78d2dae7.png","id":1,"original_url":"https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/2078"}],"blocks":[{"type":"rich_text","block_id":"18h2d","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"SPIR-V kernel support just landed in Clover: "},{"type":"link","url":"https://gitlab.freedesktop.org/mesa/mesa/-/merge_requests/2078"}]}]}],"reactions":[{"name":"cool","users":["UDSU53PEG"],"count":1}]},{"client_msg_id":"20f060f0-582e-4763-9bd6-ebea723a0bc0","type":"message","text":"I wonder if it would support GPUCompiler output OOTB...","user":"UMY1LV01G","ts":"1609701304.148900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0OcFd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I wonder if it would support GPUCompiler output OOTB..."}]}]}]},{"client_msg_id":"4e4e4a8d-0a2d-40fb-a693-ae48ddc3d415","type":"message","text":"GPU office hours in an hour!","user":"U67BJLYCS","ts":"1609775650.150000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5kI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"GPU office hours in an hour!"}]}]}]},{"client_msg_id":"e0725d8f-458b-4e2d-8f72-9feaa54e4c8a","type":"message","text":"since people probably won't be upgrading to 1.6 immediately, I'm creating an release of CUDA.jl for 1.5 with most changes from the master branch backported to it: <https://github.com/JuliaGPU/CUDA.jl/pull/623>\nnow would be a good time for testing, or to create a PR if you want a specific feature/bugfix to be part of that release.","user":"U68A3ASP9","ts":"1609832816.151800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"X9O0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"since people probably won't be upgrading to 1.6 immediately, I'm creating an release of CUDA.jl for 1.5 with most changes from the master branch backported to it: "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/pull/623"},{"type":"text","text":"\nnow would be a good time for testing, or to create a PR if you want a specific feature/bugfix to be part of that release."}]}]}]},{"client_msg_id":"545257b8-8805-465d-b4d5-1c7ad650f630","type":"message","text":"<https://rosenzweig.io/blog/asahi-gpu-part-1.html>","user":"UDGT4PM41","ts":"1610046156.153800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hFnZ","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://rosenzweig.io/blog/asahi-gpu-part-1.html"}]}]}],"thread_ts":"1610046156.153800","reply_count":1,"reply_users_count":1,"latest_reply":"1610047108.154500","reply_users":["U6A0PD8CR"],"subscribed":false},{"client_msg_id":"da211520-4a12-41aa-82a0-f47b9eead1e9","type":"message","text":"\"Dissecting the Apple M1 GPU, part I\"","user":"UDGT4PM41","ts":"1610046183.154400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1qfk","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"\"Dissecting the Apple M1 GPU, part I\""}]}]}]},{"client_msg_id":"ebbafe9b-dbb9-4b10-a66f-c5106c07a7b0","type":"message","text":"Any ideas for getting more output from a `CUDA.KernelException(CUDA.CuDevice(0))`?","user":"UM30MT6RF","ts":"1610049227.155300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WSML","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Any ideas for getting more output from a "},{"type":"text","text":"CUDA.KernelException(CUDA.CuDevice(0))","style":{"code":true}},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"06034694-ece4-4899-aa84-ee2af46f6d22","type":"message","text":"that's the host part, there should be a device print before that","user":"U68A3ASP9","ts":"1610049491.155900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gmI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"that's the host part, there should be a device print before that"}]}]}]},{"client_msg_id":"816d950e-4cb0-45de-8ce3-7afd0019c5da","type":"message","text":"unless you're running with `-g0`","user":"U68A3ASP9","ts":"1610049497.156100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YTn+D","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"unless you're running with "},{"type":"text","text":"-g0","style":{"code":true}}]}]}],"thread_ts":"1610049497.156100","reply_count":14,"reply_users_count":2,"latest_reply":"1610050910.160100","reply_users":["UM30MT6RF","U68A3ASP9"],"subscribed":false},{"client_msg_id":"63fc4c2e-6c8c-4c25-894f-9f606e0350a5","type":"message","text":"I get a backtrace, but not more:\n```ERROR: CUDA.KernelException(CUDA.CuDevice(0))\nStacktrace:\n  [1] check_exceptions()\n    @ CUDA /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/src/compiler/exceptions.jl:94\n  [2] prepare_cuda_call()\n    @ CUDA /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/src/state.jl:85\n  [3] initialize_api()\n    @ CUDA /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/lib/cudadrv/error.jl:92\n  [4] macro expansion\n    @ /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/lib/cudadrv/libcuda.jl:975 [inlined]\n  [5] macro expansion\n    @ /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/lib/cudadrv/error.jl:102 [inlined]\n  [6] cuStreamWaitEvent\n    @ /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/lib/utils/call.jl:26 [inlined]\n  [7] wait\n    @ /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/lib/cudadrv/events.jl:77 [inlined]\n  [8] wait\n    @ /local/scratch/ssd/sschaub/.julia/packages/KernelAbstractions/z2oKi/src/backends/cuda.jl:74 [inlined]\n  [9] #68#𝒜𝒸𝓉!\n    @ /local/scratch/ssd/sschaub/.julia/packages/Tullio/u3myB/src/macro.jl:1169 [inlined]\n [10] #68#𝒜𝒸𝓉!\n[...]```","user":"UM30MT6RF","ts":"1610049850.156900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ypmw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I get a backtrace, but not more:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: CUDA.KernelException(CUDA.CuDevice(0))\nStacktrace:\n  [1] check_exceptions()\n    @ CUDA /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/src/compiler/exceptions.jl:94\n  [2] prepare_cuda_call()\n    @ CUDA /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/src/state.jl:85\n  [3] initialize_api()\n    @ CUDA /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/lib/cudadrv/error.jl:92\n  [4] macro expansion\n    @ /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/lib/cudadrv/libcuda.jl:975 [inlined]\n  [5] macro expansion\n    @ /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/lib/cudadrv/error.jl:102 [inlined]\n  [6] cuStreamWaitEvent\n    @ /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/lib/utils/call.jl:26 [inlined]\n  [7] wait\n    @ /local/scratch/ssd/sschaub/.julia/packages/CUDA/tZgkK/lib/cudadrv/events.jl:77 [inlined]\n  [8] wait\n    @ /local/scratch/ssd/sschaub/.julia/packages/KernelAbstractions/z2oKi/src/backends/cuda.jl:74 [inlined]\n  [9] #68#𝒜𝒸𝓉!\n    @ /local/scratch/ssd/sschaub/.julia/packages/Tullio/u3myB/src/macro.jl:1169 [inlined]\n [10] #68#𝒜𝒸𝓉!\n[...]"}]}]}]},{"client_msg_id":"2dfb2eb5-7d16-4521-8536-626c640dde3b","type":"message","text":"Hi all,\nI'm looking to use a GPU CG method with an ILU preconditioner. I'm currently using a CPU CG method through IterativeSolvers and ILU through *<https://github.com/haampie/IncompleteLU.jl|IncompleteLU.jl>* . The CG method from IterativeSolvers is easily extended to GPUs however, some work is required on IncompleteLU.jl to implement it on a GPU. I think only `forward_substitution` and `backward_substitution` methods are required to extend this. I've tried using `sv2!` with little success. Can anyone give me a hand?","user":"U01CZ5M383E","ts":"1610065370.164500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"mxao1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi all,\nI'm looking to use a GPU CG method with an ILU preconditioner. I'm currently using a CPU CG method through IterativeSolvers and ILU through "},{"type":"link","url":"https://github.com/haampie/IncompleteLU.jl","text":"IncompleteLU.jl","style":{"bold":true}},{"type":"text","text":" . The CG method from IterativeSolvers is easily extended to GPUs however, some work is required on IncompleteLU.jl to implement it on a GPU. I think only "},{"type":"text","text":"forward_substitution","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"backward_substitution","style":{"code":true}},{"type":"text","text":" methods are required to extend this. I've tried using "},{"type":"text","text":"sv2!","style":{"code":true}},{"type":"text","text":" with little success. Can anyone give me a hand?"}]}]}]},{"client_msg_id":"bf6d7784-cd26-4954-b000-f24e5654e325","type":"message","text":"I have also tried built-in CUDA ILU with no success (`ilu02`)","user":"U01CZ5M383E","ts":"1610065586.165000","team":"T68168MUP","edited":{"user":"U01CZ5M383E","ts":"1610065611.000000"},"blocks":[{"type":"rich_text","block_id":"0B4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I have also tried built-in CUDA ILU with no success ("},{"type":"text","text":"ilu02","style":{"code":true}},{"type":"text","text":")"}]}]}]},{"client_msg_id":"ff01381b-91bf-4877-9671-30732952b395","type":"message","text":"Yeah, I have never tried that myself. In the end it's just 2 sparse triangular matrices L and U that should be copied to the GPU. I can't recall why IncompleteLU implements the forward &amp; backward solves itself and doesn't use UnitLowerTriangular and such. I'm also not aware if there is a sparse triangular solver provided by cusparse you could use","user":"U6N6VQE30","ts":"1610067848.172700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sgvT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah, I have never tried that myself. In the end it's just 2 sparse triangular matrices L and U that should be copied to the GPU. I can't recall why IncompleteLU implements the forward & backward solves itself and doesn't use UnitLowerTriangular and such. I'm also not aware if there is a sparse triangular solver provided by cusparse you could use"}]}]}]},{"client_msg_id":"5d0b04f1-2e19-4e8b-b7ca-7fc6343ac1cc","type":"message","text":"I really like EllipsisNotation.jl, but unfortunately it looks like it causes CuArray views that would otherwise not need a wrapper to acquire one, which screws up some of my downstream code:\n```julia&gt; using CUDA, EllipsisNotation\n\njulia&gt; x = cu(rand(10));\n\njulia&gt; view(x, 1:2)\n2-element CuArray{Float32,1}:\n 0.06164578\n 0.95716375\n\njulia&gt; view(x, 1:2, ..)\n2-element view(::CuArray{Float32,1}, 1:2) with eltype Float32:\n 0.06164578\n 0.95716375```\nNot knowing which repo is the right one file an Issue in, figured Id ask here, any ideas?","user":"UUMJUCYRK","ts":"1610075375.175900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5d4V","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I really like EllipsisNotation.jl, but unfortunately it looks like it causes CuArray views that would otherwise not need a wrapper to acquire one, which screws up some of my downstream code:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> using CUDA, EllipsisNotation\n\njulia> x = cu(rand(10));\n\njulia> view(x, 1:2)\n2-element CuArray{Float32,1}:\n 0.06164578\n 0.95716375\n\njulia> view(x, 1:2, ..)\n2-element view(::CuArray{Float32,1}, 1:2) with eltype Float32:\n 0.06164578\n 0.95716375"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Not knowing which repo is the right one file an Issue in, figured Id ask here, any ideas?"}]}]}]},{"client_msg_id":"f3336360-3275-4bc2-84cb-f8ff05b7cdc0","type":"message","text":"EllipsisNotation.jl probably needs overloads for `view` with GPUs, since it's specialized in CUDA.jl","user":"U69BL50BF","ts":"1610078238.176500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jLXp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"EllipsisNotation.jl probably needs overloads for "},{"type":"text","text":"view","style":{"code":true}},{"type":"text","text":" with GPUs, since it's specialized in CUDA.jl"}]}]}]},{"client_msg_id":"D2ED59B0-5BD8-47C7-99C8-08AC89B90202","type":"message","text":"Does the master branch of CUDA.jl work on Julia 1.6-nightly?","user":"U7THT3TM3","ts":"1610101498.177500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DJw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does the master branch of CUDA.jl work on Julia 1.6-nightly?"}]}]}]},{"client_msg_id":"834EC604-66F2-45ED-99FE-C800677B5141","type":"message","text":"If so, would be great to get a release","user":"U7THT3TM3","ts":"1610101543.177800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YpE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If so, would be great to get a release"}]}]}]},{"client_msg_id":"2BB84F2E-069D-4181-98B4-F1CA429155DA","type":"message","text":"Context: <https://github.com/JuliaGPU/KernelAbstractions.jl/pull/177#issuecomment-756655092|https://github.com/JuliaGPU/KernelAbstractions.jl/pull/177#issuecomment-756655092>","user":"U7THT3TM3","ts":"1610101574.178100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OVsfM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Context: "},{"type":"link","url":"https://github.com/JuliaGPU/KernelAbstractions.jl/pull/177#issuecomment-756655092","text":"https://github.com/JuliaGPU/KernelAbstractions.jl/pull/177#issuecomment-756655092"}]}]}]},{"client_msg_id":"f28f30c3-fcdf-46b6-860a-eb3b70ddd94c","type":"message","text":"I'm first going to release a version for 1.5, wasn't planning on doing 1.6 anytime soon. why not use a Manifest and use CUDA.jl by git sha?","user":"U68A3ASP9","ts":"1610105415.178600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3OTw2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm first going to release a version for 1.5, wasn't planning on doing 1.6 anytime soon. why not use a Manifest and use CUDA.jl by git sha?"}]}]}]},{"client_msg_id":"4716143A-94E0-4377-BFB0-0034A5403970","type":"message","text":"CUDA master requires 1.6, right?\n\nIf we check in a Manifest that uses CUDA master, then this pins all CI jobs to the package versions in that Manifest.","user":"U7THT3TM3","ts":"1610105512.179800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"77F/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CUDA master requires 1.6, right?\n"},{"type":"text","text":"\n"},{"type":"text","text":"If we check in a Manifest that uses CUDA master, then this pins all CI jobs to the package versions in that Manifest."}]}]}]},{"client_msg_id":"E5453593-2F1B-46D6-BD60-4840DDA52B50","type":"message","text":"And that Manifest can't be used on Julia 1.5.","user":"U7THT3TM3","ts":"1610105536.180500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=Omp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And that Manifest can't be used on Julia 1.5."}]}]}]},{"client_msg_id":"3702b2ab-8001-4166-b30d-2abc338812b2","type":"message","text":"ah, right","user":"U68A3ASP9","ts":"1610105542.180700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oIqDE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ah, right"}]}]}]},{"client_msg_id":"b9658741-74ac-4005-8349-e4a6a92a716f","type":"message","text":"that's an annoying limitation of manifests, tbh","user":"U68A3ASP9","ts":"1610105576.182000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uyGm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"that's an annoying limitation of manifests, tbh"}]}]}]},{"client_msg_id":"E4CB6468-A6F4-480E-A297-B7FCCF50E9CF","type":"message","text":"I had an idea once that we should have separate Manifests for different Julia versions. Because manifests are not portable between Julia versions","user":"U7THT3TM3","ts":"1610105579.182200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"d4DH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I had an idea once that we should have separate Manifests for different Julia versions. Because manifests are not portable between Julia versions"}]}]}]},{"client_msg_id":"4CC6EF7B-4E39-4352-97DF-3DF878B86EA0","type":"message","text":"So you'd have \n\n`Manifest.1.5.toml`\n`Manifest.1.6.toml`\n\nEtc","user":"U7THT3TM3","ts":"1610105592.182600","team":"T68168MUP","edited":{"user":"U7THT3TM3","ts":"1610105671.000000"},"blocks":[{"type":"rich_text","block_id":"p=Wcy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So you'd have \n\n"},{"type":"text","text":"Manifest.1.5.toml","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"Manifest.1.6.toml","style":{"code":true}},{"type":"text","text":"\n"},{"type":"text","text":"\nEtc"}]}]}],"thread_ts":"1610105592.182600","reply_count":4,"reply_users_count":2,"latest_reply":"1610105970.188900","reply_users":["U68A3ASP9","U7THT3TM3"],"subscribed":false},{"client_msg_id":"1943F412-7D34-4EC6-B2CA-D5FF09BA4D11","type":"message","text":"And then Julia would load the matching manifest","user":"U7THT3TM3","ts":"1610105604.183000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"w=L9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And then Julia would load the matching manifest"}]}]}]},{"client_msg_id":"1ED0583B-A051-404D-A6E8-CD08317414F8","type":"message","text":"The response to this idea was mixed. ","user":"U7THT3TM3","ts":"1610105617.183400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"i=9KW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The response to this idea was mixed. "}]}]}]},{"client_msg_id":"919F1DBD-838D-4D33-8DB4-C45710B1D57A","type":"message","text":"So probably it won't be adopted.","user":"U7THT3TM3","ts":"1610105648.184200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"WRs7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So probably it won't be adopted."}]}]}]},{"client_msg_id":"D4CC9596-003F-497A-8CC3-9544E1F3E2DC","type":"message","text":"Here's what we could do for now. In the buildkite file (<https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/.buildkite/pipeline.yml|https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/.buildkite/pipeline.yml>), after the \"JuliaCI/julia#v0.6\" and before the \"JuliaCI/julia-test#v0.3\", I would add a line that looks like this:\n\n`julia -e 'using Pkg; pkgstr\"] add CUDA#master\"` ","user":"U7THT3TM3","ts":"1610105883.186500","team":"T68168MUP","edited":{"user":"U7THT3TM3","ts":"1610105905.000000"},"blocks":[{"type":"rich_text","block_id":"g3T","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Here's what we could do for now. In the buildkite file ("},{"type":"link","url":"https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/.buildkite/pipeline.yml","text":"https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/.buildkite/pipeline.yml"},{"type":"text","text":"), after the \"JuliaCI/julia#v0.6\" and before the \"JuliaCI/julia-test#v0.3\", I would add a line that looks like this:\n\n"},{"type":"text","text":"julia -e 'using Pkg; pkgstr\"] add CUDA#m","style":{"code":true}},{"type":"text","text":"aster","style":{"code":true}},{"type":"text","text":"\"","style":{"code":true}},{"type":"text","text":" "}]}]}]},{"client_msg_id":"A38E6345-19F5-4C6D-A8EE-7D030F8639E1","type":"message","text":"Is it possible to run arbitrary commands in the buildkite pipeline?","user":"U7THT3TM3","ts":"1610105894.187000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UyOYN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is it possible to run arbitrary commands in the buildkite pipeline?"}]}]}]},{"client_msg_id":"e8b8fd32-4395-417c-8471-2ef36322d895","type":"message","text":"no. there's only a single command, the plugins add hooks to execute before that. so this command would be executed after having instantiated, and override the actual test command","user":"U68A3ASP9","ts":"1610106058.189900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"H/FZL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"no. there's only a single command, the plugins add hooks to execute before that. so this command would be executed after having instantiated, and override the actual test command"}]}]}]},{"client_msg_id":"cf685244-9a39-43e1-bf23-161b34dc3309","type":"message","text":"but I'll create a release, nobody is using 1.6 anyway","user":"U68A3ASP9","ts":"1610106075.190300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"XJ1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"but I'll create a release, nobody is using 1.6 anyway"}]}]}],"reactions":[{"name":"heart","users":["U7THT3TM3"],"count":1}]},{"client_msg_id":"FA732284-EA69-4CC2-961D-2ECE5BC39C99","type":"message","text":"Long term, it might actually be worth adding the \"per-Julia-version manifests\" feature to Buildkite at least","user":"U7THT3TM3","ts":"1610106116.191400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Q8/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Long term, it might actually be worth adding the \"per-Julia-version manifests\" feature to Buildkite at least"}]}]}]},{"client_msg_id":"14A6185C-9597-4767-8646-415DB182588C","type":"message","text":"We can mull it over for a while","user":"U7THT3TM3","ts":"1610106131.191800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gwia","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We can mull it over for a while"}]}]}]},{"client_msg_id":"30a84e08-61c6-4a22-8e66-7103fc5379ed","type":"message","text":"Has anyone looked into using the newly added `reinterpret(reshape, ...)` on `CuArrays`? I am running into issues with KernelAbstractions and am wondering whether that might be a CUDA issue: <https://github.com/JuliaGPU/KernelAbstractions.jl/issues/179#issuecomment-756714700>","user":"UM30MT6RF","ts":"1610111815.194200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uy2oO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Has anyone looked into using the newly added "},{"type":"text","text":"reinterpret(reshape, ...)","style":{"code":true}},{"type":"text","text":" on "},{"type":"text","text":"CuArrays","style":{"code":true}},{"type":"text","text":"? I am running into issues with KernelAbstractions and am wondering whether that might be a CUDA issue: "},{"type":"link","url":"https://github.com/JuliaGPU/KernelAbstractions.jl/issues/179#issuecomment-756714700"}]}]}]},{"client_msg_id":"1f1b622d-db78-4124-9069-6491bee77213","type":"message","text":"Maybe the best solution here would be to produce a `CuArray` as well instead of a wrapper?","user":"UM30MT6RF","ts":"1610111935.195300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"l/Lm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Maybe the best solution here would be to produce a "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":" as well instead of a wrapper?"}]}]}]},{"client_msg_id":"2e074ac1-d506-47c8-9e28-a673743020c5","type":"message","text":"Hi all,\nDoes anyone have any ideas on how to fix this code so that the GPU output matches the CPU CG + CPU `\\`? On Julia 1.5 with CUDA 2.0.2\n```using CUDA, CUDA.CUSPARSE, CUDA.CUSOLVER\nusing LinearAlgebra\nusing SparseArrays\nusing IncompleteLU\nusing IterativeSolvers\nCUDA.allowscalar(false)\n\nval = sprand(200,200,0.05);\nA_cpu = val*val'\nb_cpu = rand(200)\n\nA_gpu = CuSparseMatrixCSC(A_cpu)\nb_gpu = CuArray(b_cpu)\n\nPrecilu = ilu(A_cpu, τ = 3.0)\n\nimport Base: ldiv!\nfunction LinearAlgebra.ldiv!(y::CuArray, P::LUgpu, x::CuArray)\n  copyto!(y, x)\n  sv2!('N', 'L', 1.0, P, y, 'O')\n  sv2!('N', 'U', 1.0, P, y, 'O')\n  return y\nend\n\nfunction LinearAlgebra.ldiv!(P::LUgpu, x::CuArray)\n  sv2!('N', 'L', 1.0, P, x, 'O')\n  sv2!('N', 'U', 1.0, P, x, 'O')\n  return x\nend\n\n# function LinearAlgebra.ldiv!(_lu::LUperso, rhs::CUDA.CuArray)\n# \t_x = UpperTriangular(_lu.Ut) \\ (LowerTriangular(_lu.L) \\ rhs)\n# \trhs .= vec(_x)\n# \t# CUDA.unsafe_free!(_x)\n# \trhs\n# end\n#\n# function LinearAlgebra.ldiv!(yrhs::CuArray,_lu::LUperso, rhs::CuArray)\n# \tcopyto!(yrhs,rhs)\n# \t_x = UpperTriangular(_lu.Ut) \\ (LowerTriangular(_lu.L) \\ rhs)\n# \trhs .= vec(_x)\n# \t# CUDA.unsafe_free!(_x)\n# \trhs\n# end\n\nstruct LUgpu\n\tL\n\tUt\t# transpose of U in LU decomposition\nend\n\n\nP = LUperso(LowerTriangular(CuSparseMatrixCSC(I+Precilu.L)), UpperTriangular(CuSparseMatrixCSC(sparse(Precilu.U'))));\nval = cg(A_gpu,b_gpu,verbose=true,Pl=P,tol=10^-7,maxiter=1000)\n\nP_cpu = ilu(A_cpu,τ=3.0)\nval = cg(A_cpu,b_cpu;verbose=true,Pl=P_cpu,tol=10^-7,maxiter=1000)\nA_cpu\\b_cpu ```","user":"U01CZ5M383E","ts":"1610153168.199800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"p9E","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi all,\nDoes anyone have any ideas on how to fix this code so that the GPU output matches the CPU CG + CPU "},{"type":"text","text":"\\","style":{"code":true}},{"type":"text","text":"? On Julia 1.5 with CUDA 2.0.2\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using CUDA, CUDA.CUSPARSE, CUDA.CUSOLVER\nusing LinearAlgebra\nusing SparseArrays\nusing IncompleteLU\nusing IterativeSolvers\nCUDA.allowscalar(false)\n\nval = sprand(200,200,0.05);\nA_cpu = val*val'\nb_cpu = rand(200)\n\nA_gpu = CuSparseMatrixCSC(A_cpu)\nb_gpu = CuArray(b_cpu)\n\nPrecilu = ilu(A_cpu, τ = 3.0)\n\nimport Base: ldiv!\nfunction LinearAlgebra.ldiv!(y::CuArray, P::LUgpu, x::CuArray)\n  copyto!(y, x)\n  sv2!('N', 'L', 1.0, P, y, 'O')\n  sv2!('N', 'U', 1.0, P, y, 'O')\n  return y\nend\n\nfunction LinearAlgebra.ldiv!(P::LUgpu, x::CuArray)\n  sv2!('N', 'L', 1.0, P, x, 'O')\n  sv2!('N', 'U', 1.0, P, x, 'O')\n  return x\nend\n\n# function LinearAlgebra.ldiv!(_lu::LUperso, rhs::CUDA.CuArray)\n# \t_x = UpperTriangular(_lu.Ut) \\ (LowerTriangular(_lu.L) \\ rhs)\n# \trhs .= vec(_x)\n# \t# CUDA.unsafe_free!(_x)\n# \trhs\n# end\n#\n# function LinearAlgebra.ldiv!(yrhs::CuArray,_lu::LUperso, rhs::CuArray)\n# \tcopyto!(yrhs,rhs)\n# \t_x = UpperTriangular(_lu.Ut) \\ (LowerTriangular(_lu.L) \\ rhs)\n# \trhs .= vec(_x)\n# \t# CUDA.unsafe_free!(_x)\n# \trhs\n# end\n\nstruct LUgpu\n\tL\n\tUt\t# transpose of U in LU decomposition\nend\n\n\nP = LUperso(LowerTriangular(CuSparseMatrixCSC(I+Precilu.L)), UpperTriangular(CuSparseMatrixCSC(sparse(Precilu.U'))));\nval = cg(A_gpu,b_gpu,verbose=true,Pl=P,tol=10^-7,maxiter=1000)\n\nP_cpu = ilu(A_cpu,τ=3.0)\nval = cg(A_cpu,b_cpu;verbose=true,Pl=P_cpu,tol=10^-7,maxiter=1000)\nA_cpu\\b_cpu "}]}]}]},{"client_msg_id":"94898089-1969-4F52-B660-A03E7B41E7AA","type":"message","text":"Is the newly released CUDA.jl version 2.4.0 meant to be compatible with Julia 1.6?\n\nThe compat entry is `julia = \"~1.5\"`, which unfortunately excludes Julia 1.6.","user":"U7THT3TM3","ts":"1610241890.201500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KiE6B","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is the newly released CUDA.jl version 2.4.0 meant to be compatible with Julia 1.6?\n"},{"type":"text","text":"\n"},{"type":"text","text":"The compat entry is "},{"type":"text","text":"julia = \"~1.5\"","style":{"code":true}},{"type":"text","text":", which unfortunately excludes Julia 1.6."}]}]}]},{"client_msg_id":"6862B773-446C-48C4-BB16-6E8EA985263E","type":"message","text":"Oh I see... CUDA.jl 2.4.0 was the release for Julia 1.5","user":"U7THT3TM3","ts":"1610241927.202100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"s7ZHW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oh I see... CUDA.jl 2.4.0 was the release for Julia 1.5"}]}]}]},{"client_msg_id":"2B8E2C0A-E732-4363-8188-21F63C1FCE88","type":"message","text":"CUDA.jl 2.5.0 will be for Julia 1.6, but we are having trouble registering in because of Registrator","user":"U7THT3TM3","ts":"1610241947.202800","team":"T68168MUP","edited":{"user":"U7THT3TM3","ts":"1610241951.000000"},"blocks":[{"type":"rich_text","block_id":"WXQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CUDA.jl 2.5.0 will be for Julia 1.6, but we are having trouble registering in because of Registrator"}]}]}]},{"client_msg_id":"ab803024-8cdb-439c-bbe8-a6d28bbb6447","type":"message","text":"what docker or singularity container should I use? is there a 1.5 image?","user":"UAH43TMUN","ts":"1610346131.204800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wMl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"what docker or singularity container should I use? is there a 1.5 image?"}]}]}]},{"client_msg_id":"f07fd6a4-3821-4e5c-b009-4a2635a35489","type":"message","text":"Hi, I am trying to use `CUDA.cudaDeviceGetAttribute()` on `CUDA.cudaDevAttrMaxThreadsPerBlock` but how do I feed the first argument?\n\n```function cudaDeviceGetAttribute(value, attr, device)\n    ccall(\"extern cudaDeviceGetAttribute\", llvmcall, cudaError_t,\n          (Ptr{Cint}, cudaDeviceAttr, Cint),\n          value, attr, device)\nend```\ni.e. I'm wondering how to get a pointer for the first argument:\n`CUDA.cudaDeviceGetAttribute(????, CUDA.cudaDevAttrMaxThreadsPerBlock, Cint(0))`","user":"UG1KB4NLD","ts":"1610384752.207300","team":"T68168MUP","edited":{"user":"UG1KB4NLD","ts":"1610384973.000000"},"blocks":[{"type":"rich_text","block_id":"Us7n","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, I am trying to use "},{"type":"text","text":"CUDA.cudaDeviceGetAttribute()","style":{"code":true}},{"type":"text","text":" on "},{"type":"text","text":"CUDA.cudaDevAttrMaxThreadsPerBlock","style":{"code":true}},{"type":"text","text":" but how do I feed the first argument?\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"function cudaDeviceGetAttribute(value, attr, device)\n    ccall(\"extern cudaDeviceGetAttribute\", llvmcall, cudaError_t,\n          (Ptr{Cint}, cudaDeviceAttr, Cint),\n          value, attr, device)\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"i.e. I'm wondering how to get a pointer for the first argument:\n"},{"type":"text","text":"CUDA.cudaDeviceGetAttribute(????, CUDA.cudaDevAttrMaxThreadsPerBlock, Cint(0))","style":{"code":true}}]}]}],"thread_ts":"1610384752.207300","reply_count":15,"reply_users_count":3,"latest_reply":"1610387814.210400","reply_users":["U68A3ASP9","UG1KB4NLD","U67BJLYCS"],"subscribed":false},{"client_msg_id":"e8f569e6-af81-4788-9719-f86178c06ec4","type":"message","text":"To update the `CUdevice_attribute_enum`, I\n1. `cd /res/wrap`\n2. `julia`\n3. `] activated .`\n4. `include(\"wrap.jl\")`\n\nbut the output feels like it's not installing the latest version of everything, since it installs according to the local `Project.toml` and `Manifest.toml` files. Should I modify these two files (feels tidious) or update the packages later by `] up` later (but it is taking too much time)? Are there other methods?\n\nThe output now (BTW the downloads for both `CUDA` and `CUDA_full` were stuck at 17.5%. The first ctrl+c makes the download continue to run, the second ctrl+c makes it abort):\n```(@v1.7) pkg&gt; activate .\n  Activating environment at `~/.julia/dev/CUDA/res/wrap/Project.toml`\n\n\nStacktrace:\n [1] _require(pkg::Base.PkgId)\n   @ Base ./loading.jl:986\n [2] require(uuidkey::Base.PkgId)\n   @ Base ./loading.jl:910\n [3] require(into::Module, mod::Symbol)\n   @ Base ./loading.jl:897\n [4] include(fname::String)\n   @ Base.MainInclude ./client.jl:451\n [5] top-level scope\n   @ REPL[3]:1\nin expression starting at /home/qyu/.julia/dev/CUDA/res/wrap/wrap.jl:370\n\n(wrap) pkg&gt; instantiate\n   Installed Clang_jll ──────────── v9.0.1+4\n   Installed OrderedCollections ─── v1.3.0\n   Installed CUDNN_CUDA110_jll ──── v8.0.2+0\n   Installed CUDA_full_jll ──────── v11.0.3+0\n   Installed CUTENSOR_CUDA110_jll ─ v1.2.0+0\n   Installed CUDA_jll ───────────── v11.0.2+1\n   Installed Clang ──────────────── v0.12.0\n Downloading artifact: CUDA\n^C┌ Error: curl_multi_remove_handle: 1                    ]  17.5 %\n└ @ Downloads.Curl /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.7/Do  Downloaded artifact: CUDA\n  Downloaded artifact: CUDA\n  Downloaded artifact: CUTENSOR_CUDA110\n  Downloaded artifact: CUDNN_CUDA110\n  Downloaded artifact: Clang\n Downloading artifact: CUDA_full\n^C┌ Error: curl_multi_remove_handle: 1                    ]  17.5 %\n  Downloaded artifact: CUDA_full\n Downloading artifact: CUDA_full\njulia: src/unix/poll.c:117: uv_poll_start: Assertion `!(((handle)-&gt;flags &amp; (UV_HANDLE_CLOSING | UV_HANDLE_CLOSED)) != 0)' failed.\n\nsignal (6): Aborted```","user":"UG1KB4NLD","ts":"1610402295.215100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SXd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"To update the "},{"type":"text","text":"CUdevice_attribute_enum","style":{"code":true}},{"type":"text","text":", I\n1. "},{"type":"text","text":"cd /res/wrap","style":{"code":true}},{"type":"text","text":"\n2. "},{"type":"text","text":"julia","style":{"code":true}},{"type":"text","text":"\n3. "},{"type":"text","text":"] activated .","style":{"code":true}},{"type":"text","text":"\n4. "},{"type":"text","text":"include(\"wrap.jl\")","style":{"code":true}},{"type":"text","text":"\n\nbut the output feels like it's not installing the latest version of everything, since it installs according to the local "},{"type":"text","text":"Project.toml","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"Manifest.toml","style":{"code":true}},{"type":"text","text":" files. Should I modify these two files (feels tidious) or update the packages later by "},{"type":"text","text":"] up","style":{"code":true}},{"type":"text","text":" later (but it is taking too much time)? Are there other methods?\n\nThe output now (BTW the downloads for both "},{"type":"text","text":"CUDA","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"CUDA_full","style":{"code":true}},{"type":"text","text":" were stuck at 17.5%. The first ctrl+c makes the download continue to run, the second ctrl+c makes it abort):\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"(@v1.7) pkg> activate .\n  Activating environment at `~/.julia/dev/CUDA/res/wrap/Project.toml`\n\n\nStacktrace:\n [1] _require(pkg::Base.PkgId)\n   @ Base ./loading.jl:986\n [2] require(uuidkey::Base.PkgId)\n   @ Base ./loading.jl:910\n [3] require(into::Module, mod::Symbol)\n   @ Base ./loading.jl:897\n [4] include(fname::String)\n   @ Base.MainInclude ./client.jl:451\n [5] top-level scope\n   @ REPL[3]:1\nin expression starting at /home/qyu/.julia/dev/CUDA/res/wrap/wrap.jl:370\n\n(wrap) pkg> instantiate\n   Installed Clang_jll ──────────── v9.0.1+4\n   Installed OrderedCollections ─── v1.3.0\n   Installed CUDNN_CUDA110_jll ──── v8.0.2+0\n   Installed CUDA_full_jll ──────── v11.0.3+0\n   Installed CUTENSOR_CUDA110_jll ─ v1.2.0+0\n   Installed CUDA_jll ───────────── v11.0.2+1\n   Installed Clang ──────────────── v0.12.0\n Downloading artifact: CUDA\n^C┌ Error: curl_multi_remove_handle: 1                    ]  17.5 %\n└ @ Downloads.Curl /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.7/Do  Downloaded artifact: CUDA\n  Downloaded artifact: CUDA\n  Downloaded artifact: CUTENSOR_CUDA110\n  Downloaded artifact: CUDNN_CUDA110\n  Downloaded artifact: Clang\n Downloading artifact: CUDA_full\n^C┌ Error: curl_multi_remove_handle: 1                    ]  17.5 %\n  Downloaded artifact: CUDA_full\n Downloading artifact: CUDA_full\njulia: src/unix/poll.c:117: uv_poll_start: Assertion `!(((handle)->flags & (UV_HANDLE_CLOSING | UV_HANDLE_CLOSED)) != 0)' failed.\n\nsignal (6): Aborted"}]}]}]},{"client_msg_id":"84ac4e82-eb38-498c-ab50-a6942e8e31c5","type":"message","text":"Is there a way to get the element out of a dimension-0 CuArray without triggering a scalar indexing warning/error? Eg\n```julia&gt; CUDA.allowscalar(false)\n\njulia&gt; x = CuArray{Float64,0}(undef)\n 0-dimensional CuArray{Float64,0}:\n0.0\n\njulia&gt; x[1] # error\n\njulia&gt; x[] # error```\n","user":"UUMJUCYRK","ts":"1610413181.218700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"R/t=I","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way to get the element out of a dimension-0 CuArray without triggering a scalar indexing warning/error? Eg\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> CUDA.allowscalar(false)\n\njulia> x = CuArray{Float64,0}(undef)\n 0-dimensional CuArray{Float64,0}:\n0.0\n\njulia> x[1] # error\n\njulia> x[] # error"}]},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"3ab42ffd-4275-4298-ac9e-987d906760ee","type":"message","text":"Alternatively, some way to sum over specified dims of an array and get back a scalar if they happen to be _all_ of the dims? I was trying,\n```julia&gt; x = cu(rand(2,2));\n\njulia&gt; dropdims(sum(x, dims=(1,2)), dims=(1,2))\n 0-dimensional CuArray{Float32,0}:\n2.363763```\nObviously you can do `sum(x)` but here I need to specify the dims, just that sometimes its all of them and in those cases I want the scalar","user":"UUMJUCYRK","ts":"1610413442.220300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"urx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Alternatively, some way to sum over specified dims of an array and get back a scalar if they happen to be "},{"type":"text","text":"all ","style":{"italic":true}},{"type":"text","text":"of the dims? I was trying,\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> x = cu(rand(2,2));\n\njulia> dropdims(sum(x, dims=(1,2)), dims=(1,2))\n 0-dimensional CuArray{Float32,0}:\n2.363763"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Obviously you can do "},{"type":"text","text":"sum(x)","style":{"code":true}},{"type":"text","text":" but here I need to specify the dims, just that sometimes its all of them and in those cases I want the scalar"}]}]}]},{"client_msg_id":"33edf21d-639a-4dd4-a417-ced4ea2fbfed","type":"message","text":"Ah.. `CUDA.@allowscalar`","user":"UUMJUCYRK","ts":"1610416650.220600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ErB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah.. "},{"type":"text","text":"CUDA.@allowscalar","style":{"code":true}}]}]}]},{"client_msg_id":"7dcee73d-6e53-4b1e-9b7e-f865b855ba01","type":"message","text":"Is there a quick way to disable the GPU, so I can test my code without it? Perhaps through an environment variable?","user":"U01EK81V5GF","ts":"1610454806.225000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Uyw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a quick way to disable the GPU, so I can test my code without it? Perhaps through an environment variable?"}]}]}]},{"client_msg_id":"bd2837af-2faf-4aad-8565-73939879e754","type":"message","text":"`CUDA_VISIBLE_DEVICES=-1`","user":"U68A3ASP9","ts":"1610455600.225300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oAk6p","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CUDA_VISIBLE_DEVICES=-1","style":{"code":true}}]}]}]},{"client_msg_id":"c074bd3f-07b3-4011-accb-355371ff96a2","type":"message","text":"Weird question: do you need to have the GPU available when you download the CUDA artifacts?","user":"U7THT3TM3","ts":"1610522895.226600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"f7Vd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Weird question: do you need to have the GPU available when you download the CUDA artifacts?"}]}]}]},{"client_msg_id":"976610be-5bb3-44b8-8216-b386595ebc12","type":"message","text":"I'm on a cluster, so if the answer is no, I can just do `CUDA.versioninfo()` on a login node","user":"U7THT3TM3","ts":"1610522920.227100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Lrf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm on a cluster, so if the answer is no, I can just do "},{"type":"text","text":"CUDA.versioninfo()","style":{"code":true}},{"type":"text","text":" on a login node"}]}]}]},{"client_msg_id":"e076f612-d4d7-4403-b854-b70d38baf1aa","type":"message","text":"But if the answer is yes, I need to `salloc` to get on a GPU node first","user":"U7THT3TM3","ts":"1610522943.227800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"heXc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But if the answer is yes, I need to "},{"type":"text","text":"salloc","style":{"code":true}},{"type":"text","text":" to get on a GPU node first"}]}]}]},{"client_msg_id":"cb7a2005-c50e-457b-b7a8-80db601296b6","type":"message","text":"yes, as it needs to know which version to download, which depends on some results aquired from the CUDA driver","user":"U68A3ASP9","ts":"1610523137.228600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9Rqm","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yes, as it needs to know which version to download, which depends on some results aquired from the CUDA driver"}]}]}],"reactions":[{"name":"+1","users":["U7THT3TM3"],"count":1}]},{"client_msg_id":"46ef9c50-ba21-49ea-bf1e-1dd0b7bdf81f","type":"message","text":"technically that needs `libcuda` and not strictly a GPU, but the difference isn't implemented right now (and `libcuda` might fail to init if you don't have a GPU)","user":"U68A3ASP9","ts":"1610523171.229800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ETC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"technically that needs "},{"type":"text","text":"libcuda","style":{"code":true}},{"type":"text","text":" and not strictly a GPU, but the difference isn't implemented right now (and "},{"type":"text","text":"libcuda","style":{"code":true}},{"type":"text","text":" might fail to init if you don't have a GPU)"}]}]}]},{"client_msg_id":"f9ba2c69-61ec-4ee2-b704-357477ec7dad","type":"message","text":"note that on a cluster you may need to use the local CUDA installation, in case it is specifically optimized or tested for your platform","user":"U68A3ASP9","ts":"1610523219.231400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ENtag","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"note that on a cluster you may need to use the local CUDA installation, in case it is specifically optimized or tested for your platform"}]}]}]},{"client_msg_id":"04cf7810-1280-40e0-a44f-2056b161dd83","type":"message","text":"Follow-ip question: I have a different system (not the cluster) in which I have to build a Docker or Singularity image on a separate build server, and then transfer the image to the main system. The build server has Internet access; the main system does not. So I can't do the artifact downloading on the main system. Is there a way to just download \"all of the artifacts\" when building the Docker image on the build server? I don't mind if the image is really large; disk space is not an issue.","user":"U7THT3TM3","ts":"1610523590.234100","team":"T68168MUP","edited":{"user":"U7THT3TM3","ts":"1610523599.000000"},"blocks":[{"type":"rich_text","block_id":"qg/x","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Follow-ip question: I have a different system (not the cluster) in which I have to build a Docker or Singularity image on a separate build server, and then transfer the image to the main system. The build server has Internet access; the main system does not. So I can't do the artifact downloading on the main system. Is there a way to just download \"all of the artifacts\" when building the Docker image on the build server? I don't mind if the image is really large; disk space is not an issue."}]}]}]},{"client_msg_id":"b127e781-3b8c-4e68-8ea6-cf102ca5d9e7","type":"message","text":"You could do what PackageCompiler does -- download them all","user":"U68A3ASP9","ts":"1610523942.234900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"8mJZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You could do what PackageCompiler does -- download them all"}]}]}]},{"client_msg_id":"09565ee6-1776-4745-9bc4-3181317fae72","type":"message","text":"of course, if you know which driver version your main system has, you can download the appropriate artifact, but CUDA.jl doesn't have functionality to control that (it does have `JULIA_CUDA_VERSION` to force the CUDA toolkit version, and thus the artifact that will be downloaded, but for it to successfully initialize you might still need a working `libcuda` on your build system)","user":"U68A3ASP9","ts":"1610524020.236400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dni+T","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"of course, if you know which driver version your main system has, you can download the appropriate artifact, but CUDA.jl doesn't have functionality to control that (it does have "},{"type":"text","text":"JULIA_CUDA_VERSION","style":{"code":true}},{"type":"text","text":" to force the CUDA toolkit version, and thus the artifact that will be downloaded, but for it to successfully initialize you might still need a working "},{"type":"text","text":"libcuda","style":{"code":true}},{"type":"text","text":" on your build system)"}]}]}]},{"client_msg_id":"3cbc2cfa-f5be-4769-bd00-b0c42da138cd","type":"message","text":"Is `argmax` not allowed on `CuArray`s?\n","user":"UH9KWTTD3","ts":"1610640747.000800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3=cv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is "},{"type":"text","text":"argmax","style":{"code":true}},{"type":"text","text":" not allowed on `CuArray`s?\n"}]}]}],"thread_ts":"1610640747.000800","reply_count":1,"reply_users_count":1,"latest_reply":"1610640964.001000","reply_users":["UH9KWTTD3"],"subscribed":false},{"client_msg_id":"3cbc2cfa-f5be-4769-bd00-b0c42da138cd","type":"message","text":"```julia&gt; x = gpu([true false; false true])\n2×2 CuArray{Bool,2}:\n 1  0\n 0  1\n\njulia&gt; argmax(x)\nERROR: InvalidIRError: compiling kernel partial_mapreduce_grid(typeof(identity), CUDA.var\"#f#892\"{typeof(CUDA.max),typeof(&gt;)}, Tuple{Bool,CartesianIndex{2}}, CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, Val{false}, CuDeviceArray{Tuple{Bool,CartesianIndex{2}},3,1}, Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(tuple),Tuple{CuDeviceArray{Bool,2,1},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}}}) resulted in invalid LLVM IR\nReason: unsupported dynamic function invocation (call to min)\nStacktrace:\n [1] f at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/indexing.jl:126\n [2] multiple call sites at unknown:0\nStacktrace:\n [1] check_ir(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}, ::LLVM.Module) at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/validation.jl:123\n [2] macro expansion at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:239 [inlined]\n [3] macro expansion at /home/daruwalla/.julia/packages/TimerOutputs/ZmKD7/src/TimerOutput.jl:206 [inlined]\n [4] codegen(::Symbol, ::GPUCompiler.CompilerJob; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:237\n [5] compile(::Symbol, ::GPUCompiler.CompilerJob; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:39\n [6] compile at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:35 [inlined]\n [7] cufunction_compile(::GPUCompiler.FunctionSpec; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/compiler/execution.jl:302\n [8] cufunction_compile(::GPUCompiler.FunctionSpec) at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/compiler/execution.jl:297\n [9] check_cache(::Dict{UInt64,Any}, ::Any, ::Any, ::GPUCompiler.FunctionSpec{typeof(CUDA.partial_mapreduce_grid),Tuple{typeof(identity),CUDA.var\"#f#892\"{typeof(CUDA.max),typeof(&gt;)},Tuple{Bool,CartesianIndex{2}},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}},Val{false},CuDeviceArray{Tuple{Bool,CartesianIndex{2}},3,1},Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(tuple),Tuple{CuDeviceArray{Bool,2,1},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/cache.jl:40\n [10] partial_mapreduce_grid at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/mapreduce.jl:87 [inlined]\n [11] cached_compilation at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/cache.jl:65 [inlined]\n [12] cufunction(::typeof(CUDA.partial_mapreduce_grid), ::Type{Tuple{typeof(identity),CUDA.var\"#f#892\"{typeof(CUDA.max),typeof(&gt;)},Tuple{Bool,CartesianIndex{2}},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}},Val{false},CuDeviceArray{Tuple{Bool,CartesianIndex{2}},3,1},Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(tuple),Tuple{CuDeviceArray{Bool,2,1},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/compiler/execution.jl:289\n [13] cufunction at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/compiler/execution.jl:286 [inlined]\n [14] macro expansion at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/compiler/execution.jl:100 [inlined]\n [15] mapreducedim!(::typeof(identity), ::CUDA.var\"#f#892\"{typeof(CUDA.max),typeof(&gt;)}, ::CuArray{Tuple{Bool,CartesianIndex{2}},2}, ::Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(tuple),Tuple{CuArray{Bool,2},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}}}; init::Tuple{Bool,CartesianIndex{2}}) at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/mapreduce.jl:192\n [16] _mapreduce(::typeof(tuple), ::CUDA.var\"#f#892\"{typeof(CUDA.max),typeof(&gt;)}, ::CuArray{Bool,2}, ::CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}; dims::Colon, init::Tuple{Bool,CartesianIndex{2}}) at /home/daruwalla/.julia/packages/GPUArrays/WV76E/src/host/mapreduce.jl:62\n [17] #mapreduce#15 at /home/daruwalla/.julia/packages/GPUArrays/WV76E/src/host/mapreduce.jl:28 [inlined]\n [18] #findminmax#889 at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/indexing.jl:133 [inlined]\n [19] #findmax#895 at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/indexing.jl:143 [inlined]\n [20] #argmax#656 at ./reducedim.jl:952 [inlined]\n [21] argmax(::CuArray{Bool,2}) at ./reducedim.jl:952\n [22] top-level scope at REPL[47]:1```","user":"UH9KWTTD3","ts":"1610640747.000900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3=cv-rU1kS","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> x = gpu([true false; false true])\n2×2 CuArray{Bool,2}:\n 1  0\n 0  1\n\njulia> argmax(x)\nERROR: InvalidIRError: compiling kernel partial_mapreduce_grid(typeof(identity), CUDA.var\"#f#892\"{typeof(CUDA.max),typeof(>)}, Tuple{Bool,CartesianIndex{2}}, CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}, Val{false}, CuDeviceArray{Tuple{Bool,CartesianIndex{2}},3,1}, Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(tuple),Tuple{CuDeviceArray{Bool,2,1},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}}}) resulted in invalid LLVM IR\nReason: unsupported dynamic function invocation (call to min)\nStacktrace:\n [1] f at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/indexing.jl:126\n [2] multiple call sites at unknown:0\nStacktrace:\n [1] check_ir(::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget,CUDA.CUDACompilerParams}, ::LLVM.Module) at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/validation.jl:123\n [2] macro expansion at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:239 [inlined]\n [3] macro expansion at /home/daruwalla/.julia/packages/TimerOutputs/ZmKD7/src/TimerOutput.jl:206 [inlined]\n [4] codegen(::Symbol, ::GPUCompiler.CompilerJob; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:237\n [5] compile(::Symbol, ::GPUCompiler.CompilerJob; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:39\n [6] compile at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:35 [inlined]\n [7] cufunction_compile(::GPUCompiler.FunctionSpec; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/compiler/execution.jl:302\n [8] cufunction_compile(::GPUCompiler.FunctionSpec) at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/compiler/execution.jl:297\n [9] check_cache(::Dict{UInt64,Any}, ::Any, ::Any, ::GPUCompiler.FunctionSpec{typeof(CUDA.partial_mapreduce_grid),Tuple{typeof(identity),CUDA.var\"#f#892\"{typeof(CUDA.max),typeof(>)},Tuple{Bool,CartesianIndex{2}},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}},Val{false},CuDeviceArray{Tuple{Bool,CartesianIndex{2}},3,1},Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(tuple),Tuple{CuDeviceArray{Bool,2,1},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}}}}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/cache.jl:40\n [10] partial_mapreduce_grid at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/mapreduce.jl:87 [inlined]\n [11] cached_compilation at /home/daruwalla/.julia/packages/GPUCompiler/uTpNx/src/cache.jl:65 [inlined]\n [12] cufunction(::typeof(CUDA.partial_mapreduce_grid), ::Type{Tuple{typeof(identity),CUDA.var\"#f#892\"{typeof(CUDA.max),typeof(>)},Tuple{Bool,CartesianIndex{2}},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}},Val{false},CuDeviceArray{Tuple{Bool,CartesianIndex{2}},3,1},Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(tuple),Tuple{CuDeviceArray{Bool,2,1},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}}}}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/compiler/execution.jl:289\n [13] cufunction at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/compiler/execution.jl:286 [inlined]\n [14] macro expansion at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/compiler/execution.jl:100 [inlined]\n [15] mapreducedim!(::typeof(identity), ::CUDA.var\"#f#892\"{typeof(CUDA.max),typeof(>)}, ::CuArray{Tuple{Bool,CartesianIndex{2}},2}, ::Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2},Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},typeof(tuple),Tuple{CuArray{Bool,2},CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}}}; init::Tuple{Bool,CartesianIndex{2}}) at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/mapreduce.jl:192\n [16] _mapreduce(::typeof(tuple), ::CUDA.var\"#f#892\"{typeof(CUDA.max),typeof(>)}, ::CuArray{Bool,2}, ::CartesianIndices{2,Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}}; dims::Colon, init::Tuple{Bool,CartesianIndex{2}}) at /home/daruwalla/.julia/packages/GPUArrays/WV76E/src/host/mapreduce.jl:62\n [17] #mapreduce#15 at /home/daruwalla/.julia/packages/GPUArrays/WV76E/src/host/mapreduce.jl:28 [inlined]\n [18] #findminmax#889 at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/indexing.jl:133 [inlined]\n [19] #findmax#895 at /home/daruwalla/.julia/packages/CUDA/FlHUF/src/indexing.jl:143 [inlined]\n [20] #argmax#656 at ./reducedim.jl:952 [inlined]\n [21] argmax(::CuArray{Bool,2}) at ./reducedim.jl:952\n [22] top-level scope at REPL[47]:1"}]}]}]},{"client_msg_id":"3c7bc7ad-03ec-416a-8dc7-e79cf10b70ce","type":"message","text":"Is it a current limitation of Zygote+CUDA that this triggers scalar indexing, and/or is there any way to make this work?\n```julia&gt; foo(x,A) = x&gt;0.5 ? A : 0\nfoo (generic function with 2 methods)\n\njulia&gt; arr = cu(rand(10));\n\njulia&gt; gradient(A -&gt; sum(foo.(arr,A)), 1.)\nERROR: scalar getindex is disallowed```\n","user":"UUMJUCYRK","ts":"1610945972.017600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"i61","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is it a current limitation of Zygote+CUDA that this triggers scalar indexing, and/or is there any way to make this work?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> foo(x,A) = x>0.5 ? A : 0\nfoo (generic function with 2 methods)\n\njulia> arr = cu(rand(10));\n\njulia> gradient(A -> sum(foo.(arr,A)), 1.)\nERROR: scalar getindex is disallowed"}]},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"b3b6f61c-aad0-41c3-877b-1b547e9d497e","type":"message","text":"Ah.. if you write it so that its type-stable when a ForwardDiff.Dual goes through it, it works:\n```foo(x::T, A::T) where {T} = x&gt;0.5 ? A : zero(T)\narr = cu(rand(10));\ngradient(A -&gt; sum(foo.(arr,A)), 1f0)```\n","user":"UUMJUCYRK","ts":"1610966325.018700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"yJt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah.. if you write it so that its type-stable when a ForwardDiff.Dual goes through it, it works:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"foo(x::T, A::T) where {T} = x>0.5 ? A : zero(T)\narr = cu(rand(10));\ngradient(A -> sum(foo.(arr,A)), 1f0)"}]},{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"2678ec7f-97f4-4a04-9d07-22dbbe28de75","type":"message","text":"How do I write a CUDA.jl kernel, that returns a scalar? I tried the following, which does not work:\n```module MWE\nusing CUDA\nfunction give_1_kernel!(out)\n    out[] = 1f0\n    return\nend\nfunction give_1()\n    out = CuRef(0f0)\n    @cuda give_1_kernel!(out)\n    out\nend\n\ngive_1()\n\nend\nGPU compilation of kernel give_1_kernel!(CUDA.CuRefArray{Float32,CuArray{Float32,1}}) failed\nKernelError: kernel returns a value of type `Union{}`\n\nMake sure your kernel function ends in `return`, `return nothing` or `nothing`.\nIf the returned value is of type `Union{}`, your Julia code probably throws an exception.\nInspect the code with `@device_code_warntype` for more details.```","user":"UKZ78GR8Q","ts":"1611041950.021600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j/q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How do I write a CUDA.jl kernel, that returns a scalar? I tried the following, which does not work:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"module MWE\nusing CUDA\nfunction give_1_kernel!(out)\n    out[] = 1f0\n    return\nend\nfunction give_1()\n    out = CuRef(0f0)\n    @cuda give_1_kernel!(out)\n    out\nend\n\ngive_1()\n\nend\nGPU compilation of kernel give_1_kernel!(CUDA.CuRefArray{Float32,CuArray{Float32,1}}) failed\nKernelError: kernel returns a value of type `Union{}`\n\nMake sure your kernel function ends in `return`, `return nothing` or `nothing`.\nIf the returned value is of type `Union{}`, your Julia code probably throws an exception.\nInspect the code with `@device_code_warntype` for more details."}]}]}],"thread_ts":"1611041950.021600","reply_count":13,"reply_users_count":3,"latest_reply":"1611070539.025100","reply_users":["U01C10FSAM8","UKZ78GR8Q","U68A3ASP9"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"can anyone confirm this?","user":"U01C10FSAM8","ts":"1611058650.024100","thread_ts":"1611041950.021600","root":{"client_msg_id":"2678ec7f-97f4-4a04-9d07-22dbbe28de75","type":"message","text":"How do I write a CUDA.jl kernel, that returns a scalar? I tried the following, which does not work:\n```module MWE\nusing CUDA\nfunction give_1_kernel!(out)\n    out[] = 1f0\n    return\nend\nfunction give_1()\n    out = CuRef(0f0)\n    @cuda give_1_kernel!(out)\n    out\nend\n\ngive_1()\n\nend\nGPU compilation of kernel give_1_kernel!(CUDA.CuRefArray{Float32,CuArray{Float32,1}}) failed\nKernelError: kernel returns a value of type `Union{}`\n\nMake sure your kernel function ends in `return`, `return nothing` or `nothing`.\nIf the returned value is of type `Union{}`, your Julia code probably throws an exception.\nInspect the code with `@device_code_warntype` for more details.```","user":"UKZ78GR8Q","ts":"1611041950.021600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j/q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How do I write a CUDA.jl kernel, that returns a scalar? I tried the following, which does not work:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"module MWE\nusing CUDA\nfunction give_1_kernel!(out)\n    out[] = 1f0\n    return\nend\nfunction give_1()\n    out = CuRef(0f0)\n    @cuda give_1_kernel!(out)\n    out\nend\n\ngive_1()\n\nend\nGPU compilation of kernel give_1_kernel!(CUDA.CuRefArray{Float32,CuArray{Float32,1}}) failed\nKernelError: kernel returns a value of type `Union{}`\n\nMake sure your kernel function ends in `return`, `return nothing` or `nothing`.\nIf the returned value is of type `Union{}`, your Julia code probably throws an exception.\nInspect the code with `@device_code_warntype` for more details."}]}]}],"thread_ts":"1611041950.021600","reply_count":13,"reply_users_count":3,"latest_reply":"1611070539.025100","reply_users":["U01C10FSAM8","UKZ78GR8Q","U68A3ASP9"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"sHkyY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"can anyone confirm this?"}]}]}],"client_msg_id":"eff2177a-b1ad-45e5-9409-74195bdfba34","reactions":[{"name":"heart","users":["UKZ78GR8Q"],"count":1}]},{"client_msg_id":"49fdbfa2-5a3d-4cf2-8070-abeda8c5280b","type":"message","text":"Is there any general rule of thumb for choosing the number of workgroups/threads when using KernelAbstractions? If I have an O(n) reduction in the kernel, should I just set it to the number of available CUDA cores? If I set it to the length of the ndrange, will it overschedule? Or is the only way to find the sweet spot really just to benchmark it?","user":"UM30MT6RF","ts":"1611136960.035300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HfrQc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there any general rule of thumb for choosing the number of workgroups/threads when using KernelAbstractions? If I have an O(n) reduction in the kernel, should I just set it to the number of available CUDA cores? If I set it to the length of the ndrange, will it overschedule? Or is the only way to find the sweet spot really just to benchmark it?"}]}]}]},{"client_msg_id":"985d2002-b5e6-422d-960d-b6fd76735768","type":"message","text":"the underlying APIs have tools to calculate the ideal workgroups/threads, e.g. CUDA has the occupancy API, oneAPI has a suggest_groupsize, etc","user":"U68A3ASP9","ts":"1611137302.035800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"al3A","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"the underlying APIs have tools to calculate the ideal workgroups/threads, e.g. CUDA has the occupancy API, oneAPI has a suggest_groupsize, etc"}]}]}]},{"client_msg_id":"bb7bb7d1-a85f-40f1-9820-c5c3cd0558e2","type":"message","text":"you generally can't set it to the max because, at least in the case of CUDA, it is restricted by other factors (e.g. the device max threads may be 1024, but if your kernel uses a lot of registers or shared memory you may not be able to launch as many)","user":"U68A3ASP9","ts":"1611137339.036600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VsqbN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you generally can't set it to the max because, at least in the case of CUDA, it is restricted by other factors (e.g. the device max threads may be 1024, but if your kernel uses a lot of registers or shared memory you may not be able to launch as many)"}]}]}]},{"client_msg_id":"8b99904d-7682-45f4-9a18-d9d7fedacc39","type":"message","text":"Thanks for the answer! Do you have a link where I can find out more about the occupancy API?","user":"UM30MT6RF","ts":"1611137462.037400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UMwi1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for the answer! Do you have a link where I can find out more about the occupancy API?"}]}]}],"thread_ts":"1611137462.037400","reply_count":1,"reply_users_count":1,"latest_reply":"1611137655.038100","reply_users":["U68A3ASP9"],"subscribed":false},{"client_msg_id":"4502901a-3211-4b3e-abe8-48d9abd70c8f","type":"message","text":"<https://github.com/JuliaGPU/CUDA.jl/blob/35150564fac7848cdf4fc3b7e74d3cca201eb442/src/indexing.jl#L32-L36>","user":"U68A3ASP9","ts":"1611137615.037600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Cgv","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/blob/35150564fac7848cdf4fc3b7e74d3cca201eb442/src/indexing.jl#L32-L36"}]}]}],"thread_ts":"1611137615.037600","reply_count":1,"reply_users_count":1,"latest_reply":"1611137831.039100","reply_users":["U68A3ASP9"],"subscribed":false},{"client_msg_id":"c6ba9a7c-c716-4f87-b6ee-9936c85d6489","type":"message","text":"that's a pretty new interface though, it used to be a `config=...` argument to `@cuda`","user":"U68A3ASP9","ts":"1611137640.038000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"wl2r","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"that's a pretty new interface though, it used to be a "},{"type":"text","text":"config=...","style":{"code":true}},{"type":"text","text":" argument to "},{"type":"text","text":"@cuda","style":{"code":true}}]}]}]},{"client_msg_id":"11e85465-50c2-46d0-be29-139e096ad689","type":"message","text":"Ah, cool! So I should be able to call `launch_configuration` on my `KernelAbstractions.@kernel` as well?","user":"UM30MT6RF","ts":"1611137815.039000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IJj4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah, cool! So I should be able to call "},{"type":"text","text":"launch_configuration","style":{"code":true}},{"type":"text","text":" on my "},{"type":"text","text":"KernelAbstractions.@kernel","style":{"code":true}},{"type":"text","text":" as well?"}]}]}]},{"client_msg_id":"f56e783f-d749-4040-900f-c984494dc938","type":"message","text":"I don't think KA just returns the underlying CUDA kernel, so no. The library should probably be adjusted to retain that info, but right now I'm not sure what the best approach would be. <@U67BJLYCS> should be able to elaborate in a couple of hours","user":"U68A3ASP9","ts":"1611137911.040500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oD9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I don't think KA just returns the underlying CUDA kernel, so no. The library should probably be adjusted to retain that info, but right now I'm not sure what the best approach would be. "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":" should be able to elaborate in a couple of hours"}]}]}],"reactions":[{"name":"+1","users":["UM30MT6RF"],"count":1}]},{"client_msg_id":"db7cb2ca-fdf8-46f6-8e01-b8655b956a38","type":"message","text":"Yeah KA has been designed more around static workgroup sizes","user":"U67BJLYCS","ts":"1611146437.041500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"gHr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah KA has been designed more around static workgroup sizes"}]}]}]},{"client_msg_id":"d5a4ccca-c094-4e41-a9d7-3ef15d1414b6","type":"message","text":"<https://github.com/JuliaGPU/KernelAbstractions.jl/blob/ad2cd8c388143c0333802525063ce6d25ffbfc59/src/backends/cuda.jl#L150|https://github.com/JuliaGPU/KernelAbstractions.jl/blob/ad2cd8c388143c0333802525063ce6d25ffbfc59/src/backends/cuda.jl#L150>","user":"U67BJLYCS","ts":"1611146438.041700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"96O+2","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/JuliaGPU/KernelAbstractions.jl/blob/ad2cd8c388143c0333802525063ce6d25ffbfc59/src/backends/cuda.jl#L150","text":"https://github.com/JuliaGPU/KernelAbstractions.jl/blob/ad2cd8c388143c0333802525063ce6d25ffbfc59/src/backends/cuda.jl#L150"}]}]}]},{"client_msg_id":"4011f313-fb2f-470f-931c-9b095a815dde","type":"message","text":"There is a chicken and an egg problem here","user":"U67BJLYCS","ts":"1611146496.042400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"dJ/s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"There is a chicken and an egg problem here"}]}]}]},{"client_msg_id":"373dea20-6b89-49f9-9317-dbffc135e28a","type":"message","text":"And the question; what is the right way of selecting the group size for the CPU","user":"U67BJLYCS","ts":"1611146571.042700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kf7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And the question; what is the right way of selecting the group size for the CPU"}]}]}],"thread_ts":"1611146571.042700","reply_count":1,"reply_users_count":1,"latest_reply":"1611311887.010400","reply_users":["UM30MT6RF"],"subscribed":false},{"client_msg_id":"c7ce31ce-58e9-4930-8c97-53f8375a0afb","type":"message","text":"Which I don't have a good answer too","user":"U67BJLYCS","ts":"1611146585.043100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Najb8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Which I don't have a good answer too"}]}]}]},{"client_msg_id":"8db7de75-365e-4e8e-88a5-885d15b5ad4c","type":"message","text":"But yeah we could lift the cuda kernel creation to instantiation time","user":"U67BJLYCS","ts":"1611146644.043900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"EsIr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But yeah we could lift the cuda kernel creation to instantiation time"}]}]}]},{"client_msg_id":"c4a61047-8f24-40c9-a8f4-7e11213c6aa8","type":"message","text":"And then add a similar api","user":"U67BJLYCS","ts":"1611146656.044300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4MsQR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And then add a similar api"}]}]}]},{"client_msg_id":"2214a435-8c74-42f9-af95-1e3177648bf1","type":"message","text":"Ah, I see, so you typically just tune that by hand? It would be really nice if Tullio.jl could just choose a sensible default for you. Right now, it defaults to 256, which I am not sure is always a good idea.","user":"UM30MT6RF","ts":"1611147591.046400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3WPGY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah, I see, so you typically just tune that by hand? It would be really nice if Tullio.jl could just choose a sensible default for you. Right now, it defaults to 256, which I am not sure is always a good idea."}]}]}]},{"client_msg_id":"71d911ef-86c7-4da0-91b6-11113ef57c3e","type":"message","text":"Yeah in CLIMA we have a algorithm design that limits the size","user":"U67BJLYCS","ts":"1611152528.047600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"OP4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah in CLIMA we have a algorithm design that limits the size"}]}]}]},{"client_msg_id":"21d131b6-4533-4345-9c8b-facdd62ff8a3","type":"message","text":"With the new CUDA non spawning kernel this shouldn't be too hard to add","user":"U67BJLYCS","ts":"1611152561.048400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2aA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"With the new CUDA non spawning kernel this shouldn't be too hard to add"}]}]}]},{"client_msg_id":"54d9eb51-8e0d-4b70-a6e5-05a5aa42f52b","type":"message","text":"I welcome any contributions :)","user":"U67BJLYCS","ts":"1611152577.048800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Wnl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I welcome any contributions :)"}]}]}],"thread_ts":"1611152577.048800","reply_count":1,"reply_users_count":1,"latest_reply":"1611314114.010600","reply_users":["UM30MT6RF"],"subscribed":false},{"client_msg_id":"890e498a-71dd-4c48-acbf-0e87cde60450","type":"message","text":"Is there an ideal way of exiting Julia, especially when using CUDA.jl?\nI have been encountering this problem since the past 3 days. Whenever I load CUDA.jl into the Julia REPL on a fresh boot, it runs perfectly and I can initialize arrays using `y=CUDA.fill(1.0f0,1000)` with no problem whatsoever.\nBut then when I quit Julia and start Julia again, load the package, and try to initialize another array, it gives me an error which is seemingly popular-\n```ERROR: CUDA.jl did not successfully initialize, and is not usable.\nIf you did not see any other error message, try again in a new session\nwith the JULIA_DEBUG environment variable set to 'CUDA'.```","user":"UTDSTSANP","ts":"1611163655.051900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eZW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there an ideal way of exiting Julia, especially when using CUDA.jl?\nI have been encountering this problem since the past 3 days. Whenever I load CUDA.jl into the Julia REPL on a fresh boot, it runs perfectly and I can initialize arrays using "},{"type":"text","text":"y=CUDA.fill(1.0f0,1000)","style":{"code":true}},{"type":"text","text":" with no problem whatsoever.\nBut then when I quit Julia and start Julia again, load the package, and try to initialize another array, it gives me an error which is seemingly popular-\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: CUDA.jl did not successfully initialize, and is not usable.\nIf you did not see any other error message, try again in a new session\nwith the JULIA_DEBUG environment variable set to 'CUDA'."}]}]}],"thread_ts":"1611163655.051900","reply_count":5,"reply_users_count":2,"latest_reply":"1611235330.001700","reply_users":["UTDSTSANP","U68A3ASP9"],"subscribed":false},{"client_msg_id":"cca8e87b-2874-48d1-a97c-e6898eb25a81","type":"message","text":"I'm wanting to run a RL algorithm (PPO), where the data gathering step is run on multiple processes to speed it up. However, if I try to run the inference step for a batch of states (ie `actions = model[states]` ) on the gpu, I get this error:\n```CUBLASError: the GPU program failed to execute (code 13, CUBLAS_STATUS_EXECUTION_FAILED)```\nDoes anyone know the reason for this error, and how I could get around it? I have tried making a copy of the model for each process, but this doesn't seem to make a difference.","user":"UQR5DTD99","ts":"1611175170.054600","team":"T68168MUP","edited":{"user":"UQR5DTD99","ts":"1611175246.000000"},"blocks":[{"type":"rich_text","block_id":"EbW9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm wanting to run a RL algorithm (PPO), where the data gathering step is run on multiple processes to speed it up. However, if I try to run the inference step for a batch of states (ie "},{"type":"text","text":"actions = model[states]","style":{"code":true}},{"type":"text","text":" ) on the gpu, I get this error:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"CUBLASError: the GPU program failed to execute (code 13, CUBLAS_STATUS_EXECUTION_FAILED)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Does anyone know the reason for this error, and how I could get around it? I have tried making a copy of the model for each process, but this doesn't seem to make a difference."}]}]}],"thread_ts":"1611175170.054600","reply_count":8,"reply_users_count":3,"latest_reply":"1611252041.001900","reply_users":["U68A3ASP9","UQR5DTD99","UC4QQPG4A"],"subscribed":false},{"client_msg_id":"f66814ca-4c93-44ec-ba7b-2ac3e3931e65","type":"message","text":"How stable is CUDA#master supposed to be against 1.6-beta1 ? Should I be reporting issues I come across, or is it too early / already known?","user":"UUMJUCYRK","ts":"1611192563.000800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Eapc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How stable is CUDA#master supposed to be against 1.6-beta1 ? Should I be reporting issues I come across, or is it too early / already known?"}]}]}],"thread_ts":"1611192563.000800","reply_count":3,"reply_users_count":2,"latest_reply":"1611222495.001500","reply_users":["U68A3ASP9","UUMJUCYRK"],"subscribed":false},{"client_msg_id":"29bec3c6-9424-411f-b3e7-98297e9849d2","type":"message","text":"Is it normal for\n\n```z = @view u[1:1, :]```\nwhere u is a CuArray to not return a CuArray ?\n\nAnd therefore breaks everything later with\n\n```ERROR: ArgumentError: cannot take the CPU address of a CUDA.CuArray{Float32,2}```\n?","user":"UKJSNT1QR","ts":"1611264202.003100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"XLhE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is it normal for\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"z = @view u[1:1, :]"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nwhere u is a CuArray to not return a CuArray ?\n\nAnd therefore breaks everything later with\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: ArgumentError: cannot take the CPU address of a CUDA.CuArray{Float32,2}"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"?"}]}]}]},{"client_msg_id":"756c1273-5cb3-415b-95e3-12adc09f1a49","type":"message","text":"It returns a\n```view(::CUDA.CuArray{Float32,2}, 1:1, :)```\ninstead.\n\nAnd yes I know u[1:1, :] is ugly, but a man gotta do what a man gotta do :slightly_smiling_face:","user":"UKJSNT1QR","ts":"1611264297.004100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"17QZ4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It returns a\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"view(::CUDA.CuArray{Float32,2}, 1:1, :)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"instead.\n\nAnd yes I know u[1:1, :] is ugly, but a man gotta do what a man gotta do "},{"type":"emoji","name":"slightly_smiling_face"}]}]}]},{"client_msg_id":"80178e40-b38b-48dc-82c1-f14b3cf688b0","type":"message","text":"you generally shouldn't ever expect `view` to return a CuArray, but for inference performance reasons we represent contiguous views as CuArray instances (for now). if your view isn't contiguous, or CUDA.jl doesn't detect it as such, you'll just get a SubArray.","user":"U68A3ASP9","ts":"1611264961.005700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vSMDg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you generally shouldn't ever expect "},{"type":"text","text":"view","style":{"code":true}},{"type":"text","text":" to return a CuArray, but for inference performance reasons we represent contiguous views as CuArray instances (for now). if your view isn't contiguous, or CUDA.jl doesn't detect it as such, you'll just get a SubArray."}]}]}]},{"client_msg_id":"b5a5f813-7aa7-4806-9de8-b3e6b51daad1","type":"message","text":"and the \"therefor breaks everything\" is false. many kernels just work for SubArray{&lt;:CuArray}, and even some CUBLAS (e.a.) calls do too; but support for that (ie. for strided inputs) is fairly now and not available for all APIs.","user":"U68A3ASP9","ts":"1611265012.006700","team":"T68168MUP","edited":{"user":"U68A3ASP9","ts":"1611265020.000000"},"blocks":[{"type":"rich_text","block_id":"2BB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"and the \"therefor breaks everything\" is false. many kernels just work for SubArray{<:CuArray}, and even some CUBLAS (e.a.) calls do too; but support for that (ie. for strided inputs) is fairly now and not available for all APIs."}]}]}]},{"client_msg_id":"4c23d68d-5e60-48a9-823e-01aa23e78c68","type":"message","text":"I ran into a weird error while using <#C7T968HRU|diffeq-bridged> and CUDA\nTL;DR: sometimes solvers using ForwardDiff Duals error out when combined with CUDA. This doesn't happen for `Dense` with tanh nor `GRUCell`, it does however happen for `LSTMCell` and a custom layer that mimics the LSTM.\nImportantly it doesn't happen when evaluated on CPU\nMWE\n```using CUDA, Flux, OrdinaryDiffEq, DiffEqSensitivity, Random\nCUDA.allowscalar(false)\n\nRandom.seed!(0)\ntspan = Float32.((0, 100))\nbasic_tgrad(u,p,t) = zero(u)\n\nnhidden = 4\nnin = 2\nnn = Flux.LSTMCell(nin,nhidden) |&gt; gpu\np, re = Flux.destructure(nn)\n\nU = randn(2*nhidden) |&gt; cu\nwrap(u) = (u[1:nhidden],u[nhidden+1:2nhidden])\nunwrap(u) = cat(u[1]...,dims=1)\ndudt_(u,p,t) = re(p)(wrap(u), CUDA.zeros(nin) ) |&gt; unwrap\nff = ODEFunction{false}(dudt_,tgrad=basic_tgrad)\nprob = ODEProblem{false}(ff,U,tspan,p)\nsolve(prob,Rosenbrock23();sense=InterpolatingAdjoint(autojacvec=ZygoteVJP()))```\nError message:\n```ERROR: MethodError: no method matching unsafe_convert(::Type{Float64}, ::ForwardDiff.Dual{Nothing,Float64,8})```\nEnvironment info:\nFlux 0.11.2\nCUDA 2.4.0\nNNlib 0.7.12","user":"UPM0H43C7","ts":"1611281003.007900","team":"T68168MUP","edited":{"user":"UPM0H43C7","ts":"1611281216.000000"},"blocks":[{"type":"rich_text","block_id":"F0m5i","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I ran into a weird error while using "},{"type":"channel","channel_id":"C7T968HRU"},{"type":"text","text":" and CUDA\nTL;DR: sometimes solvers using ForwardDiff Duals error out when combined with CUDA. This doesn't happen for "},{"type":"text","text":"Dense","style":{"code":true}},{"type":"text","text":" with tanh nor "},{"type":"text","text":"GRUCell","style":{"code":true}},{"type":"text","text":", it does however happen for "},{"type":"text","text":"LSTMCell","style":{"code":true}},{"type":"text","text":" and a custom layer that mimics the LSTM.\nImportantly it doesn't happen when evaluated on CPU\nMWE\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using CUDA, Flux, OrdinaryDiffEq, DiffEqSensitivity, Random\nCUDA.allowscalar(false)\n\nRandom.seed!(0)\ntspan = Float32.((0, 100))\nbasic_tgrad(u,p,t) = zero(u)\n\nnhidden = 4\nnin = 2\nnn = Flux.LSTMCell(nin,nhidden) |> gpu\np, re = Flux.destructure(nn)\n\nU = randn(2*nhidden) |> cu\nwrap(u) = (u[1:nhidden],u[nhidden+1:2nhidden])\nunwrap(u) = cat(u[1]...,dims=1)\ndudt_(u,p,t) = re(p)(wrap(u), CUDA.zeros(nin) ) |> unwrap\nff = ODEFunction{false}(dudt_,tgrad=basic_tgrad)\nprob = ODEProblem{false}(ff,U,tspan,p)\nsolve(prob,Rosenbrock23();sense=InterpolatingAdjoint(autojacvec=ZygoteVJP()))"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Error message:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: MethodError: no method matching unsafe_convert(::Type{Float64}, ::ForwardDiff.Dual{Nothing,Float64,8})"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Environment info:\nFlux 0.11.2\nCUDA 2.4.0\nNNlib 0.7.12"}]}]}],"thread_ts":"1611281003.007900","reply_count":8,"reply_users_count":4,"latest_reply":"1611317979.010800","reply_users":["UPM0H43C7","U67BJLYCS","U69BL50BF","UC4QQPG4A"],"subscribed":false},{"client_msg_id":"d8ec5452-f348-4a54-b640-25975d3d7306","type":"message","text":"Tiny announcement of bindings to the NVIDIA AMGX library (multigrid solver) at <https://github.com/JuliaGPU/AMGX.jl>.","user":"U67D54KS8","ts":"1611308776.010200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"2r6","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Tiny announcement of bindings to the NVIDIA AMGX library (multigrid solver) at "},{"type":"link","url":"https://github.com/JuliaGPU/AMGX.jl"},{"type":"text","text":"."}]}]}],"reactions":[{"name":"100","users":["U68A3ASP9","U680THK2S","U017XL92LJG"],"count":3}]},{"client_msg_id":"6418a582-69f0-4909-9f0f-60c317293984","type":"message","text":"for those people using CUDA.jl with Julia tasks/threads: now would be a good time to test or comment on <https://github.com/JuliaGPU/CUDA.jl/pull/662>\nTL;DR: per-task streams and synchronization, which makes it possible to overlap GPU computations of individual tasks.","user":"U68A3ASP9","ts":"1611330728.012900","team":"T68168MUP","edited":{"user":"U68A3ASP9","ts":"1611330737.000000"},"blocks":[{"type":"rich_text","block_id":"wVV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"for those people using CUDA.jl with Julia tasks/threads: now would be a good time to test or comment on "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/pull/662"},{"type":"text","text":"\nTL;DR: per-task streams and synchronization, which makes it possible to overlap GPU computations of individual tasks."}]}]}],"thread_ts":"1611330728.012900","reply_count":2,"reply_users_count":2,"latest_reply":"1611386533.018500","reply_users":["U01DD7Z0D89","U68A3ASP9"],"subscribed":false,"reactions":[{"name":"party_wizard","users":["U67BJLYCS","ULD19UCPK","U017XL92LJG"],"count":3},{"name":"tada","users":["U01G39CC63F"],"count":1}]},{"client_msg_id":"f6633ac1-b5d2-4830-b55f-3f19ee0fa58a","type":"message","text":"I'm trying to write a kernel function in which I want to update columns of a pre-allocated array `result`  using  `ForwardDiff.jacobian!(result, f!, y, x)` , with some in-place function `f!` . However, I keep on getting the `unsupported dynamic function invocation` error message. I have been trying to search for a bug in my code but so far I haven't been able to fix it. So I was wondering if perhaps I'm trying to do something that's not supported :neutral_face:","user":"UCT34GL7M","ts":"1611331557.015800","team":"T68168MUP","edited":{"user":"UCT34GL7M","ts":"1611331581.000000"},"blocks":[{"type":"rich_text","block_id":"gtRL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm trying to write a kernel function in which I want to update columns of a pre-allocated array "},{"type":"text","text":"result","style":{"code":true}},{"type":"text","text":"  using  "},{"type":"text","text":"ForwardDiff.jacobian!(result, f!, y, x)","style":{"code":true}},{"type":"text","text":" , with some in-place function "},{"type":"text","text":"f!","style":{"code":true}},{"type":"text","text":" . However, I keep on getting the "},{"type":"text","text":"unsupported dynamic function invocation","style":{"code":true}},{"type":"text","text":" error message. I have been trying to search for a bug in my code but so far I haven't been able to fix it. So I was wondering if perhaps I'm trying to do something that's not supported "},{"type":"emoji","name":"neutral_face"}]}]}]},{"client_msg_id":"71646c46-df20-4855-adf0-e343356a2b8f","type":"message","text":"Use `@device_code_typed` to see what the compiler thinks about your code","user":"U67BJLYCS","ts":"1611332446.016500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"cfZn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Use "},{"type":"text","text":"@device_code_typed","style":{"code":true}},{"type":"text","text":" to see what the compiler thinks about your code"}]}]}]},{"client_msg_id":"ae3e70eb-b971-467d-a458-389204f80b6a","type":"message","text":"I get the same error when trying to do that. Here's a MWE (which I'm running on Julia-1.6 and CUDA#master)\n```using CUDA\nusing StaticArrays\nusing ForwardDiff\n\n# in-place function f! and its Jacobian\nf!(y, v) = y[:] .= v' * v\n∇f!(dy, y, v) = ForwardDiff.jacobian!(dy, f!, y, v)\n\n# cpu test\nv = @SVector rand(Float32, 3)\ny = zeros(Float32, 10);\ndy = zeros(Float32, 10, length(v));\nf!(y,v);\n∇f!(dy, y, v);\n\n# gpu functions\nfunction gpu_f!(y, v)\n    threadIdx().x == 1 &amp;&amp; f!(y,v)\n    return nothing\nend\n\nfunction gpu_∇f!(dy, y, v)\n    threadIdx().x == 1 &amp;&amp; ∇f!(dy,y,v)\n    return nothing\nend\n\n# gpu test\ny_d = cu(zeros(10));\ndy_d = cu(zeros(10,length(v)));\n\n@cuda threads=256 gpu_f!(y_d, v)\n\n@device_code_typed @cuda  threads=256 gpu_∇f!(dy_d, y_d, v)```","user":"UCT34GL7M","ts":"1611334323.017200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j/1P","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I get the same error when trying to do that. Here's a MWE (which I'm running on Julia-1.6 and CUDA#master)\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using CUDA\nusing StaticArrays\nusing ForwardDiff\n\n# in-place function f! and its Jacobian\nf!(y, v) = y[:] .= v' * v\n∇f!(dy, y, v) = ForwardDiff.jacobian!(dy, f!, y, v)\n\n# cpu test\nv = @SVector rand(Float32, 3)\ny = zeros(Float32, 10);\ndy = zeros(Float32, 10, length(v));\nf!(y,v);\n∇f!(dy, y, v);\n\n# gpu functions\nfunction gpu_f!(y, v)\n    threadIdx().x == 1 && f!(y,v)\n    return nothing\nend\n\nfunction gpu_∇f!(dy, y, v)\n    threadIdx().x == 1 && ∇f!(dy,y,v)\n    return nothing\nend\n\n# gpu test\ny_d = cu(zeros(10));\ndy_d = cu(zeros(10,length(v)));\n\n@cuda threads=256 gpu_f!(y_d, v)\n\n@device_code_typed @cuda  threads=256 gpu_∇f!(dy_d, y_d, v)"}]}]}]},{"type":"message","subtype":"thread_broadcast","text":"This probably won't have an effect on <https://github.com/JuliaGPU/CUDA.jl/issues/595>, right? (Not able to test the PR branch myself right now, sorry.)","user":"U01DD7Z0D89","ts":"1611353431.018200","thread_ts":"1611330728.012900","root":{"client_msg_id":"6418a582-69f0-4909-9f0f-60c317293984","type":"message","text":"for those people using CUDA.jl with Julia tasks/threads: now would be a good time to test or comment on <https://github.com/JuliaGPU/CUDA.jl/pull/662>\nTL;DR: per-task streams and synchronization, which makes it possible to overlap GPU computations of individual tasks.","user":"U68A3ASP9","ts":"1611330728.012900","team":"T68168MUP","edited":{"user":"U68A3ASP9","ts":"1611330737.000000"},"blocks":[{"type":"rich_text","block_id":"wVV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"for those people using CUDA.jl with Julia tasks/threads: now would be a good time to test or comment on "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/pull/662"},{"type":"text","text":"\nTL;DR: per-task streams and synchronization, which makes it possible to overlap GPU computations of individual tasks."}]}]}],"thread_ts":"1611330728.012900","reply_count":2,"reply_users_count":2,"latest_reply":"1611386533.018500","reply_users":["U01DD7Z0D89","U68A3ASP9"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"MTa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This probably won't have an effect on "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/issues/595"},{"type":"text","text":", right? (Not able to test the PR branch myself right now, sorry.)"}]}]}],"client_msg_id":"8882a938-55d7-4720-97e7-06062537c61e"},{"client_msg_id":"2775ebfc-75f9-4a79-9954-b2a07f8ec14e","type":"message","text":"Am I understanding correctly that in KernelAbstractions, `workgroupsize` is basically the number of threads per block?","user":"UM30MT6RF","ts":"1611417943.019500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FpPe","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Am I understanding correctly that in KernelAbstractions, "},{"type":"text","text":"workgroupsize","style":{"code":true}},{"type":"text","text":" is basically the number of threads per block?"}]}]}]},{"client_msg_id":"e56d5f86-c941-4346-9937-98debf2f4e0a","type":"message","text":"And if `workgroupsize` is specified as an integer, would it perhaps make sense to thread over the second dimension as well, if the `workgroupsize` is larger than the first dimension of the iteration space?","user":"UM30MT6RF","ts":"1611418395.021200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iod","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"And if "},{"type":"text","text":"workgroupsize","style":{"code":true}},{"type":"text","text":" is specified as an integer, would it perhaps make sense to thread over the second dimension as well, if the "},{"type":"text","text":"workgroupsize","style":{"code":true}},{"type":"text","text":" is larger than the first dimension of the iteration space?"}]}]}]},{"client_msg_id":"5af37b44-23c6-4b46-8b2d-fc90681bc346","type":"message","text":"Right prod(workgroupsize()) is the number of threads in a block","user":"U67BJLYCS","ts":"1611419117.022300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"HpI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Right prod(workgroupsize()) is the number of threads in a block"}]}]}]},{"client_msg_id":"3d2ece81-03e9-4380-8726-fda3c49ffc71","type":"message","text":"Uhm, potentially yes","user":"U67BJLYCS","ts":"1611419179.022800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/+s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Uhm, potentially yes"}]}]}]},{"client_msg_id":"c1b20da5-9093-433e-9c0e-d7b1f885e0d9","type":"message","text":"I haven't thought about that","user":"U67BJLYCS","ts":"1611419224.023500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4Q/j","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I haven't thought about that"}]}]}]},{"client_msg_id":"58a871b0-1d0a-4b2b-a517-04a423f14369","type":"message","text":"It's a bit challenging since the shape of the workgroupsize is actually used for the kernel","user":"U67BJLYCS","ts":"1611419246.024200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kky1z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It's a bit challenging since the shape of the workgroupsize is actually used for the kernel"}]}]}]},{"client_msg_id":"7470a39a-b3f2-4251-bd31-971a2d1757e0","type":"message","text":"Even if it's dynamic?","user":"UM30MT6RF","ts":"1611419349.024500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TnWe","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Even if it's dynamic?"}]}]}]},{"client_msg_id":"a497d463-fa6c-470a-811a-6d2ca77c9eaf","type":"message","text":"Or do you mean in the Cassette pass?","user":"UM30MT6RF","ts":"1611419456.024900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JKd5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Or do you mean in the Cassette pass?"}]}]}]},{"client_msg_id":"429dba30-73d3-48aa-abdc-9d86905ccbcc","type":"message","text":"It depends on what the kernel does","user":"U67BJLYCS","ts":"1611420844.025600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MnEpy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It depends on what the kernel does"}]}]}]},{"client_msg_id":"cb89371f-1c10-45e5-a1ae-80c5331474b8","type":"message","text":"If the kernel uses @private","user":"U67BJLYCS","ts":"1611420854.026000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Mgy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If the kernel uses @private"}]}]}]},{"client_msg_id":"f967b372-7557-4748-9622-b5f38f55e938","type":"message","text":"That requires querying the workgroupsize","user":"U67BJLYCS","ts":"1611420868.026400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hTw5O","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That requires querying the workgroupsize"}]}]}],"thread_ts":"1611420868.026400","reply_count":4,"reply_users_count":2,"latest_reply":"1611456661.031700","reply_users":["UM30MT6RF","U67BJLYCS"],"subscribed":false,"reactions":[{"name":"+1","users":["UM30MT6RF"],"count":1}]},{"client_msg_id":"f5729deb-9eea-4609-924e-aed8331fde28","type":"message","text":"you also may not want to use too many threads if you don't need to, as it can hurt achieved occupancy","user":"U68A3ASP9","ts":"1611421921.027100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"BNLj0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you also may not want to use too many threads if you don't need to, as it can hurt achieved occupancy"}]}]}],"reactions":[{"name":"+1","users":["UM30MT6RF"],"count":1}]},{"client_msg_id":"f4ca3357-f125-4c0e-8dcd-3269c9bb188c","type":"message","text":"but that depends on the specifics of your kernel","user":"U68A3ASP9","ts":"1611421928.027300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Zo6q","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"but that depends on the specifics of your kernel"}]}]}]},{"client_msg_id":"65c33f2d-44d4-47f5-952a-c046e085bbcb","type":"message","text":"What do I need to download and install in order to get `nv-nsight-cu-cli` on my machine?","user":"U7THT3TM3","ts":"1611445625.028000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JY0Z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What do I need to download and install in order to get "},{"type":"text","text":"nv-nsight-cu-cli","style":{"code":true}},{"type":"text","text":" on my machine?"}]}]}]},{"client_msg_id":"8b0bd70b-95ba-44f5-9fe7-2bd1c1b443b6","type":"message","text":"Also, on a completely different note.\n\nOn the CPU, OpenBLAS and MKL do not support matmul with integer matrices.\n\nOn the GPU, does cuBLAS support matmul with integer matrices?","user":"U7THT3TM3","ts":"1611456383.029100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"d8=to","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Also, on a completely different note.\n\nOn the CPU, OpenBLAS and MKL do not support matmul with integer matrices.\n\nOn the GPU, does cuBLAS support matmul with integer matrices?"}]}]}]},{"client_msg_id":"c77eaec0-c0f5-4b57-a56a-590d9931fc98","type":"message","text":"I've been trying to use `CUDA.CUBLAS.gemmEx!`, but it keeps giving me errors:\n```julia&gt; CUDA.CUBLAS.gemmEx!('N', 'N', alpha, a, b, beta, c);\nERROR: ArgumentError: gemmEx does not support Int32=Int32*Int32\n\njulia&gt; CUDA.CUBLAS.gemmEx!('N', 'N', alpha, a, b, beta, c);\nERROR: ArgumentError: gemmEx does not support Int16=Int16*Int16```","user":"U7THT3TM3","ts":"1611456406.029700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7+vmU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I've been trying to use "},{"type":"text","text":"CUDA.CUBLAS.gemmEx!","style":{"code":true}},{"type":"text","text":", but it keeps giving me errors:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> CUDA.CUBLAS.gemmEx!('N', 'N', alpha, a, b, beta, c);\nERROR: ArgumentError: gemmEx does not support Int32=Int32*Int32\n\njulia> CUDA.CUBLAS.gemmEx!('N', 'N', alpha, a, b, beta, c);\nERROR: ArgumentError: gemmEx does not support Int16=Int16*Int16"}]}]}]},{"client_msg_id":"7ec13857-2ffa-4041-8d41-872c6a554d25","type":"message","text":"I don't think it does","user":"U67BJLYCS","ts":"1611456417.029900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"phLPL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I don't think it does"}]}]}]},{"client_msg_id":"dcc556da-5cfb-43d3-871d-d045185b7254","type":"message","text":"You could give <https://github.com/JuliaGPU/GemmKernels.jl|https://github.com/JuliaGPU/GemmKernels.jl>","user":"U67BJLYCS","ts":"1611456486.030200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vhsR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You could give "},{"type":"link","url":"https://github.com/JuliaGPU/GemmKernels.jl","text":"https://github.com/JuliaGPU/GemmKernels.jl"}]}]}]},{"client_msg_id":"c8474ce1-21bb-4f95-939f-841ef654de2f","type":"message","text":"A go","user":"U67BJLYCS","ts":"1611456488.030400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"eWv=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"A go"}]}]}]},{"client_msg_id":"700cfcc6-d502-485e-ad30-2259388832b8","type":"message","text":"Or use the generic fallback","user":"U67BJLYCS","ts":"1611456496.030800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"AEe9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Or use the generic fallback"}]}]}]},{"client_msg_id":"8810d084-18bb-4a0c-b0a9-1cca1cfc9af2","type":"message","text":"Yep I'm about to try GemmKernels","user":"U7THT3TM3","ts":"1611456512.031100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ti/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yep I'm about to try GemmKernels"}]}]}]},{"client_msg_id":"0df45c95-541a-465a-b2b5-4fd2191b1094","type":"message","text":"No such luck :disappointed:","user":"U7THT3TM3","ts":"1611456755.032100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Ya+RS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"No such luck "},{"type":"emoji","name":"disappointed"}]}]}]},{"client_msg_id":"b41a21ef-78f7-4895-8654-afb2e09bc49e","type":"message","text":"```julia&gt; GemmKernels.BLAS.gemmEx!('N', 'N', alpha, a, b, beta, c_gemmkernels)\nERROR: MethodError: no method matching shared_layout_ab(::Type{CuArray{Int16,2}}, ::Val{false})\nClosest candidates are:\n  shared_layout_ab(::Type{CuArray{Float16,N}}, ::Any) where N at /users/daluthge/.julia/packages/GemmKernels/s61GT/src/blas.jl:13\n  shared_layout_ab(::Type{LinearAlgebra.Diagonal{Float16,CuArray{Float16,N}}}, ::Any) where {N, P} at /users/daluthge/.julia/packages/GemmKernels/s61GT/src/blas.jl:14\nStacktrace:\n [1] gemmEx!(::Char, ::Char, ::Int16, ::CuArray{Int16,2}, ::CuArray{Int16,2}, ::Int16, ::CuArray{Int16,2}) at /users/daluthge/.julia/packages/GemmKernels/s61GT/src/blas.jl:46\n [2] top-level scope at REPL[68]:1```","user":"U7THT3TM3","ts":"1611456769.032300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"d3h","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> GemmKernels.BLAS.gemmEx!('N', 'N', alpha, a, b, beta, c_gemmkernels)\nERROR: MethodError: no method matching shared_layout_ab(::Type{CuArray{Int16,2}}, ::Val{false})\nClosest candidates are:\n  shared_layout_ab(::Type{CuArray{Float16,N}}, ::Any) where N at /users/daluthge/.julia/packages/GemmKernels/s61GT/src/blas.jl:13\n  shared_layout_ab(::Type{LinearAlgebra.Diagonal{Float16,CuArray{Float16,N}}}, ::Any) where {N, P} at /users/daluthge/.julia/packages/GemmKernels/s61GT/src/blas.jl:14\nStacktrace:\n [1] gemmEx!(::Char, ::Char, ::Int16, ::CuArray{Int16,2}, ::CuArray{Int16,2}, ::Int16, ::CuArray{Int16,2}) at /users/daluthge/.julia/packages/GemmKernels/s61GT/src/blas.jl:46\n [2] top-level scope at REPL[68]:1"}]}]}]},{"client_msg_id":"329bc424-e961-4388-baa0-5b8e36ba0d28","type":"message","text":"Which implies that GemmKernels also doesn't support integer matrices","user":"U7THT3TM3","ts":"1611456783.032700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"B2sIO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Which implies that GemmKernels also doesn't support integer matrices"}]}]}]},{"client_msg_id":"a8e11a1e-986d-4801-9f3d-c5c4ac3179d5","type":"message","text":"<https://github.com/JuliaGPU/GemmKernels.jl/issues/64>","user":"U7THT3TM3","ts":"1611457500.032900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sJS","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/JuliaGPU/GemmKernels.jl/issues/64"}]}]}]},{"client_msg_id":"f4cc9d30-8b54-4c2a-8ebe-3db57edd772a","type":"message","text":"gemmEx does support some integer scenario's. i8*i8=i32, and i8*i8=i32. so not that useful; GemmKernels would be great here (cc <@UPMLA9F9S>)","user":"U68A3ASP9","ts":"1611473779.033900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"u0o7I","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"gemmEx does support some integer scenario's. i8*i8=i32, and i8*i8=i32. so not that useful; GemmKernels would be great here (cc "},{"type":"user","user_id":"UPMLA9F9S"},{"type":"text","text":")"}]}]}]},{"client_msg_id":"a606ffea-f79b-4fba-a862-3228eea1ccd0","type":"message","text":"also note that those gemmEx versions I mentioned above have requirements in input sizes, and one at least is broken in CUDA 11.2: <https://github.com/JuliaGPU/CUDA.jl/blob/7ff1e2c4d8d645d4433b802262e0218b3f1a69f5/lib/cublas/wrappers.jl#L765-L770>","user":"U68A3ASP9","ts":"1611473861.034500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1B5nz","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"also note that those gemmEx versions I mentioned above have requirements in input sizes, and one at least is broken in CUDA 11.2: "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/blob/7ff1e2c4d8d645d4433b802262e0218b3f1a69f5/lib/cublas/wrappers.jl#L765-L770"}]}]}]},{"client_msg_id":"df4e5b97-d95e-4135-b7f2-9cac6acc4672","type":"message","text":"you normally don't notice that because it gracefully falls back to the (super slow) GPUArrays.jl gemm","user":"U68A3ASP9","ts":"1611473881.034900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"E3Y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you normally don't notice that because it gracefully falls back to the (super slow) GPUArrays.jl gemm"}]}]}]},{"client_msg_id":"8c4f9cfc-eb46-47c3-83fe-72ae5433a5cc","type":"message","text":"When I initialize `y=CuArray([1.0f0,2.0f0,3.0f0,4.0f0,5.0f0])`  and then diagonalize it into a matrix `Diagonal(y)` , I get the following output -\n```julia&gt; Diagonal(y)\n5×5 Diagonal{Float32,CuArray{Float32,1}}:\n 1.0   ⋅    ⋅    ⋅    ⋅ \n  ⋅   2.0   ⋅    ⋅    ⋅ \n  ⋅    ⋅   3.0   ⋅    ⋅ \n  ⋅    ⋅    ⋅   4.0   ⋅ \n  ⋅    ⋅    ⋅    ⋅   5.0```\nDo the `.`s mean the matrix is sparse?","user":"UTDSTSANP","ts":"1611498511.037400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bwu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"When I initialize "},{"type":"text","text":"y=CuArray([1.0f0,2.0f0,3.0f0,4.0f0,5.0f0])","style":{"code":true}},{"type":"text","text":"  and then diagonalize it into a matrix "},{"type":"text","text":"Diagonal(y)","style":{"code":true}},{"type":"text","text":" , I get the following output -\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> Diagonal(y)\n5×5 Diagonal{Float32,CuArray{Float32,1}}:\n 1.0   ⋅    ⋅    ⋅    ⋅ \n  ⋅   2.0   ⋅    ⋅    ⋅ \n  ⋅    ⋅   3.0   ⋅    ⋅ \n  ⋅    ⋅    ⋅   4.0   ⋅ \n  ⋅    ⋅    ⋅    ⋅   5.0"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Do the "},{"type":"text","text":".","style":{"code":true}},{"type":"text","text":"s mean the matrix is sparse?"}]}]}]},{"client_msg_id":"4d2237fc-6a84-4782-9682-c0d1a6968afb","type":"message","text":"It means only the diagonal is stored, yes.","user":"U67D54KS8","ts":"1611500098.037800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"6e04","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It means only the diagonal is stored, yes."}]}]}],"thread_ts":"1611500098.037800","reply_count":3,"reply_users_count":2,"latest_reply":"1611502901.039600","reply_users":["UTDSTSANP","U67D54KS8"],"subscribed":false,"reactions":[{"name":"+1","users":["UTDSTSANP"],"count":1}]},{"client_msg_id":"5e9d7359-9cf3-44e6-b490-060597f6d2ab","type":"message","text":"How to do 3d linear interpolation on gpu? I am looking for something like this:\n```interpolate!(out, grid_values::CuArray{T,3}, grid_axes::NTuple{3, AbstractRange}, x,y,z)```\npytorch has some equivalent functionality:\n<https://pytorch.org/docs/stable/nn.functional.html#grid-sample>","user":"UKZ78GR8Q","ts":"1611561630.040800","team":"T68168MUP","edited":{"user":"UKZ78GR8Q","ts":"1611561651.000000"},"blocks":[{"type":"rich_text","block_id":"S0p3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"How to do 3d linear interpolation on gpu? I am looking for something like this:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"interpolate!(out, grid_values::CuArray{T,3}, grid_axes::NTuple{3, AbstractRange}, x,y,z)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"pytorch has some equivalent functionality:\n"},{"type":"link","url":"https://pytorch.org/docs/stable/nn.functional.html#grid-sample"}]}]}]},{"client_msg_id":"ee41f35c-63eb-4505-bfde-d9668e21b430","type":"message","text":"Following up an example from a few days ago on using ForwardDiff inside of CUDA kernels, I have a slightly different MWE now:\nI have a function in-place function `f!(y,x)` that fills up a vector `y` based on the input `x`. When I make `y` a CuArray like below, the function runs fine on GPU. Also, when I use `ForwardDiff.jacobian!`  like in the code below, I can get the code to run on the GPU. However, when I try to call `ForwardDiff.jacobian!`  from within a kernel function (`g!`), I get a `Reason: unsupported dynamic function invocation (call to jacobian!(result::Union{AbstractArray, DiffResults.DiffResult}, f!, y::AbstractArray, x::AbstractArray) in ForwardDiff` error. I'm trying to understand the stacktrace but it's way over my head (calls to `compile`, `codegen`, `check_ir` functions `GPUCompiler`) so I was wondering if there's someone here who might know how to deal with the problem.\n```# julia-1.6\nusing CUDA # master\nusing ForwardDiff # v0.10.15\nusing StaticArrays # v1.0.1\n\nx = @SVector rand(Float32, 3);\ny = cu(zeros(256));\n\nf!(y, x) = y[:] .= x'*x;\nf!(y,x); # runs\n\ndy = cu(zeros(256,3))\nForwardDiff.jacobian!(dy,f!,y,x); # also runs\n\nfunction g!(dy,y,x)\n    threadIdx().x == 1 &amp;&amp; ForwardDiff.jacobian!(dy,f!,y,x)\n    nothing\nend\n\n@cuda threads=1 g!(dy,y,x) # doesn't run\n@device_code_warntype @cuda threads=1 g!(dy,y,x)```","user":"UCT34GL7M","ts":"1611570192.046900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"BYHx4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Following up an example from a few days ago on using ForwardDiff inside of CUDA kernels, I have a slightly different MWE now:\nI have a function in-place function "},{"type":"text","text":"f!(y,x)","style":{"code":true}},{"type":"text","text":" that fills up a vector "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" based on the input "},{"type":"text","text":"x","style":{"code":true}},{"type":"text","text":". When I make "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" a CuArray like below, the function runs fine on GPU. Also, when I use "},{"type":"text","text":"ForwardDiff.jacobian!","style":{"code":true}},{"type":"text","text":"  like in the code below, I can get the code to run on the GPU. However, when I try to call "},{"type":"text","text":"ForwardDiff.jacobian!","style":{"code":true}},{"type":"text","text":"  from within a kernel function ("},{"type":"text","text":"g!","style":{"code":true}},{"type":"text","text":"), I get a "},{"type":"text","text":"Reason: unsupported dynamic function invocation (call to jacobian!(result::Union{AbstractArray, DiffResults.DiffResult}, f!, y::AbstractArray, x::AbstractArray) in ForwardDiff","style":{"code":true}},{"type":"text","text":" error. I'm trying to understand the stacktrace but it's way over my head (calls to "},{"type":"text","text":"compile","style":{"code":true}},{"type":"text","text":", "},{"type":"text","text":"codegen","style":{"code":true}},{"type":"text","text":", "},{"type":"text","text":"check_ir","style":{"code":true}},{"type":"text","text":" functions "},{"type":"text","text":"GPUCompiler","style":{"code":true}},{"type":"text","text":") so I was wondering if there's someone here who might know how to deal with the problem.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"# julia-1.6\nusing CUDA # master\nusing ForwardDiff # v0.10.15\nusing StaticArrays # v1.0.1\n\nx = @SVector rand(Float32, 3);\ny = cu(zeros(256));\n\nf!(y, x) = y[:] .= x'*x;\nf!(y,x); # runs\n\ndy = cu(zeros(256,3))\nForwardDiff.jacobian!(dy,f!,y,x); # also runs\n\nfunction g!(dy,y,x)\n    threadIdx().x == 1 && ForwardDiff.jacobian!(dy,f!,y,x)\n    nothing\nend\n\n@cuda threads=1 g!(dy,y,x) # doesn't run\n@device_code_warntype @cuda threads=1 g!(dy,y,x)"}]}]}],"thread_ts":"1611570192.046900","reply_count":10,"reply_users_count":2,"latest_reply":"1611577890.052200","reply_users":["U68A3ASP9","UCT34GL7M"],"subscribed":false},{"client_msg_id":"6cc47365-08dc-47ae-b335-b4df6e5782a3","type":"message","text":"I'd like to compute `pairwise(Euclidean(), X, Y, dims = 2)` efficiently on a GPU, where `X` and `Y` are matrices of size e.g. `3072 x 1024` . Any tips how to get this? Do I need to write my own kernel?","user":"UB2NJGVK6","ts":"1611572679.050500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pD0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'd like to compute "},{"type":"text","text":"pairwise(Euclidean(), X, Y, dims = 2)","style":{"code":true}},{"type":"text","text":" efficiently on a GPU, where "},{"type":"text","text":"X","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"Y","style":{"code":true}},{"type":"text","text":" are matrices of size e.g. "},{"type":"text","text":"3072 x 1024","style":{"code":true}},{"type":"text","text":" . Any tips how to get this? Do I need to write my own kernel?"}]}]}],"thread_ts":"1611572679.050500","reply_count":7,"reply_users_count":2,"latest_reply":"1611651075.053300","reply_users":["U68A3ASP9","UB2NJGVK6"],"subscribed":false},{"client_msg_id":"309b053d-e9e0-4189-b492-60c813d92ece","type":"message","text":"Does anyone know how `CUDA.jl` handles shared memory between GPU and CPU? I have the suspicion that it ignores that it is shared, thereby allocating more memory than actually is free and crashing. Unfortunately, I’m not expert enough to be able to prove that somehow…","user":"UNZKG0909","ts":"1611688634.053900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Et0=F","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does anyone know how "},{"type":"text","text":"CUDA.jl","style":{"code":true}},{"type":"text","text":" handles shared memory between GPU and CPU? I have the suspicion that it ignores that it is shared, thereby allocating more memory than actually is free and crashing. Unfortunately, I’m not expert enough to be able to prove that somehow…"}]}]}],"thread_ts":"1611688634.053900","reply_count":9,"reply_users_count":3,"latest_reply":"1611733958.055900","reply_users":["U6A0PD8CR","UNZKG0909","U68A3ASP9"],"subscribed":false},{"client_msg_id":"d47e0d08-8d2a-49f9-8369-0c06f0c7ec97","type":"message","text":"If I have some data in unified memory, and each of several GPU in parallel takes it and does some series of non-mutating operations using it as input, is it true that there's no extra overhead as compared to if the starting input was in regular GPU memory?","user":"UUMJUCYRK","ts":"1611737157.057900","team":"T68168MUP","edited":{"user":"UUMJUCYRK","ts":"1611737170.000000"},"blocks":[{"type":"rich_text","block_id":"FTi9Y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"If I have some data in unified memory, and each of several GPU in parallel takes it and does some series of non-mutating operations using it as input, is it true that there's no extra overhead as compared to if the starting input was in regular GPU memory?"}]}]}]},{"client_msg_id":"93d38eee-9099-4ff5-9002-f474ccbfba7f","type":"message","text":"depends: <https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-multi-gpu>","user":"U68A3ASP9","ts":"1611737441.058300","team":"T68168MUP","attachments":[{"title":"Programming Guide :: CUDA Toolkit Documentation","title_link":"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-multi-gpu","text":"The programming guide to the CUDA model and interface.","fallback":"Programming Guide :: CUDA Toolkit Documentation","from_url":"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-multi-gpu","service_name":"docs.nvidia.com","id":1,"original_url":"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-multi-gpu"}],"blocks":[{"type":"rich_text","block_id":"lgHzK","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"depends: "},{"type":"link","url":"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-multi-gpu"}]}]}]},{"client_msg_id":"00a823b1-a08e-412f-9c7b-ce469a9351b2","type":"message","text":"It depends on latest AMD hardware too, as data which was created by the CPU can't be in the infinity cache","user":"U9MD78Z9N","ts":"1611737530.059700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/6V","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It depends on latest AMD hardware too, as data which was created by the CPU can't be in the infinity cache"}]}]}],"thread_ts":"1611737530.059700","reply_count":1,"reply_users_count":1,"latest_reply":"1611757361.071400","reply_users":["U6A0PD8CR"],"subscribed":false},{"client_msg_id":"bc37f80c-4b37-4fda-a5ae-ed31b9ce02ad","type":"message","text":"Thanks, that's helpful. The description for &lt;6.x sounds suitable for me, although the one for &gt;6.x (what I have) somehow is less clear, but I guess it can't be worse? Basically I am thinking of\n```unified_memory_array = &lt;...&gt; \n\nThreads.@threads for dev in devices()\n    device!(dev)\n    compute(unified_memory_array)\nend```\nwhere `compute` is a \"long\" running series of GPU-local operations (which don't mutate `unified_memory_array` or need to intercommunicate in any way).","user":"UUMJUCYRK","ts":"1611738036.062700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Xr=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks, that's helpful. The description for <6.x sounds suitable for me, although the one for >6.x (what I have) somehow is less clear, but I guess it can't be worse? Basically I am thinking of\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"unified_memory_array = <...> \n\nThreads.@threads for dev in devices()\n    device!(dev)\n    compute(unified_memory_array)\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"where "},{"type":"text","text":"compute","style":{"code":true}},{"type":"text","text":" is a \"long\" running series of GPU-local operations (which don't mutate "},{"type":"text","text":"unified_memory_array","style":{"code":true}},{"type":"text","text":" or need to intercommunicate in any way)."}]}]}]},{"client_msg_id":"8dcd4c88-8ee7-40f0-a495-454c45b48f9a","type":"message","text":"I was just slightly scared to see that e.g. `2 .* unified_memory_array` returns another unified memory array, but I suppose the result will be physically on whatever GPU computed it and from that point on theres no overhead for that GPU to do other things with this array?","user":"UUMJUCYRK","ts":"1611738155.063800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Zi8t","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I was just slightly scared to see that e.g. "},{"type":"text","text":"2 .* unified_memory_array","style":{"code":true}},{"type":"text","text":" returns another unified memory array, but I suppose the result will be physically on whatever GPU computed it and from that point on theres no overhead for that GPU to do other things with this array?"}]}]}]},{"client_msg_id":"f478c383-9923-469c-83fb-c1f5db4f74ef","type":"message","text":"&lt;sm_60 is OK as long as you have peer to peer, if not it'll keep the array in CPU memory and use slow PCI-E reads","user":"U68A3ASP9","ts":"1611738363.064400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"An5h","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"<sm_60 is OK as long as you have peer to peer, if not it'll keep the array in CPU memory and use slow PCI-E reads"}]}]}]},{"client_msg_id":"83f871ae-29dd-4053-88d5-e3a7a9cbe0db","type":"message","text":"&gt; sm_60 is stricly better because then your array, or the pages you're working with, will transparantly migrate to the GPU that is processing them","user":"U68A3ASP9","ts":"1611738385.065000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QbRr+","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"sm_60 is stricly better because then your array, or the pages you're working with, will transparantly migrate to the GPU that is processing them"}]}]}]},{"client_msg_id":"be5904a8-a2d3-4312-9859-ee20c057dbc0","type":"message","text":"&gt; I was just slightly scared to see that e.g. 2 .* unified_memory_array returns another unified memory array,\nCuArray doesn't currently really support unified memory, so these allocations will be tied to a device, and not be unified","user":"U68A3ASP9","ts":"1611738430.065700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/rBQ","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"I was just slightly scared to see that e.g. 2 .* unified_memory_array returns another unified memory array,"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"CuArray doesn't currently really support unified memory, so these allocations will be tied to a device, and not be unified"}]}]}]},{"type":"message","text":"do you mean that just the constructor doesn't support unified? once allocated, doesn't the fact that e.g. here I can access `y` from device 1 mean the result was in unified memory?","files":[{"id":"F01LQ6QPECQ","created":1611738630,"timestamp":1611738630,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"UUMJUCYRK","editable":false,"size":65370,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01LQ6QPECQ/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01LQ6QPECQ/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01LQ6QPECQ-e6ef08d6bb/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01LQ6QPECQ-e6ef08d6bb/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01LQ6QPECQ-e6ef08d6bb/image_360.png","thumb_360_w":360,"thumb_360_h":219,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01LQ6QPECQ-e6ef08d6bb/image_480.png","thumb_480_w":480,"thumb_480_h":292,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01LQ6QPECQ-e6ef08d6bb/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01LQ6QPECQ-e6ef08d6bb/image_720.png","thumb_720_w":720,"thumb_720_h":438,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01LQ6QPECQ-e6ef08d6bb/image_800.png","thumb_800_w":800,"thumb_800_h":486,"original_w":954,"original_h":580,"thumb_tiny":"AwAdADDQ/GgfWlxznH6UuaADn1o59qM0ZoAOfajmjNGaAG4+bP8AWncegpvO7rxTsfWgAzRmlooATNGaWigD/9k=","permalink":"https://julialang.slack.com/files/UUMJUCYRK/F01LQ6QPECQ/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01LQ6QPECQ-1d7571f0de","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"GkY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"do you mean that just the constructor doesn't support unified? once allocated, doesn't the fact that e.g. here I can access "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" from device 1 mean the result was in unified memory?"}]}]}],"user":"UUMJUCYRK","display_as_bot":false,"ts":"1611738650.066600"},{"client_msg_id":"c8683e18-dc9c-4f5e-839c-c158310612f5","type":"message","text":"huh","user":"U68A3ASP9","ts":"1611739191.067000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zVU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"huh"}]}]}]},{"client_msg_id":"f15c8593-25c6-4a32-a6d3-66051d56e93f","type":"message","text":"`CUDA.is_managed` returns `false`, so I'm surprised this doesn't trigger an illegal memory access","user":"U68A3ASP9","ts":"1611739500.067400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9sGo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CUDA.is_managed","style":{"code":true}},{"type":"text","text":" returns "},{"type":"text","text":"false","style":{"code":true}},{"type":"text","text":", so I'm surprised this doesn't trigger an illegal memory access"}]}]}]},{"client_msg_id":"c59411d0-d7d0-48f8-bf74-7497765a852b","type":"message","text":"I guess with UVA, `memcpy` knows the device of origin and so you don't need to switch devices","user":"U68A3ASP9","ts":"1611739549.068000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"C4h","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I guess with UVA, "},{"type":"text","text":"memcpy","style":{"code":true}},{"type":"text","text":" knows the device of origin and so you don't need to switch devices"}]}]}]},{"client_msg_id":"e86c3fd1-d0ea-44d2-b5bc-e8453ca8d66e","type":"message","text":"The relevant `memcpy` here is the one to copy `y` to CPU to print it? Indeed `2 .* y` is an illegal memory access. In any case, that all makes sense to me then I think, and its also all I need (I don't need to later work on `y` from a different GPU)","user":"UUMJUCYRK","ts":"1611739974.069400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LeuON","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The relevant "},{"type":"text","text":"memcpy","style":{"code":true}},{"type":"text","text":" here is the one to copy "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" to CPU to print it? Indeed "},{"type":"text","text":"2 .* y","style":{"code":true}},{"type":"text","text":" is an illegal memory access. In any case, that all makes sense to me then I think, and its also all I need (I don't need to later work on "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" from a different GPU)"}]}]}]},{"client_msg_id":"df1b4296-9439-4c3b-b223-ee6b2956050b","type":"message","text":"yeah correct","user":"U68A3ASP9","ts":"1611739990.069600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PNlMq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yeah correct"}]}]}]},{"client_msg_id":"d1b163ff-f5b9-4ab3-99ff-a07f4f52aa17","type":"message","text":"if you want to test out unified memory for more, I'd recommend trying to switch the CUDA.jl allocator to unified memory altogether","user":"U68A3ASP9","ts":"1611740014.070200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5hc5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"if you want to test out unified memory for more, I'd recommend trying to switch the CUDA.jl allocator to unified memory altogether"}]}]}]},{"client_msg_id":"10b8a7f7-32a1-4b5f-9fdf-66addd5a7c70","type":"message","text":"is that a built in option or do you mean hacking it?","user":"UUMJUCYRK","ts":"1611740313.070400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"q=Fx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is that a built in option or do you mean hacking it?"}]}]}]},{"client_msg_id":"5e9573f0-03b1-4580-872f-a35436daf000","type":"message","text":"hacking it, it should be a tiny change","user":"U68A3ASP9","ts":"1611740506.070600","team":"T68168MUP","edited":{"user":"U68A3ASP9","ts":"1611740509.000000"},"blocks":[{"type":"rich_text","block_id":"FZRO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hacking it, it should be a tiny change"}]}]}],"thread_ts":"1611740506.070600","reply_count":1,"reply_users_count":1,"latest_reply":"1611740514.070800","reply_users":["U68A3ASP9"],"subscribed":false,"reactions":[{"name":"+1","users":["UUMJUCYRK"],"count":1}]},{"client_msg_id":"3cae42dc-94ca-449f-8412-9cf029a38774","type":"message","text":"cool, will play around with it, thanks!","user":"UUMJUCYRK","ts":"1611740540.071300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5rZGI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"cool, will play around with it, thanks!"}]}]}]},{"client_msg_id":"537cccc4-f743-4dfb-af41-7eade5e02eaa","type":"message","text":"Does `diagm(cu(rand(10)))` make a CuArray? (I was trying to debug something via CI :cry:  and something was making an Array…)","user":"UD0NS8PDF","ts":"1611777449.072200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ddKYo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does "},{"type":"text","text":"diagm(cu(rand(10)))","style":{"code":true}},{"type":"text","text":" make a CuArray? (I was trying to debug something via CI "},{"type":"emoji","name":"cry"},{"type":"text","text":"  and something was making an Array…)"}]}]}]},{"client_msg_id":"c2591968-7408-4897-94c8-7b4d951f2b64","type":"message","text":"```ulia&gt; diagm(cu(rand(10)))\n┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`\n└ @ GPUArrays ~/.julia/packages/GPUArrays/WV76E/src/host/indexing.jl:43\n10×10 Array{Float32,2}:```","user":"U6N6VQE30","ts":"1611780003.072400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"w+t","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ulia> diagm(cu(rand(10)))\n┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`\n└ @ GPUArrays ~/.julia/packages/GPUArrays/WV76E/src/host/indexing.jl:43\n10×10 Array{Float32,2}:"}]}]}],"thread_ts":"1611780003.072400","reply_count":2,"reply_users_count":1,"latest_reply":"1611780740.072900","reply_users":["UD0NS8PDF"],"subscribed":false},{"client_msg_id":"c8f8ffa2-1fe8-473c-82ff-ff6de609bcdd","type":"message","text":"I got a dreaded device compatibility error when updating CUDA.\n```┌ Warning: Your Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 &lt; 5.0).\n│ Some functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n└ @ CUDA ~/.julia/packages/CUDA/wTQsK/src/state.jl:251```\nIs there a table mapping GPUs/compute versions to CUDA versions, or should I just be doing binary search to find the latest CUDA version that doesn't throw this error?","user":"U73ACR3TQ","ts":"1611795078.074200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kr9Y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I got a dreaded device compatibility error when updating CUDA.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"┌ Warning: Your Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n│ Some functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n└ @ CUDA ~/.julia/packages/CUDA/wTQsK/src/state.jl:251"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a table mapping GPUs/compute versions to CUDA versions, or should I just be doing binary search to find the latest CUDA version that doesn't throw this error?"}]}]}]},{"client_msg_id":"1b6dfb38-547e-4b21-a50f-8c9425cbe42a","type":"message","text":"that's a warning, not an error.","user":"U68A3ASP9","ts":"1611818365.074700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UXY1k","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"that's a warning, not an error."}]}]}]},{"client_msg_id":"e6ae833d-b8ad-4b96-b1e8-57ffebe81fbe","type":"message","text":"I should probably add a table to the README, but the incompatibility doesn't matter much right now. you should be fine.","user":"U68A3ASP9","ts":"1611818452.075300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PDG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I should probably add a table to the README, but the incompatibility doesn't matter much right now. you should be fine."}]}]}]},{"client_msg_id":"7228d4e6-8a6f-48ad-88aa-f095f76785e2","type":"message","text":"I would like to ask, as BLAS.nrm2 is the BLAS function for calculating the 2-norm on the cpu, for CuArrays it should be CUBLAS.nrm2 right?","user":"UTDSTSANP","ts":"1611842910.076700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lbxf/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I would like to ask, as BLAS.nrm2 is the BLAS function for calculating the 2-norm on the cpu, for CuArrays it should be CUBLAS.nrm2 right?"}]}]}],"thread_ts":"1611842910.076700","reply_count":1,"reply_users_count":1,"latest_reply":"1611843474.077600","reply_users":["U68A3ASP9"],"subscribed":false},{"client_msg_id":"128d2141-37f0-492e-98a1-89a911a4afdb","type":"message","text":"Does anyone know why random number generator with fixed seeds are so slow?\n```julia&gt; e = zeros(Float32, 100000, 100);\n\njulia&gt; e2 = cu(e);\n\njulia&gt; seed = rand(UInt64);\n\njulia&gt; @btime Random.seed!(seed);rand!(e);\n  10.654 μs (2 allocations: 112 bytes)\n\njulia&gt; @btime CUDA.seed!(seed);rand!(e2);\n  6.114 ms (18 allocations: 51.84 KiB)```\nLike that a 1000x factor slower","user":"U010WA4SZK5","ts":"1611843143.077500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iaj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does anyone know why random number generator with fixed seeds are so slow?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> e = zeros(Float32, 100000, 100);\n\njulia> e2 = cu(e);\n\njulia> seed = rand(UInt64);\n\njulia> @btime Random.seed!(seed);rand!(e);\n  10.654 μs (2 allocations: 112 bytes)\n\njulia> @btime CUDA.seed!(seed);rand!(e2);\n  6.114 ms (18 allocations: 51.84 KiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Like that a 1000x factor slower"}]}]}]}]}