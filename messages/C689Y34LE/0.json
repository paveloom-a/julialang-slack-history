{"cursor": 3, "messages": [{"client_msg_id":"688b7b6e-4705-4458-80f2-a3878c5335d0","type":"message","text":"I think we also figured out what the problem could be. The problem is related to the architecture (ppc64le).\nAccording to the documentation of <https://julialang.org/downloads/#currently_supported_platforms>\nCUDA.jl does not support Power 9 AC922. \n\nWe were wondering if we could get some help installing the CUDA.jl. \n\nThank you and sorry for the long message.","user":"U015TV044AV","ts":"1613410443.183500","team":"T68168MUP","edited":{"user":"U015TV044AV","ts":"1613410457.000000"},"blocks":[{"type":"rich_text","block_id":"MRs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think we also figured out what the problem could be. The problem is related to the architecture (ppc64le).\nAccording to the documentation of "},{"type":"link","url":"https://julialang.org/downloads/#currently_supported_platforms"},{"type":"text","text":"\nCUDA.jl does not support Power 9 AC922. \n\nWe were wondering if we could get some help installing the CUDA.jl. \n\nThank you and sorry for the long message."}]}]}]},{"client_msg_id":"6ebc8cd3-1509-4118-86df-34a831f0cf92","type":"message","text":"<@U67BJLYCS> is the person to ask","user":"U674T3KB3","ts":"1613410532.184400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"T9jjE","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":" is the person to ask"}]}]}]},{"client_msg_id":"3dd0b04d-a98a-4068-9e30-bffdd8797c07","type":"message","text":"It definitely works, but there may be quirks, since it's not a mainstream platform","user":"U674T3KB3","ts":"1613410550.185200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SR0dv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"It definitely works, but there may be quirks, since it's not a mainstream platform"}]}]}],"reactions":[{"name":"+1","users":["UEP056STX"],"count":1}]},{"client_msg_id":"4909949d-17a9-487f-8993-c53a9f131596","type":"message","text":"<@U015TV044AV> we are running Julia on our local powerpc cluster at MIT","user":"U67BJLYCS","ts":"1613411360.186100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"k9fnJ","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U015TV044AV"},{"type":"text","text":" we are running Julia on our local powerpc cluster at MIT"}]}]}]},{"client_msg_id":"4a799ce3-906d-4833-b8b3-fb142bc02a56","type":"message","text":"You might want to set `export JULIA_CUDA_USE_BINARYBUILDER=false`","user":"U67BJLYCS","ts":"1613411435.186400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FQa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You might want to set "},{"type":"text","text":"export JULIA_CUDA_USE_BINARYBUILDER=false","style":{"code":true}}]}]}]},{"client_msg_id":"74d9644b-8a93-4a18-8c33-748ab200d19b","type":"message","text":"but CUDA_jll 11+ is available for PPC","user":"U67BJLYCS","ts":"1613412120.186900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"X9j","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"but CUDA_jll 11+ is available for PPC"}]}]}]},{"client_msg_id":"d23cc184-bc2e-4b54-b98e-cc1770a247d4","type":"message","text":"So what issue are you running into?","user":"U67BJLYCS","ts":"1613412131.187200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"CBqp1","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So what issue are you running into?"}]}]}]},{"client_msg_id":"3382ded7-8fc6-42d3-acab-ba32959d4191","type":"message","text":"<@U68A3ASP9> do I understand correctly that the issue basically is that CUDA 10 only was distributed as an rpm, for PPC?","user":"U67BJLYCS","ts":"1613413774.188000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"TwhG","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":" do I understand correctly that the issue basically is that CUDA 10 only was distributed as an rpm, for PPC?"}]}]}]},{"client_msg_id":"73937155-4ac1-49a3-a491-625bb93df452","type":"message","text":"Hey, do I understand it right that if I want to run my code on CUDA I should use CUDAArrays and write all my algorithms using array calculations mostly?","user":"U7PD3M3L5","ts":"1613470136.190600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"e1uo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey, do I understand it right that if I want to run my code on CUDA I should use CUDAArrays and write all my algorithms using array calculations mostly?"}]}]}]},{"client_msg_id":"231ebdc3-2a1f-478d-8ee7-da081440220e","type":"message","text":"I am a bit confused since how I imagine a GPU I should not be using normal Julia code with `if`  statements or `for`  loops with complicated conditions right?","user":"U7PD3M3L5","ts":"1613470186.191700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FgT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I am a bit confused since how I imagine a GPU I should not be using normal Julia code with "},{"type":"text","text":"if","style":{"code":true}},{"type":"text","text":"  statements or "},{"type":"text","text":"for","style":{"code":true}},{"type":"text","text":"  loops with complicated conditions right?"}]}]}]},{"client_msg_id":"2abdda4e-b333-483d-baa9-4ddde3c1cae3","type":"message","text":"<https://juliagpu.gitlab.io/CUDA.jl/usage/overview/>","user":"U68A3ASP9","ts":"1613470991.191900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=DMkc","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://juliagpu.gitlab.io/CUDA.jl/usage/overview/"}]}]}]},{"client_msg_id":"d54733cf-3fcb-4cea-a541-cd6bed33e4a8","type":"message","text":"you can use kernels with conditionals and everything, but the same restrictions as CUDA C apply then (i.e. you probably want to avoid those as they can be slow)","user":"U68A3ASP9","ts":"1613471080.192500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Dzg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you can use kernels with conditionals and everything, but the same restrictions as CUDA C apply then (i.e. you probably want to avoid those as they can be slow)"}]}]}]},{"client_msg_id":"dd702b83-0b8d-4442-aafb-4d869fad7468","type":"message","text":"Thank you, the link helped a bit and the other docs there look promising","user":"U7PD3M3L5","ts":"1613471343.193300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"pVpZj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thank you, the link helped a bit and the other docs there look promising"}]}]}]},{"client_msg_id":"33f700e0-e27f-4a13-b1e0-f941f90d3c76","type":"message","text":"Is using Flux on the GPU as easy and performant as using PyTorch? Or basically what do I get for GPU programming when switching from python to julia. For CPU it is pretty clear: I can just write most stuff in for loops etc and not worry. In Pytorch I am doing nearly everything with predefined objects that operate kernels on the GPU anyways, how does julia make things easier here?","user":"U7PD3M3L5","ts":"1613471443.195100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1hLli","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is using Flux on the GPU as easy and performant as using PyTorch? Or basically what do I get for GPU programming when switching from python to julia. For CPU it is pretty clear: I can just write most stuff in for loops etc and not worry. In Pytorch I am doing nearly everything with predefined objects that operate kernels on the GPU anyways, how does julia make things easier here?"}]}]}]},{"client_msg_id":"524f60ff-e88f-4129-9f80-6cbaa3b7f534","type":"message","text":"Not trying to rant or something, I just want to understand","user":"U7PD3M3L5","ts":"1613471463.195600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"oU/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not trying to rant or something, I just want to understand"}]}]}]},{"client_msg_id":"96ba99e6-e934-4a04-ae39-31e165a38aa7","type":"message","text":"you generally need to stick to vectorized array expressions to get portability across CPU/GPU (but in Julia that's a much more powerful set of operations). and when you want to optimize, you can write your GPU kernels in Julia too, which is the main advantage over other languages (where you need to use CUDA C).\nwrt. performance compared to pytorch, we probably aren't as fast because we don't have as many optimized kernels, and there can be some issues with the GC when running close to OOM, but better check in some of the machine learning channels for specific numbers.","user":"U68A3ASP9","ts":"1613471972.196100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YIv/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you generally need to stick to vectorized array expressions to get portability across CPU/GPU (but in Julia that's a much more powerful set of operations). and when you want to optimize, you can write your GPU kernels in Julia too, which is the main advantage over other languages (where you need to use CUDA C).\nwrt. performance compared to pytorch, we probably aren't as fast because we don't have as many optimized kernels, and there can be some issues with the GC when running close to OOM, but better check in some of the machine learning channels for specific numbers."}]}]}]},{"client_msg_id":"ca070fcf-e8c7-4e81-9119-538c5b674b42","type":"message","text":"So the advantages go beyond the array kernels. There is a very rich ecosystem of GPU capable packages, thanks in part to the ease with which one can write compatible code, this increases the modelling surface quite significantly.\n\nThere are further optimisations that we can get from existing cuda c kernels as in cudnn, but often one can write code to optimise it without going into cuda, directly in Julia.\n\nThere's also the question of complexity. Tim mentioned how one can write array operations for GPU compatible code. This means that code you write can be transformed easily to GPU kernels, and you can use ad on it the same way. The whole composable nature of it also allows for the same optimisation tricks that the compilers can do on the code generated, since it's all Julia code","user":"UC4QQPG4A","ts":"1613478627.204600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Vz9U","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"So the advantages go beyond the array kernels. There is a very rich ecosystem of GPU capable packages, thanks in part to the ease with which one can write compatible code, this increases the modelling surface quite significantly.\n\nThere are further optimisations that we can get from existing cuda c kernels as in cudnn, but often one can write code to optimise it without going into cuda, directly in Julia.\n\nThere's also the question of complexity. Tim mentioned how one can write array operations for GPU compatible code. This means that code you write can be transformed easily to GPU kernels, and you can use ad on it the same way. The whole composable nature of it also allows for the same optimisation tricks that the compilers can do on the code generated, since it's all Julia code"}]}]}]},{"client_msg_id":"27c1125c-1933-488b-ad38-fb55323a2065","type":"message","text":"Thanks for the answers, that shed some light. All in all this means that if I can write my code completly in PyTorch i.e. in Tensor notation then probably pytorch is faster since their kernels are more optimized. If I'd need something custom, I will have it easier in Julia since I can just write Julia code/use existing libraries?","user":"U7PD3M3L5","ts":"1613487521.206600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"CA9","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks for the answers, that shed some light. All in all this means that if I can write my code completly in PyTorch i.e. in Tensor notation then probably pytorch is faster since their kernels are more optimized. If I'd need something custom, I will have it easier in Julia since I can just write Julia code/use existing libraries?"}]}]}]},{"client_msg_id":"8762e1ff-635f-48da-b855-92fa57a9c8b5","type":"message","text":"I haven't followed its development very closely but Torch.jl also exists which means you can stay inside julia and use the C++ pytorch library","user":"U01C3624SGJ","ts":"1613487982.208200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sWA2V","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I haven't followed its development very closely but Torch.jl also exists which means you can stay inside julia and use the C++ pytorch library"}]}]}]},{"client_msg_id":"860087dc-e10c-4e2d-9f8a-36517effe9a7","type":"message","text":"<@U7PD3M3L5> that's not generally true","user":"U69BL50BF","ts":"1613488367.208400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"h537A","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U7PD3M3L5"},{"type":"text","text":" that's not generally true"}]}]}]},{"client_msg_id":"3bddbe18-ae34-45f4-a333-8d0779757d08","type":"message","text":"if you're doing a lot of broadcasted expressions then the codegen of CUDA.jl will definitely lead to more optimized code because of the kernel fusion","user":"U69BL50BF","ts":"1613488397.209100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ELnB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"if you're doing a lot of broadcasted expressions then the codegen of CUDA.jl will definitely lead to more optimized code because of the kernel fusion"}]}]}]},{"client_msg_id":"6720fef2-97ea-4142-b42c-5b92fb83a37d","type":"message","text":"But there are some specifically good kernels that PyTorch more readily uses. I think the RNN ones IIRC. There was some stuff in the CNN kernels but I think those were fixed up just 2 weeks ago (<@U68A3ASP9> would know that part). It's the RNN kernels that don't do all of the fusions PyTorch goes though.","user":"U69BL50BF","ts":"1613488458.210200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"0aKB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But there are some specifically good kernels that PyTorch more readily uses. I think the RNN ones IIRC. There was some stuff in the CNN kernels but I think those were fixed up just 2 weeks ago ("},{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":" would know that part). It's the RNN kernels that don't do all of the fusions PyTorch goes though."}]}]}]},{"client_msg_id":"757d34a5-dede-437f-b4a2-3f30a6786973","type":"message","text":"I thought the Pytorch RNNs just shell out to cuDNN?","user":"UMY1LV01G","ts":"1613493060.210700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j2TZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I thought the Pytorch RNNs just shell out to cuDNN?"}]}]}]},{"client_msg_id":"a76cec61-7bf1-4ea0-98ea-5b300fa14abc","type":"message","text":"they fuse in some nice way IIRC right <@UC4QQPG4A>?","user":"U69BL50BF","ts":"1613493152.211000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LVx8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"they fuse in some nice way IIRC right "},{"type":"user","user_id":"UC4QQPG4A"},{"type":"text","text":"?"}]}]}],"thread_ts":"1613493152.211000","reply_count":4,"reply_users_count":2,"latest_reply":"1613496605.219300","reply_users":["UMY1LV01G","UC4QQPG4A"],"subscribed":false},{"client_msg_id":"782e5c6d-682b-49ab-b7c1-83630e8e922d","type":"message","text":"This is a bit of an odd request, but I'm looking for the machinery to flip `c.colVal` and `c.rowPtr`  to `c.rowVal` and `c.colPtr` where `c` is a sparse matrix. I'm using this structure to represent a graph on the gpu, and this \"transposes\" the graph. I was wondering, where can I find a function that does that without having to build sparse matrices? It is a bit wasteful to allocate a vector of floats that I don't use.\n\nIt would be great for me to have something more \"hackable\", where I can play with the implementation (also because I'm working with multigraphs, so not all the assumptions for the `CuSparseMatrixCSR` format hold. I started playing with my own implementation but it seemed a bit tricky. From what I understand, the current sorting implementation (<https://github.com/JuliaGPU/CUDA.jl/pull/431/files>) does not have an optimized version for integer arrays with few unique values, which is the one I would need.","user":"U6BJ9E351","ts":"1613493902.217300","team":"T68168MUP","edited":{"user":"U6BJ9E351","ts":"1613494031.000000"},"blocks":[{"type":"rich_text","block_id":"F/H1T","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"This is a bit of an odd request, but I'm looking for the machinery to flip "},{"type":"text","text":"c.colVal","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"c.rowPtr","style":{"code":true}},{"type":"text","text":"  to "},{"type":"text","text":"c.rowVal","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"c.colPtr","style":{"code":true}},{"type":"text","text":" where "},{"type":"text","text":"c","style":{"code":true}},{"type":"text","text":" is a sparse matrix. I'm using this structure to represent a graph on the gpu, and this \"transposes\" the graph. I was wondering, where can I find a function that does that without having to build sparse matrices? It is a bit wasteful to allocate a vector of floats that I don't use.\n\nIt would be great for me to have something more \"hackable\", where I can play with the implementation (also because I'm working with multigraphs, so not all the assumptions for the "},{"type":"text","text":"CuSparseMatrixCSR","style":{"code":true}},{"type":"text","text":" format hold. I started playing with my own implementation but it seemed a bit tricky. From what I understand, the current sorting implementation ("},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/pull/431/files"},{"type":"text","text":") does not have an optimized version for integer arrays with few unique values, which is the one I would need."}]}]}]},{"client_msg_id":"f15c9771-9257-414c-8aee-401806a2fcdb","type":"message","text":"you can always use a lazy transpose/adjoint to turn the columns to rows","user":"U85JBUGGP","ts":"1613495829.218900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"1a+0s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you can always use a lazy transpose/adjoint to turn the columns to rows"}]}]}]},{"client_msg_id":"42b6dc92-7083-4e1d-96f6-5f846eb65532","type":"message","text":"Hey everyone, I'm trying to install CUDA.jl without artifacts and I'm getting the following exception:\n`CUDA.jl does not yet support CUDA with nvdisasm 11.0.167`\nDiscussions on GitHub suggest that this was fixed for 11.0, but I'm still getting it, maybe something went wrong on further updates? Any help would be much appreciated!","user":"U01NHTMSEHG","ts":"1613643051.224700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/WNJr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hey everyone, I'm trying to install CUDA.jl without artifacts and I'm getting the following exception:\n"},{"type":"text","text":"CUDA.jl does not yet support CUDA with nvdisasm 11.0.167","style":{"code":true}},{"type":"text","text":"\nDiscussions on GitHub suggest that this was fixed for 11.0, but I'm still getting it, maybe something went wrong on further updates? Any help would be much appreciated!"}]}]}]},{"client_msg_id":"96fbb82f-f7d5-4e5e-acbf-a41869fdb40d","type":"message","text":"I also couldn't get it to work with artifacts so maybe I'm doing something wrong...","user":"U01NHTMSEHG","ts":"1613643102.225200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vz4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I also couldn't get it to work with artifacts so maybe I'm doing something wrong..."}]}]}]},{"client_msg_id":"2c67042a-4184-429c-bd65-3fccd4976380","type":"message","text":"NVM, did a clean install, installation with artifacts works now (though non-artifact installation is still throwing the same error)","user":"U01NHTMSEHG","ts":"1613651365.226200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LeDRq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"NVM, did a clean install, installation with artifacts works now (though non-artifact installation is still throwing the same error)"}]}]}]},{"client_msg_id":"4766f4b6-3dee-47b6-9472-3c42cf22dcbc","type":"message","text":"What's the best way to get a single element from a `CuArray` if I disallow scalar indexing? I can hack it through a `copyto!` with a temporary `Array`, but wondering if there was a more elegant approach.","user":"UCRHP2GHE","ts":"1613685299.229300","team":"T68168MUP","edited":{"user":"UCRHP2GHE","ts":"1613685334.000000"},"blocks":[{"type":"rich_text","block_id":"xcBaa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What's the best way to get a single element from a "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":" if I disallow scalar indexing? I can hack it through a "},{"type":"text","text":"copyto!","style":{"code":true}},{"type":"text","text":" with a temporary "},{"type":"text","text":"Array","style":{"code":true}},{"type":"text","text":", but wondering if there was a more elegant approach."}]}]}],"thread_ts":"1613685299.229300","reply_count":1,"reply_users_count":1,"latest_reply":"1613686106.229500","reply_users":["UM30MT6RF"],"subscribed":false},{"client_msg_id":"7af3b557-eb74-4104-9dd6-4452bc4ebe33","type":"message","text":"Is there a workaround for <https://github.com/JuliaGPU/KernelAbstractions.jl/issues/7>? I need to compute the maximum of a given value (which is thread dependent) within a block and could only think of `CUDA.atomic_max!`  as a way to do that","user":"U6BJ9E351","ts":"1613732616.231000","team":"T68168MUP","edited":{"user":"U6BJ9E351","ts":"1613732627.000000"},"blocks":[{"type":"rich_text","block_id":"Y2S","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a workaround for "},{"type":"link","url":"https://github.com/JuliaGPU/KernelAbstractions.jl/issues/7"},{"type":"text","text":"? I need to compute the maximum of a given value (which is thread dependent) within a block and could only think of "},{"type":"text","text":"CUDA.atomic_max!","style":{"code":true}},{"type":"text","text":"  as a way to do that"}]}]}]},{"client_msg_id":"c7059b03-8a80-410e-a797-7ed7874bf3c8","type":"message","text":"No I don't think there is a work around, besides implementing either portable atomics or block level reductions in KA","user":"U67BJLYCS","ts":"1613737438.232700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UMH8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"No I don't think there is a work around, besides implementing either portable atomics or block level reductions in KA"}]}]}]},{"client_msg_id":"7b7338a4-5fe5-4685-95bb-97fd4dedbf95","type":"message","text":"Not impossible, just hard","user":"U67BJLYCS","ts":"1613737491.233200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=5T","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not impossible, just hard"}]}]}]},{"client_msg_id":"2ba08f83-3b73-4c3e-9c8a-ebba14886b63","type":"message","text":"Is there a way of profiling a device function? In my kernel function, I have several device functions, and I would like to profile them. nvprof() or other similar tool seems to give statistics of a kernel function only. In NVIDIA document, they seem to recommend to use clock() inside a device function to profile. CUDA.jl has a similar function?","user":"U01FXSDEXN3","ts":"1613766676.235800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Tnbp","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a way of profiling a device function? In my kernel function, I have several device functions, and I would like to profile them. nvprof() or other similar tool seems to give statistics of a kernel function only. In NVIDIA document, they seem to recommend to use clock() inside a device function to profile. CUDA.jl has a similar function?"}]}]}]},{"client_msg_id":"c1e3bcae-e3fb-488f-bcc8-cd0d0b1c12d3","type":"message","text":"NVidia profiling has PC profiling","user":"U67BJLYCS","ts":"1613770233.236300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IFkl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"NVidia profiling has PC profiling"}]}]}],"thread_ts":"1613770233.236300","reply_count":3,"reply_users_count":2,"latest_reply":"1613770807.236900","reply_users":["U01FXSDEXN3","U67BJLYCS"],"subscribed":false},{"client_msg_id":"292632b6-c949-4d35-a28d-736460e7f174","type":"message","text":"sorry for spamming this channel with KernelAbstractions question, but I'm a bit new to GPU computing and I'm finding out there is a bit of a learning curve. I was trying to figure out, is the relationship between Cartesian and Linear indexing the same as on the CPU? So basically things like `ndrange = (32, 32), groupsize = (16, 16)` (to write on an array of size `ndrange` ) would be slightly slower than `ndrange = (1024,), groupsize = (256,)`  (unless one has to manually use `fldmod1` to get back the cartesian indices inside the kernel)? Or are there some performance advantages in having a 2D groupsize?","user":"U6BJ9E351","ts":"1613841602.004400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vvq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"sorry for spamming this channel with KernelAbstractions question, but I'm a bit new to GPU computing and I'm finding out there is a bit of a learning curve. I was trying to figure out, is the relationship between Cartesian and Linear indexing the same as on the CPU? So basically things like "},{"type":"text","text":"ndrange = (32, 32), groupsize = (16, 16)","style":{"code":true}},{"type":"text","text":" (to write on an array of size "},{"type":"text","text":"ndrange","style":{"code":true}},{"type":"text","text":" ) would be slightly slower than "},{"type":"text","text":"ndrange = (1024,), groupsize = (256,)","style":{"code":true}},{"type":"text","text":"  (unless one has to manually use "},{"type":"text","text":"fldmod1","style":{"code":true}},{"type":"text","text":" to get back the cartesian indices inside the kernel)? Or are there some performance advantages in having a 2D groupsize?"}]}]}]},{"client_msg_id":"3c3367b0-74c2-449f-927e-a7af0c237aff","type":"message","text":"<https://discourse.julialang.org/t/how-to-generate-a-random-number-in-cuda-kernel-function/50364|this> discourse post by Tim Besard in nov 2020 says that random number generation is not supported in GPU kernels right now. Is that still the case?","user":"U011V2YN59N","ts":"1613844813.005600","team":"T68168MUP","attachments":[{"service_name":"JuliaLang","title":"How to generate a random number in CUDA kernel function","title_link":"https://discourse.julialang.org/t/how-to-generate-a-random-number-in-cuda-kernel-function/50364","text":"All the rand like function in CUDA.jl will get a CuArray. But memory allocation is not allowed in a kernel function. Thus, a CuArray can not be generated. So, are there any method to get a pure random number instead of an Array.","fallback":"JuliaLang: How to generate a random number in CUDA kernel function","thumb_url":"https://aws1.discourse-cdn.com/business5/uploads/julialang/original/2X/1/12829a7ba92b924d4ce81099cbf99785bee9b405.png","fields":[{"title":"Reading time","value":"1 mins :clock2:","short":true},{"title":"Likes","value":"7 :heart:","short":true}],"ts":1605690854,"from_url":"https://discourse.julialang.org/t/how-to-generate-a-random-number-in-cuda-kernel-function/50364","thumb_width":408,"thumb_height":263,"service_icon":"https://aws1.discourse-cdn.com/business5/uploads/julialang/optimized/2X/6/6ca888e296f59ca2a599807f7d5edd489e3d1829_2_180x180.png","id":1,"original_url":"https://discourse.julialang.org/t/how-to-generate-a-random-number-in-cuda-kernel-function/50364"}],"blocks":[{"type":"rich_text","block_id":"zHupm","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://discourse.julialang.org/t/how-to-generate-a-random-number-in-cuda-kernel-function/50364","text":"this"},{"type":"text","text":" discourse post by Tim Besard in nov 2020 says that random number generation is not supported in GPU kernels right now. Is that still the case?"}]}]}]},{"client_msg_id":"78ef8434-de1b-4d23-91ec-e9a932bef7fd","type":"message","text":"With a bit of care you can have random numbers on the GPU","user":"U67BJLYCS","ts":"1613846094.007500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jjn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"With a bit of care you can have random numbers on the GPU"}]}]}]},{"client_msg_id":"a2af553e-756e-45ac-9ca4-15b6f7622288","type":"message","text":"<https://juliafolds.github.io/FoldsCUDA.jl/dev/examples/monte_carlo_pi/>","user":"U67BJLYCS","ts":"1613846094.007700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Wfn","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://juliafolds.github.io/FoldsCUDA.jl/dev/examples/monte_carlo_pi/"}]}]}]},{"client_msg_id":"b2358d3b-9a40-4cef-8463-22e8d9b0d2b2","type":"message","text":"mm my gpu does not like that example :disappointed:","user":"U011V2YN59N","ts":"1613846510.008200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KC7S","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"mm my gpu does not like that example "},{"type":"emoji","name":"disappointed"}]}]}]},{"client_msg_id":"a9d3957b-304b-4def-84f4-506f36e45ff0","type":"message","text":"In general actually, `test CUDA`  produces many errors.. is it worth opening an issue?","user":"U011V2YN59N","ts":"1613846560.009000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bqQM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In general actually, "},{"type":"text","text":"test CUDA","style":{"code":true}},{"type":"text","text":"  produces many errors.. is it worth opening an issue?"}]}]}]},{"type":"message","text":"I wrote this tiny wrapper which \"adapts\" an array to unified memory instead of per-device memory. Its not quite right w.r.t. GC yet (but I think I can fix, granted I'm a noob here), but curious if something like this would be useful to have in CUDA, at least as a stopgap until more first-class unified memory support is achieved?","files":[{"id":"F01NSJ0PK1U","created":1614038063,"timestamp":1614038063,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"UUMJUCYRK","editable":false,"size":105967,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01NSJ0PK1U/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01NSJ0PK1U/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01NSJ0PK1U-651f9c7346/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01NSJ0PK1U-651f9c7346/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01NSJ0PK1U-651f9c7346/image_360.png","thumb_360_w":360,"thumb_360_h":331,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01NSJ0PK1U-651f9c7346/image_480.png","thumb_480_w":480,"thumb_480_h":441,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01NSJ0PK1U-651f9c7346/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01NSJ0PK1U-651f9c7346/image_720.png","thumb_720_w":720,"thumb_720_h":662,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01NSJ0PK1U-651f9c7346/image_800.png","thumb_800_w":800,"thumb_800_h":735,"original_w":892,"original_h":820,"thumb_tiny":"AwAsADC+3BpOc8Zp31Umlz7GgBoBPXNOx7mj8DS0AN2+5pcClooAbjJ6GlAApMZPQ0oGKAFzRmjNGaADNGaM0ZoAbjJzg0oGBSY+Y0qDigBc0ZpaKAEzRmlooA//2Q==","permalink":"https://julialang.slack.com/files/UUMJUCYRK/F01NSJ0PK1U/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01NSJ0PK1U-33a829397b","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"QYmt","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I wrote this tiny wrapper which \"adapts\" an array to unified memory instead of per-device memory. Its not quite right w.r.t. GC yet (but I think I can fix, granted I'm a noob here), but curious if something like this would be useful to have in CUDA, at least as a stopgap until more first-class unified memory support is achieved?"}]}]}],"user":"UUMJUCYRK","display_as_bot":false,"ts":"1614038094.011300"},{"client_msg_id":"7223d48a-7572-4c62-8f98-32f6005d5b98","type":"message","text":"Hi, can someone enable buildkite for <https://github.com/JuliaFolds/FoldsKernelAbstractions.jl>? (ping <@U68A3ASP9>, <@U67BJLYCS>, <@U7THT3TM3>)","user":"UC7AF7NSU","ts":"1614038411.012100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+O+M4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, can someone enable buildkite for "},{"type":"link","url":"https://github.com/JuliaFolds/FoldsKernelAbstractions.jl"},{"type":"text","text":"? (ping "},{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":", "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":", "},{"type":"user","user_id":"U7THT3TM3"},{"type":"text","text":")"}]}]}]},{"client_msg_id":"2f34f1d9-a7bc-4b25-a8f1-cb879f63edf9","type":"message","text":"also, can I access both Nividia and AMD machines from this repo? if that's not possible, accessing only Nvidia machine is still great","user":"UC7AF7NSU","ts":"1614038979.013600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Huj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"also, can I access both Nividia and AMD machines from this repo? if that's not possible, accessing only Nvidia machine is still great"}]}]}],"thread_ts":"1614038979.013600","reply_count":1,"reply_users_count":1,"latest_reply":"1614039098.013700","reply_users":["U6A0PD8CR"],"subscribed":false},{"client_msg_id":"2c697392-2b59-4f44-bd68-4601ea1ca546","type":"message","text":"Julian is working on it now","user":"UC7AF7NSU","ts":"1614039673.014100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"tGOn","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Julian is working on it now"}]}]}]},{"client_msg_id":"bd5b7d0c-99a7-4c98-ba8f-9b3d2da564ef","type":"message","text":"We still have trouble fully enabling it: <https://julialang.slack.com/archives/C689Y34LE/p1614041651014200?thread_ts=1614038411.012100&amp;cid=C689Y34LE>","user":"UC7AF7NSU","ts":"1614041979.014800","team":"T68168MUP","attachments":[{"from_url":"https://julialang.slack.com/archives/C689Y34LE/p1614041651014200?thread_ts=1614038411.012100&amp;cid=C689Y34LE","fallback":"[February 22nd, 2021 4:54 PM] jpsamaroo: I'm not an admin, so I can't make the pipeline public. Can one of the above-mentioned admins make it so?","ts":"1614041651.014200","author_id":"U6A0PD8CR","author_subname":"Julian Samaroo","channel_id":"C689Y34LE","channel_name":"gpu","is_msg_unfurl":true,"is_reply_unfurl":true,"text":"I'm not an admin, so I can't make the pipeline public. Can one of the above-mentioned admins make it so?","author_name":"Julian Samaroo","author_link":"https://julialang.slack.com/team/U6A0PD8CR","author_icon":"https://avatars.slack-edge.com/2017-07-18/215660769271_7ef635ba687405f810d2_48.jpg","mrkdwn_in":["text"],"id":1,"original_url":"https://julialang.slack.com/archives/C689Y34LE/p1614041651014200?thread_ts=1614038411.012100&amp;cid=C689Y34LE","footer":"From a thread in #gpu"}],"blocks":[{"type":"rich_text","block_id":"jyZ3s","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"We still have trouble fully enabling it: "},{"type":"link","url":"https://julialang.slack.com/archives/C689Y34LE/p1614041651014200?thread_ts=1614038411.012100&cid=C689Y34LE"}]}]}]},{"client_msg_id":"c17b295b-3668-48ef-833d-25aa750335ab","type":"message","text":"I'm trying to skip GPU CI on buildkite with PR labels. I'm trying\n\n```if: |\n  !(\n      build.pull_request.labels includes \"no cuda\" ||\n      build.pull_request.labels includes \"no gpu\"\n  )```\nBut this doesn't work. Does anyone make similar thing work? Is `if: build.message !~ /\\[skip tests\\]/` more reliable?\n\n<https://github.com/JuliaFolds/FoldsKernelAbstractions.jl/pull/5>","user":"UC7AF7NSU","ts":"1614047660.017400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Q=WjT","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm trying to skip GPU CI on buildkite with PR labels. I'm trying\n\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"if: |\n  !(\n      build.pull_request.labels includes \"no cuda\" ||\n      build.pull_request.labels includes \"no gpu\"\n  )"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\nBut this doesn't work. Does anyone make similar thing work? Is "},{"type":"text","text":"if: build.message !~ /\\[skip tests\\]/","style":{"code":true}},{"type":"text","text":" more reliable?\n\n"},{"type":"link","url":"https://github.com/JuliaFolds/FoldsKernelAbstractions.jl/pull/5"}]}]}]},{"client_msg_id":"f51ee45f-e7b8-44ee-a9db-86d5371442a9","type":"message","text":"I think I understood the strategy for performant matmul here (<https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/examples/performant_matmul.jl|https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/examples/performant_matmul.jl>), that example is very instructive. I'm trying to wrap my head around whether strategy is also effective for things like `conv`.\n\nIn particular, would one just use a similar tiling strategy summing over `tile1[i, k] * tile2[k, j]`, where there are as many values of `k` as there are \"compatible pixels\" (size filter times size image), or is it more effective to make fewer tiles and sum over `tile1[i, k] * tile2[k + s, j]` for `s` varying among the indices of the filter?","user":"U6BJ9E351","ts":"1614074563.028000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"KSRmu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I think I understood the strategy for performant matmul here ("},{"type":"link","url":"https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/examples/performant_matmul.jl","text":"https://github.com/JuliaGPU/KernelAbstractions.jl/blob/master/examples/performant_matmul.jl"},{"type":"text","text":"), that example is very instructive. I'm trying to wrap my head around whether strategy is also effective for things like "},{"type":"text","text":"conv","style":{"code":true}},{"type":"text","text":".\n\nIn particular, would one just use a similar tiling strategy summing over "},{"type":"text","text":"tile1[i, k] * tile2[k, j]","style":{"code":true}},{"type":"text","text":", where there are as many values of "},{"type":"text","text":"k","style":{"code":true}},{"type":"text","text":" as there are \"compatible pixels\" (size filter times size image), or is it more effective to make fewer tiles and sum over "},{"type":"text","text":"tile1[i, k] * tile2[k + s, j]","style":{"code":true}},{"type":"text","text":" for "},{"type":"text","text":"s ","style":{"code":true}},{"type":"text","text":"varying among the indices of the filter?"}]}]}]},{"client_msg_id":"8a932b1e-211c-4c20-8e07-9082e0f9ee15","type":"message","text":"The first strategy (pretend it's `matmul`) sounds a bit naive, but given that the \"copying the tiles\" part has lower computational complexity, I was wondering whether it was enough to get things packed together to speed up the multiplication part (with `@simd` and `@inbounds`). (I hope this is not too fuzzy/unclear!)","user":"U6BJ9E351","ts":"1614074781.032800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9fSC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The first strategy (pretend it's "},{"type":"text","text":"matmul","style":{"code":true}},{"type":"text","text":") sounds a bit naive, but given that the \"copying the tiles\" part has lower computational complexity, I was wondering whether it was enough to get things packed together to speed up the multiplication part (with "},{"type":"text","text":"@simd","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"@inbounds","style":{"code":true}},{"type":"text","text":"). (I hope this is not too fuzzy/unclear!)"}]}]}],"thread_ts":"1614074781.032800","reply_count":1,"reply_users_count":1,"latest_reply":"1614075180.032900","reply_users":["U6BJ9E351"],"subscribed":false},{"client_msg_id":"5dcf9bb3-b392-41fe-8315-956730d923aa","type":"message","text":"<@UC7AF7NSU> does tapir have any implications for gpu / accelerator programming ?","user":"UDGT4PM41","ts":"1614091461.034000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"AWvNU","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"UC7AF7NSU"},{"type":"text","text":" does tapir have any implications for gpu / accelerator programming ?"}]}]}],"thread_ts":"1614091461.034000","reply_count":5,"reply_users_count":2,"latest_reply":"1614092106.035100","reply_users":["UC7AF7NSU","UDGT4PM41"],"subscribed":false},{"client_msg_id":"84de9945-58a8-4479-a59e-35a8337b1bf9","type":"message","text":"I've been trying to get one-GPU-per-thread parallelization working (mainly to avoid paying the N-process Julia startup cost of the standard one-GPU-per-process way which is otherwise very robust). Between unified memory, <https://github.com/JuliaGPU/CUDA.jl/issues/731|#731> getting fixed, and recent stuff on master, quite a lot is working well! I'm stuck on the following thing though, wondering if I might get some hints of what might be going wrong that could help get a MWE. The general outline of what I'm running is\n```x = move_to_unified_memory(CuArray(...))\n\nThreads.@sync begin\n    Threads.@spawn begin\n        device!(0)\n        some_gpu_code(x)\n    end\n    Threads.@spawn begin\n        device!(1)\n        some_gpu_code(x)\n    end\nend```\nFor `some_gpu_code` being simple FFTs or array broadcasts, it works. For my more complex actual calculations (which in theory should just be a whole bunch of these chained together in some complex way), I get the segfault I attach in the thread. Any ideas? This is CUDA#master.","user":"UUMJUCYRK","ts":"1614162324.041500","team":"T68168MUP","edited":{"user":"UUMJUCYRK","ts":"1614162388.000000"},"blocks":[{"type":"rich_text","block_id":"/hNZ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I've been trying to get one-GPU-per-thread parallelization working (mainly to avoid paying the N-process Julia startup cost of the standard one-GPU-per-process way which is otherwise very robust). Between unified memory, "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/issues/731","text":"#731"},{"type":"text","text":" getting fixed, and recent stuff on master, quite a lot is working well! I'm stuck on the following thing though, wondering if I might get some hints of what might be going wrong that could help get a MWE. The general outline of what I'm running is\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"x = move_to_unified_memory(CuArray(...))\n\nThreads.@sync begin\n    Threads.@spawn begin\n        device!(0)\n        some_gpu_code(x)\n    end\n    Threads.@spawn begin\n        device!(1)\n        some_gpu_code(x)\n    end\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"For "},{"type":"text","text":"some_gpu_code","style":{"code":true}},{"type":"text","text":" being simple FFTs or array broadcasts, it works. For my more complex actual calculations (which in theory should just be a whole bunch of these chained together in some complex way), I get the segfault I attach in the thread. Any ideas? This is CUDA#master."}]}]}],"thread_ts":"1614162324.041500","reply_count":18,"reply_users_count":2,"latest_reply":"1614164847.046500","reply_users":["UUMJUCYRK","U68A3ASP9"],"subscribed":false},{"client_msg_id":"4a863dbc-d55a-48d0-bf3f-e1f93f5786d5","type":"message","text":"Is there a straightforward way to get poisson samples in GPU kernels?\n\nI know that `CUDA.rand_poisson!` exists but that seems to be host only. I saw the example by <@UC7AF7NSU> that uses `Random123.jl` to get random numbers in kernels, and that works for simple things like samples from geometric distributions, but it seems like sampling from poisson efficiently is quite complex, so I am wondering if this has already been done somewhere.","user":"U011V2YN59N","ts":"1614170758.051000","team":"T68168MUP","edited":{"user":"U011V2YN59N","ts":"1614172157.000000"},"blocks":[{"type":"rich_text","block_id":"Cyq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a straightforward way to get poisson samples in GPU kernels?\n\nI know that "},{"type":"text","text":"CUDA.rand_poisson!","style":{"code":true}},{"type":"text","text":" exists but that seems to be host only. I saw the example by "},{"type":"user","user_id":"UC7AF7NSU"},{"type":"text","text":" that uses "},{"type":"text","text":"Random123.jl","style":{"code":true}},{"type":"text","text":" to get random numbers in kernels, and that works for simple things like samples from geometric distributions, but it seems like sampling from poisson efficiently is quite complex, so I am wondering if this has already been done somewhere."}]}]}]},{"client_msg_id":"2d66fbac-50b2-4758-930a-8c677ae1f953","type":"message","text":"I couldn't get the poisson sampler in Distributions.jl to work out of the box, either, because it depends on `randexp`, which uses the tables for normal variates in `Base`, which are not `CuArrays`. I suppose I could just copy and paste all that with tweaks, but again I am wondering if this has been done somewhere.","user":"U011V2YN59N","ts":"1614172274.052700","team":"T68168MUP","edited":{"user":"U011V2YN59N","ts":"1614172302.000000"},"blocks":[{"type":"rich_text","block_id":"tYvF","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I couldn't get the poisson sampler in Distributions.jl to work out of the box, either, because it depends on "},{"type":"text","text":"randexp","style":{"code":true}},{"type":"text","text":", which uses the tables for normal variates in "},{"type":"text","text":"Base","style":{"code":true}},{"type":"text","text":", which are not "},{"type":"text","text":"CuArrays","style":{"code":true}},{"type":"text","text":". I suppose I could just copy and paste all that with tweaks, but again I am wondering if this has been done somewhere."}]}]}]},{"type":"message","text":"<https://github.com/vosen/ZLUDA> That looks fun.","user":"U9MD78Z9N","ts":"1614258546.054500","team":"T68168MUP","thread_ts":"1614258546.054500","reply_count":1,"reply_users_count":1,"latest_reply":"1614258962.054600","reply_users":["U01C3624SGJ"],"subscribed":false},{"client_msg_id":"093682f8-1f50-45dc-bf1b-37def0aa93af","type":"message","text":"Tried to run buildkite with julia v1.4; is this even possible?\nI get an error that I can’t interpret. I guess the error is clear that I need to file an issue :wink:\n`LoadError: CUDA.jl does not yet support CUDA with nvdisasm 11.1.74; please file an issue.`\n\n<https://buildkite.com/julialang/geophysicalflows-dot-jl/builds/105#cae0280c-c34c-49e8-b75c-f718b2d4f2b5>","user":"UBY3ENVBK","ts":"1614503194.060000","team":"T68168MUP","attachments":[{"service_name":"Buildkite","title":"GeophysicalFlows.jl #105 · JuliaLang","title_link":"https://buildkite.com/julialang/geophysicalflows-dot-jl/builds/105#cae0280c-c34c-49e8-b75c-f718b2d4f2b5","text":"julia 1.4","fallback":"Buildkite: GeophysicalFlows.jl #105 · JuliaLang","thumb_url":"https://avatars.githubusercontent.com/u/7112768?v=4","from_url":"https://buildkite.com/julialang/geophysicalflows-dot-jl/builds/105#cae0280c-c34c-49e8-b75c-f718b2d4f2b5","thumb_width":301,"thumb_height":301,"service_icon":"https://buildkite.com/favicon.ico","id":1,"original_url":"https://buildkite.com/julialang/geophysicalflows-dot-jl/builds/105#cae0280c-c34c-49e8-b75c-f718b2d4f2b5"}],"blocks":[{"type":"rich_text","block_id":"9Lw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Tried to run buildkite with julia v1.4; is this even possible?\nI get an error that I can’t interpret. I guess the error is clear that I need to file an issue "},{"type":"emoji","name":"wink"},{"type":"text","text":"\n"},{"type":"text","text":"LoadError: CUDA.jl does not yet support CUDA with nvdisasm 11.1.74; please file an issue.","style":{"code":true}},{"type":"text","text":"\n\n"},{"type":"link","url":"https://buildkite.com/julialang/geophysicalflows-dot-jl/builds/105#cae0280c-c34c-49e8-b75c-f718b2d4f2b5"}]}]}]},{"client_msg_id":"86de8e28-e2cc-405a-99e7-049a87e51a30","type":"message","text":"Old version of CUDA.jl. Either upgrade or use the cuda tag to request an older version. You can also force it to use artifacts, that's normally disabled on ci to save on network traffic.","user":"U68A3ASP9","ts":"1614506110.062200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"77NEA","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Old version of CUDA.jl. Either upgrade or use the cuda tag to request an older version. You can also force it to use artifacts, that's normally disabled on ci to save on network traffic."}]}]}]},{"client_msg_id":"dc5774a9-56b1-49e2-97d1-81f051c7ccde","type":"message","text":"Biweekly office hours happening in 45minutes","user":"U67BJLYCS","ts":"1614615460.066100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lZs","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Biweekly office hours happening in 45minutes"}]}]}],"thread_ts":"1614615460.066100","reply_count":3,"reply_users_count":3,"latest_reply":"1614615552.067800","reply_users":["U9MD78Z9N","U6A0PD8CR","U67BJLYCS"],"subscribed":false},{"client_msg_id":"bf1c5ff4-69f3-4e94-a43e-15ac6cec72bc","type":"message","text":"link in the channel description","user":"U67BJLYCS","ts":"1614615468.066400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"JLP","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"link in the channel description"}]}]}]},{"client_msg_id":"c143f3ed-5e3e-4b86-a4f7-bb0be7269146","type":"message","text":"I use `GPUArrays.default_rgn(CuArray)` to generate random numbers for a `CuArray` and found that it can only generate a maximum of 256 random numbers at one time. Starting form 257th, it’s the repeat of the first 256 numbers. Below is an example:\n```using CUDA, Random\nusing CUDA: GPUArrays\na = zeros(256*3) |&gt; CuArray\nrand!(GPUArrays.default_rng(CuArray), a)\na[1:256] == a[257:512] # true\na[1:256] == a[513:end] # true```\nI’m wondering is there a way to avoid this?","user":"U019U0QCF7F","ts":"1614615646.069100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7Oks","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I use "},{"type":"text","text":"GPUArrays.default_rgn(CuArray)","style":{"code":true}},{"type":"text","text":" to generate random numbers for a "},{"type":"text","text":"CuArray","style":{"code":true}},{"type":"text","text":" and found that it can only generate a maximum of 256 random numbers at one time. Starting form 257th, it’s the repeat of the first 256 numbers. Below is an example:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using CUDA, Random\nusing CUDA: GPUArrays\na = zeros(256*3) |> CuArray\nrand!(GPUArrays.default_rng(CuArray), a)\na[1:256] == a[257:512] # true\na[1:256] == a[513:end] # true"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I’m wondering is there a way to avoid this?"}]}]}]},{"client_msg_id":"e777fe10-f572-4a87-9d0a-2cc2cd42a3c7","type":"message","text":"<@U019U0QCF7F> Not sure about `GPUArrays.default_rgn` but why not use `Base.rand` and `Base.randn`? Could just do\n```a = rand(256*3) |&gt; CuArray```","user":"UEP056STX","ts":"1614616926.070000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Gj00","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U019U0QCF7F"},{"type":"text","text":" Not sure about "},{"type":"text","text":"GPUArrays.default_rgn","style":{"code":true}},{"type":"text","text":" but why not use "},{"type":"text","text":"Base.rand","style":{"code":true}},{"type":"text","text":" and "},{"type":"text","text":"Base.randn","style":{"code":true}},{"type":"text","text":"? Could just do\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"a = rand(256*3) |> CuArray"}]}]}],"thread_ts":"1614616926.070000","reply_count":1,"reply_users_count":1,"latest_reply":"1614618632.070500","reply_users":["U019U0QCF7F"],"subscribed":false}]}