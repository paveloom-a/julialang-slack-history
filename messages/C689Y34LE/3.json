{"cursor": 2, "messages": [{"client_msg_id":"8dcd4c88-8ee7-40f0-a495-454c45b48f9a","type":"message","text":"I was just slightly scared to see that e.g. `2 .* unified_memory_array` returns another unified memory array, but I suppose the result will be physically on whatever GPU computed it and from that point on theres no overhead for that GPU to do other things with this array?","user":"UUMJUCYRK","ts":"1611738155.063800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Zi8t","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I was just slightly scared to see that e.g. "},{"type":"text","text":"2 .* unified_memory_array","style":{"code":true}},{"type":"text","text":" returns another unified memory array, but I suppose the result will be physically on whatever GPU computed it and from that point on theres no overhead for that GPU to do other things with this array?"}]}]}]},{"client_msg_id":"f478c383-9923-469c-83fb-c1f5db4f74ef","type":"message","text":"&lt;sm_60 is OK as long as you have peer to peer, if not it'll keep the array in CPU memory and use slow PCI-E reads","user":"U68A3ASP9","ts":"1611738363.064400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"An5h","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"<sm_60 is OK as long as you have peer to peer, if not it'll keep the array in CPU memory and use slow PCI-E reads"}]}]}]},{"client_msg_id":"83f871ae-29dd-4053-88d5-e3a7a9cbe0db","type":"message","text":"&gt; sm_60 is stricly better because then your array, or the pages you're working with, will transparantly migrate to the GPU that is processing them","user":"U68A3ASP9","ts":"1611738385.065000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"QbRr+","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"sm_60 is stricly better because then your array, or the pages you're working with, will transparantly migrate to the GPU that is processing them"}]}]}]},{"client_msg_id":"be5904a8-a2d3-4312-9859-ee20c057dbc0","type":"message","text":"&gt; I was just slightly scared to see that e.g. 2 .* unified_memory_array returns another unified memory array,\nCuArray doesn't currently really support unified memory, so these allocations will be tied to a device, and not be unified","user":"U68A3ASP9","ts":"1611738430.065700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/rBQ","elements":[{"type":"rich_text_quote","elements":[{"type":"text","text":"I was just slightly scared to see that e.g. 2 .* unified_memory_array returns another unified memory array,"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"CuArray doesn't currently really support unified memory, so these allocations will be tied to a device, and not be unified"}]}]}]},{"type":"message","text":"do you mean that just the constructor doesn't support unified? once allocated, doesn't the fact that e.g. here I can access `y` from device 1 mean the result was in unified memory?","files":[{"id":"F01LQ6QPECQ","created":1611738630,"timestamp":1611738630,"name":"image.png","title":"image.png","mimetype":"image/png","filetype":"png","pretty_type":"PNG","user":"UUMJUCYRK","editable":false,"size":65370,"mode":"hosted","is_external":false,"external_type":"","is_public":true,"public_url_shared":false,"display_as_bot":false,"username":"","url_private":"https://files.slack.com/files-pri/T68168MUP-F01LQ6QPECQ/image.png","url_private_download":"https://files.slack.com/files-pri/T68168MUP-F01LQ6QPECQ/download/image.png","thumb_64":"https://files.slack.com/files-tmb/T68168MUP-F01LQ6QPECQ-e6ef08d6bb/image_64.png","thumb_80":"https://files.slack.com/files-tmb/T68168MUP-F01LQ6QPECQ-e6ef08d6bb/image_80.png","thumb_360":"https://files.slack.com/files-tmb/T68168MUP-F01LQ6QPECQ-e6ef08d6bb/image_360.png","thumb_360_w":360,"thumb_360_h":219,"thumb_480":"https://files.slack.com/files-tmb/T68168MUP-F01LQ6QPECQ-e6ef08d6bb/image_480.png","thumb_480_w":480,"thumb_480_h":292,"thumb_160":"https://files.slack.com/files-tmb/T68168MUP-F01LQ6QPECQ-e6ef08d6bb/image_160.png","thumb_720":"https://files.slack.com/files-tmb/T68168MUP-F01LQ6QPECQ-e6ef08d6bb/image_720.png","thumb_720_w":720,"thumb_720_h":438,"thumb_800":"https://files.slack.com/files-tmb/T68168MUP-F01LQ6QPECQ-e6ef08d6bb/image_800.png","thumb_800_w":800,"thumb_800_h":486,"original_w":954,"original_h":580,"thumb_tiny":"AwAdADDQ/GgfWlxznH6UuaADn1o59qM0ZoAOfajmjNGaAG4+bP8AWncegpvO7rxTsfWgAzRmlooATNGaWigD/9k=","permalink":"https://julialang.slack.com/files/UUMJUCYRK/F01LQ6QPECQ/image.png","permalink_public":"https://slack-files.com/T68168MUP-F01LQ6QPECQ-1d7571f0de","is_starred":false,"has_rich_preview":false}],"upload":false,"blocks":[{"type":"rich_text","block_id":"GkY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"do you mean that just the constructor doesn't support unified? once allocated, doesn't the fact that e.g. here I can access "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" from device 1 mean the result was in unified memory?"}]}]}],"user":"UUMJUCYRK","display_as_bot":false,"ts":"1611738650.066600"},{"client_msg_id":"c8683e18-dc9c-4f5e-839c-c158310612f5","type":"message","text":"huh","user":"U68A3ASP9","ts":"1611739191.067000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zVU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"huh"}]}]}]},{"client_msg_id":"f15c8593-25c6-4a32-a6d3-66051d56e93f","type":"message","text":"`CUDA.is_managed` returns `false`, so I'm surprised this doesn't trigger an illegal memory access","user":"U68A3ASP9","ts":"1611739500.067400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"9sGo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CUDA.is_managed","style":{"code":true}},{"type":"text","text":" returns "},{"type":"text","text":"false","style":{"code":true}},{"type":"text","text":", so I'm surprised this doesn't trigger an illegal memory access"}]}]}]},{"client_msg_id":"c59411d0-d7d0-48f8-bf74-7497765a852b","type":"message","text":"I guess with UVA, `memcpy` knows the device of origin and so you don't need to switch devices","user":"U68A3ASP9","ts":"1611739549.068000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"C4h","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I guess with UVA, "},{"type":"text","text":"memcpy","style":{"code":true}},{"type":"text","text":" knows the device of origin and so you don't need to switch devices"}]}]}]},{"client_msg_id":"e86c3fd1-d0ea-44d2-b5bc-e8453ca8d66e","type":"message","text":"The relevant `memcpy` here is the one to copy `y` to CPU to print it? Indeed `2 .* y` is an illegal memory access. In any case, that all makes sense to me then I think, and its also all I need (I don't need to later work on `y` from a different GPU)","user":"UUMJUCYRK","ts":"1611739974.069400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LeuON","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"The relevant "},{"type":"text","text":"memcpy","style":{"code":true}},{"type":"text","text":" here is the one to copy "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" to CPU to print it? Indeed "},{"type":"text","text":"2 .* y","style":{"code":true}},{"type":"text","text":" is an illegal memory access. In any case, that all makes sense to me then I think, and its also all I need (I don't need to later work on "},{"type":"text","text":"y","style":{"code":true}},{"type":"text","text":" from a different GPU)"}]}]}]},{"client_msg_id":"df1b4296-9439-4c3b-b223-ee6b2956050b","type":"message","text":"yeah correct","user":"U68A3ASP9","ts":"1611739990.069600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PNlMq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yeah correct"}]}]}]},{"client_msg_id":"d1b163ff-f5b9-4ab3-99ff-a07f4f52aa17","type":"message","text":"if you want to test out unified memory for more, I'd recommend trying to switch the CUDA.jl allocator to unified memory altogether","user":"U68A3ASP9","ts":"1611740014.070200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5hc5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"if you want to test out unified memory for more, I'd recommend trying to switch the CUDA.jl allocator to unified memory altogether"}]}]}]},{"client_msg_id":"10b8a7f7-32a1-4b5f-9fdf-66addd5a7c70","type":"message","text":"is that a built in option or do you mean hacking it?","user":"UUMJUCYRK","ts":"1611740313.070400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"q=Fx","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"is that a built in option or do you mean hacking it?"}]}]}]},{"client_msg_id":"5e9573f0-03b1-4580-872f-a35436daf000","type":"message","text":"hacking it, it should be a tiny change","user":"U68A3ASP9","ts":"1611740506.070600","team":"T68168MUP","edited":{"user":"U68A3ASP9","ts":"1611740509.000000"},"blocks":[{"type":"rich_text","block_id":"FZRO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hacking it, it should be a tiny change"}]}]}],"thread_ts":"1611740506.070600","reply_count":1,"reply_users_count":1,"latest_reply":"1611740514.070800","reply_users":["U68A3ASP9"],"subscribed":false,"reactions":[{"name":"+1","users":["UUMJUCYRK"],"count":1}]},{"client_msg_id":"3cae42dc-94ca-449f-8412-9cf029a38774","type":"message","text":"cool, will play around with it, thanks!","user":"UUMJUCYRK","ts":"1611740540.071300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5rZGI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"cool, will play around with it, thanks!"}]}]}]},{"client_msg_id":"537cccc4-f743-4dfb-af41-7eade5e02eaa","type":"message","text":"Does `diagm(cu(rand(10)))` make a CuArray? (I was trying to debug something via CI :cry:  and something was making an Array…)","user":"UD0NS8PDF","ts":"1611777449.072200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ddKYo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does "},{"type":"text","text":"diagm(cu(rand(10)))","style":{"code":true}},{"type":"text","text":" make a CuArray? (I was trying to debug something via CI "},{"type":"emoji","name":"cry"},{"type":"text","text":"  and something was making an Array…)"}]}]}]},{"client_msg_id":"c2591968-7408-4897-94c8-7b4d951f2b64","type":"message","text":"```ulia&gt; diagm(cu(rand(10)))\n┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`\n└ @ GPUArrays ~/.julia/packages/GPUArrays/WV76E/src/host/indexing.jl:43\n10×10 Array{Float32,2}:```","user":"U6N6VQE30","ts":"1611780003.072400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"w+t","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ulia> diagm(cu(rand(10)))\n┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`\n└ @ GPUArrays ~/.julia/packages/GPUArrays/WV76E/src/host/indexing.jl:43\n10×10 Array{Float32,2}:"}]}]}],"thread_ts":"1611780003.072400","reply_count":2,"reply_users_count":1,"latest_reply":"1611780740.072900","reply_users":["UD0NS8PDF"],"subscribed":false},{"client_msg_id":"c8f8ffa2-1fe8-473c-82ff-ff6de609bcdd","type":"message","text":"I got a dreaded device compatibility error when updating CUDA.\n```┌ Warning: Your Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 &lt; 5.0).\n│ Some functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n└ @ CUDA ~/.julia/packages/CUDA/wTQsK/src/state.jl:251```\nIs there a table mapping GPUs/compute versions to CUDA versions, or should I just be doing binary search to find the latest CUDA version that doesn't throw this error?","user":"U73ACR3TQ","ts":"1611795078.074200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"kr9Y","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I got a dreaded device compatibility error when updating CUDA.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"┌ Warning: Your Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n│ Some functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n└ @ CUDA ~/.julia/packages/CUDA/wTQsK/src/state.jl:251"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Is there a table mapping GPUs/compute versions to CUDA versions, or should I just be doing binary search to find the latest CUDA version that doesn't throw this error?"}]}]}]},{"client_msg_id":"1b6dfb38-547e-4b21-a50f-8c9425cbe42a","type":"message","text":"that's a warning, not an error.","user":"U68A3ASP9","ts":"1611818365.074700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UXY1k","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"that's a warning, not an error."}]}]}]},{"client_msg_id":"e6ae833d-b8ad-4b96-b1e8-57ffebe81fbe","type":"message","text":"I should probably add a table to the README, but the incompatibility doesn't matter much right now. you should be fine.","user":"U68A3ASP9","ts":"1611818452.075300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PDG","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I should probably add a table to the README, but the incompatibility doesn't matter much right now. you should be fine."}]}]}]},{"client_msg_id":"7228d4e6-8a6f-48ad-88aa-f095f76785e2","type":"message","text":"I would like to ask, as BLAS.nrm2 is the BLAS function for calculating the 2-norm on the cpu, for CuArrays it should be CUBLAS.nrm2 right?","user":"UTDSTSANP","ts":"1611842910.076700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lbxf/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I would like to ask, as BLAS.nrm2 is the BLAS function for calculating the 2-norm on the cpu, for CuArrays it should be CUBLAS.nrm2 right?"}]}]}],"thread_ts":"1611842910.076700","reply_count":1,"reply_users_count":1,"latest_reply":"1611843474.077600","reply_users":["U68A3ASP9"],"subscribed":false},{"client_msg_id":"128d2141-37f0-492e-98a1-89a911a4afdb","type":"message","text":"Does anyone know why random number generator with fixed seeds are so slow?\n```julia&gt; e = zeros(Float32, 100000, 100);\n\njulia&gt; e2 = cu(e);\n\njulia&gt; seed = rand(UInt64);\n\njulia&gt; @btime Random.seed!(seed);rand!(e);\n  10.654 μs (2 allocations: 112 bytes)\n\njulia&gt; @btime CUDA.seed!(seed);rand!(e2);\n  6.114 ms (18 allocations: 51.84 KiB)```\nLike that a 1000x factor slower","user":"U010WA4SZK5","ts":"1611843143.077500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"iaj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does anyone know why random number generator with fixed seeds are so slow?\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> e = zeros(Float32, 100000, 100);\n\njulia> e2 = cu(e);\n\njulia> seed = rand(UInt64);\n\njulia> @btime Random.seed!(seed);rand!(e);\n  10.654 μs (2 allocations: 112 bytes)\n\njulia> @btime CUDA.seed!(seed);rand!(e2);\n  6.114 ms (18 allocations: 51.84 KiB)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Like that a 1000x factor slower"}]}]}]},{"client_msg_id":"ea41910a-1458-46b5-a522-38aee7b3477c","type":"message","text":"Digression: in @btime, why do we put a $ when passing the parameters in the function? I tried both with and without putting the $ and it gives the same benchmarks literally","user":"UTDSTSANP","ts":"1611843650.079300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"sMNC","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Digression: in @btime, why do we put a $ when passing the parameters in the function? I tried both with and without putting the $ and it gives the same benchmarks literally"}]}]}]},{"client_msg_id":"a4e87888-ec50-44b1-89ca-d9cee8b69117","type":"message","text":"interpolation to avoid overhead of accessing global values, see the BenchmarkTools.jl README","user":"U68A3ASP9","ts":"1611843711.079900","team":"T68168MUP","edited":{"user":"U68A3ASP9","ts":"1611843716.000000"},"blocks":[{"type":"rich_text","block_id":"9d8T","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"interpolation to avoid overhead of accessing global values, see the BenchmarkTools.jl README"}]}]}],"thread_ts":"1611843711.079900","reply_count":1,"reply_users_count":1,"latest_reply":"1611843752.080100","reply_users":["UTDSTSANP"],"subscribed":false},{"client_msg_id":"cdec4cef-5262-4c52-b411-ce547441b795","type":"message","text":"What are `DenseCuArray` s and `StridedCuArray`s ? I just can't seem to find any documentation about these datatypes, could someone help please?","user":"UTDSTSANP","ts":"1611844867.083600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j1uH","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What are "},{"type":"text","text":"DenseCuArray","style":{"code":true}},{"type":"text","text":" s and "},{"type":"text","text":"StridedCuArray","style":{"code":true}},{"type":"text","text":"s ? I just can't seem to find any documentation about these datatypes, could someone help please?"}]}]}]},{"client_msg_id":"59ccc1a1-9503-4467-9e35-5e18b66a9e57","type":"message","text":"unions for contiguous vs strided memory, mirrorring Base's StridedArray (which is basically a non-contiguous view)","user":"U68A3ASP9","ts":"1611845307.084600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"N/gO","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"unions for contiguous vs strided memory, mirrorring Base's StridedArray (which is basically a non-contiguous view)"}]}]}]},{"client_msg_id":"899ecde0-d963-442c-a43f-5d7d3380023c","type":"message","text":"<https://julialang.slack.com/archives/C680MM7D4/p1611872031254800|https://julialang.slack.com/archives/C680MM7D4/p1611872031254800>","user":"UDGT4PM41","ts":"1611872391.086600","team":"T68168MUP","attachments":[{"from_url":"https://julialang.slack.com/archives/C680MM7D4/p1611872031254800","fallback":"[January 28th, 2021 2:13 PM] arikatzpro: Can someone please eli5 parallel scan?","ts":"1611872031.254800","author_id":"UDGT4PM41","author_subname":"Ari Katz","channel_id":"C680MM7D4","channel_name":"random","is_msg_unfurl":true,"text":"Can someone please eli5 parallel scan?","author_name":"Ari Katz","author_link":"https://julialang.slack.com/team/UDGT4PM41","author_icon":"https://secure.gravatar.com/avatar/a9e84ba6e7b9db667ae3371c11a07dfe.jpg?s=48&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0022-48.png","mrkdwn_in":["text"],"id":1,"original_url":"https://julialang.slack.com/archives/C680MM7D4/p1611872031254800","footer":"Posted in #random"}],"blocks":[{"type":"rich_text","block_id":"shlM9","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://julialang.slack.com/archives/C680MM7D4/p1611872031254800","text":"https://julialang.slack.com/archives/C680MM7D4/p1611872031254800"}]}]}]},{"client_msg_id":"9ce2f14e-99d0-4a91-a81c-b83beb0f2012","type":"message","text":"Does there exist a method to find out the absolute value element-wise in a CuArray?","user":"UTDSTSANP","ts":"1611901521.087800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xQZS=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does there exist a method to find out the absolute value element-wise in a CuArray?"}]}]}]},{"client_msg_id":"7f2a6b57-80c3-4a7b-835b-34244df18ac0","type":"message","text":"are you just looking for `abs.(u)`?","user":"U69BL50BF","ts":"1611901572.088200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GiM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"are you just looking for "},{"type":"text","text":"abs.(u)","style":{"code":true}},{"type":"text","text":"?"}]}]}]},{"client_msg_id":"f20d6658-4c40-4008-8bb2-d877f9a4d1d2","type":"message","text":"That was nice. I thought I would again encounter the scalar getindex error. Thank you Chris!","user":"UTDSTSANP","ts":"1611902030.088800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rd65C","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That was nice. I thought I would again encounter the scalar getindex error. Thank you Chris!"}]}]}]},{"client_msg_id":"65f748fd-10b6-40f7-8918-8461b49cbf82","type":"message","text":"Basic operation like `c = a .+ b` failed. Both a and b are small cuda arrays.\n\nsignal (11): Segmentation fault\nin expression starting at REPL[4]:1\nfesetenv at ./julia-1.6.0-beta1/bin/../lib/julia/libopenlibm.so (unknown line)\nunknown function (ip: 0x7f2049e7cf)\nAllocations: 44366399 (Pool: 44353014; Big: 13385); GC: 49\nSegmentation fault (core dumped)\n\nJulia : 1.6.0 beta\nCUDA : v2.6.1 installed through package manager.\n\nConfiguration aarch64. NVIDIA AGX hardware.\nIs this expected ? Given the uncommon configuration relative to regular x86-64 pc.","user":"UKREUAYEM","ts":"1611984331.090100","team":"T68168MUP","edited":{"user":"UKREUAYEM","ts":"1611985601.000000"},"blocks":[{"type":"rich_text","block_id":"vMGv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Basic operation like "},{"type":"text","text":"c = a .+ b ","style":{"code":true}},{"type":"text","text":"failed. Both a and b are small cuda arrays.\n\nsignal (11): Segmentation fault\nin expression starting at REPL[4]:1\nfesetenv at ./julia-1.6.0-beta1/bin/../lib/julia/libopenlibm.so (unknown line)\nunknown function (ip: 0x7f2049e7cf)\nAllocations: 44366399 (Pool: 44353014; Big: 13385); GC: 49\nSegmentation fault (core dumped)\n\nJulia : 1.6.0 beta\nCUDA : v2.6.1 installed through package manager.\n\nConfiguration aarch64. NVIDIA AGX hardware.\nIs this expected ? Given the uncommon configuration relative to regular x86-64 pc."}]}]}]},{"client_msg_id":"a2a20885-0453-4edd-84b2-7bdefdb989f2","type":"message","text":"Hm, I thought I had fixed that: <https://github.com/JuliaLang/julia/issues/38427>\nwhere do you get openlibm from? Is this a custom Julia build?\neither way, it's a bug in Julia, so you can re-open that issue or file a new one if it persists.","user":"U68A3ASP9","ts":"1612017297.091800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jsUd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hm, I thought I had fixed that: "},{"type":"link","url":"https://github.com/JuliaLang/julia/issues/38427"},{"type":"text","text":"\nwhere do you get openlibm from? Is this a custom Julia build?\neither way, it's a bug in Julia, so you can re-open that issue or file a new one if it persists."}]}]}],"thread_ts":"1612017297.091800","reply_count":1,"reply_users_count":1,"latest_reply":"1612024709.091900","reply_users":["UKREUAYEM"],"subscribed":false},{"client_msg_id":"29629506-d933-48cf-998d-498b4dc5eb6a","type":"message","text":"I'm playing around with KernelAbstractions,and am liking it a lot so far. I have a basic doubt as to whether something is possible. I understand that in a kernel, the function is run asynchronously and in parallel on all the indices in the `ndrange`.\n\nImagine that the result is not completely independent from the order, but to compute the kernel on a given index, I necessarily need to have computed some other indices before (the overall dependency structure will be some directed acyclic graph). Is there a way to tell KernelAbstractions to follow this order (which still allows for a lot of parallelism) or should one do a manual layering, do a kernel on a chunk, wait on the result, do another kernel, and so on?","user":"U6BJ9E351","ts":"1612126021.096900","team":"T68168MUP","edited":{"user":"U6BJ9E351","ts":"1612126290.000000"},"blocks":[{"type":"rich_text","block_id":"L19Tj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm playing around with KernelAbstractions,and am liking it a lot so far. I have a basic doubt as to whether something is possible. I understand that in a kernel, the function is run asynchronously and in parallel on all the indices in the "},{"type":"text","text":"ndrange","style":{"code":true}},{"type":"text","text":".\n\nImagine that the result is not completely independent from the order, but to compute the kernel on a given index, I necessarily need to have computed some other indices before (the overall dependency structure will be some directed acyclic graph). Is there a way to tell KernelAbstractions to follow this order (which still allows for a lot of parallelism) or should one do a manual layering, do a kernel on a chunk, wait on the result, do another kernel, and so on?"}]}]}],"thread_ts":"1612126021.096900","reply_count":5,"reply_users_count":2,"latest_reply":"1612173224.103700","reply_users":["UC7AF7NSU","U6BJ9E351"],"subscribed":false},{"client_msg_id":"49f6f6b6-7182-43f6-9f72-266b7409c517","type":"message","text":"You would need to define it as a operation of multiple kernels","user":"U67BJLYCS","ts":"1612126917.097900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"Sg0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You would need to define it as a operation of multiple kernels"}]}]}]},{"client_msg_id":"cf104ba8-35c1-4afc-be4b-9d57be40f8b1","type":"message","text":"Or I need to think real hard and add dynamic parallelism like CUDA has","user":"U67BJLYCS","ts":"1612126948.098700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"XzIv0","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Or I need to think real hard and add dynamic parallelism like CUDA has"}]}]}]},{"client_msg_id":"ad51cebb-c618-43e4-99d1-646708fd719a","type":"message","text":"ah, I see, so this (<https://juliagpu.github.io/CUDA.jl/dev/api/kernel/#Dynamic-parallelism>) is the low-level building block one would use to achieve that? I think it'd be useful to have a more high-level DAG-based formalism implemented on top of it, but I have no idea how easy / hard that is","user":"U6BJ9E351","ts":"1612128322.100200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"rMI8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"ah, I see, so this ("},{"type":"link","url":"https://juliagpu.github.io/CUDA.jl/dev/api/kernel/#Dynamic-parallelism"},{"type":"text","text":") is the low-level building block one would use to achieve that? I think it'd be useful to have a more high-level DAG-based formalism implemented on top of it, but I have no idea how easy / hard that is"}]}]}],"thread_ts":"1612128322.100200","reply_count":19,"reply_users_count":3,"latest_reply":"1612187366.105900","reply_users":["U6A0PD8CR","UC7AF7NSU","U6BJ9E351"],"subscribed":false},{"client_msg_id":"1949e392-13d8-4d41-bd98-9821d429a299","type":"message","text":"Could dagger lower to that ?","user":"UDGT4PM41","ts":"1612133871.100700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vMV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Could dagger lower to that ?"}]}]}],"thread_ts":"1612133871.100700","reply_count":1,"reply_users_count":1,"latest_reply":"1612142439.100800","reply_users":["U6A0PD8CR"],"subscribed":false},{"type":"message","subtype":"thread_broadcast","text":"Not sure what kind of DAGs you are talking about but is it possible to express it as a single kernel (even with some indexing tricks)? If so, maybe that's possible to formalize them as iterator combinators (like `zip`/`product`) and define a fold on it.","user":"UC7AF7NSU","ts":"1612145604.101600","thread_ts":"1612126021.096900","root":{"client_msg_id":"29629506-d933-48cf-998d-498b4dc5eb6a","type":"message","text":"I'm playing around with KernelAbstractions,and am liking it a lot so far. I have a basic doubt as to whether something is possible. I understand that in a kernel, the function is run asynchronously and in parallel on all the indices in the `ndrange`.\n\nImagine that the result is not completely independent from the order, but to compute the kernel on a given index, I necessarily need to have computed some other indices before (the overall dependency structure will be some directed acyclic graph). Is there a way to tell KernelAbstractions to follow this order (which still allows for a lot of parallelism) or should one do a manual layering, do a kernel on a chunk, wait on the result, do another kernel, and so on?","user":"U6BJ9E351","ts":"1612126021.096900","team":"T68168MUP","edited":{"user":"U6BJ9E351","ts":"1612126290.000000"},"blocks":[{"type":"rich_text","block_id":"L19Tj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm playing around with KernelAbstractions,and am liking it a lot so far. I have a basic doubt as to whether something is possible. I understand that in a kernel, the function is run asynchronously and in parallel on all the indices in the "},{"type":"text","text":"ndrange","style":{"code":true}},{"type":"text","text":".\n\nImagine that the result is not completely independent from the order, but to compute the kernel on a given index, I necessarily need to have computed some other indices before (the overall dependency structure will be some directed acyclic graph). Is there a way to tell KernelAbstractions to follow this order (which still allows for a lot of parallelism) or should one do a manual layering, do a kernel on a chunk, wait on the result, do another kernel, and so on?"}]}]}],"thread_ts":"1612126021.096900","reply_count":5,"reply_users_count":2,"latest_reply":"1612173224.103700","reply_users":["UC7AF7NSU","U6BJ9E351"],"subscribed":false},"blocks":[{"type":"rich_text","block_id":"U4TB","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Not sure what kind of DAGs you are talking about but is it possible to express it as a single kernel (even with some indexing tricks)? If so, maybe that's possible to formalize them as iterator combinators (like "},{"type":"text","text":"zip","style":{"code":true}},{"type":"text","text":"/"},{"type":"text","text":"product","style":{"code":true}},{"type":"text","text":") and define a fold on it."}]}]}],"client_msg_id":"eb905366-e52f-4d0f-923c-a9c307238570"},{"client_msg_id":"46b8c28b-972f-4595-8432-a0ae675630ac","type":"message","text":"Timely reminder JuliaGPU office hours in 2h","user":"U67BJLYCS","ts":"1612191191.106500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=jU4","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Timely reminder JuliaGPU office hours in 2h"}]}]}]},{"client_msg_id":"673be344-c4e7-444a-849c-ea385a7bead4","type":"message","text":"happening now","user":"U67BJLYCS","ts":"1612199536.106700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"bY9+L","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"happening now"}]}]}]},{"client_msg_id":"63e0c827-1577-4762-8faa-d6ef8531adb3","type":"message","text":"hey I tried checking out the new stream/task functionality using <https://github.com/JuliaGPU/CUDA.jl/pull/662#issuecomment-765273035|this example> but I'm not seeing the same behavior - mine looks like it's all running in one stream. Why might that be?Screen Shot 2021-02-03 at 5.00.24 PM","user":"U01G39CC63F","ts":"1612389717.109700","team":"T68168MUP","edited":{"user":"U01G39CC63F","ts":"1612389743.000000"},"blocks":[{"type":"rich_text","block_id":"pvsg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"hey I tried checking out the new stream/task functionality using "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/pull/662#issuecomment-765273035","text":"this example"},{"type":"text","text":" but I'm not seeing the same behavior - mine looks like it's all running in one stream. Why might that be?Screen Shot 2021-02-03 at 5.00.24 PM"}]}]}],"thread_ts":"1612389717.109700","reply_count":3,"reply_users_count":2,"latest_reply":"1612420878.112700","reply_users":["U01G39CC63F","U68A3ASP9"],"subscribed":false},{"client_msg_id":"db0d29ab-6f1b-4b52-8311-f2e2c8a0103f","type":"message","text":"<@U68A3ASP9> I was looking for the `whitelist` in `GPUCompiler`, but it seems to have disappeared between the latest release and master (and I can’t even find where it happened)","user":"U67G3QRJM","ts":"1612391646.112000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xre","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":" I was looking for the "},{"type":"text","text":"whitelist","style":{"code":true}},{"type":"text","text":" in "},{"type":"text","text":"GPUCompiler","style":{"code":true}},{"type":"text","text":", but it seems to have disappeared between the latest release and master (and I can’t even find where it happened)"}]}]}],"thread_ts":"1612391646.112000","reply_count":4,"reply_users_count":2,"latest_reply":"1612423047.113200","reply_users":["U67G3QRJM","U68A3ASP9"],"subscribed":false},{"client_msg_id":"5014175e-2ea2-497c-9d96-796b7a274a3f","type":"message","text":"```ERROR: CUDA error: a PTX JIT compilation failed (code 218, ERROR_INVALID_PTX)\nptxas application ptx input, line 2440; error   : Entry function '_Z27julia_broadcast_kernel_994815CuKernelContext8SubArrayI4DualIv7Float64Li15EELi1E13CuDeviceArrayIS1_IvS2_Li15EELi1ELi1EE5TupleIS3_I5Int64Li1ELi1EEELifalseEE11BroadcastedIvS4_I5OneToIS5_EE8_897_898IS1_IvS2_Li15EEES4_I8ExtrudedIS0_IS2_Li1ES3_IS2_Li1ELi1EES4_IS3_IS5_Li1ELi1EEELifalseEES4_I4BoolES4_IS5_EES4_I8PartialsILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_EEEES5_' uses too much parameter space (0x1958 bytes, 0x1100 max).\nptxas fatal   : Ptx assembly aborted due to errors```\nDoes this ring a bell to anyone? I have no idea what the parameter space is but it looks like a compiler limit of sorts. I use this function <https://github.com/JuliaDiff/ForwardDiff.jl/blob/a884ddd1f71f54db765cf41e6ecc5a38fd6db70e/src/apiutils.jl#L82>\n\nFor small `N` it seems to produce the expected result. With larger `N` (here `N=53`) this seems to break. A way to circumvent it is to write a kernel instead of using the ForwardDiff API.","user":"ULL3KSGBS","ts":"1612541798.119600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"vGIza","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"ERROR: CUDA error: a PTX JIT compilation failed (code 218, ERROR_INVALID_PTX)\nptxas application ptx input, line 2440; error   : Entry function '_Z27julia_broadcast_kernel_994815CuKernelContext8SubArrayI4DualIv7Float64Li15EELi1E13CuDeviceArrayIS1_IvS2_Li15EELi1ELi1EE5TupleIS3_I5Int64Li1ELi1EEELifalseEE11BroadcastedIvS4_I5OneToIS5_EE8_897_898IS1_IvS2_Li15EEES4_I8ExtrudedIS0_IS2_Li1ES3_IS2_Li1ELi1EES4_IS3_IS5_Li1ELi1EEELifalseEES4_I4BoolES4_IS5_EES4_I8PartialsILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_ES11_ILi15ES2_EEEES5_' uses too much parameter space (0x1958 bytes, 0x1100 max).\nptxas fatal   : Ptx assembly aborted due to errors"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Does this ring a bell to anyone? I have no idea what the parameter space is but it looks like a compiler limit of sorts. I use this function "},{"type":"link","url":"https://github.com/JuliaDiff/ForwardDiff.jl/blob/a884ddd1f71f54db765cf41e6ecc5a38fd6db70e/src/apiutils.jl#L82"},{"type":"text","text":"\n\nFor small "},{"type":"text","text":"N","style":{"code":true}},{"type":"text","text":" it seems to produce the expected result. With larger "},{"type":"text","text":"N","style":{"code":true}},{"type":"text","text":" (here "},{"type":"text","text":"N=53","style":{"code":true}},{"type":"text","text":") this seems to break. A way to circumvent it is to write a kernel instead of using the ForwardDiff API."}]}]}]},{"client_msg_id":"4cc24d95-3f47-497c-8691-fc717e077887","type":"message","text":"you're using a huge tuple as argument","user":"U68A3ASP9","ts":"1612542114.119900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"YrWv","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you're using a huge tuple as argument"}]}]}]},{"client_msg_id":"1f922629-8318-43de-b29c-2f9f5db6624d","type":"message","text":"bits types passed to kernels are passed by value, not by reference, and there's a limit: 4K IIRC","user":"U68A3ASP9","ts":"1612542135.120400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"epBTR","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"bits types passed to kernels are passed by value, not by reference, and there's a limit: 4K IIRC"}]}]}]},{"client_msg_id":"56e18ee3-7e89-4ebd-9c34-4439c789d5fb","type":"message","text":"you could pass the value as a single-valued array. Ref could work too, but isn't supported very well, so might result in other issues.","user":"U68A3ASP9","ts":"1612542202.121000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"SEwP=","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you could pass the value as a single-valued array. Ref could work too, but isn't supported very well, so might result in other issues."}]}]}]},{"client_msg_id":"d0bd9c18-efd1-4a1d-8ab5-6be97fdd47c7","type":"message","text":"<@U68A3ASP9> <@U67BJLYCS> <@U7THT3TM3> how do I set up buildkite? I think I set up buildkite and github for my account/organization but <https://buildkite.com/organizations/julialang/repository-providers> says \"Page not found\" for me. I'm guessing you need to add my repository?\n\nI'd like to use it in <https://github.com/JuliaFolds/FoldsCUDA.jl> and maybe at some point in <https://github.com/tkf/UnionArrays.jl>","user":"UC7AF7NSU","ts":"1612681159.121500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"j+DpH","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":" "},{"type":"user","user_id":"U67BJLYCS"},{"type":"text","text":" "},{"type":"user","user_id":"U7THT3TM3"},{"type":"text","text":" how do I set up buildkite? I think I set up buildkite and github for my account/organization but "},{"type":"link","url":"https://buildkite.com/organizations/julialang/repository-providers"},{"type":"text","text":" says \"Page not found\" for me. I'm guessing you need to add my repository?\n\nI'd like to use it in "},{"type":"link","url":"https://github.com/JuliaFolds/FoldsCUDA.jl"},{"type":"text","text":" and maybe at some point in "},{"type":"link","url":"https://github.com/tkf/UnionArrays.jl"}]}]}]},{"client_msg_id":"bd8fa02b-ee2a-484a-9b69-ed2ddbf3db1f","type":"message","text":"You'll need to temporarily add a Buildkite admin as an owner of the organization that owns the repo (JuliaFolds, in this case). Once the setup is done, you can convert them from an owner back to a regular member.","user":"U7THT3TM3","ts":"1612681398.122500","team":"T68168MUP","edited":{"user":"U7THT3TM3","ts":"1612681446.000000"},"blocks":[{"type":"rich_text","block_id":"Yib","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You'll need to temporarily add a Buildkite admin as an owner of the organization that owns the repo (JuliaFolds, in this case). Once the setup is done, you can convert them from an owner back to a regular member."}]}]}]},{"client_msg_id":"83bf9804-6d6c-4912-99c2-f07ef418015f","type":"message","text":"Also, Buildkite is really inconvenient to use for repos owned by users. I'd recommend moving UnionArrays.jl to the JuliaFolds org. Then the setup is easy.","user":"U7THT3TM3","ts":"1612681436.123300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"/Qc","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Also, Buildkite is really inconvenient to use for repos owned by users. I'd recommend moving UnionArrays.jl to the JuliaFolds org. Then the setup is easy."}]}]}]},{"client_msg_id":"8dec640e-8eb1-4f30-af59-3fbf210fd4d1","type":"message","text":"Thanks! I invited you as an owner and moved <https://github.com/tkf/UnionArrays.jl|UnionArrays.jl> there","user":"UC7AF7NSU","ts":"1612683588.123800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xbJSL","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Thanks! I invited you as an owner and moved "},{"type":"link","url":"https://github.com/tkf/UnionArrays.jl","text":"UnionArrays.jl"},{"type":"text","text":" there"}]}]}]},{"client_msg_id":"38e5f80e-bd41-447c-9e82-69b24033acb5","type":"message","text":"<https://github.com/JuliaFolds/FoldsCUDA.jl/pull/39>","user":"U7THT3TM3","ts":"1612684479.124000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"cauv","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://github.com/JuliaFolds/FoldsCUDA.jl/pull/39"}]}]}]},{"client_msg_id":"562e1372-e07b-4728-a66e-782644da4d14","type":"message","text":"You can convert me back to a regular member now BTW","user":"U7THT3TM3","ts":"1612684493.124300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"enPY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You can convert me back to a regular member now BTW"}]}]}]},{"client_msg_id":"bd0b75f2-14b2-4442-811e-b24ccb3f279b","type":"message","text":"Wow, you created the config. Thanks a lot!","user":"UC7AF7NSU","ts":"1612684540.124500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"GWT3","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Wow, you created the config. Thanks a lot!"}]}]}]},{"client_msg_id":"0c22c7cb-62a6-46ef-9314-09254d13c8de","type":"message","text":"I didn't set up coverage. If you want coverage, follow the instructions here: <https://github.com/JuliaGPU/buildkite/blob/main/README.md#using-secrets>","user":"U7THT3TM3","ts":"1612684589.125100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"xBMtd","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I didn't set up coverage. If you want coverage, follow the instructions here: "},{"type":"link","url":"https://github.com/JuliaGPU/buildkite/blob/main/README.md#using-secrets"}]}]}]},{"client_msg_id":"c609595a-60c9-4782-837f-10e3dcd6fc4b","type":"message","text":"gotcha","user":"UC7AF7NSU","ts":"1612684688.125300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"VEU","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"gotcha"}]}]}]},{"client_msg_id":"9edb20a0-08de-4485-a999-d2cb0eacbec6","type":"message","text":"Am I supposed to be able to see <https://buildkite.com/julialang/foldscuda-dot-jl/builds/4>? It says \"page not found\" for me. The link is from GitHub's build status","user":"UC7AF7NSU","ts":"1612684705.125700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"7H=jy","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Am I supposed to be able to see "},{"type":"link","url":"https://buildkite.com/julialang/foldscuda-dot-jl/builds/4"},{"type":"text","text":"? It says \"page not found\" for me. The link is from GitHub's build status"}]}]}]},{"client_msg_id":"25F54228-1EEF-4FA3-8642-3DEEFAED3B3E","type":"message","text":"Ah, I forgot to make the pipeline public.","user":"U7THT3TM3","ts":"1612685109.126300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"o7E","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Ah, I forgot to make the pipeline public."}]}]}]},{"client_msg_id":"9A27B939-C20E-4B14-87C9-BE33C70B8DDA","type":"message","text":"Okay, I made the pipeline public.","user":"U7THT3TM3","ts":"1612685115.126600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"UMY","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Okay, I made the pipeline public."}]}]}]},{"client_msg_id":"338575D7-58F0-4377-B992-3419818B43E5","type":"message","text":"Try it now.","user":"U7THT3TM3","ts":"1612685119.126800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=iw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Try it now."}]}]}]},{"client_msg_id":"1938cfad-7c93-47aa-b311-bdd2e2c13731","type":"message","text":"Works now!","user":"UC7AF7NSU","ts":"1612685169.127000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"LlS","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Works now!"}]}]}],"reactions":[{"name":"+1","users":["U7THT3TM3"],"count":1}]},{"client_msg_id":"3aa2f81e-e493-4407-9647-44242535cb5a","type":"message","text":"<@U7THT3TM3> what would need to be done to finish setting up buildkite here: <https://github.com/jump-dev/SCS.jl/pull/202> (I'm not the owner of the repo, I just contributed GPU/CUDA capabilities) cc: <@U014UPHHPM0>","user":"UHBF252VC","ts":"1612707847.128600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"uSfJo","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U7THT3TM3"},{"type":"text","text":" what would need to be done to finish setting up buildkite here: "},{"type":"link","url":"https://github.com/jump-dev/SCS.jl/pull/202"},{"type":"text","text":" (I'm not the owner of the repo, I just contributed GPU/CUDA capabilities) cc: "},{"type":"user","user_id":"U014UPHHPM0"}]}]}]},{"client_msg_id":"B7B302BD-D15C-4D03-8BAF-6DC3D6CDD9AE","type":"message","text":"Hmm I'm not sure. <@U68A3ASP9> ","user":"U7THT3TM3","ts":"1612732682.129300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"MjRq","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hmm I'm not sure. "},{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":" "}]}]}]},{"client_msg_id":"bb02326f-1fa7-4601-879b-f8f7317066a7","type":"message","text":"the Buildkite integration isn't set up on jump-dev. so me or <@U7THT3TM3> need to become an admin of the github jump-dev org temporarily.","user":"U68A3ASP9","ts":"1612769606.130000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lpI","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"the Buildkite integration isn't set up on jump-dev. so me or "},{"type":"user","user_id":"U7THT3TM3"},{"type":"text","text":" need to become an admin of the github jump-dev org temporarily."}]}]}]},{"client_msg_id":"6bd03791-8a5e-4735-b7f0-2044804bfe8c","type":"message","text":"I always get this warning when I run CUDA.jl for the first time:\n```/usr/bin/nvidia-modprobe: unrecognized option: \"-f\"\n\nERROR: Invalid commandline, please run `/usr/bin/nvidia-modprobe --help` for usage information.\n\n/usr/bin/nvidia-modprobe: unrecognized option: \"-f\"\n\nERROR: Invalid commandline, please run `/usr/bin/nvidia-modprobe --help` for usage information.```\nEverything still works just fine, will this have any other potential consequences? If not, is there a way to disable this?","user":"UM30MT6RF","ts":"1612789597.133200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"NyTV","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I always get this warning when I run CUDA.jl for the first time:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"/usr/bin/nvidia-modprobe: unrecognized option: \"-f\"\n\nERROR: Invalid commandline, please run `/usr/bin/nvidia-modprobe --help` for usage information.\n\n/usr/bin/nvidia-modprobe: unrecognized option: \"-f\"\n\nERROR: Invalid commandline, please run `/usr/bin/nvidia-modprobe --help` for usage information."}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Everything still works just fine, will this have any other potential consequences? If not, is there a way to disable this?"}]}]}]},{"client_msg_id":"faa0350a-26b5-4826-9f51-c660234eb7ab","type":"message","text":"CUDA.jl doesn't run `modprobe` or anything, this is your local system set-up","user":"U68A3ASP9","ts":"1612789789.133700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"zPE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CUDA.jl doesn't run "},{"type":"text","text":"modprobe","style":{"code":true}},{"type":"text","text":" or anything, this is your local system set-up"}]}]}]},{"client_msg_id":"ad7e8142-f3ec-49a8-9707-8f5437d47b0f","type":"message","text":"where did you get your NVIDIA driver from?","user":"U68A3ASP9","ts":"1612789837.134200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lWTvr","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"where did you get your NVIDIA driver from?"}]}]}]},{"client_msg_id":"d78db6e2-09f2-4aa1-b801-c5599e0698d0","type":"message","text":"Oh, I see. I did not set up the CUDA drivers on that computer, but I will ask","user":"UM30MT6RF","ts":"1612789895.135200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=X2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Oh, I see. I did not set up the CUDA drivers on that computer, but I will ask"}]}]}]},{"client_msg_id":"e1b199d1-9696-45da-91ba-26d5b299ddcc","type":"message","text":"I'm working to benchmark basic MMA calculations for varying input types and whether or not tensor cores are available. However, I'm struggling to determine when a CUBLAS gemmEx function is utilizing tensor cores for integer computations. When I run Float32 matrix calculations, there is an expected ~3.6x increase when allowing for tensor core usage vs when the pedantic math flag is used. For Int8 calculations though, the runtime is nearly identical. I thought CUBLAS supported dispatching appropriate Int8 matrix multiplications to tensor cores. Is this not the case? Can anyone point out some error I'm making?\n\nI've also somewhat looked into what's required to add Integer support into GemmKernels by wrapping the PTX instructions for each possible type and matrix size combos. However, if I were to continue the way all the PTX instructions are wrapped with intrinsics, the Turing/Ampere tensor core matrix size and type combos would make the work difficult. Generating the possible combos while not trying to generate invalid combos seems like it would lead to a large amount of if statements. Could the system be revamped to be a list of all possibilities rather than nested iterations through type and sizes? This also seems somewhat redundant. Perhaps when I get these benchmarks working, I'll start honest work on that addition, but per this thread (<https://github.com/JuliaGPU/GemmKernels.jl/issues/64>), maybe that's not worth the work if some other method is preferred. <@U68A3ASP9> <@UPMLA9F9S> Any thoughts?\n\nThe end goal for my work is to create a real-time correlator for a radio astronomy array. This involves large amounts of correlation on Complex Int8 data. We're looking for any way to accelerate these computations and see tensor cores as a promising route. I'm trying to come up with initial benchmarks to assess feasibility.\n\nCUDA versioninfo() output:\n```julia&gt; CUDA.versioninfo()\nCUDA toolkit 11.1.1, artifact installation\nCUDA driver 11.1.0\nNVIDIA driver 455.23.4\n\nLibraries: \n- CUBLAS: 11.3.0\n- CURAND: 10.2.2\n- CUFFT: 10.3.0\n- CUSOLVER: 11.0.1\n- CUSPARSE: 11.3.0\n- CUPTI: 14.0.0\n- NVML: 11.0.0+455.23.4\n- CUDNN: 8.0.4 (for CUDA 11.1.0)\n- CUTENSOR: 1.2.1 (for CUDA 11.1.0)\n\nToolchain:\n- Julia: 1.5.3\n- LLVM: 9.0.1\n- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4\n- Device support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75\n\n2 devices:\n  0: GeForce RTX 3090 (sm_86, 22.463 GiB / 23.700 GiB available)\n  1: GeForce GTX 1080 Ti (sm_61, 10.914 GiB / 10.917 GiB available)```\n\nMost relevant code:\n```# CUBLAS math flags:\n# CUBLAS.cublasSetMathMode(CUBLAS.handle(), CUBLAS.CUBLAS_PEDANTIC_MATH)\n# CUBLAS.cublasSetMathMode(CUBLAS.handle(), CUBLAS.CUBLAS_TENSOR_OP_MATH)\n\n# Test integer tensor core speedup\nfunction test_mma_cublas_i8(_size)\n    a     = rand(Int8, _size)\n    b     = rand(Int8, _size)\n    c     = rand(Int32, _size)\n\n    a_dev = CuArray(a)\n    b_dev = CuArray(b)\n    c_dev = CuArray(c)\n    d_dev = similar(c_dev)\n\n    CUDA.CUBLAS.gemmEx!('N', 'N', 1, a_dev, b_dev, 1, c_dev)\n\n    bench = @benchmark CUDA.@sync blocking=false CUDA.CUBLAS.gemmEx!($'N', $'N', $1, $a_dev, $b_dev, $1, $c_dev)\nend```\n\nFull benchmarking code:\n","user":"U01C67AS6F7","ts":"1612820663.139600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DI7","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I'm working to benchmark basic MMA calculations for varying input types and whether or not tensor cores are available. However, I'm struggling to determine when a CUBLAS gemmEx function is utilizing tensor cores for integer computations. When I run Float32 matrix calculations, there is an expected ~3.6x increase when allowing for tensor core usage vs when the pedantic math flag is used. For Int8 calculations though, the runtime is nearly identical. I thought CUBLAS supported dispatching appropriate Int8 matrix multiplications to tensor cores. Is this not the case? Can anyone point out some error I'm making?\n\nI've also somewhat looked into what's required to add Integer support into GemmKernels by wrapping the PTX instructions for each possible type and matrix size combos. However, if I were to continue the way all the PTX instructions are wrapped with intrinsics, the Turing/Ampere tensor core matrix size and type combos would make the work difficult. Generating the possible combos while not trying to generate invalid combos seems like it would lead to a large amount of if statements. Could the system be revamped to be a list of all possibilities rather than nested iterations through type and sizes? This also seems somewhat redundant. Perhaps when I get these benchmarks working, I'll start honest work on that addition, but per this thread ("},{"type":"link","url":"https://github.com/JuliaGPU/GemmKernels.jl/issues/64"},{"type":"text","text":"), maybe that's not worth the work if some other method is preferred. "},{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":" "},{"type":"user","user_id":"UPMLA9F9S"},{"type":"text","text":" Any thoughts?\n\nThe end goal for my work is to create a real-time correlator for a radio astronomy array. This involves large amounts of correlation on Complex Int8 data. We're looking for any way to accelerate these computations and see tensor cores as a promising route. I'm trying to come up with initial benchmarks to assess feasibility.\n\nCUDA versioninfo() output:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> CUDA.versioninfo()\nCUDA toolkit 11.1.1, artifact installation\nCUDA driver 11.1.0\nNVIDIA driver 455.23.4\n\nLibraries: \n- CUBLAS: 11.3.0\n- CURAND: 10.2.2\n- CUFFT: 10.3.0\n- CUSOLVER: 11.0.1\n- CUSPARSE: 11.3.0\n- CUPTI: 14.0.0\n- NVML: 11.0.0+455.23.4\n- CUDNN: 8.0.4 (for CUDA 11.1.0)\n- CUTENSOR: 1.2.1 (for CUDA 11.1.0)\n\nToolchain:\n- Julia: 1.5.3\n- LLVM: 9.0.1\n- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4\n- Device support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75\n\n2 devices:\n  0: GeForce RTX 3090 (sm_86, 22.463 GiB / 23.700 GiB available)\n  1: GeForce GTX 1080 Ti (sm_61, 10.914 GiB / 10.917 GiB available)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\n\nMost relevant code:\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"# CUBLAS math flags:\n# CUBLAS.cublasSetMathMode(CUBLAS.handle(), CUBLAS.CUBLAS_PEDANTIC_MATH)\n# CUBLAS.cublasSetMathMode(CUBLAS.handle(), CUBLAS.CUBLAS_TENSOR_OP_MATH)\n\n# Test integer tensor core speedup\nfunction test_mma_cublas_i8(_size)\n    a     = rand(Int8, _size)\n    b     = rand(Int8, _size)\n    c     = rand(Int32, _size)\n\n    a_dev = CuArray(a)\n    b_dev = CuArray(b)\n    c_dev = CuArray(c)\n    d_dev = similar(c_dev)\n\n    CUDA.CUBLAS.gemmEx!('N', 'N', 1, a_dev, b_dev, 1, c_dev)\n\n    bench = @benchmark CUDA.@sync blocking=false CUDA.CUBLAS.gemmEx!($'N', $'N', $1, $a_dev, $b_dev, $1, $c_dev)\nend"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"\n\nFull benchmarking code:\n"}]}]}]},{"client_msg_id":"e1b199d1-9696-45da-91ba-26d5b299ddcc","type":"message","text":"```[3:35 PM] using CUDA, LinearAlgebra, Test, BenchmarkTools\n\nfunction test_mma_cpu(type, s1, s2)\n    println(\"Benchmarking CPU multiplication ($(type), ($s1, $s2))\")\n    @benchmark LinearAlgebra.mul!(\n        $(zeros(type, s1, s2)),\n        $(rand(type, s1, s2)),\n        $(rand(type, s1, s2)))\n end\n\n function test_mma_gpu(type, s1, s2)\n    println(\"Benchmarking Native GPU multiplication ($(type), ($s1, $s2))\")\n    @benchmark CUDA.@sync blocking=false CUBLAS.mul!(\n        $(CUDA.zeros(type, s1, s2)),\n          $(cu(rand(type, s1, s2))),\n          $(cu(rand(type, s1, s2))))\n end\n\nfunction test_mma_cublas_i8(_size=(16,16))\n    a     = rand(Int8, _size)\n    b     = rand(Int8, _size)\n    c     = rand(Int32, _size)\n\n    a_dev = CuArray(a)\n    b_dev = CuArray(b)\n    c_dev = CuArray(c)\n    d_dev = similar(c_dev)\n\n    CUDA.CUBLAS.gemmEx!('N', 'N', 1, a_dev, b_dev, 1, c_dev)\n\n    bench = @benchmark CUDA.@sync blocking=false CUDA.CUBLAS.gemmEx!($'N', $'N', $1, $a_dev, $b_dev, $1, $c_dev)\nend\n\nfunction kernel_wmma(a_dev, b_dev, c_dev, d_dev)\n    conf = WMMA.Config{16, 16, 16, Float32}\n\n    a_frag = WMMA.load_a(pointer(a_dev), 16, WMMA.ColMajor, conf)\n    b_frag = WMMA.load_b(pointer(b_dev), 16, WMMA.ColMajor, conf)\n    c_frag = WMMA.load_c(pointer(c_dev), 16, WMMA.ColMajor, conf)\n\n    d_frag = WMMA.mma(a_frag, b_frag, c_frag, conf)\n\n    WMMA.store_d(pointer(d_dev), d_frag, 16, WMMA.ColMajor, conf)\n\n    return\nend\n\nfunction test_mma_wmma(_size=(16,16))\n    println(\"Benchmarking WMMA GPU multiplication ( $(_size))\")\n\n    a     = rand(Float16, _size)\n    b     = rand(Float16, _size)\n    c     = rand(Float32, _size)\n\n    a_dev = CuArray(a)\n    b_dev = CuArray(b)\n    c_dev = CuArray(c)\n    d_dev = similar(c_dev)\n\n    @cuda threads=32 kernel_wmma(a_dev, b_dev, c_dev, d_dev)\n    d = Array(d_dev)\n\n    @test all(isapprox.(a * b + c, d; rtol=0.01))\n\n    bench = @benchmark CUDA.@sync blocking=false begin\n        @cuda threads=1 kernel_wmma($a_dev, $b_dev, $c_dev, $d_dev)\n    end\n    return bench\nend\n\nfunction kernel_llvm(a_dev, b_dev, c_dev, d_dev)\n    a_frag = WMMA.llvm_wmma_load_a_col_m16n16k16_global_stride_f16(pointer(a_dev), 16)\n    b_frag = WMMA.llvm_wmma_load_b_col_m16n16k16_global_stride_f16(pointer(b_dev), 16)\n    c_frag = WMMA.llvm_wmma_load_c_col_m16n16k16_global_stride_f32(pointer(c_dev), 16)\n\n    d_frag = WMMA.llvm_wmma_mma_col_col_m16n16k16_f32_f32(a_frag, b_frag, c_frag)\n\n    WMMA.llvm_wmma_store_d_col_m16n16k16_global_stride_f32(pointer(d_dev), d_frag, 16)\n    return\nend\n\nfunction test_mma_llvm(size=(16,16))\n    println(\"Benchmarking LLVM GPU multiplication ($(size))\")\n\n    # Generate input matrices\n    a     = rand(Float16, size)\n    b     = rand(Float16, size)\n    c     = rand(Float32, size)\n\n    a_dev = CuArray(a)\n    b_dev = CuArray(b)\n    c_dev = CuArray(c)\n    d_dev = similar(c_dev)\n\n    @cuda threads=32 kernel_llvm(a_dev, b_dev, c_dev, d_dev)\n    # Verify calculations\n    @test all(isapprox.(a * b + c, Array(d_dev); rtol=0.01))\n\n    bench = @benchmark CUDA.@sync blocking=false begin\n        @cuda threads=32 kernel_llvm($a_dev, $b_dev, $c_dev, $d_dev)\n    end\n    return bench\nend\n\n\n\nfunction run_mma_bench(_type=Float16, _size=(16,16))\n    println(\"Benchmarking MMA with type: $(_type) of size: $(_size)\\n\")\n    println(\"CUDA device: $(CUDA.device())\")\n\n    bench_cpu  = test_mma_cpu(_type, _size[1], _size[2]);\n    bench_gpu  = test_mma_gpu(_type, _size[1], _size[2]);\n    bench_wmma = test_mma_wmma(_size)\n    bench_llvm = test_mma_llvm(_size)\n\n    cpu_mean  = mean(bench_cpu.times) / 1000.0\n    gpu_mean  = mean(bench_gpu.times) / 1000.0\n    wmma_mean = mean(bench_wmma.times)/ 1000.0\n    llvm_mean = mean(bench_llvm.times)/ 1000.0\n\n    println(\"\\nAverage compute times (us):\")\n    println(\"CPU:  $cpu_mean\")\n    println(\"GPU:  $gpu_mean  Speedup: $(cpu_mean / gpu_mean)\")\n    println(\"WMMA: $wmma_mean  Speedup: $(cpu_mean / wmma_mean)\")\n    println(\"LLVM: $llvm_mean  Speedup: $(cpu_mean / llvm_mean)\")\n    println(\"\\nSpeedup using Tensor Cores vs Native GPU: $(gpu_mean / ((wmma_mean + llvm_mean) / 2))\")\nend\n\n# Set tensor math:\n# CUBLAS.cublasSetMathMode(CUBLAS.handle(), CUBLAS.CUBLAS_TENSOR_OP_MATH)\n# CUBLAS_PEDANTIC_MATH (prevents tensor core usage), CUBLAS_DEFAULT_MATH (dispatches to TCs if possible) ```","user":"U01C67AS6F7","ts":"1612820663.139700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DI7-JrxL","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"[3:35 PM] using CUDA, LinearAlgebra, Test, BenchmarkTools\n\nfunction test_mma_cpu(type, s1, s2)\n    println(\"Benchmarking CPU multiplication ($(type), ($s1, $s2))\")\n    @benchmark LinearAlgebra.mul!(\n        $(zeros(type, s1, s2)),\n        $(rand(type, s1, s2)),\n        $(rand(type, s1, s2)))\n end\n\n function test_mma_gpu(type, s1, s2)\n    println(\"Benchmarking Native GPU multiplication ($(type), ($s1, $s2))\")\n    @benchmark CUDA.@sync blocking=false CUBLAS.mul!(\n        $(CUDA.zeros(type, s1, s2)),\n          $(cu(rand(type, s1, s2))),\n          $(cu(rand(type, s1, s2))))\n end\n\nfunction test_mma_cublas_i8(_size=(16,16))\n    a     = rand(Int8, _size)\n    b     = rand(Int8, _size)\n    c     = rand(Int32, _size)\n\n    a_dev = CuArray(a)\n    b_dev = CuArray(b)\n    c_dev = CuArray(c)\n    d_dev = similar(c_dev)\n\n    CUDA.CUBLAS.gemmEx!('N', 'N', 1, a_dev, b_dev, 1, c_dev)\n\n    bench = @benchmark CUDA.@sync blocking=false CUDA.CUBLAS.gemmEx!($'N', $'N', $1, $a_dev, $b_dev, $1, $c_dev)\nend\n\nfunction kernel_wmma(a_dev, b_dev, c_dev, d_dev)\n    conf = WMMA.Config{16, 16, 16, Float32}\n\n    a_frag = WMMA.load_a(pointer(a_dev), 16, WMMA.ColMajor, conf)\n    b_frag = WMMA.load_b(pointer(b_dev), 16, WMMA.ColMajor, conf)\n    c_frag = WMMA.load_c(pointer(c_dev), 16, WMMA.ColMajor, conf)\n\n    d_frag = WMMA.mma(a_frag, b_frag, c_frag, conf)\n\n    WMMA.store_d(pointer(d_dev), d_frag, 16, WMMA.ColMajor, conf)\n\n    return\nend\n\nfunction test_mma_wmma(_size=(16,16))\n    println(\"Benchmarking WMMA GPU multiplication ( $(_size))\")\n\n    a     = rand(Float16, _size)\n    b     = rand(Float16, _size)\n    c     = rand(Float32, _size)\n\n    a_dev = CuArray(a)\n    b_dev = CuArray(b)\n    c_dev = CuArray(c)\n    d_dev = similar(c_dev)\n\n    @cuda threads=32 kernel_wmma(a_dev, b_dev, c_dev, d_dev)\n    d = Array(d_dev)\n\n    @test all(isapprox.(a * b + c, d; rtol=0.01))\n\n    bench = @benchmark CUDA.@sync blocking=false begin\n        @cuda threads=1 kernel_wmma($a_dev, $b_dev, $c_dev, $d_dev)\n    end\n    return bench\nend\n\nfunction kernel_llvm(a_dev, b_dev, c_dev, d_dev)\n    a_frag = WMMA.llvm_wmma_load_a_col_m16n16k16_global_stride_f16(pointer(a_dev), 16)\n    b_frag = WMMA.llvm_wmma_load_b_col_m16n16k16_global_stride_f16(pointer(b_dev), 16)\n    c_frag = WMMA.llvm_wmma_load_c_col_m16n16k16_global_stride_f32(pointer(c_dev), 16)\n\n    d_frag = WMMA.llvm_wmma_mma_col_col_m16n16k16_f32_f32(a_frag, b_frag, c_frag)\n\n    WMMA.llvm_wmma_store_d_col_m16n16k16_global_stride_f32(pointer(d_dev), d_frag, 16)\n    return\nend\n\nfunction test_mma_llvm(size=(16,16))\n    println(\"Benchmarking LLVM GPU multiplication ($(size))\")\n\n    # Generate input matrices\n    a     = rand(Float16, size)\n    b     = rand(Float16, size)\n    c     = rand(Float32, size)\n\n    a_dev = CuArray(a)\n    b_dev = CuArray(b)\n    c_dev = CuArray(c)\n    d_dev = similar(c_dev)\n\n    @cuda threads=32 kernel_llvm(a_dev, b_dev, c_dev, d_dev)\n    # Verify calculations\n    @test all(isapprox.(a * b + c, Array(d_dev); rtol=0.01))\n\n    bench = @benchmark CUDA.@sync blocking=false begin\n        @cuda threads=32 kernel_llvm($a_dev, $b_dev, $c_dev, $d_dev)\n    end\n    return bench\nend\n\n\n\nfunction run_mma_bench(_type=Float16, _size=(16,16))\n    println(\"Benchmarking MMA with type: $(_type) of size: $(_size)\\n\")\n    println(\"CUDA device: $(CUDA.device())\")\n\n    bench_cpu  = test_mma_cpu(_type, _size[1], _size[2]);\n    bench_gpu  = test_mma_gpu(_type, _size[1], _size[2]);\n    bench_wmma = test_mma_wmma(_size)\n    bench_llvm = test_mma_llvm(_size)\n\n    cpu_mean  = mean(bench_cpu.times) / 1000.0\n    gpu_mean  = mean(bench_gpu.times) / 1000.0\n    wmma_mean = mean(bench_wmma.times)/ 1000.0\n    llvm_mean = mean(bench_llvm.times)/ 1000.0\n\n    println(\"\\nAverage compute times (us):\")\n    println(\"CPU:  $cpu_mean\")\n    println(\"GPU:  $gpu_mean  Speedup: $(cpu_mean / gpu_mean)\")\n    println(\"WMMA: $wmma_mean  Speedup: $(cpu_mean / wmma_mean)\")\n    println(\"LLVM: $llvm_mean  Speedup: $(cpu_mean / llvm_mean)\")\n    println(\"\\nSpeedup using Tensor Cores vs Native GPU: $(gpu_mean / ((wmma_mean + llvm_mean) / 2))\")\nend\n\n# Set tensor math:\n# CUBLAS.cublasSetMathMode(CUBLAS.handle(), CUBLAS.CUBLAS_TENSOR_OP_MATH)\n# CUBLAS_PEDANTIC_MATH (prevents tensor core usage), CUBLAS_DEFAULT_MATH (dispatches to TCs if possible) "}]}]}]},{"client_msg_id":"e1b199d1-9696-45da-91ba-26d5b299ddcc","type":"message","text":"","user":"U01C67AS6F7","ts":"1612820663.139800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DI7-JrxL-pe7","elements":[{"type":"rich_text_section","elements":[]}]}]},{"client_msg_id":"1497e32b-9874-4695-a33f-1900f439e0b4","type":"message","text":"Some Googling suggests that (at least as of May 2018) people were having trouble getting CUBLAS to use Tensor Cores for Int8 GEMM: <https://forums.developer.nvidia.com/t/cublas-int8-tensor-core-mode-vs-fp16-mode/70510>","user":"U7THT3TM3","ts":"1612823218.140900","team":"T68168MUP","attachments":[{"service_name":"NVIDIA Developer Forums","title":"cuBLAS INT8 tensor core mode vs. FP16 mode","title_link":"https://forums.developer.nvidia.com/t/cublas-int8-tensor-core-mode-vs-fp16-mode/70510","text":"Hi all, I recently acquired an RTX card and was testing the new INT8 tensor core mode supported by Turing. I put together a simple test program (based on the “Programming Tensor Cores” devblogs article) to compare the execution times of INT8 mode vs. FP16 mode using the tensor cores. Strangely the execution times of tensor-FP16 mode and tensor-INT8 mode are practically the same. I was expecting much better execution times for tensor-INT8 mode since it’s supposed to have nearly twice the through...","fallback":"NVIDIA Developer Forums: cuBLAS INT8 tensor core mode vs. FP16 mode","thumb_url":"https://aws1.discourse-cdn.com/nvidia/original/2X/8/8f17cc8f1a724d6ecea8a197a267ec8a05ef1490.png","ts":1550251569,"from_url":"https://forums.developer.nvidia.com/t/cublas-int8-tensor-core-mode-vs-fp16-mode/70510","thumb_width":237,"thumb_height":42,"service_icon":"https://aws1.discourse-cdn.com/nvidia/optimized/2X/8/819b2855e1f1f3249e77dc713405cf77d1eda57c_2_180x180.png","id":1,"original_url":"https://forums.developer.nvidia.com/t/cublas-int8-tensor-core-mode-vs-fp16-mode/70510"}],"blocks":[{"type":"rich_text","block_id":"Qnl","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Some Googling suggests that (at least as of May 2018) people were having trouble getting CUBLAS to use Tensor Cores for Int8 GEMM: "},{"type":"link","url":"https://forums.developer.nvidia.com/t/cublas-int8-tensor-core-mode-vs-fp16-mode/70510"}]}]}],"thread_ts":"1612823218.140900","reply_count":1,"reply_users_count":1,"latest_reply":"1612823809.141800","reply_users":["U01C67AS6F7"],"subscribed":false,"reactions":[{"name":"thumbsup_all","users":["U01C67AS6F7"],"count":1}]},{"client_msg_id":"c426a042-6254-4c47-8d45-92b35735ee79","type":"message","text":"In Feb 2019, someone wrote: \"Thanks Mod, you’re probably right that tensor-INT8 mode is not yet supported under CUBLAS.\"","user":"U7THT3TM3","ts":"1612823248.141300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"ZtRf","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"In Feb 2019, someone wrote: \"Thanks Mod, you’re probably right that tensor-INT8 mode is not yet supported under CUBLAS.\""}]}]}]},{"client_msg_id":"44b84003-86a9-4493-8a33-63df61d79b7a","type":"message","text":"But presumably something has changed two years later?","user":"U7THT3TM3","ts":"1612823429.141700","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"DpXN","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"But presumably something has changed two years later?"}]}]}]},{"client_msg_id":"492bf307-425d-4fcb-a60b-d9e045a7b508","type":"message","text":"int8 gemmex is broken again in cuda 11.2","user":"U68A3ASP9","ts":"1612853008.142200","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"5XD4N","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"int8 gemmex is broken again in cuda 11.2"}]}]}]},{"client_msg_id":"5e9dbac0-ccf0-437c-9b88-e3982adc2700","type":"message","text":"see <https://github.com/JuliaGPU/CUDA.jl/blob/4eb99b9f53acfc02a01f92d4a0a2b219bf8994cc/lib/cublas/wrappers.jl#L745-L810>, where we decide which mode to use","user":"U68A3ASP9","ts":"1612853011.142500","team":"T68168MUP","edited":{"user":"U68A3ASP9","ts":"1612853017.000000"},"blocks":[{"type":"rich_text","block_id":"HfW","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"see "},{"type":"link","url":"https://github.com/JuliaGPU/CUDA.jl/blob/4eb99b9f53acfc02a01f92d4a0a2b219bf8994cc/lib/cublas/wrappers.jl#L745-L810"},{"type":"text","text":", where we decide which mode to use"}]}]}]},{"client_msg_id":"5e9a628a-5ac0-4974-b8d5-99e0e8869277","type":"message","text":"you can always run with `JULIA_DEBUG=CUBLAS` to see which gemmEx mode the `mul!` boils down to","user":"U68A3ASP9","ts":"1612853030.143000","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fvw","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"you can always run with "},{"type":"text","text":"JULIA_DEBUG=CUBLAS","style":{"code":true}},{"type":"text","text":" to see which gemmEx mode the "},{"type":"text","text":"mul!","style":{"code":true}},{"type":"text","text":" boils down to"}]}]}]},{"client_msg_id":"03ab2590-48c1-45b2-b508-6ec21cca5b13","type":"message","text":"<@U68A3ASP9> i was exploring functions in CUDA and a call normcdf aborts julia directly\n`CUDA.normcdf(1.0)`\n```julia&gt; CUDA.normcdf(1.0)\nFATAL ERROR: Symbol \"__nv_normcdf\"not found\nsignal (6): Aborted\nin expression starting at REPL[6]:1\nraise at /lib/aarch64-linux-gnu/libc.so.6 (unknown line)\nraise at /lib/aarch64-linux-gnu/libc.so.6 (unknown line)\nAllocations: 13204160 (Pool: 13201587; Big: 2573); GC: 8\nAborted (core dumped)```\njulia 1.6.-rc1","user":"UKREUAYEM","ts":"1612932179.147400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"IruH","elements":[{"type":"rich_text_section","elements":[{"type":"user","user_id":"U68A3ASP9"},{"type":"text","text":" i was exploring functions in CUDA and a call normcdf aborts julia directly\n"},{"type":"text","text":"CUDA.normcdf(1.0)","style":{"code":true}},{"type":"text","text":"\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> CUDA.normcdf(1.0)\nFATAL ERROR: Symbol \"__nv_normcdf\"not found\nsignal (6): Aborted\nin expression starting at REPL[6]:1\nraise at /lib/aarch64-linux-gnu/libc.so.6 (unknown line)\nraise at /lib/aarch64-linux-gnu/libc.so.6 (unknown line)\nAllocations: 13204160 (Pool: 13201587; Big: 2573); GC: 8\nAborted (core dumped)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"julia 1.6.-rc1"}]}]}],"thread_ts":"1612932179.147400","reply_count":2,"reply_users_count":1,"latest_reply":"1612932404.147800","reply_users":["UKREUAYEM"],"subscribed":false},{"client_msg_id":"7564c151-bb26-4179-b5e9-5876d3586c3c","type":"message","text":"That is expected","user":"U67BJLYCS","ts":"1612932669.151800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"aC1I","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"That is expected"}]}]}],"thread_ts":"1612932669.151800","reply_count":2,"reply_users_count":2,"latest_reply":"1612932991.153700","reply_users":["UKREUAYEM","U67BJLYCS"],"subscribed":false},{"client_msg_id":"1d42e970-39af-4aec-8746-220a3387a4a9","type":"message","text":"You are calling a GPU only function on the CPU","user":"U67BJLYCS","ts":"1612932684.152400","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"o=5z","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"You are calling a GPU only function on the CPU"}]}]}],"reactions":[{"name":"+1","users":["UKREUAYEM"],"count":1}]},{"client_msg_id":"6a48551c-9ca8-4a63-9308-c6055d61061a","type":"message","text":"I struggled with rand number generation in CUDA kernel. I somehow resolved it by passing precomputed random values and passing it as an argument to the kernel. But this only works with `CUDA.rand(len)`; I couldn't use `CUDA.randn(len)`; In my case I really need gaussian random variable in kernel code. Is there any other way to do it.","user":"UKREUAYEM","ts":"1612932832.153400","team":"T68168MUP","edited":{"user":"UKREUAYEM","ts":"1612933091.000000"},"blocks":[{"type":"rich_text","block_id":"sFu","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I struggled with rand number generation in CUDA kernel. I somehow resolved it by passing precomputed random values and passing it as an argument to the kernel. But this only works with "},{"type":"text","text":"CUDA.rand(len)","style":{"code":true}},{"type":"text","text":"; I couldn't use "},{"type":"text","text":"CUDA.randn(len)","style":{"code":true}},{"type":"text","text":"; In my case I really need gaussian random variable in kernel code. Is there any other way to do it."}]}]}],"thread_ts":"1612932832.153400","reply_count":1,"reply_users_count":1,"latest_reply":"1612933703.155300","reply_users":["UKREUAYEM"],"subscribed":false},{"client_msg_id":"e5024f99-56cf-41ce-adf3-a8690a3b504f","type":"message","text":"```julia&gt; rand(2, 100) |&gt; gpu\nERROR: CUDA error: an illegal memory access was encountered (code 700, ERROR_ILLEGAL_ADDRESS)```\nAnyone knows why this may be happening ?","user":"UKJSNT1QR","ts":"1613052819.157100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"+2JF","elements":[{"type":"rich_text_preformatted","elements":[{"type":"text","text":"julia> rand(2, 100) |> gpu\nERROR: CUDA error: an illegal memory access was encountered (code 700, ERROR_ILLEGAL_ADDRESS)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"Anyone knows why this may be happening ?"}]}]}]},{"client_msg_id":"2b410c82-a5cd-4800-bbd2-08260a48b1fc","type":"message","text":"CUDA errors are sticky, so anything bad you might have done before will remain until you restart julia","user":"U68A3ASP9","ts":"1613052866.157500","team":"T68168MUP","edited":{"user":"U68A3ASP9","ts":"1613052871.000000"},"blocks":[{"type":"rich_text","block_id":"5Je","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"CUDA errors are sticky, so anything bad you might have done before will remain until you restart julia"}]}]}]},{"client_msg_id":"4933b1bf-9dad-43cf-bca2-8b7017d8d443","type":"message","text":"Seems to be the case, thanks","user":"UKJSNT1QR","ts":"1613053178.157800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"hM9BQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Seems to be the case, thanks"}]}]}]},{"client_msg_id":"8905aff6-cdb5-47de-8728-c0c1d97fbe01","type":"message","text":"What usually causes ERROR_ILLEGAL_ADDRESS ?","user":"UKJSNT1QR","ts":"1613053242.158100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"FVMa","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What usually causes ERROR_ILLEGAL_ADDRESS ?"}]}]}]},{"client_msg_id":"d9037a88-0761-4b87-ab82-0e89dcfb5523","type":"message","text":"I've found the line causing it but before I dive in ... :slightly_smiling_face:","user":"UKJSNT1QR","ts":"1613053260.158500","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"PJgQb","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"I've found the line causing it but before I dive in ... "},{"type":"emoji","name":"slightly_smiling_face"}]}]}]},{"client_msg_id":"17ae5807-d2bc-4a4f-b20f-570173cfaef1","type":"message","text":"Out of bounds memory access is a potential source for that error","user":"U67BJLYCS","ts":"1613054601.159100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"fpne2","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Out of bounds memory access is a potential source for that error"}]}]}]},{"client_msg_id":"ede5703d-c830-4918-9e80-d7a3db999618","type":"message","text":"yeah, watch out with those `@inbounds` :slightly_smiling_face:","user":"U68A3ASP9","ts":"1613055997.159300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"lpYHh","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"yeah, watch out with those "},{"type":"text","text":"@inbounds","style":{"code":true}},{"type":"text","text":" "},{"type":"emoji","name":"slightly_smiling_face"}]}]}]},{"client_msg_id":"ef958ea3-a029-4906-a4c2-4c2359f28864","type":"message","text":"Does anybody know why the following code generates an illegal memory access error (code 700, ERROR_ILLEGAL_ADDRESS)? I restarted Julia and just ran the following.\n```using CUDA\n\nfunction kernel(n::Int, arr::CuDeviceArray{Float64})\n    x = @cuDynamicSharedMem(Float64, n)\n    xl = @cuDynamicSharedMem(Float64, n, n*sizeof(Float64))\n\n    I = blockIdx().x\n\n    xl[5] = arr[I]\n    xl[6] = arr[I]\n    CUDA.sync_threads()\n\n    return\nend\n\nn = 8\nnblk = 9\narr = CuArray{Float64}(undef, nblk)\ncopyto!(arr, ones(nblk))\n\nCUDA.@sync @cuda threads=32 blocks=nblk shmem=((2*n)*sizeof(Float64)) kernel(n,arr)```\nI'm using Julia 1.5.2 and CUDA.jl 2.3.0.","user":"U01FXSDEXN3","ts":"1613057505.161700","team":"T68168MUP","edited":{"user":"U01FXSDEXN3","ts":"1613058140.000000"},"blocks":[{"type":"rich_text","block_id":"ugg","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Does anybody know why the following code generates an illegal memory access error (code 700, ERROR_ILLEGAL_ADDRESS)? I restarted Julia and just ran the following.\n"}]},{"type":"rich_text_preformatted","elements":[{"type":"text","text":"using CUDA\n\nfunction kernel(n::Int, arr::CuDeviceArray{Float64})\n    x = @cuDynamicSharedMem(Float64, n)\n    xl = @cuDynamicSharedMem(Float64, n, n*sizeof(Float64))\n\n    I = blockIdx().x\n\n    xl[5] = arr[I]\n    xl[6] = arr[I]\n    CUDA.sync_threads()\n\n    return\nend\n\nn = 8\nnblk = 9\narr = CuArray{Float64}(undef, nblk)\ncopyto!(arr, ones(nblk))\n\nCUDA.@sync @cuda threads=32 blocks=nblk shmem=((2*n)*sizeof(Float64)) kernel(n,arr)"}]},{"type":"rich_text_section","elements":[{"type":"text","text":"I'm using Julia 1.5.2 and CUDA.jl 2.3.0."}]}]}],"thread_ts":"1613057505.161700","reply_count":8,"reply_users_count":2,"latest_reply":"1613059785.163700","reply_users":["U68A3ASP9","U01FXSDEXN3"],"subscribed":false},{"client_msg_id":"8047a107-2fdf-411c-8fca-d2b3eae5877c","type":"message","text":"Hi!\nIs there any material for experienced julia programmers but absolute GPU newbies  out there ?\nMy institute just got a few beefy gpus and encouraged people to try them out.\nI'm wondering what parts of rresearch might be suitable.","user":"U9769MVA7","ts":"1613119938.168300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"nBN8","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi!\nIs there any material for experienced julia programmers but absolute GPU newbies  out there ?\nMy institute just got a few beefy gpus and encouraged people to try them out.\nI'm wondering what parts of rresearch might be suitable."}]}]}]},{"client_msg_id":"81988fcc-e058-4d9a-86e0-444554163a5d","type":"message","text":"there's the introductory tutorial: <https://juliagpu.github.io/CUDA.jl/stable/tutorials/introduction/>\na couple of more were in development here, <https://github.com/Ellipse0934/CUDATutorials.jl>, but havent' been finished yet","user":"U68A3ASP9","ts":"1613120105.168800","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"74FKo","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"there's the introductory tutorial: "},{"type":"link","url":"https://juliagpu.github.io/CUDA.jl/stable/tutorials/introduction/"},{"type":"text","text":"\na couple of more were in development here, "},{"type":"link","url":"https://github.com/Ellipse0934/CUDATutorials.jl"},{"type":"text","text":", but havent' been finished yet"}]}]}],"thread_ts":"1613120105.168800","reply_count":1,"reply_users_count":1,"latest_reply":"1613120224.168900","reply_users":["U9769MVA7"],"subscribed":false,"reactions":[{"name":"heart","users":["U9769MVA7"],"count":1}]},{"client_msg_id":"ff5231eb-a2db-4331-be2c-3275d312cd89","type":"message","text":"<https://arxiv.org/pdf/2008.11326.pdf>","user":"U67BJLYCS","ts":"1613145507.171600","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"3RU","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://arxiv.org/pdf/2008.11326.pdf"}]}]}]},{"client_msg_id":"393c0c54-1bec-43d4-a8bc-f4d6ef6f020c","type":"message","text":"<https://twitter.com/ahcohen/status/1360330829440901124>","user":"UDGT4PM41","ts":"1613165004.171800","team":"T68168MUP","attachments":[{"fallback":"<https://twitter.com/ahcohen|@ahcohen>: <https://twitter.com/dancherp|@dancherp> <https://twitter.com/srush_nlp|@srush_nlp> <https://twitter.com/pragmaticml|@pragmaticml> <https://twitter.com/_rockt|@_rockt> <https://twitter.com/ftynse|@ftynse> See also the longer term SAIR project.\n<https://github.com/google-research/structured-additive-IR>\nBuilding on the strengths of MLIR + polyhedral compilation + going beyond integer linear programming (ILP) as an optimization engine. Stay tuned for further announcements.","ts":1613163118,"author_name":"Albert Cohen","author_link":"https://twitter.com/ahcohen/status/1360330829440901124","author_icon":"https://pbs.twimg.com/profile_images/1614409278/albert_cohen_cut_normal.png","author_subname":"@ahcohen","text":"<https://twitter.com/dancherp|@dancherp> <https://twitter.com/srush_nlp|@srush_nlp> <https://twitter.com/pragmaticml|@pragmaticml> <https://twitter.com/_rockt|@_rockt> <https://twitter.com/ftynse|@ftynse> See also the longer term SAIR project.\n<https://github.com/google-research/structured-additive-IR>\nBuilding on the strengths of MLIR + polyhedral compilation + going beyond integer linear programming (ILP) as an optimization engine. Stay tuned for further announcements.","service_name":"twitter","service_url":"https://twitter.com/","from_url":"https://twitter.com/ahcohen/status/1360330829440901124","id":1,"original_url":"https://twitter.com/ahcohen/status/1360330829440901124","footer":"Twitter","footer_icon":"https://a.slack-edge.com/80588/img/services/twitter_pixel_snapped_32.png"}],"blocks":[{"type":"rich_text","block_id":"d5H","elements":[{"type":"rich_text_section","elements":[{"type":"link","url":"https://twitter.com/ahcohen/status/1360330829440901124"}]}]}]},{"client_msg_id":"d4fbb579-a0e3-41a9-8f53-a88e90722ba6","type":"message","text":"Hi, is there a dict like structure on GPU?","user":"UCD4Z3NJZ","ts":"1613247700.172300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"NYQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi, is there a dict like structure on GPU?"}]}]}]},{"client_msg_id":"cfd5020b-a6f4-450b-a365-59a1c111b6bb","type":"message","text":"in Julia? not that I know. seems hard to implement without proper device-side memory management.","user":"U68A3ASP9","ts":"1613248854.173100","team":"T68168MUP","edited":{"user":"U68A3ASP9","ts":"1613248860.000000"},"blocks":[{"type":"rich_text","block_id":"rrj","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"in Julia? not that I know. seems hard to implement without proper device-side memory management."}]}]}]},{"client_msg_id":"831351da-8cad-4972-819a-9d091701e6ef","type":"message","text":"Yeah, thank you for confirming.","user":"UCD4Z3NJZ","ts":"1613250361.174100","team":"T68168MUP","edited":{"user":"UCD4Z3NJZ","ts":"1613250371.000000"},"blocks":[{"type":"rich_text","block_id":"67aID","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Yeah, thank you for confirming."}]}]}]},{"client_msg_id":"d955b51e-d091-4855-80bf-75cb9c8dcac3","type":"message","text":"From what I understand, in KernelAbstractions, one can just pass an `ndrange` of indices, and KernelAbstractions will take care of splitting that range among threads. Is there a way to know how indices are split (or to impose manually how they are divided)? The use case are heterogeneous computations, where the kernel is more expensive on some indices than others, and one may want to make sure that \"easy\" and \"hard\" indices are more or less equally distributed among threads","user":"U6BJ9E351","ts":"1613319178.177900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"151v","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"From what I understand, in KernelAbstractions, one can just pass an "},{"type":"text","text":"ndrange","style":{"code":true}},{"type":"text","text":" of indices, and KernelAbstractions will take care of splitting that range among threads. Is there a way to know how indices are split (or to impose manually how they are divided)? The use case are heterogeneous computations, where the kernel is more expensive on some indices than others, and one may want to make sure that \"easy\" and \"hard\" indices are more or less equally distributed among threads"}]}]}]},{"client_msg_id":"448f1920-e0e5-4a84-a23d-8463a8bf2dfc","type":"message","text":"GPU bi-weekly in an hour","user":"U67BJLYCS","ts":"1613404663.180300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"=mZN5","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"GPU bi-weekly in an hour"}]}]}]},{"client_msg_id":"c690a40c-778c-4f87-948e-7b782cc6e957","type":"message","text":"What's the best way to compute a `mapreduce`-type thing like a dot-product over non-contiguous CuArray views?","user":"UEP056STX","ts":"1613409346.182000","team":"T68168MUP","edited":{"user":"UEP056STX","ts":"1613409611.000000"},"blocks":[{"type":"rich_text","block_id":"MB/","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"What's the best way to compute a "},{"type":"text","text":"mapreduce","style":{"code":true}},{"type":"text","text":"-type thing like a dot-product over non-contiguous CuArray views?"}]}]}]},{"client_msg_id":"0a52e237-e88a-47fb-bade-790c056c9d91","type":"message","text":"Hi Everyone, I am writing this message in connection with the issue we have with CUDA.jl.","user":"U015TV044AV","ts":"1613410414.182900","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"jjdQ","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Hi Everyone, I am writing this message in connection with the issue we have with CUDA.jl."}]}]}]},{"client_msg_id":"d5e31ca1-3c38-4a92-a8d4-d0c9ac5c406d","type":"message","text":"Let me introduce myself. I work at CINECA, Bologna. Last year our center was in the top 10 supercomputers in the world.\nI came to know about Julia last year through JuliaCon. We have been using pushing Julia activity here at our Center. We are organizing a Julia course.","user":"U015TV044AV","ts":"1613410427.183100","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"70HKM","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"Let me introduce myself. I work at CINECA, Bologna. Last year our center was in the top 10 supercomputers in the world.\nI came to know about Julia last year through JuliaCon. We have been using pushing Julia activity here at our Center. We are organizing a Julia course."}]}]}]},{"client_msg_id":"468f0c45-e004-42fd-abfb-9ed4e374f16b","type":"message","text":"My colleague (soon we will join the chat as well, his name is *Francesco*) and I were trying to install the Julia CUDA package. But we have some issues with that.","user":"U015TV044AV","ts":"1613410434.183300","team":"T68168MUP","blocks":[{"type":"rich_text","block_id":"4=xE","elements":[{"type":"rich_text_section","elements":[{"type":"text","text":"My colleague (soon we will join the chat as well, his name is "},{"type":"text","text":"Francesco","style":{"bold":true}},{"type":"text","text":") and I were trying to install the Julia CUDA package. But we have some issues with that."}]}]}]}]}